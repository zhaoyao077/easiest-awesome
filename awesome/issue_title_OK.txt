[SPARK-41818][SPARK-42000][CONNECT] Fix saveAsTable to find the default source
[SPARK-42427][SQL] ANSI MODE: Conv should return an error if the internal conversion overflows
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In ANSI SQL mode, function Conv() should return an error if the internal conversion overflowsFor example, before the change:```> select conv('fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 16, 10)18446744073709551615```After the change```> select conv('fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 16, 10)org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select conv('fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 16, 10)       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Similar to the other SQL functions, this PR shows the overflow errors of `conv()` to users under ANSI SQL mode, instead of returning an unexpected number.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, function `conv()` will return an error if the internal conversion overflows### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UTs
[SPARK-41999][CONNECT][PYTHON] Fix bucketBy/sortBy to properly use the first column name
[SPARK-42428][CONNECT][PYTHON] Standardize __repr__ of CommonInlineUserDefinedFunction
[SPARK-42429][BUILD] Fix a sbt compile error for `getArgument` when using IntelliJ
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When build with IntelliJ, I hit the following error from time to time:```spark/sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala:149:18value getArgument is not a member of org.mockito.invocation.InvocationOnMock      invocation.getArgument[Identifier](0).name match {```Sometimes the error can be solved by rebuilt with sbt or maven. But the best might be just avoid using the method that causes this compilation error.### Why are the changes needed?Make the life easier for IDE users.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual and existing tests
[SPARK-42430][SQL][DOC] Add documentation for TimestampNTZ type
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add documentation for TimestampNTZ type### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Add documentation for the new data type TimestampNTZ.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Build docs and preview:<img width=\"782\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/218656254-096df429-851d-4046-8a6f-f368819c405b.png\"><img width=\"777\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/218656277-e8cfe747-2c45-476d-b70f-83c654e0b0f2.png\">
[SPARK-41818][SPARK-42000][CONNECT][FOLLOWUP] Fix leaked test case
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->After https://github.com/apache/spark/pull/40000, the connect framework make format as optional but miss updating some test case.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->recover CI<img width=\"1052\" alt=\"image\" src=\"https://user-images.githubusercontent.com/12025282/218662255-fd8de2f9-f64e-43c6-8e19-0f2268882b44.png\">### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->pass CI
[SPARK-41818][SPARK-42000][CONNECT][TESTS][FOLLOWUP] Fix failed test in SparkConnectProtoSuite
[SPARK-42431][CONNECT] Avoid calling `LogicalPlan.output` before analysis in `Union`
[SPARK-40238][PYTHON] Support scaleUpFactor and initialNumPartition in pySpark RDD take API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support scaleUpFactor and initialNumPartition in pySpark RDD take API, follow-up issue [SPARK-40211](https://issues.apache.org/jira/browse/SPARK-40211) and pr #37661 .### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->By setting this new configuration to a high value we could effectively mitigate the “run multiple jobs” overhead in pySpark RDD take behavior.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add a new test case.
[SPARK-42433][PYTHON][CONNECT] Add `array_insert` to Connect
[SPARK-42406][PROTOBUF] Fix recursive depth setting for Protobuf functions
[SPARK-42434][PYTHON][CONNECT] `array_append` should accept `Any` value
[SPARK-42367][CONNECT][PYTHON] `DataFrame.drop` should handle duplicated columns properly
[SPARK-42435][UI] Update DataTables to 1.13.2
[SPARK-42437][PYTHON][CONNECT] PySpark catalog.cacheTable will allow to specify storage level
[SPARK-42436][SQL] Improve multiTransform to generate alternatives dynamically
[SPARK-42439] [SQL] Make createJobDescription in FileWrite.toBatch not lazy
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42439][SQL] In v2 writes, make createJobDescription in FileWrite.toBatch not lazy
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In v2 writes, make createJobDescription in FileWrite.toBatch not lazy### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->There is a difference in behavior between v1 writes and v2 writes in the order of events happening when configuring the file writer and the committer.v1:writer.prepareWrite()committer.setupJob()v2:committer.setupJob()writer.prepareWrite() This is because the `prepareWrite()` call (that is the one performing the call `job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])`)happens as part of the `createWriteJobDescription` which is `lazy val` in the `toBatch` call and therefore is evaluated after the `committer.setupJob` at the end of the `toBatch`This causes issues when evaluating the committer as some elements might be missing, for example the aforementioned output format class not being set, causing the committer being set up as generic write instead of parquet write. The fix is very simple and it is to make the `createJobDescription` call non-lazy### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing test.
[SPARK-42440][CONNECT] Initial set of Dataframe APIs for Scala Client
[SPARK-41775][PYTHON][FOLLOW-UP] Updating docs for readability 
[SPARK-42342][PYTHON][CONNECT][TEST] Fix FunctionsParityTests.test_raise_error to call the proper test
[SPARK-42442][SQL] Use spark.sql.timestampType for data source inference
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->With the configuration `spark.sql.timestampType`,  TIMESTAMP in Spark is a user-specified alias associated with one of the TIMESTAMP_LTZ and TIMESTAMP_NTZ variations. This is quite complicated to Spark users.There is another option `spark.sql.sources.timestampNTZTypeInference.enabled` for schema inference. I would like to introduce it in https://github.com/apache/spark/pull/40005 but having two flags seems too much. After thoughts, I decide to merge `spark.sql.sources.timestampNTZTypeInference.enabled` into `spark.sql.timestampType` and let  `spark.sql.timestampType` control the schema inference behavior.We can have followups to add data source options \"inferTimestampNTZType\" for CSV/JSON/partiton column like JDBC data source did.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make the new feature simpler.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the feature is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UTI also tried ```git grep spark.sql.sources.timestampNTZTypeInference.enabledgit grep INFER_TIMESTAMP_NTZ_IN_DATA_SOURCES```to make sure the flag INFER_TIMESTAMP_NTZ_IN_DATA_SOURCES is removed.
[SPARK-42443][SQL] Remove unused object in DataFrameAggregateSuite
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove constructed but never used variable in test case.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Codebase cleanup.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42426][CONNECT] Fix DataFrameWriter.insertInto to call the corresponding method instead of saveAsTable
[SPARK-42457][CONNECT] Adding SparkSession#read
[SPARK-42401][SQL][FOLLOWUP] Always set `containsNull=true` for `array_insert`
[SPARK-42441][CONNECT] Scala Client add Column APIs
[SPARK-38324][SQL] The second range is not [0, 59] in the day time ANSI interval
[SPARK-42431][CONNECT][FOLLOWUP] Use `Distinct` to delay analysis for `Union` 
[SPARK-42002][CONNECT][FOLLOWUP] Remove unused imports
[SPARK-42445][R] Fix SparkR `install.spark` function
[SPARK-42446][DOCS][PYTHON]Updating PySpark documentation to enhance usability
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Updates to the PySpark documentation web site:- Fixing typo on the Getting Started page (Version => Versions)- Capitalizing \"In/Out\" in the DataFrame Quick Start notebook- Adding \"(Legacy)\" to the Spark Streaming heading on the Spark Streaming page- Reorganizing the User Guide page to list PySpark guides first, minor language updates, and removing links to legacy streaming and RDD programming guides to not promote these as prominently and focus on the recommended APIs<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To improve usability of the PySpark doc website by adding guidance (calling out legacy APIs), fixing a few language issues, and making PySpark content more prominent.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing PySpark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built and manually reviewed/tested the PySpark documentation web site locally.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42447][INFRA] Remove Hadoop 2 GitHub Action job
[SPARK-41151][FOLLOW-UP][SQL] Improve the doc of `_metadata` generated columns nullability implementation
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add a doc of how `_metadata` nullability is implemented for generated metadata columns.### Why are the changes needed?Improve readability### Does this PR introduce _any_ user-facing change?No### How was this patch tested?N/A
[SPARK-42448][SQL] Fix spark sql shell prompt for current db
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The CliSessionState does not contain the current database info, we shall use spark's `catalog.currentDatabase` ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, when users use spark-sql and switch database, the prompt now show the correct one instead of `default`### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally tested```textmatespark-sql (default)> create database abc;Time taken: 0.24 secondsspark-sql (default)> use abc;Time taken: 0.027 secondsspark-sql (ABC)>````
[MINOR][PYTHON][DOCS] Add `applyInPandasWithState` to API references
[WIP][CONNECT] Test `RootAllocator` memory leak
[SPARK-42451][SQL][TESTS] Simplifies the filter conditions of `testingVersions` in `HiveExternalCatalogVersionsSuite`
[SPARK-42399] [SQL] Support big numbers for conv function (get rid of overflow)
[SPARK-42453][CONNECT] Implement function max in Scala client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR tries to implement functions.max for Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing scala client testing framework.
[SPARK-42455][SQL] Rename JDBC option inferTimestampNTZType as preferTimestampNTZ
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Similar with https://github.com/apache/spark/pull/37327, this PR renames the JDBC data source option `inferTimestampNTZType` as `preferTimestampNTZ`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It is simpler and more straightforward. Also, it is consistent with the CSV data source option introduced in https://github.com/apache/spark/pull/37327,### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the TimestampNTZ project is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-39904][SQL][FOLLOW-UP] Rename CSV option `prefersDate` as `preferDate`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Rename the CSV data source option `prefersDate` as `preferDate`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->All the CSV data source options doesn't have a `s` on the verb in the naming. For example, `inferSchema`, `ignoreLeadingWhiteSpace` and `ignoreTrailingWhiteSpace`. The renaming makes the naming consistent.Also, the title of JIRA https://issues.apache.org/jira/browse/SPARK-39904 uses `preferDate` as well.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the data source option is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-42456][DOCS][PYTHON] Consolidating the PySpark version upgrade note pages into a single page to make it easier to read
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Creating a new PySpark migration guide sub page and consolidating the existing 9 separate pages into this one new page. This makes it easier to take a look across multiple version upgrades by simply scrolling on the page instead of having to navigate back and forth. Note that this is similar to the Spark Core Migration Guide page here:https://spark.apache.org/docs/latest/core-migration-guide.htmlAlso updating the existing main Migration Guide page to point to this new sub page and making some minor language updates to help readers.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To improve usability of the PySpark doc site.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing PySpark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built and tested the PySpark documentation web site locally.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-41591][PYTHON][FOLLOW-UP] Remove gRPC version check for Distributor
[SPARK-41817][CONNECT][PYTHON][TEST] Enable the doctest pyspark.sql.connect.readwriter.DataFrameReader.option
[SPARK-42459][CONNECT] Create pyspark.sql.connect.utils to keep common codes
[SPARK-42460][CONNECT] Clean-up results in ClientE2ETestSuite
[SPARK-42398][SQL] Refine default column value DS v2 interface
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The current default value DS V2 API is a bit inconsistent. The `createTable` API only takes `StructType`, so implementations must know the special metadata key of the default value to access it. The `TableChange` API has the default value as an individual field.This API adds a new `Column` interface, which holds both current default (as a SQL string) and exist default (as a v2 literal). `createTable` API now takes `Column`. This avoids the need of special metadata key and is also more extensible when adding more special cols like generated cols. This is also type-safe and makes sure the exist default is literal. The implementation is free to decide how to encode and store default values. Note: backward compatibility is taken care of.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->better DS v2 API for default value### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-42461][CONNECT] Scala Client implement first batch of functions
[SPARK-42462][K8S] Prevent `docker-image-tool.sh` from publishing OCI manifests
[SPARK-42463][YARN][TESTS] Clean up the third-party Java files copy introduced by SPARK-27180
[SPARK-42470][SQL] Remove unused declarations from Hive module
[SPARK-42477][CONNECT][PYTHON] accept user_agent in spark connect's connection string
[SPARK-42464][CONNECT] Fix ProtoToPlanTestSuite for Scala 2.13
[SPARK-42465][CONNECT] ProtoToPlanTestSuite should use analyzed plans instead of parsed plans.
[SPARK-42468][CONNECT] Implement agg by (String, String)*
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Starting to support basic aggregation in Scala client. The first step is to support aggregation by strings.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-39859][SQL] Support v2 DESCRIBE TABLE EXTENDED for columns
[SPARK-42469][SQL] Update MSSQL Dialect to use parentheses for TOP and add tests for Limit clause
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR is a follow-up for https://issues.apache.org/jira/browse/SPARK-42131. It adds parentheses in TOP clause in MSSQL dialect as they are only omitted for backward compatibility and required otherwise: https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql?view=sql-server-ver16#compatibility-support.I also added tests to check Limit clause translation for Oracle and MSSQL dialects.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Updates TOP to include round brackets.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I added unit tests.
[SPARK-42002][CONNECT][PYTHON][FOLLOWUP] Enable tests in ReadwriterV2ParityTests
[SPARK-42482][CONNECT] Scala Client Write API V1
[SPARK-42474][CORE][K8S] Add extraJVMOptions JVM GC option K8s test cases
[CONNECT] Eager Execution of DF.sql()
[SPARK-42478] Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-41448](https://issues.apache.org/jira/browse/SPARK-41448) make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
[SPARK-42382][BUILD] Upgrade `cyclonedx-maven-plugin` to 2.7.5
[SPARK-42498] [CONNECT][PYTHON] Reduce spark connect service retries
[SPARK-42476][CONNECT][DOCS] Complete Spark Connect API reference
[SPARK-42380][BUILD] Upgrade maven to 3.9.0
[SPARK-42480][SQL] Improve the performance of drop partitions
[SPARK-42481][CONNECT] Implement agg.{max,min,mean,count,avg,sum}
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Adding more API to `agg` including max,min,mean,count,avg,sum.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-41818][CONNECT][PYTHON][FOLLOWUP][TEST] Enable a doctest for DataFrame.write
[SPARK-42483][TESTS] Regenerate benchmark results
[SPARK-42484] [SQL] UnsafeRowUtils better error message
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Showing the essential information when throwing `InvalidUnsafeRowException`. Including where the check failed, and status of the `unsafeRow` and `expctedSchema`Example output:```[UnsafeRowStatus] expectedSchema: StructType(StructField(key1,IntegerType,false),StructField(key2,IntegerType,false),StructField(sum(key1),IntegerType,false),StructField(sum(key2),IntegerType,false)), expectedSchemaNumFields: 4, numFields: 4, bitSetWidthInBytes: 8, rowSizeInBytes: 40 fieldStatus: [UnsafeRowFieldStatus] index: 0, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1[UnsafeRowFieldStatus] index: 1, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1[UnsafeRowFieldStatus] index: 2, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1[UnsafeRowFieldStatus] index: 3, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Right now if such error happens, it's hard to track where it errored, and what the misbehaved row & schema looks like. With this change these information are more clear.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests
[SPARK-42430][DOC][FOLLOW-UP] Revise the java doc for TimestampNTZ & ANSI interval types
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As https://github.com/apache/spark/pull/40005#pullrequestreview-1299089504 pointed out, the java doc for data type recommends using factory methods provided in org.apache.spark.sql.types.DataTypes. Since the ANSI interval types missed the `DataTypes` as well, this PR also revise their doc.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Unify the data type doc### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Local preview<img width=\"826\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/219821685-321c2fd1-6248-4930-9c61-eec68f0dcb50.png\">
[SPARK-42518] [CONNECT] Scala Client DataFrameWriterV2
[SPARK-42048][PYTHON][CONNECT] Fix the alias name for numpy literals
[SPIP][POC] Driver scaling: parallel schedulers
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?SPIP: https://docs.google.com/document/d/1_MVEpGxz6U_CNqKArR1M1l2oP-3I7O67grfwPtniLaA/edit?usp=sharingPOC of scaling Spark Driver via parallel schedulers.Uses multiple groups of `CoarseGrainedSchedulerBackend, TaskSchedulerImpl`<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Low performance of Spark Driver with multiple large jobs.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Configs for enabling parallel schedulers:`spark.driver.schedulers.parallelism` - number of parallel schedulers, no value or <= 1 will disable parallelism<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Comparison tests with spark-sql processes of same parallelism level <!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
Update scalastyle-config.xml
[SPARK-42486][BUILD] Upgrade `ZooKeeper` to 3.6.4
[SPARK-42406][SQL] Fix check for missing required fields of to_protobuf
[SPARK-42487][BUILD] Upgrade Netty to 4.1.89 
[SPARK-42488][BUILD] Upgrade commons-crypto to 1.2.0
[SPARK-42489][BUILD] Upgrdae scala-parser-combinators from 2.1.1 to 2.2.0
[SPARK-42490][BUILD] Upgrade protobuf-java from 3.21.12 to 3.22.0 
[SPARK-42492][SQL] Add new function filter_value
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new higher order function `filter_value` which takes a column to validate and a function that takes the result of that column and returns a boolean indicating whether to keep the value or return null. This is semantically the same as a `when` expression passing the column into a validation function, except it guarantees to only evaluate the initial column once. The idea was taken from the Scala `Option.filter`, open to other names if anyone has a better idea.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Conditionally evaluated expressions are currently not candidates for subexpression elimination. This can lead to a lot of duplicate evaluations of expressions when doing common data cleaning tasks, such as only keeping a value if it matches some validation checks. It gets worse when multiple different checks are chained together, and you can end up with a single expensive expression being evaluated numerous times.https://github.com/apache/spark/pull/32987 attempts to solve this by allowing conditionally evaluated expressions to be candidates for subexpression elimination, however I have not been able to get that merged in the past 1.5 years. I still think that is valuable and useful, especially as an opt-in behavior, but this is an alternative option to help improve performance of these kinds of data validation tasks.A custom implementation of `NullIf` could help as well, however it would only support exact equals checks, where this can support any logic you need to do validation.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Adds a new function.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UTs.
[MINOR][SQL] Fix typo and whitespaces in SQLConf
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Fix `SIMPLIFIEID` -> `SIMPLIFIED`- Fix indentation and whitespaces around a few `val` definitions### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix typo and code formatting### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests pass
[SPARK-42493][DOCS][PYTHON] Make Python the first tab for code examples - Spark SQL, DataFrames and Datasets Guide
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Making Python the first tab for code examples in the Spark SQL, DataFrames and Datasets Guide.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Python is the easiest approachable and most popular language so this change moves it to the first tab (showing by default) for code examples.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing Spark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?I built the website locally and manually tested the pages.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42427][SQL][TESTS][FOLLOW-UP] Remove duplicate overflow test for conv
[SPARK-42495][CONNECT] Scala Client add Misc, String, and Date/Time functions
[SPARK-41741][SQL] Encode the string using the UTF_8 charset in ParquetFilters
[SPARK-41952][SQL] Fix Parquet zstd off-heap memory leak as a workaround for PARQUET-2160
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SPARK-41952 was raised for a while, but unfortunately, the Parquet community does not publish the patched version yet, as a workaround, we can fix the issue on the Spark side first.We encountered this memory issue when migrating data from parquet/snappy to parquet/zstd, Spark executors always occupy unreasonable off-heap memory and have a high risk of being killed by NM.See more discussions at https://github.com/apache/parquet-mr/pull/982 and https://github.com/apache/iceberg/pull/5681### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The issue is fixed in the parquet community [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160), but the patched version is not available yet.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it's bug fix.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing UT should cover the correctness check, I also verified this patch by scanning a large parquet/zstd table.```spark-shell --executor-cores 4 --executor-memory 6g --conf spark.executor.memoryOverhead=2g``````spark.sql(\"select sum(hash(*)) from parquet_zstd_table \").show(false)```- before this patchAll executors get killed by NM quickly.```ERROR YarnScheduler: Lost executor 1 on hadoop-xxxx.****.org: Container killed by YARN for exceeding physical memory limits. 8.2 GB of 8 GB physical memory used. Consider boosting spark.executor.memoryOverhead.```<img width=\"1872\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26535726/220031678-e9060244-5586-4f0c-8fe7-55bb4e20a580.png\">- after this patchQuery runs well, no executor gets killed.<img width=\"1881\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26535726/220031917-4fe38c07-b38f-49c6-a982-2091a6c2a8ed.png\">
[SPARK-42475][CONNECT][DOCS] Getting Started: Live Notebook for Spark Connect
[SPARK-42500][SQL] ConstantPropagation supports more cases
[SPARK-41812][SPARK-41823][CONNECT][SQL][SCALA] Add PlanId to Scala Client
[SPARK-XXX][SQL][TESTS][3.4] Reduce the degree of concurrency during ORC schema merge conflict tests
[SPARK-XXX][SQL][TESTS] Reduce the degree of concurrency during ORC schema merge conflict tests
[SPARK-42508][CONNECT][ML] Extract the common .ml classes to `mllib-common`
[SPARK-42504][SQL] NestedColumnAliasing support pruning adjacent projects
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr improves the `NestedColumnAliasing` to support prune the adjacent project nodes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->CollapseProject won't combine adjacent projects into one, e.g. non-cheap expression has been accessed more than once with the below project. Then there would be possible to appear some adjacent project nodes that `NestedColumnAliasing` does not support pruning.It's a common case with lateral column alias that it would push down an extra project, e.g.```sqlCREATE TABLE t (c struct<c1: int, c2 string>) USING PARQUET;SELECT c.c1, c.c1 + 1 as x, x + 1 FROM t;-- before*(1) Project [c#15.c1 AS c1#53, x#47, (x#47 + 1) AS (lateralAliasReference(x) + 1)#55]+- *(1) Project [c#15, (c#15.c1 + 1) AS x#47]   +- *(1) ColumnarToRow      +- FileScan parquet spark_catalog.default.t[c#15] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c:struct<c1:int,c2:string>>-- after*(1) Project [_extract_c1#38 AS c1#34, x#27, (x#27 + 1) AS (lateralAliasReference(x) + 1)#37]+- *(1) Project [c#33.c1 AS _extract_c1#38, (c#33.c1 + 1) AS x#27]   +- *(1) ColumnarToRow      +- FileScan parquet spark_catalog.default.t[c#33] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c:struct<c1:int>>```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add new test
[WIP][CONNECT] Scala client add collection functions
[SPARK-42506][SQL] Fix Sort's maxRowsPerPartition if maxRows does not exist
[SPARK-42507][SQL][TESTS] Simplify ORC schema merging conflict error check
[MINOR][TESTS] Avoid NPE in an anonym SparkListener in DataFrameReaderWriterSuite
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Avoid the following NPE in an anonym SparkListener in DataFrameReaderWriterSuite, as job desc may be absent```java.lang.NullPointerException\tat java.util.concurrent.ConcurrentLinkedQueue.checkNotNull(ConcurrentLinkedQueue.java:920)\tat java.util.concurrent.ConcurrentLinkedQueue.offer(ConcurrentLinkedQueue.java:327)\tat java.util.concurrent.ConcurrentLinkedQueue.add(ConcurrentLinkedQueue.java:297)\tat org.apache.spark.sql.test.DataFrameReaderWriterSuite$$anon$2.onJobStart(DataFrameReaderWriterSuite.scala:1151)\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1462)\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Test Improvement### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-42516][SQL] Always capture the session time zone config while creating views
[SPARK-42510][CONNECT][PYTHON] Implement `DataFrame.mapInPandas`
[SPARK-42514][CONNECT] Scala Client add partition transforms functions
[SPARK-42002][CONNECT][FOLLOW-UP] Add Required/Optional notions to writer v2 proto
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Follow existing proto style guide, we should always add `Required/Optional` to proto documentation.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve documentation.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42520][CONNECT] Support basic Window API in Scala client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support Window orderby, partitionby, rowsbetween/rangebetween.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42521][SQL] Add NULLs for INSERTs with user-specified lists of fewer columns than the target table
[SPARK-42522][CONNECT] Fix DataFrameWriterV2 to find the default source
[SPARK-41775][PYTHON][FOLLOW-UP] Updating docs for readability
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Added minor UI fixes.<img width=\"732\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/220488925-eda62d80-d54d-41e9-a9ec-53d02b6fb94d.png\"><img width=\"725\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/220488948-929b1c35-4da7-4317-9883-078c2a57896a.png\"><img width=\"693\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/220488975-fdc34ae5-a539-4557-993c-d740232b29b5.png\">### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->For easy to read documentation.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42524][BUILD] Upgrade numpy and pandas in the release Dockerfile
[SPARK-41933][FOLLOWUP][CONNECT] Correct an error message
[WIP][SPARK-42509][SQL] WindowGroupLimitExec supports codegen
[SPARK-42513][SQL] Push down topK through join
[SPARK-42525][SQL] Collapse two adjacent windows with semantically-same partition/order
[SPARK-41391][SQL] The output column name of groupBy.agg(count_distinct) is incorrect
[SPARK-42427][SQL][TESTS][FOLLOW-UP] Disable ANSI for several conv test cases in MathFunctionsSuite
[SPARK-26365][K8S] In kuberentes cluster mode, spark submit should pass driver exit code
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Find spark driver container exit code and set as spark submit exit code.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Many users face this problem [SPARK-26365](https://issues.apache.org/jira/browse/SPARK-26365)> When launching apps using spark-submit in a kubernetes cluster, if the Spark applications fails (returns exit code = 1 for example), spark-submit will still exit gracefully and return exit code = 0.As a comparison, in yarn cluster mode, spark submit will throw exception when application be failed or killed.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->YesBefore, when application (driver container) on kubernetes fail, spark-submit still return 0 as exit code.After this, spark submit will use driver's exit code or throw exception when not found driver's exit code.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->In local; If need unit test, willing to add
[SPARK-42526][ML] Add Classifier.getNumClasses back
[SPARK-42527][CONNECT] Scala Client add Window functions
[SPARK-42528][CORE] Optimize PercentileHeap
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Reimplement `PercentileHeap` such that:- the percentile value is always in the `topHeap`, this speeds up `percentile` access- rebalance the heaps more efficiently by checking which heap should grow due to the new insertion and doing a rebalance based on target heap sizes- the heaps are java PriorityQueue's *without* comparators. Comparator call overhead slows down `poll`/`offer` by more than 2x. Instead implement a max-heap by `poll`/`offer` on the negated domain of numbers.### Why are the changes needed?`PercentileHeap` is heavy weight enough to cause scheduling delays if inserted inside the scheduler loop.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added more extensive unittests.
[SPARK-42349][PYTHON] Support pandas cogroup with multiple df
[SPARK-42272][CONNEC][TESTS][FOLLOW-UP] Do not cache local port in SparkConnectService
[SPARK-37980][SQL] Access row_index via _metadata if possible in tests
[SPARK-42468][CONNECT][FOLLOW-UP] Add .agg variants in Dataset
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `.agg` in Dataset in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-40822][SQL] Stable derived column aliases
[SPARK-42530][PYSPARK][DOCS] Remove Hadoop 2 from PySpark installation guide
[SPARK-42466][K8S]: Cleanup k8s upload directory when job terminates
[SPARK-42529][CONNECT] Support Cube and Rollup in Scala client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support Cube and Rollup in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API Coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42531][CONNECT] Scala Client Add Collections Functions
[SPARK-42150][K8S][DOCS][FOLLOWUP] Use v1.7.0 in docs
[SPARK-42532][K8S][DOCS] Update YuniKorn docs with v1.2
[SPARK-42533][CONNECT][Scala] Add ssl for Scala client
[SPARK-42534][SQL] Fix DB2Dialect Limit clause
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The PR fixes DB2 Limit clause syntax. Although DB2 supports LIMIT keyword, it seems that this support varies across databases and versions and the recommended way is to use `FETCH FIRST x ROWS ONLY`. In fact, some versions don't support LIMIT at all. Doc: https://www.ibm.com/docs/en/db2/11.5?topic=subselect-fetch-clause, usage example: https://www.mullinsconsulting.com/dbu_0502.htm.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fixes the incorrect Limit clause which could cause errors when using against DB2 versions that don't support LIMIT.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I added a unit test and an integration test to cover this functionality.
[SPARK-42444][PYTHON] `DataFrame.drop` should handle duplicated columns properly
[SPARK-42515][BUILD][CONNECT][TESTS] Make `write table` in `ClientE2ETestSuite` sbt local test pass
[SPARK-42049][SQL][FOLLOWUP] Always filter away invalid ordering/partitioning
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a follow-up of https://github.com/apache/spark/pull/37525 . When the project list has aliases, we go to the `projectExpression` branch which filters away invalid partitioning/ordering that reference non-existing attributes in the current plan node. However, this filtering is missing when the project list has no alias, where we directly return the child partitioning/ordering.This PR fixes it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to make sure we always return valid output partitioning/ordering.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[SPARK-41793][SQL] Incorrect result for window frames defined by a range clause on large decimals
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use `DecimalAddNoOverflowCheck` instead of `Add` to craete bound ordering for window range frame### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Before 3.4, the `Add` did not check overflow. Instead, we always wrapped `Add` with a `CheckOverflow`. After https://github.com/apache/spark/pull/36698, we make `Add` check overflow by itself. However, the bound ordering of window range frame uses `Add` to calculate the boundary that is used to determine which input row lies within the frame boundaries of an output row. Then the behavior is changed with an extra overflow check. Technically，We could allow an overflowing value if it is just an intermediate result. So this pr use `DecimalAddNoOverflowCheck` to replace the `Add` to restore the previous behavior.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, restore the previous(before 3.4) behavior### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-39859][SQL][FOLLOWUP] Only get ColStats when isExtended is true in Describe Column
[3.3][SPARK-42286][SPARK-41991][SPARK-42473][SQL] Fallback to previous codegen code path for complex expr with CAST
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR fixes the internal error `Child is not Cast or ExpressionProxy of Cast` for valid `CaseWhen` expr with `Cast` expr in its branches.Specifically, after SPARK-39865, an improved error msg for overflow exception during table insert was introduced. The improvement covers `Cast` expr and `ExpressionProxy` expr, but `CaseWhen` and other complex ones are not covered. An example below hits an internal error today.```create table t1 as select x FROM values (1), (2), (3) as tab(x);create table t2 (x Decimal(9, 0));insert into t2 select 0 - (case when x = 1 then 1 else x end) from t1 where x = 1;```To fix the query failure, we decide to fall back to the previous handling if the expr is not a simple `Cast` expr or `ExpressionProxy` expr.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To fix the query regression introduced in SPARK-39865.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No. We just fall back to the previous error msg if the expression involving `Cast` is not a simple one.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?- Added Unit test.- Removed one test case for the `Child is not Cast or ExpressionProxy of Cast` internal error, as now we do not check if the child has a `Cast` expression and fall back to the previous error message.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42406] Terminate Protobuf recursive fields by dropping the field
[SPARK-41171][SQL] Infer and push down window limit through window if partitionSpec is empty
[SPARK-42538][CONNECT] Make `sql.functions#lit` function support more types
[SPARK-42539][SQL][HIVE] Eliminate separate classloader when using 'builtin' Hive version for metadata client
[SPARK-42541][CONNECT] Support Pivot with provided pivot column values
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support Pivot with provided pivot column values. Not supporting Pivot without providing column values because that requires to do max value check which depends on the implementation of Spark configuration in Spark Connect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42120][SQL] Add built-in table-valued function json_tuple
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds a new builtin table-valued function `json_tuple`.### Why are the changes needed?To improve the usability of table-valued generator functions.### Does this PR introduce _any_ user-facing change?Yes. After this PR, the table-valued generator function `json_tuple` can be used in the FROM clause of a query.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL query tests.
[SPARK-42543][CONNECT] Specify protocol for UDF artifact transfer in JVM/Scala client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces a new client-streaming RPC service \"AddArtifacts\" to handle the transfer of artifacts from the client to the server. New message types `AddArtifactsRequest` and `AddArtifactsResponse` are added that specify the format of artifact transfer.An artifact is defined by its `name` and `data` fields.- `name`  - The name of the artifact is expected in the form of a \"Relative Path\" that is made up of a sequence of directories and the final file element.  - Examples of \"Relative Path\"s: `jars/test.jar`,  `classes/xyz.class`, `abc.xyz`, `a/b/X.jar`.  - The server is expected to maintain the hierarchy of files as defined by their name. (i.e The relative path of the file on the server's filesystem will be the same as the name of the provided artifact).- `data`  - The raw data of the artifact.The intention behind the `name` format is to add extensibility to the approach. Through this scheme, the server can maintain the hierarchy/grouping of files in any way the client specifies as well as transfer different \"forms\" of artifacts without needing any updates to the protocol/code itself.The protocol supports batching and chunking (due to gRPC size limits) of artifacts as required.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In the decoupled client-server architecture of Spark Connect, a remote client may use a local JAR or a new class in their UDF that may not be present on the server. To handle these cases of missing \"artifacts\ a protocol for artifact transfer is needed to move the required artifacts from the client side over to the server side.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42544][CONNNECT] Spark Connect Scala Client: support parameterized SQL
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support parameterized SQL API in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42122][SQL] Add built-in table-valued function stack
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds a new builtin table-valued function `stack`.### Why are the changes needed?To improve the usability of table-valued generator functions.### Does this PR introduce _any_ user-facing change?Yes. After this PR, the table-valued generator function `stack` can be used in the FROM clause of a query.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL query tests.
[SPARK-41834][CONNECT] Implement SparkSession.conf
[SPARK-42121][SQL] Add built-in table-valued functions posexplode, posexplode_outer, json_tuple and stack
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds new builtin table-valued functions `posexplode`, `posexplode_outer`, `json_tuple` and `stack`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To improve the usability of table-valued generator functions. Now all generate functions can be used as table value functions.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. After this PR, 4 new table-valued generator functions can be used in the FROM clause of a query.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL query tests
[SPARK-42545][K8S][DOCS] Remove `experimental` from `Volcano` docs
[SPARK-42547][PYTHON] Make PySpark working with Python 3.7
[SPARK-42548][SQL] Add ReferenceAllColumns to skip rewriting attributes
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new trait `ReferenceAllColumns ` that overrides `references` using children output. Then we can skip it during rewriting attributes in transformUpWithNewOutput.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->There are two reasons with this new trait:1. it's dangerous to call `references` on an unresolved plan that all of references come from children2. it's unnecessary to rewrite its attributes that all of references come from children### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->prevent potential bug### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test and pass CI
[SPARK-42534][SQL][3.4] Fix DB2Dialect Limit clause
[SPARK-41823][CONNECT] Scala Client resolve ambiguous columns in Join
[SPARK-42551][SQL] Support more subexpression elimination cases
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Support more subexpression elimination cases.How to do subexpression elimination:* Get all common expressions from input expressions. Recursively visits all subexpressions regardless of whether the current expression is a conditional expression.* For each common expression:  * Add a new boolean variable subExprInit_n to indicate whether we have  already evaluated the common expression, and reset it to false at the start of operator.consume()  * Add a new wrapper subExpr function for common subexpression.```javaprivate void subExpr_n(${argList}) { if (!subExprInit_n) {   ${eval.code}   subExprInit_n = true;   subExprIsNull_n = ${eval.isNull};   subExprValue_n = ${eval.value}; }}```  * Replace all the common subexpression with the wrapper function `subExpr_n(argList)`.## New support subexpression elimination patterns* Support subexpression elimination with conditional expressions```sqlSELECT case when v + 2 > 1 then 1            when v + 1 > 2 then 2            when v + 1 > 3 then 3 END vvFROM values(1) as t2(v)```We can reuse the result of expression  v + 1```sqlSELECT a, max(if(a > 0, b + c, null)) max_bc, min(if(a > 1, b + c, null)) min_bcFROM values(1, 1, 1) as t(a, b, c)GROUP BY a```We can reuse the result of expression  b + c* Support subexpression elimination in FilterExec```sqlSELECT * FROM (  SELECT v * v + 1 v1 from values(1) as t2(v)) twhere v1 > 5 and v1 < 10```We can reuse the result of expression  v * v + 1* Support subexpression elimination in JoinExec```sqlSELECT * FROM values(1, 1) as t1(a, b) join values(1, 2) as t2(x, y)ON b * y between 2 and 3``` We can reuse the result of expression  b * y* Support subexpression elimination in ExpandExec```sqlSELECT a, count(b),   \tcount(distinct case when b > 1 then b + c else null end) as count_bc_1,   \tcount(distinct case when b < 0 then b + c else null end) as count_bc_2FROM values(1, 1, 1) as t(a, b, c)GROUP BY a```We can reuse the result of expression  b + c### Why are the changes needed?Support more subexpression elimination cases, improve performance.For example, TPCDS q23b query,  we can reuse the result of projection `sum(ss_quantity * ss_sales_price)` in the if expressions:```if (isnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])))   input[0, decimal(28,2), true]else    (input[0, decimal(28,2), true] + cast(knownnotnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])) as decimal(28,2)))```q4, q62, q67 are similar to the above.| Query    | Time with PR | Time without PR | Time diff | Percentage ||----------|--------------|-----------------|-----------|------------|| q1.sql   | 36.97        | 36.623          | -0.347    | 99.06%     || q2.sql   | 42.699       | 41.741          | -0.958    | 97.76%     || q3.sql   | 6.038        | 6.111           | 0.073     | 101.21%    || q4.sql   | 273.849      | 323.799         | 49.95     | 118.24%    || q5.sql   | 76.202       | 76.353          | 0.151     | 100.20%    || q6.sql   | 8.451        | 9.011           | 0.56      | 106.63%    || q7.sql   | 13.017       | 12.954          | -0.063    | 99.52%     || q8.sql   | 10.22        | 11.027          | 0.807     | 107.90%    || q9.sql   | 72.843       | 72.019          | -0.824    | 98.87%     || q10.sql  | 13.233       | 14.028          | 0.795     | 106.01%    || q11.sql  | 112.252      | 112.983         | 0.731     | 100.65%    || q12.sql  | 3.036        | 3.488           | 0.452     | 114.89%    || q13.sql  | 12.471       | 12.911          | 0.44      | 103.53%    || q14a.sql | 210.201      | 220.12          | 9.919     | 104.72%    || q14b.sql | 185.374      | 187.57          | 2.196     | 101.18%    || q15.sql  | 10.189       | 10.338          | 0.149     | 101.46%    || q16.sql  | 80.756       | 82.503          | 1.747     | 102.16%    || q17.sql  | 28.523       | 28.567          | 0.044     | 100.15%    || q18.sql  | 13.417       | 14.271          | 0.854     | 106.37%    || q19.sql  | 6.366        | 6.53            | 0.164     | 102.58%    || q20.sql  | 3.427        | 4.939           | 1.512     | 144.12%    || q21.sql  | 2.096        | 2.16            | 0.064     | 103.05%    || q22.sql  | 14.4         | 14.01           | -0.39     | 97.29%     || q23a.sql | 507.253      | 545.185         | 37.932    | 107.48%    || q23b.sql | 707.054      | 768.148         | 61.094    | 108.64%    || q24a.sql | 193.116      | 193.793         | 0.677     | 100.35%    || q24b.sql | 177.109      | 179.54          | 2.431     | 101.37%    || q25.sql  | 22.264       | 22.949          | 0.685     | 103.08%    || q26.sql  | 8.68         | 8.973           | 0.293     | 103.38%    || q27.sql  | 8.535        | 8.558           | 0.023     | 100.27%    || q28.sql  | 101.953      | 102.713         | 0.76      | 100.75%    || q29.sql  | 75.392       | 76.211          | 0.819     | 101.09%    || q30.sql  | 12.265       | 13.508          | 1.243     | 110.13%    || q31.sql  | 26.477       | 26.965          | 0.488     | 101.84%    || q32.sql  | 3.393        | 3.507           | 0.114     | 103.36%    || q33.sql  | 6.909        | 7.277           | 0.368     | 105.33%    || q34.sql  | 8.41         | 8.572           | 0.162     | 101.93%    || q35.sql  | 34.214       | 36.822          | 2.608     | 107.62%    || q36.sql  | 9.027        | 9.79            | 0.763     | 108.45%    || q37.sql  | 36.076       | 36.753          | 0.677     | 101.88%    || q38.sql  | 71.768       | 74.473          | 2.705     | 103.77%    || q39a.sql | 7.753        | 7.617           | -0.136    | 98.25%     || q39b.sql | 6.365        | 7.229           | 0.864     | 113.57%    || q40.sql  | 16.588       | 17.164          | 0.576     | 103.47%    || q41.sql  | 1.162        | 1.188           | 0.026     | 102.24%    || q42.sql  | 2.3          | 2.561           | 0.261     | 111.35%    || q43.sql  | 7.407        | 7.605           | 0.198     | 102.67%    || q44.sql  | 28.939       | 30.473          | 1.534     | 105.30%    || q45.sql  | 9.796        | 9.634           | -0.162    | 98.35%     || q46.sql  | 9.496        | 9.692           | 0.196     | 102.06%    || q47.sql  | 27.087       | 27.151          | 0.064     | 100.24%    || q48.sql  | 14.524       | 14.889          | 0.365     | 102.51%    || q49.sql  | 21.466       | 21.572          | 0.106     | 100.49%    || q50.sql  | 194.755      | 195.052         | 0.297     | 100.15%    || q51.sql  | 37.493       | 38.56           | 1.067     | 102.85%    || q52.sql  | 2.227        | 2.28            | 0.053     | 102.38%    || q53.sql  | 5.375        | 5.437           | 0.062     | 101.15%    || q54.sql  | 12.556       | 13.015          | 0.459     | 103.66%    || q55.sql  | 2.341        | 2.809           | 0.468     | 119.99%    || q56.sql  | 7.424        | 7.207           | -0.217    | 97.08%     || q57.sql  | 17.606       | 17.797          | 0.191     | 101.08%    || q58.sql  | 6.169        | 6.374           | 0.205     | 103.32%    || q59.sql  | 27.602       | 27.744          | 0.142     | 100.51%    || q60.sql  | 7.04         | 7.459           | 0.419     | 105.95%    || q61.sql  | 7.838        | 7.816           | -0.022    | 99.72%     || q62.sql  | 9.726        | 10.762          | 1.036     | 110.65%    || q63.sql  | 4.816        | 5.176           | 0.36      | 107.48%    || q64.sql  | 253.937      | 261.034         | 7.097     | 102.79%    || q65.sql  | 78.942       | 78.373          | -0.569    | 99.28%     || q66.sql  | 15.199       | 14.73           | -0.469    | 96.91%     || q67.sql  | 926.049      | 1022.971        | 96.922    | 110.47%    || q68.sql  | 7.932        | 7.977           | 0.045     | 100.57%    || q69.sql  | 12.101       | 14.699          | 2.598     | 121.47%    || q70.sql  | 20.7         | 20.872          | 0.172     | 100.83%    || q71.sql  | 14.96        | 15.065          | 0.105     | 100.70%    || q72.sql  | 73.215       | 73.955          | 0.74      | 101.01%    || q73.sql  | 5.973        | 6.126           | 0.153     | 102.56%    || q74.sql  | 97.611       | 99.577          | 1.966     | 102.01%    || q75.sql  | 125.005      | 129.508         | 4.503     | 103.60%    || q76.sql  | 34.812       | 35.34           | 0.528     | 101.52%    || q77.sql  | 7.686        | 8.474           | 0.788     | 110.25%    || q78.sql  | 287.959      | 292.936         | 4.977     | 101.73%    || q79.sql  | 8.401        | 9.616           | 1.215     | 114.46%    || q80.sql  | 59.371       | 60.051          | 0.68      | 101.15%    || q81.sql  | 18.452       | 19.499          | 1.047     | 105.67%    || q82.sql  | 64.093       | 65.032          | 0.939     | 101.47%    || q83.sql  | 4.675        | 4.867           | 0.192     | 104.11%    || q84.sql  | 10.456       | 10.816          | 0.36      | 103.44%    || q85.sql  | 12.347       | 12.77           | 0.423     | 103.43%    || q86.sql  | 6.537        | 6.843           | 0.306     | 104.68%    || q87.sql  | 77.427       | 77.876          | 0.449     | 100.58%    || q88.sql  | 83.082       | 83.385          | 0.303     | 100.36%    || q89.sql  | 6.645        | 6.801           | 0.156     | 102.35%    || q90.sql  | 7.841        | 7.883           | 0.042     | 100.54%    || q91.sql  | 3.88         | 4.129           | 0.249     | 106.42%    || q92.sql  | 3.044        | 3.271           | 0.227     | 107.46%    || q93.sql  | 361.149      | 365.883         | 4.734     | 101.31%    || q94.sql  | 43.929       | 46.667          | 2.738     | 106.23%    || q95.sql  | 196.363      | 197.427         | 1.064     | 100.54%    || q96.sql  | 12.457       | 12.496          | 0.039     | 100.31%    || q97.sql  | 80.131       | 81.821          | 1.69      | 102.11%    || q98.sql  | 6.885        | 7.522           | 0.637     | 109.25%    || q99.sql  | 17.685       | 18.009          | 0.324     | 101.83%    ||          | 6788.707     | 7116.357        | 327.65    | 104.82%    |One of our production query which has 19 case when  expressions, it's query time changed from 1.1 hour to 42 seconds.![image](https://user-images.githubusercontent.com/3626747/227448668-8a58ff33-3296-4a95-984b-292af47532ca.png)A simplify benchmark of the above production query.```    spark.range(1, 2000000, 1, 1)      .selectExpr(        \"cast(id + 1 as decimal) as a\        \"cast(id + 2 as decimal) as b\        \"cast(id + 3 as decimal) as c\        \"cast(id + 4 as decimal) as d\")      .createOrReplaceTempView(\"tab\")    runBenchmark(\"Subexpression elimination in ProjectExec\") {      val benchmark =        new Benchmark(\"Subexpression elimination in ProjectExec\ 2000000, output = output)      benchmark.addCase(s\"Test query\") { _ =>        val query =          s\"\"\"             |SELECT a, b, c, d,             |       a * b / c as s1,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END s2,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |       ELSE 0 END s3,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s4,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                      CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |                 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s5             |FROM tab             |\"\"\".stripMargin        spark.sql(query).noop()      }      benchmark.run()```Local benchmark result:Before this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         9713           9900         263          0.2        4856.7       1.0X```After this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         1238           1307          98          8.1         123.8       1.0X```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Exists UT.
[MINOR][CONNECT] Typo fixes & update comment
[SPARK-41725][CONNECT] Eager Execution of DF.sql()
[SPARK-42565][SS] Error log improvement for the lock acquisition of RocksDB state store instance
[SPARK-42566][SS] RocksDB StateStore lock acquisition should happen after getting input iterator from inputRDD
[SPARK-42567][SS][SQL] Track load time for state store provider and log warning if it exceeds threshold
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Track load time for state store provider and log warning if it exceeds threshold### Why are the changes needed?We have seen that the initial state store provider load can be blocked by external factors such as filesystem initialization. This log enables us to track cases where this load takes too long and we log a warning in such cases.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Augmented some of the tests to verify the logging is working as expected.Sample logs:```14:58:51.784 WARN org.apache.spark.sql.execution.streaming.state.StateStore: Loaded state store provider in loadTimeMs=2049 for storeId=StateStoreId[ checkpointRootLocation=file:/Users/anish.shrigondekar/spark/spark/target/tmp/streaming.metadata-1f2ff296-1ece-4a0c-b4b4-48aa0e909b49/state, operatorId=0, partitionId=2, storeName=default ] and queryRunId=a4063603-3929-4340-9920-eca206ebec3614:58:53.838 WARN org.apache.spark.sql.execution.streaming.state.StateStore: Loaded state store provider in loadTimeMs=2046 for storeId=StateStoreId[ checkpointRootLocation=file:/Users/anish.shrigondekar/spark/spark/target/tmp/streaming.metadata-1f2ff296-1ece-4a0c-b4b4-48aa0e909b49/state, operatorId=0, partitionId=3, storeName=default ] and queryRunId=a4063603-3929-4340-9920-eca206ebec3614:58:55.885 WARN org.apache.spark.sql.execution.streaming.state.StateStore: Loaded state store provider in loadTimeMs=2044 for storeId=StateStoreId[ checkpointRootLocation=file:/Users/anish.shrigondekar/spark/spark/target/tmp/streaming.metadata-1f2ff296-1ece-4a0c-b4b4-48aa0e909b49/state, operatorId=0, partitionId=4, storeName=default ] and queryRunId=a4063603-3929-4340-9920-eca206ebec36```
[SPARK-42569][CONNECT] Throw unsupported exceptions for non-supported API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Match https://github.com/apache/spark/blob/6a2433070e60ad02c69ae45706a49cdd0b88a082/python/pyspark/sql/connect/dataframe.py#L1500 to throw unsupported exceptions in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better indicating a API is not supported yet.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42568][CONNECT] Fix SparkConnectStreamHandler to handle configs properly while planning
[SPARK-42570][CONNECT][PYTHON] Fix DataFrameReader to use the default source
[SPARK-42561][CONNECT] Add temp view API to Dataset
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add temp view API to Dataset### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42573][Connect][Scala] Enable binary compatibility tests on all major client APIs
[SPARK-42575][Connect][Scala] Make all client tests to extend from ConnectFunSuite
[SPARK-42574][CONNECT][PYTHON] Fix toPandas to handle duplicated column names
[SPARK-42598][TEST] Refactor TPCH schema to separate file similar to TPCDS for code reuse
[SPARK-42569][CONNECT][FOLLOW-UP] Throw unsupported exceptions for persist
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Follow up https://github.com/apache/spark/pull/40164 to also throw unsupported operation exception for `persist`. Right now we are ok to depends on the `StorageLevel` in core module but in the future that shall be refactored and moved to a common module.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better way to indicate a non-supported API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42576][CONNECT] Add 2nd groupBy method to Dataset
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `groupBy(col1: String, cols: String*)` to Scala client Dataset API.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42573][CONNECT][FOLLOW-UP] fix broken build after variable rename
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix BUILD caused by https://github.com/apache/spark/pull/40168. This is not intentional when a PR changes stuff where another PR replies on old code but already pass the tests. In this case if either PR is merged without causing a conflict then another PR can be merged without knowing that it will cause a BUILD break.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix BUILD### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42580][CONNECT] Scala client add client side typed APIs
[SPARK-42564][CONNECT] Implement SparkSession.version and SparkSession.time
[SPARK-42583][SQL] Remove the outer join if they are all distinct aggregate functions
[MINOR][DOCS] Remove `Jenkins` from web page.
[SPARK-42560][CONNECT] Add ColumnName class
[SPARK-42587][CONNECT][TESTS] Use wrapper versions for SBT and Maven in `connect` module tests
[SPARK-42589][CONNECT][TESTS] Exclude `RelationalGroupedDataset.apply` from `CompatibilitySuite`
[SPARK-42588][SQL] Collapse two adjacent windows with the equivalent partition/order expressions in two withColumn()
[SPARK-42587][CONNECT][TESTS][FOLLOWUP] Fix `scalafmt` failure
[SPARK-42569][CONNECT] Throw exceptions for unsupported session API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Throw exceptions for unsupported session API:1. newSession2. getActiveSession3. getDefaultSession4. active### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->Better indicate an API is not supported to users.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42586][CONNECT] Add RuntimeConfig for Scala Client
[SPARK-42581][CONNECT] Add SQLImplicits.
[SPARK-42572][SQL][SS] Fix behavior for StateStoreProvider.validateStateRowFormat
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->https://github.com/apache/spark/pull/40073 accidentally changed the relationship of the two `if` statement in `StateStoreProvider.validateStateRowFormat`. Before they were inclusive, i.e. ```if (a) {  // <code>  if (b) {    // <code>  }}```It was changed to parallel, i.e.```if (a) {  // <code>}if (b) {    // <code>}```This PR change it back to the original behavior and add a unit test to prevent it in the future.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->As above.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit test
[SPARK-42592][SS][DOCS] Document how to perform chained time window aggregations
[SPARK-27483] [SS] [SQL] Move fallback logic for StreamingRelation(V2) to an analyzer rule
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Mainly refactoring. Move the logic of resolving StreamingRelation versions to a new rule. Concretely, create a new node named `VersionUnresolvedRelation`, which will be resolved to `StreamingRelation` or `StreamingRelationV2` with rule `ResolveDataSourceVersion` in analyzer. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better Code Structure.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests should be enough.
[SPARK-42597][SQL] Support unwrap date type to timestamp type
[SPARK-42599][CONNECT][INFRA] Introduce `dev/connect-jvm-client-mima-check`  instead of `CompatibilitySuite`
[SPARK-42600][SQL] CatalogImpl.currentDatabase shall use NamespaceHelper instead of MultipartIdentifierHelper
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->v2 catalog default namespace may be empty```javaException in thread \"main\" org.apache.spark.sql.AnalysisException: Multi-part identifier cannot be empty.\tat org.apache.spark.sql.errors.QueryCompilationErrors$.emptyMultipartIdentifierError(QueryCompilationErrors.scala:1887)\tat org.apache.spark.sql.connector.catalog.CatalogV2Implicits$MultipartIdentifierHelper.<init>(CatalogV2Implicits.scala:152)\tat org.apache.spark.sql.connector.catalog.CatalogV2Implicits$.MultipartIdentifierHelper(CatalogV2Implicits.scala:150)\tat org.apache.spark.sql.internal.CatalogImpl.currentDatabase(CatalogImpl.scala:65)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally verified
[SPARK-42528] Optimize PercentileHeap further by removing averaging
[SPARK-42602][CORE] Add reason argument to TaskScheduler.cancelTasks
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change adds a `reason: String` to the argument list of `TaskScheduler.cancelTasks`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Currently all tasks killed by `TaskScheduler.cancelTasks` will have a `TaskEndReason` \"TaskKilled (Stage cancelled)\". To better differentiate reasons for stage cancellations (e.g. user-initiated or caused by task failures in the stage), we could add a reason argument in `TaskScheduler.cancelTasks`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated unit tests.
[SPARK-42553][SQL] Ensure at least one time unit after \"interval\"
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->THis PR aims to ensure \"at least one time unit should be given for interval literal\" by modifying SqlBaseParser### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->INTERVAL is a Non-Reserved keyword in spark. But when I run```shellscala> spark.sql(\"select interval from mytable\")```I get```org.apache.spark.sql.catalyst.parser.ParseException:at least one time unit should be given for interval literal(line 1, pos 7)== SQL ==select interval from mytable-------^^^  at org.apache.spark.sql.errors.QueryParsingErrors$.invalidIntervalLiteralError(QueryParsingErrors.scala:196)......```It is a bug because \"Non-Reserved keywords\" have a special meaning in particular contexts and can be used as identifiers in other contexts. So by design, INTERVAL can be used as a column name.Currently the interval's grammar is```interval    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)?    ;```There is no need to make the time unit nullable, we can ensure \"at least one time unit should be given for interval literal\" if the interval's grammar is```interval    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)    ;```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->local test```shellscala> val myDF = spark.sparkContext.makeRDD(1 to 5).toDF(\"interval\")myDF: org.apache.spark.sql.DataFrame = [interval: int]scala> myDF.createOrReplaceTempView(\"mytable\")scala> spark.sql(\"select interval from mytable;\").show()+--------+|interval|+--------+|       1||       2||       3||       4||       5|+--------+```
[SPARK-42603][SQL] Set spark.sql.legacy.createHiveTableByDefault  to false.
[SPARK-42605][CONNECT] Add TypedColumn
[SPARK-42337][SQL][FOLLOWUP] Update the error message for INVALID_TEMP_OBJ_REFERENCE
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR is a follow-up for #39910. It updates the error message of the error class INVALID_TEMP_OBJ_REFERENCE.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error message more user-friendly.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. This PR updates the error message for INVALID_TEMP_OBJ_REFERENCE.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
[SPARK-42596][CORE][YARN] OMP_NUM_THREADS not set to number of executor cores by default
[SPARK-42542][CONNECT] Support Pivot without providing pivot column values
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `Pivot` API when pivot column values are not provided. The decision here is that we push everything into server thus does not do max value validation for the pivot column on the client sides (both Scala and Python) now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42510][CONNECT][PYTHON][TEST] Enable more `DataFrame.mapInPandas` parity tests
[SPARK-42608][SQL] Use full inner field names in resolution errors
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes `TableOutputResolver` use full names for inner fields in resolution errors.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to avoid confusion when there are multiple inner fields with the same name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
[SPARK-42612][CONNECT][PYTHON][TESTS] Enable more parity tests related to functions
[SPARK-42601][SQL] New physical type Decimal128 for DecimalType
[SPARK-42610][CONNECT] Add encoders to SQLImplicits
[SPARK-42611][SQL] Insert char/varchar length checks for inner fields during resolution
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds  char/varchar length checks for inner fields during resolution when struct fields are reordered.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These checks are needed to handle nested char/varchar columns correctly. Prior to this change, we would lose the raw type information when constructing nested attributes. As a result, we will not insert proper char/varchar length checks.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests that would previously fail.
[SPARK-42614][CONNECT] Make constructors private[sql]
[SPARK-42592][SS][DOCS][FOLLOWUP] Add missed commit on reflecting review comment
[SPARK-42427][SQL][TESTS][FOLLOW-UP] Disable ANSI for one more conv test case in MathFunctionsSuite
[SPARK-42615][CONNECT][PYTHON] Refactor the AnalyzePlan RPC and add `session.version`
[SPARK-42616][SQL] SparkSQLCLIDriver shall only close started hive sessionState
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->the hive sessionState initiated in SparkSQLCLIDriver will be started later in HiveClient during communicating with HMS if necessary. There are some cases that it will not get started:- fail early before reaching HiveClient- HiveClient is not used, e.g., v2 catalog only- ...### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bugfix, an app will end up with unexpected states, e.g.,```javabin/spark-sql -c spark.sql.catalogImplementation=in-memory -e \"select 1\"23/02/28 13:40:22 WARN Utils: Your hostname, hulk.local resolves to a loopback address: 127.0.0.1; using 10.221.102.180 instead (on interface en0)23/02/28 13:40:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another addressSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/02/28 13:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark master: local[*], Application Id: local-16775628240271Time taken: 2.578 seconds, Fetched 1 row(s)23/02/28 13:40:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist23/02/28 13:40:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist23/02/28 13:40:29 WARN Hive: Failed to register all functions.java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3901)\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:395)\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:339)\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:319)\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288)\tat org.apache.hadoop.hive.ql.session.SessionState.unCacheDataNucleusClassLoaders(SessionState.java:1596)\tat org.apache.hadoop.hive.ql.session.SessionState.close(SessionState.java:1586)\tat org.apache.hadoop.hive.cli.CliSessionState.close(CliSessionState.java:66)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.$anonfun$main$2(SparkSQLCLIDriver.scala:153)\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat scala.util.Try$.apply(Try.scala:213)\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\tat java.lang.Thread.run(Thread.java:750)Caused by: java.lang.reflect.InvocationTargetException\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\t... 31 moreCaused by: MetaException(message:Version information not found in metastore. )\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:83)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\t... 36 moreCaused by: MetaException(message:Version information not found in metastore. )\tat org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:7810)\tat org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:7788)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)\tat com.sun.proxy.$Proxy37.verifySchema(Unknown Source)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:595)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)\t... 40 more```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally verified
[SPARK-42613][CORE][PYTHON][YARN] PythonRunner should set OMP_NUM_THREADS to task cpus instead of executor cores by default
[SPARK-42491][BUILD] Upgrade jetty to 9.4.51.v20230217
[SPARK-42591][SS][DOCS] Add examples of unblocked workloads after SPARK-42376
[SPARK-42593][PS] Deprecate & remove the APIs that will be removed in pandas 2.0.
[SPARK-42559][CONNECT] Implement DataFrameNaFunctions
[SPARK-42579][CONNECT] Part-1: `function.lit` support `Array[_]` dataType
[SPARK-42622][CORE] Disable substitution in values
[SPARK-42647][PYTHON] Change alias for numpy deprecated and removed types
[SPARK-41551][SQL] Dynamic/absolute path support in PathOutputCommitters
[SPARK-42615][CONNECT][FOLLOW-UP] Implement correct version API in SparkSession for Scala client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Following up on https://github.com/apache/spark/pull/40210, add correct `version` in the scala client side.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42624][PYTHON][TESTS] Reorganize imports in test_functions
[SPARK-42625][BUILD] Upgrade `zstd-jni` to 1.5.4-2
[SPARK-41868][CONNECT][PYTHON] Fix createDataFrame to support durations
[SPARK-41870][CONNECT][PYTHON] Fix createDataFrame to handle duplicated column names
[SPARK-41874][CONNECT][PYTHON] Support SameSemantics in Spark Connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support SameSemantics in Spark Connect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->SameSemantics API calls from users returns result now than throwing an exception.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[MINOR][BUILD] Delete a invalid TODO from `dev/test-dependencies.sh`
[SPARK-42628][SQL][DOCS] Add a migration note for bloom filter join
[SPARK-42629][DOCS] Update the description of default data source in the document
[WIP][SPARK-42630][CONNECT][PYTHON] Make `parse_data_type` use new proto message `DDLParse`
[SPARK-42631][CONNECT] Support custom extensions in Scala client
[SPARK-42632][CONNECT] Fix scala paths in integration tests
[SPARK-38735][SQL][TESTS] Add tests for the error class: INTERNAL_ERROR
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to add tests for the error class INTERNAL_ERROR to QueryExecutionErrorsSuite### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The changes improve test coverage, and document expected error messages in tests### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->By running the tests
[SPARK-42635][SQL] Fix the TimestampAdd expression.
[SPARK-42633][CONNECT] Make LocalRelation take an actual schema
[SPARK-42637][CONNECT] Add SparkSession.stop()
[SPARK-42458][CONNECT][PYTHON] Fixes createDataFrame to support DDL string as schema
[SPARK-42640][CONNECT] Remove stale entries from the excluding rules for CompatibilitySuite
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove stale entries from the excluding rules for CompatibilitySuite.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Keep API compatibility list in sync.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42639][CONNECT] Add createDataFrame/createDataset methods
[SPARK-42641][CONNECT][BUILD] Upgrade buf from 1.14.0 to 1.15.0
[SPARK-42643][CONNECT][PYTHON] Register Java (aggregate) user-defined functions
[SPARK-41823][CONNECT][FOLLOW-UP][TESTS] Disable ANSI mode in ProtoToParsedPlanTestSuite
[SPARK-42644][INFRA] Add `hive` dependency to `connect` module
[SPARK-42646][BUILD] Upgrade cyclonedx from 2.7.3 to 2.7.5
[SPARK-42648][BUILD] Upgrade `versions-maven-plugin` to 2.15.0
[SPARK-42649][CORE] Remove the standard Apache License header from the top of third-party source files
[SPARK-42642][DOCS][PYTHON] Updating remaining Spark documentation code examples to show Python by default
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Making Python the first tab for code examples in the Spark documentation.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?This completes the work started with [SPARK-42493].Python is the most approachable and most popular language and this change moves Python code examples to the first tab (showing by default).<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing Spark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?I built the website locally and manually tested the pages.SKIP_SCALADOC=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-41725][PYTHON][TESTS][FOLLOW-UP] Remove collect for SQL command execution in tests
[SPARK-42555][CONNECT] Add JDBC to DataFrameReader
[SPARK-42553][SQL][3.3] Ensure at least one time unit after \"interval\"
[SPARK-42654][BUILD] Upgrade dropwizard metrics 4.2.17
[SPARK-42558][CONNECT] Implement `DataFrameStatFunctions` except `bloomFilter` functions
[SPARK-42653][CONNECT] Artifact transfer from Scala/JVM client to Server
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces a mechanism to transfer artifacts (currently, local `.jar` + `.class` files) from a Spark Connect JVM/Scala client over to the server side of Spark Connect. The mechanism follows the protocol as defined in https://github.com/apache/spark/pull/40147 and supports batching (for multiple \"small\" artifacts) and chunking (for large artifacts).Note: Server-side artifact handling is not covered in this PR.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In the decoupled client-server architecture of Spark Connect, a remote client may use a local JAR or a new class in their UDF that may not be present on the server. To handle these cases of missing \"artifacts\ we implement a mechanism to transfer artifacts from the client side over to the server side as per the protocol defined in https://github.com/apache/spark/pull/40147.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, users would be able to use the `addArtifact` and `addArtifacts` methods (via a `SparkSession` instance) to transfer local files (`.jar` and `.class` extensions).### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests - located in `ArtifactSuite`. 
[SPARK-42656][CONNECT] Adding SCALA REPL shell script for JVM client
[SPARK-42655][SQL] Incorrect ambiguous column reference error
[SPARK-42609][CONNECT][TESTS] Add tests for grouping() and grouping_id() functions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add tests for grouping() and grouping_id() functions.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve testing coverage.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42630][CONNECT][PYTHON] Introduce UnparsedDataType and delay parsing DDL string until SparkConnectClient is available
[SPARK-42615][CONNECT][FOLLOWUP] Fix SparkConnectAnalyzeHandler to use withActive
[SPARK-42651][SQL] Optimize global sort to driver sort
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr adds a new physical plan `DriverSortExec`. It represents a special case of global `SortExec`  which is the root node. Then we can save one shuffle.Add a new config `spark.sql.execution.driverSortThreshold` to control if it's safe to do sort at driver side. We should make sure the max rows of plan is small enough to avoid oom and preformance regression.This optimization should work fine since It gets benefits from AQE framework that the max rows of logical plan is accurate and it can be propagated cross most of nodes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve performance### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test and do some benchmarkSort 50, 000 rows for such case `df.sort.collect`:```OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1033-azureIntel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHzDriverSort:                               Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Sort                                               1158           1355         210          0.0       23169.3       1.0XDriverSort                                          636            680          55          0.1       12715.2       1.8X```
[SPARK-42659][ML] Reimplement `FPGrowthModel.transform` with dataframe operations
[SPARK-42635][SQL][3.3] Fix the TimestampAdd expression
[SPARK-42556][CONNECT] Dataset.colregex should link a plan_id when it only matches a single column.
[SPARK-42660][SQL] Infer filters for Join produced by IN and EXISTS clause (RewritePredicateSubquery rule)
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?We should run `InferFiltersFromConstraints` again after running `RewritePredicateSubquery` rule. `RewritePredicateSubquery `rewrite IN and EXISTS queries to LEFT SEMI/LEFT ANTI joins. But we don't infer filters for these newly generated joins. We noticed in TPCH 1TB q21 by inferring filter for these new joins, one `lineitem` table scan can be reduced as `ReusedExchange` got introduce. Previously due to mismatch in filter predicates reuse was not happening.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Can improve query performance.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->PlanStability test
[SPARK-42653][CONNECT][FOLLOW-UP] Fix Scala 2.13 build failure by explicit Seq conversion
[SPARK-42500][SQL] ConstantPropagation support more cases
[SPARK-42853][DOC] Updating the Style for the Spark Docs based on the Webpage
[WIP][SPARK-42662][CONNECT][PYTHON][PS] Support `withSequenceColumn` as PySpark DataFrame internal function.
SPARK-42258][PYTHON] pyspark.sql.functions should not expose typing.cast
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?In the `pyspark.sql.functions`, we replaced `from typing import foo, bar, etc` with `import typing` and alluses of `foo`, `bar` and `etc` in that module were replaced with `typing.foo`, `typing.bar` and `typing.etc`.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Exposing methods from `typing` inside `spark.sql.functions` could lead to confusing errors on the user side, like in this example:```from pyspark.sql import SparkSessionfrom pyspark.sql import functions as fspark = SparkSession.builder.getOrCreate()df = spark.sql(\"\"\"SELECT 1 as a\"\"\")df.withColumn(\"a\ f.cast(\"STRING\ f.col(\"a\"))).printSchema()  ```The code above runs without raising any error but yields an incorrect result:```root|-- a: integer (nullable = false)```This is because `f.cast` is in fact `typing.cast` and we should have used `f.col(\"a\").cast(\"STRING\")` instead.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, if a user has made the mistake described in the example above, their code that did run (with an incorrect behaviour) will instead break after they upgrade to a version containing this change. But one could argue that it would be a good thing, since it will expose a mistake in their code.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?No new test were added.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42667][CONNECT] Spark Connect: newSession API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes an implementation of newSession API. The idea is we reuse user context(e.g. user_id), gRPC channel, etc. But differentiate different Spark Remote Session by client id, which is generated randomly. So this idea has the benefits of:1. reusing gRPC channel to not over too manny connections to the server.2. Each user can has multiple remote sessions, differentiated by client ids (or named session ids in server side).  ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42668][SS] Catch exception while trying to close compressed stream in HDFSStateStoreProvider abort
[SPARK-42215][CONNECT] Simplify Scala Client IT tests
[SPARK-42557][CONNECT] Add Broadcast to functions
[SPARK-42630][CONNECT][PYTHON] Implement data type string parser
[SPARK-42555][CONNECT][FOLLOWUP] Add the new proto msg to support the remaining jdbc API
[SPARK-42670][BUILD] Upgrade maven-surefire-plugin to 3.0.0-M9 & eliminate build warnings
[MINOR][CONNECT] Remove unused protobuf imports to eliminate build warnings
[SPARK-42671][CONNECT] Fix bug for createDataFrame from complex type schema
[SPARK-41497][CORE][Follow UP]Modify config `spark.rdd.cache.visibilityTracking.enabled` support version to 3.5.0
[SPARK-42672][PYTHON][DOCS] Document error class list
[SPARK-42673][BUILD]  Make `build/mvn` build Spark only with the verified maven version
[SPARK-42674][BUILD] Upgrade scalafmt from 3.7.1 to 3.7.2
[SPARK-42675][CONNECT][TESTS] Drop temp view after test `test temp view`
[SPARK-42577][CORE] Add max attempts limitation for stages to avoid potential infinite retry
[SPARK-42562][CONNECT] UnresolvedNamedLambdaVariable in python do not need unique names
[SPARK-42496][CONNECT][DOCS] Introduction Spark Connect at main page.
[SPARK-42478][SQL][3.2] Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory
This is a backport of https://github.com/apache/spark/pull/40064 for branch-3.2<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-41448](https://issues.apache.org/jira/browse/SPARK-41448) make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
[SPARK-42478][SQL][3.3] Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory
This is a backport of https://github.com/apache/spark/pull/40064 for branch-3.3<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-41448](https://issues.apache.org/jira/browse/SPARK-41448) make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
[WIP][SPARK-42578][CONNECT] Add JDBC to DataFrameWriter
[SPARK-42676][SS] Write temp checkpoints for streaming queries to local filesystem even if default FS is set differently
[SPARK-42677][SQL][TESTS] Fix the invalid tests for broadcast hint
[SPARK-40610][SQL] Support unwrap date type to string type
[SPARK-42681][SQL] Relax ordering constraint for ALTER TABLE ADD|REPLACE column descriptor
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Currently the grammar for ALTER TABLE ADD|REPLACE column is:```qualifiedColTypeWithPosition: name=multipartIdentifier dataType (NOT NULL)? defaultExpression? commentSpec? colPosition?;```This enforces a constraint on the order of: (NOT NULL, DEFAULT value, COMMENT value FIRST|AFTER value). We can update the grammar to allow these options in any order instead, to improve usability.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This helps make the SQL syntax more usable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the SQL syntax updates slightly.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing and new unit tests
[SPARK-42680][CONNECT][TESTS] Create the helper function withSQLConf for connect test framework
[SPARK-42412][WIP] Initial PR of Spark connect ML
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Design doc:https://docs.google.com/document/d/1V5rOgksmOnA8AsJFZ_rasSYDQuP06_vrcfp3RY_22o8/edit#### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Testing code:run command `bin/pyspark --remote local `, in python REPL, run following code:```from pyspark.ml.classification import LogisticRegression, LogisticRegressionModellor = LogisticRegression()# Set paramslor.setMaxIter(2)df0 = spark.read.format(\"libsvm\").load(\"data/mllib/sample_binary_classification_data.txt\")# Train modellor_model = lor.fit(df0)infer_df = df0.sample(0.5)# Predictionprediction_df = lor_model.transform(infer_df)prediction_df.show()# Test model attributesprint(lor_model.coefficients)print(lor_model.intercept)print(lor_model.coefficientMatrix)print(lor_model.interceptVector)# Test model summary methodsprint(lor_model.summary.featuresCol)lor_model.summary.roc.show()print(lor_model.summary.areaUnderROC)lor_model.summary.pr.show()lor_model.summary.fMeasureByThreshold.show()lor_model.summary.precisionByThreshold.show()print(lor_model.summary.weightedFalsePositiveRate)print(lor_model.summary.precisionByLabel)print(lor_model.summary.objectiveHistory)summary2 = lor_model.evaluate(infer_df)summary2.roc.show()print(summary2.precisionByLabel)# save estimatorlor.write().overwrite().save(\"/tmp/lore_001\")loaded_lor = LogisticRegression.load(\"/tmp/lore_001\")# save modellor_model.write().overwrite().save(\"/tmp/lor_001\")# load modelloaded_model = LogisticRegressionModel.read().load(\"/tmp/lor_001\")# Test loaded model transformationloaded_model.transform(infer_df).show()```
[SPARK-42595][SQL] Support query inserted partitions after insert data into table when hive.exec.dynamic.partition=true
[SPARK-42684][SQL] v2 catalog should not allow column default value by default
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Following generated columns, column default value should also have a catalog capability and v2 catalogs must explicitly declare SUPPORT_COLUMN_DEFAULT_VALUE to support it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->column default value needs dedicated handling and if a catalog simply ignores it, then query result can be wrong.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[SPARK-42683] Automatically rename conflicting metadata columns
[SPARK-42685][CORE] Optimize Utils.bytesToString routines
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Optimize `Utils.bytesToString`. Arithmetic ops on `BigInt` and `BigDecimal` are order(s) of magnitude slower than the ops on primitive types. Division is an especially slow operation and it is used en masse here.To avoid heating up the Earth while formatting byte counts for human consumption we observe that most formatting operations are not in the 10s of EiBs but on counts that fit in 64-bits and use (fastpath) 64-bit operations to format them.### Why are the changes needed?Use of `Utils.bytesToString` is prevalent through the codebase and they are mainly used in logs. If the logs are emitted then this becomes a heavyweight operation.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
[SPARK-42686][CORE] Defer formatting for debug messages in TaskMemoryManager
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Defer formatting of bytes until debug logging is required. Otherwise we are always spending cycles doing formatting irrespective of if debug logging is enabled.### Why are the changes needed?Formatting shows up in profiling scan benchmarks.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually.
[SPARK-42656][CONNECT][Followup] Improve the script to start spark-connect server
[SPARK-42665][CONNECT][Test] Mute Scala Client UDF test
[SPARK-42656][CONNECT][Followup] Spark Connect Shell
[SPARK-42687][SS] Better error message for the unsupport `pivot` operation in Streaming
[SPARK-42689][CORE][SHUFFLE] Allow ShuffleDriverComponent to declare if shuffle data is reliably stored
[SPARK-42151][SQL] Align UPDATE assignments with table attributes
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a rule to align UPDATE assignments with table attributes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed so that we can rewrite UPDATE statements into executable plans for tables that support row-level operations. In particular, our row-level mutation framework assumes Spark is responsible for building an updated version of each affected row and that row is passed back to the data source.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
[SPARK-42688][CONNECT] Rename Connect proto Request client_id to session_id
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Rename Connect proto requests `client_id` to `session_id`.On the one hand when I read `client_id` I was confused on what it is used to, even after reading the proto documentation.On the other hand,  client sides already use session_id:https://github.com/apache/spark/blob/9bf174f9722e34f13bfaede5e59f989bf2a511e9/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala#L51https://github.com/apache/spark/blob/9bf174f9722e34f13bfaede5e59f989bf2a511e9/python/pyspark/sql/connect/client.py#L522### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code readability### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-42022][CONNECT][PYTHON] Fix createDataFrame to autogenerate missing column names
[SPARK-42559][CONNECT][TESTS][FOLLOW-UP] Disable ANSI in several tests at DataFrameNaFunctionSuite.scala
[SPARK-42695][SQL] Skew join handling in stream side of broadcast hash join
[SPARK-42697][WEBUI] Fix /api/v1/applications to return total uptime instead of 0 for the duration field
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix /api/v1/applications to return total uptime instead of 0 for duration### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix REST API OneApplicationResource### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, /api/v1/applications will return the total uptime instead of 0 for the duration### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally build and run```json[ {  \"id\" : \"local-1678183638394\  \"name\" : \"SparkSQL::10.221.102.180\  \"attempts\" : [ {    \"startTime\" : \"2023-03-07T10:07:17.754GMT\    \"endTime\" : \"1969-12-31T23:59:59.999GMT\    \"lastUpdated\" : \"2023-03-07T10:07:17.754GMT\    \"duration\" : 20317,    \"sparkUser\" : \"kentyao\    \"completed\" : false,    \"appSparkVersion\" : \"3.5.0-SNAPSHOT\    \"startTimeEpoch\" : 1678183637754,    \"endTimeEpoch\" : -1,    \"lastUpdatedEpoch\" : 1678183637754  } ]} ]```
[SPARK-42698][CORE] SparkSubmit should also stop SparkContext when exit program in yarn mode and pass exitCode to AM side
[SPARK-42699][CONNECT] SparkConnectServer should make client and AM same exit code
[SPARK-42679][CONNECT][PYTHON] createDataFrame doesn't work with non-nullable schema
[SPARK-42700][BUILD] Add `h2` as test dependency of connect-server module
[SPARK-42656][SPARK SHELL][CONNECT][FOLLOWUP] Add same `ClassNotFoundException` catch to `repl.Main` for Scala 2.13
[SPARK-42692][CONNECT] Implement `Dataset.toJSON`
Update code example formatting for protobuf parsing readme
[SPARK-42704] SubqueryAlias propagates metadata columns that child outputs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The `AddMetadataColumns` analyzer rule is designed to resolve metadata columns using `LogicalPlan.metadataOutput` -- even if the plan already contains projections whose output does not specifically include a requested metadata column.Meanwhile, the `SubqueryAlias` plan node intentionally does _NOT_ propagate metadata columns automatically from a non-leaf/non-subquery child node, because the following should _NOT_ work:```scalaspark.read.table(\"t\").select(\"a\ \"b\").as(\"s\").select(\"_metadata\")```However, the current implementation is too strict in breaking the metadata chain, in case the child node's output already includes the metadata column:```scala// expected to work (and does)spark.read.table(\"t\")  .select(\"a\ \"b\").select(\"_metadata\")// by extension, this should also work (but does not)spark.read.table(\"t\").select(\"a\ \"b\ \"_metadata\").as(\"s\")  .select(\"a\ \"b\").select(\"_metadata\")```The solution is for `SubqueryAlias` to propagate metadata columns that are already in the child's output, thus preserving the `metadataOutput` chain for such columns.### Why are the changes needed?The current implementation of `SubqueryAlias` breaks the intended behavior of metadata column propagation. ### Does this PR introduce _any_ user-facing change?Yes. The following now works, where previously it did not:```scalaspark.read.table(\"t\").select(\"a\ \"b\ \"_metadata\").as(\"s\")  .select(\"a\ \"b\").select(\"_metadata\")```### How was this patch tested?New unit tests verify the expected behavior holds, with and without subqueries in the plan.
[SPARK-41775][PYTHON][FOLLOW-UP] Updating error message for training using PyTorch functions
[SPARK-42705][CONNECT] Fix spark.sql to return values from the command
[SPARK-42496][CONNECT][DOCS] Adding Spark Connect to the Spark 3.4 documentation
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Adding a Spark Connect overview page to the Spark 3.4 documentation and a short section on the Spark overview page with a link to it.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?The first version of Spark Connect is released as part of Spark 3.4.0 and this adds an overview for it to the documentation.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built the doc website locally and tested the pages.SKIP_SCALADOC=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->index.html![index html](https://user-images.githubusercontent.com/112507318/224800134-7d0093bb-261a-4abe-a3da-b1e8784854c1.png)spark-connect-overview.html![spark-connect-overview html](https://user-images.githubusercontent.com/112507318/224800167-8f6308e0-2e0e-4b15-81a8-5bd9da12d0ce.png)
[SPARK-42707][CONNECT][DOCS] Update developer documentation about API stability warning
[SPARK-42708][DOCS] Improve doc about protobuf java file can't be indexed.
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Improve README doc for developers about protobuf java file can't be indexed.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To make sure developers to start spark easier.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?NO<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?No<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42266][PYTHON] Remove the parent directory in shell.py execution when IPython is used
[SPARK-42709][PYTHON] Remove the assumption of `__file__` being available 
[SPARK-42710][CONNECT][PYTHON] Rename FrameMap proto to MapPartitions
[SPARK-42712][PYTHON][DOC] Improve docstring of mapInPandas and mapInArrow
[SPARK-42713][PYTHON][DOCS] Add '__getattr__' and '__getitem__' of DataFrame and Column to API reference
[SPARK-42690][CONNECT] Implement CSV/JSON parsing functions for Scala client
[SPARK-42702][SPARK-42623][SQL] Support parameterized query in subquery and CTE
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR fixes a few issues of parameterized query:1. replace placeholders in CTE/subqueries2. don't replace placeholders in non-DML commands as it may store the original SQL text with placeholders and we can't resolve it later (e.g. CREATE VIEW).### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->make the parameterized query feature complete### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, bug fix### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[SPARK-42716][SQL] DataSourceV2 supports reporting key-grouped partitioning without HasPartitionKey
[SPARK-42717][BUILD] Upgrade mysql-connector-java from 8.0.31 to 8.0.32
[SPARK-42706][SQL][DOCS] Document the Spark SQL error classes in user-facing documentation.
[SPARK-42718][BUILD] Upgrade rocksdbjni to 7.10.2
[MINOR][PYTHON] Change TypeVar to private symbols
[SPARK-42719][CORE] `MapOutputTracker#getMapLocation` should respect `spark.shuffle.reduceLocality.enabled`
[SPARK-42701][SQL] Add the `try_aes_decrypt()` function
[WIP][SPARK-42715][SQL] Tips for Optimizing NegativeArraySizeException
[SPARK-42721][CONNECT] RPC logging interceptor 
[SPARK-42722][CONNECT][PYTHON] Python Connect def schema() should not cache the schema
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As of now that Connect Python client cache the schema when calling `def schema()`. However this might cause stale data issue. For example:```1. Create table2. table.schema3. drop table and recreate the table with different schema4. table.schema // now this is incorrect```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->Fix the behavior when the cached schema could be stale.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This is actually a fix that users now can always see the most up-to-dated schema.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-42656][CONNECT][Followup] Fix the spark-connect script
[SPARK-42723][SQL] Support parser data type json \"timestamp_ltz\" as TimestampType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support parsing `timestamp_ltz` as `TimestampType` in schema JSON string.It also add tests for both parsing JSON/DDL for \"timestamp_ltz\" and \"timestamp_ntz\"### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`timestamp_ltz` becomes an alias for TimestampType since Spark 3.4### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the new keyword is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
[SPARK-42667][CONNECT][FOLLOW-UP] SparkSession created by newSession should not share the channel
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SparkSession created by newSession should not share the channel. This is because that a SparkSession might be called `stop` in which the channel it uses will be shutdown. If the channel is shared, other non-stop SparkSession that is sharing this channel will get into trouble.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This fixes the issue when one SparkSession is stopped to cause other active SparkSession not working in Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-42711][BUILD]Update usage info and shellcheck warn/error fix for build/sbt tool
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The build/sbt script's usage information have several error. See the info below:```(base) spark% ./build/sbt -helpUsage:  [options]  -h | -help         print this message  -v | -verbose      this runner is chattier```There is no script name after the usage. With this change, the info become:```(base) spark % ./build/sbt -helpUsage: sbt [options]  -h | -help         print this message  -v | -verbose      this runner is chattier```This changes also fix several shellcheck error. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug fix.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the user of build/sbt tool will see usage information updated.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual check the script codes.
[SPARK-42724][CONNECT][BUILD] Upgrade buf to v1.15.1
[SPARK-42725][CONNECT][PYTHON] Make LiteralExpression support array params
[SPARK-42726][CONNECT][PYTHON] Implement `DataFrame.mapInArrow`
[SPARK-42727][CORE] Fix can't executing spark commands in the root directory when local mode is specified
[SPARK-42664][CONNECT] Support `bloomFilter` function for `DataFrameStatFunctions`
[SPARK-42732][PYTHON][CONNECT] Update spark connect session `getOrCreate` behavior to check existing global `_active_spark_session` first
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update spark connect session `getOrCreate` behavior to check existing global `_active_spark_session` first, if existing, return it.Spark connect ML needs this API to get active session in some cases (e.g. fetching model attributes from server side).### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Manually.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. Implemented `pyspark.sql.connect.session.SparkSession.getActiveSession` API. ### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42735][CONNECT][SCALA] Allow passing extra confs to RemoteSparkSession
[SPARK-42604][CONNECT] Implement functions.typedlit
[SPARK-42733][CONNECT][PYTHON] Fix DataFrameWriter.save to work without path parameter
[SPARK-42739][BUILD] Ensure release tag to be pushed to release branch
[SPARK-42733][CONNECT][Followup] Write without path or table
[SPARK-42740][SQL] Fix the bug that pushdown offset or paging is invalid for some built-in dialect
[SPARK-42741][SQL] Do not unwrap casts in binary comparison when literal is null
[SPARK_42742]access apiserver by pod env
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When start spark on k8s，driver pod  use spark.kubernetes.driver.master to get apiserver address. This config  us  https://kubernetes.default.svc/ as default and do not care about the apiserver port.In our case, apiserver port is not 443 will driver will throw connectException. As k8s doc mentioned （https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/#directly-accessing-the-rest-api）, we can get master url by getting KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT_HTTPS environment variables from pod. So we add a new conf spark.kubernetes.driver.master.from.pod.env to allow driver get master url from env in cluster mode on k8s### Why are the changes needed?Add a new conf spark.kubernetes.driver.master.from.pod.env  to let the driver pod get apiserver automatically from pod env instead of by  spark.kubernetes.driver.master.### Does this PR introduce _any_ user-facing change?Yes. When user set new conf spark.kubernetes.driver.master.from.pod.env as true, the logic of driver get apiserver url will changed. In some case it will help user to get right apiserver url.By default, the conf spark.kubernetes.driver.master.from.pod.env  is false, and the driver logic changes nothing.### How was this patch tested?No. the apiserver is mocked in unit test. we tested this feature in our k8s cluster
[SPARK-42743][SQL] Support analyze TimestampNTZ columns
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support analyze TimestampNTZ columns ```ANALYZE TABLE table_name [ PARTITION clause ]    COMPUTE STATISTICS [ NOSCAN | FOR COLUMNS col1 [, ...] | FOR ALL COLUMNS ]```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Support computing statistics of TimestmapNTZ columns, which can be used for optimizations.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the TimestampNTZ type is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Update existing UT
[SPARK_42744] delete uploaded file when job finish for k8s
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Let driver delete uploaded file when job finish.### Why are the changes needed?Now there is no deletion for files uploaded by client, which causes file leaks on remote file system.### Does this PR introduce _any_ user-facing change?Yes. This PR add a new configuration spark.kubernetes.uploaded.file.delete.on.termination. By default, this configuration is false and the behavior is the same with current version. When the configuration is set to true, driver will try to delete uploaded files when job finish.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42745][SQL] Improved AliasAwareOutputExpression works with DSv2
[MINOR][SQL] Fix incorrect comment in LimitPushDownThroughWindow
[SPARK-42691][CONNECT][PYTHON] Implement Dataset.semanticHash
[SPARK-42747][ML] Fix incorrect internal status of LoR and AFT
[SPARK-42748][CONNECT] Server-side Artifact Management
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds server-side artifact management as a follow up to the client-side artifact transfer introduced in https://github.com/apache/spark/pull/40256.Note: The artifacts added on the server are visible to **all users** of the cluster. This is a limitation of the current spark architecture (unisolated classloaders).Apart from storing generic artifacts, we handle jars and classfiles in specific ways:- Jars:   - Jars may be added but not removed or overwritten.  - Added jars would be visible to **all** users/tasks/queries.- Classfiles:  - Classfiles may not be explicitly removed but are allowed to be overwritten.  - We piggyback on top of the REPL architecture to serve classfiles to the executors    -  If a REPL is initialized, classfiles are stored in the existing `spark.repl.class.outputDir` and share the URI with `spark.repl.class.uri`.    - If a REPL is not being used, we use a custom directory (root: `sparkContext. sparkConnectArtifactDirectory`) to store classfiles and point the `spark.repl.class.uri` towards it.  - Class files are visible to **all** users/tasks/queries.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->https://github.com/apache/spark/pull/40256 implements the client-side transfer of artifacts to the server but currently, the server does not process these requests.We need to implement a server-side management mechanism to handle the storage of these artifacts on the driver as well as perform further processing (such as adding jars and moving class files to the right directories).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, a new experimental API but no behavioural changes.A new method called `sparkConnectArtifactDirectory` is accessible through SparkContext (the directory storing all artifacts from SparkConnect)### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit tests.
[SPARK-42398][SQL][FOLLOWUP] DelegatingCatalogExtension should override the new createTable method
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/40049 to fix a small issue: `DelegatingCatalogExtension` should also override the new `createTable` function and call the session catalog, instead of using the default implementation.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A, too trivial.
[SPARK-42620][PS] Add `inclusive` parameter for (DataFrame|Series).between_time
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `inclusive` parameter for (DataFrame|Series).between_time to support the pandas 2.0.0### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, the API changesBefore:` (DataFrame|Series).between_time(start_time, end_time, include_start, include_end, axis)`After:` (DataFrame|Series).between_time(start_time, end_time, inclusive, axis)`### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests were updated
Revert \"[SPARK-41498] Propagate metadata through Union\"
This reverts commit 827ca9b82476552458e8ba7b01b90001895e8384.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->After more thinking, it's a bit fragile to propagate metadata columns through Union. We have added quite some new fields in the file source `_metadata` metadata column such as `row_index`, `block_start`, etc. Some are parquet only. The same thing may happen in other data sources as well. If one day one table under Union adds a new metadata column (or add a new field if the metadata column is a struct type), but other tables under Union do not have this new column, then Union can't propagate metadata columns and the query will suddenly fail to analyze.To be future-proof, let's revert this support.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to make the analysis behavior more robust.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, but propagating metadata columns through Union is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42752][PYSPARK][SQL] Make PySpark exceptions printable during initialization
[Draft] Streaming Spark Connect POC
[SPARK-42721][CONNECT][FOLLOWUP] Apply scalafmt to LoggingInterceptor
[SPARK-42755][CONNECT] Factor literal value conversion out to `connect-common`
[SPARK-42756][CONNECT][PYTHON] Helper function to convert proto literal to value in Python Client
[SPARK-42757][CONNECT] Implement textFile for DataFrameReader
[SPARK-42758][BUILD][MLLIB] Remove dependency on breeze
[SPARK-42759][BUILD] Avoid duplicated `build/apache-maven` install when target already exists
An obsolete pr, please don't pay attention
[SPARK-42761][BUILD][K8S] Upgrade `kubernetes-client` to 6.5.0
[SPARK-42762][K8S] Improve logging in K8s on disconnect when using StatefulSets
[SPARK-42763][BUILD] Upgrade ZooKeeper from 3.6.3 to 3.6.4
[SPARK-42753] ReusedExchange refers to non-existent nodes
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR addresses a rare bug with the EXPLAIN function and Spark UI that can happen when AQE takes effect with multiple ReusedExchange nodes. The bug causes the ReusedExchange to point to an unknown child since that child subtree was \"pruned\" in a previous AQE iteration. This PR fixes the issue by finding all the ReusedExchange nodes in the tree that have a `child` node that has NOT been processed in the final plan (meaning it has no ID or it has an incorrect ID generated from the previous AQE iteration). It then traverses the child subtree and generates correct IDs for them. We print this missing subtree in a new section called `Adaptively Optimized Out Exchanges`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Below is an example to demonstrate the root cause:> AdaptiveSparkPlan>   |-- SomeNode X (subquery xxx)>       |-- Exchange A>           |-- SomeNode Y>               |-- Exchange B> > Subquery:Hosting operator = SomeNode Hosting Expression = xxx dynamicpruning#388> AdaptiveSparkPlan>   |-- SomeNode M>       |-- Exchange C>           |-- SomeNode N>               |-- Exchange D> Step 1: Exchange B is materialized and the QueryStage is added to stage cacheStep 2: Exchange D reuses Exchange BStep 3: Exchange C is materialized and the QueryStage is added to stage cacheStep 4: Exchange A reuses Exchange CThen the final plan looks like:> AdaptiveSparkPlan>   |-- SomeNode X (subquery xxx)>       |-- Exchange A -> ReusedExchange (reuses Exchange C)> > > Subquery:Hosting operator = SomeNode Hosting Expression = xxx dynamicpruning#388> AdaptiveSparkPlan>   |-- SomeNode M>       |-- Exchange C -> PhotonShuffleMapStage ....>           |-- SomeNode N>               |-- Exchange D -> ReusedExchange (reuses Exchange B)> As a result, the ReusedExchange (reuses Exchange B) will refer to a non-exist node.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->**Explain Text Before and After****Before:**```+- ReusedExchange (105)(105) ReusedExchange [Reuses operator id: unknown]Output [3]: [sr_customer_sk#303, sr_store_sk#307, sum#413L]```**After:**```+- ReusedExchange (105)   +- Exchange (132)      +- * HashAggregate (131)         +- * Project (130)            +- * BroadcastHashJoin Inner BuildRight (129)               :- * Filter (128)               :  +- * ColumnarToRow (127)               :     +- Scan parquet hive_metastore.tpcds_sf1000_delta.store_returns (126)               +- ShuffleQueryStage (115), Statistics(sizeInBytes=5.7 KiB, rowCount=366, [d_date_sk#234 -> ColumnStat(Some(362),Some(2415022),Some(2488070),Some(0),Some(4),Some(4),None,2)], isRuntime=true)                  +- ReusedExchange (114)(105) ReusedExchange [Reuses operator id: 132]Output [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L](126) Scan parquet hive_metastore.tpcds_sf1000_delta.store_returnsOutput [4]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214]Batched: trueLocation: PreparedDeltaFileIndex [dbfs:/mnt/performance-datasets/2018TPC/tpcds-2.4/sf1000_delta/store_returns]PartitionFilters: [isnotnull(sr_returned_date_sk#214), dynamicpruningexpression(sr_returned_date_sk#214 IN dynamicpruning#329)]PushedFilters: [IsNotNull(sr_store_sk)]ReadSchema: struct<sr_customer_sk:int,sr_store_sk:int,sr_return_amt:decimal(7,2)>(127) ColumnarToRowInput [4]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214](128) FilterInput [4]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214]Condition : isnotnull(sr_store_sk#221)(114) ReusedExchange [Reuses operator id: 8]Output [1]: [d_date_sk#234](115) ShuffleQueryStageOutput [1]: [d_date_sk#234]Arguments: 2, Statistics(sizeInBytes=5.7 KiB, rowCount=366, [d_date_sk#234 -> ColumnStat(Some(362),Some(2415022),Some(2488070),Some(0),Some(4),Some(4),None,2)], isRuntime=true)(129) BroadcastHashJoinLeft keys [1]: [sr_returned_date_sk#214]Right keys [1]: [d_date_sk#234]Join type: InnerJoin condition: None(130) ProjectOutput [3]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225]Input [5]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214, d_date_sk#234](131) HashAggregateInput [3]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225]Keys [2]: [sr_customer_sk#217, sr_store_sk#221]Functions [1]: [partial_sum(UnscaledValue(sr_return_amt#225)) AS sum#327L]Aggregate Attributes [1]: [sum#326L]Results [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L](132) ExchangeInput [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L]Arguments: hashpartitioning(sr_store_sk#221, 200), ENSURE_REQUIREMENTS, [plan_id=1791]```**Spark UI Before and After****Before:**<img width=\"339\" alt=\"Screenshot 2023-03-10 at 10 52 46 AM\" src=\"https://user-images.githubusercontent.com/83618776/224406011-e622ad11-37e6-48c6-b556-cd5c7708e237.png\">**After:**![image](https://user-images.githubusercontent.com/83618776/224406076-4fcbf918-2a8d-4776-b91a-36815752cf2a.png)### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests were added to `ExplainSuite`. And manually tested with ExplainSuite.
[MINOR][SQL][FOLLOWUP] Fix scalastyle in LimitPushDownThroughWindow
[SPARK-42764][K8S] Parameterize the max number of attempts for driver props fetcher in KubernetesExecutorBackend
[SPARK-42765][CONNECT][PYTHON] Enable importing `pandas_udf` from `pyspark.sql.connect.functions`
[SPARK-42767][CONNECT][TESTS] Add a precondition to start connect server fallback with `in-memory` and auto ignored some tests strongly depend on hive
[SPARK-42768][SQL] Enable cached plan apply AQE by default
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr enables the `spark.sql.optimizer.canChangeCachedPlanOutputPartitioning` by default.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We have fixed all known issues when enable cache + AQE since SPARK-42101. There is no reason to skip AQE optimizing cached plan.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, the default config changed### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI
[SPARK-42766][YARN] YarnAllocator filter excluded nodes when launching containers
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In production environment, we hit an issue like this:If we request 10 containers form nodeA and nodeB, first response from Yarn return 5 contianers from nodeA and nodeB, then nodeA blacklisted, and second response from Yarn maybe return some containers from nodeA and launching containers, but when containers(Executor) setup and send register request to Driver, it will be rejected and this failure will be counted to `spark.yarn.max.executor.failures` and will casue app failed: `Max number of executor failures ($maxNumExecutorFailures) reached`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Filtering excluded nodes when launching containers to avoid failing the app.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added UT
[SPARK-42769][K8S] Add `SPARK_DRIVER_POD_IP` env variable to executor pods
[SPARK-40082] Schedule mergeFinalize when push merge shuffleMapStage retry but no running tasks
[SPARK-42771][SQL] Refactor HiveGenericUDF
[SPARK-42770][CONNECT] Add `truncatedTo(ChronoUnit.MICROS)` to make `SQLImplicitsTestSuite`  in Java 17 daily test GA task pass
[SPARK-42772][SQL] Change the default value of JDBC options about push down to true
[SPARK-42052][SQL] Codegen Support for HiveSimpleUDF
[MINOR][DOCS] Update `translate` docblock
[SPARK-42101][SQL][FOLLOWUP] Make QueryStageExec more type safe
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/39624 .`TableCacheQueryStageExec.cancel` is a noop and we can move `def cancel` out from `QueryStageExec`. Due to this movement, I renamed `ReusableQueryStageExec` to `ExchangeQueryStageExec`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->type safe### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-41359][SQL] Use `PhysicalDataType` instead of DataType in UnsafeRow
[SPARK-42773][DOCS][PYTHON] Minor update to 3.4.0 version change message for Spark Connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Changing the 3.4.0 version change message for PySpark functionality from \"Support Spark Connect\" to \"Supports Spark Connect\".<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Grammatical improvement.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, these messages are shown in the user-facing PySpark documentation.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built Spark and the documentation successfully on my computer and checked the PySpark documentation../build/sbt -Phive clean packagePRODUCTION=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->![supports_spark_connect_version_change_message](https://user-images.githubusercontent.com/112507318/224786569-d600ba83-483e-4a41-83a2-b3bf99d38af1.png)
[SPARK-42020][CONNECT][PYTHON] Support UserDefinedType in Spark Connect
[SPARK-42754][SQL][UI] Fix backward compatibility issue in nested SQL execution
[SPARK-42777][SQL] Support converting TimestampNTZ catalog stats to plan stats
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->When `spark.sql.cbo.planStats.enabled` or `spark.sql.cbo.enabled` is enabled, the logical plan will fetch row counts and column statistics from catalog.This PR is to support converting TimestampNTZ catalog stats to plan stats.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Implement a missing piece of the TimestampNTZ type.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, TimestampNTZ is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
[SPARK-42340][CONNECT][PYTHON] Implement Grouped Map API
[SPARK-42101][SQL][FOLLOWUP] Improve TableCacheQueryStage with CoalesceShufflePartitions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`CoalesceShufflePartitions` should make sure all leaves are `ExchangeQueryStageExec` to avoid collect `TableCacheQueryStage`. As we can not change the partition number of IMR.Add two tests to make sure `CoalesceShufflePartitions` works well with `TableCacheQueryStage`. Note that, these two tests work without this pr, thanks to `ValidateRequirements` the wrong plan has been reverted.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Avoid potential issue.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-42778][SQL] QueryStageExec should respect supportsRowBased
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make `QueryStageExec` respect plan.supportsRowBased### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It is a long time issue that if the plan support both columnar and row, then it would add a unnecessary `ColumnarToRow`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-42780][BUILD] Upgrade `Tink` to 1.8.0
[SPARK-42782][SQL][TESTS] Hive compatibility check for get_json_object
[SPARK-42783][SQL] Infer window group limit should run as late as possible
[SPARK-42781][DOCS][PYTHON] provide one format for writing  to kafka
[SPARK-42784] should still create subDir when the number of subDir in merge dir is less than conf
[SPARK-42786][Connect] Typed Select
[SPARK-42785][K8S][Core] When spark submit without `--deploy-mode`, avoid facing NPE in Kubernetes Case
[Do not merge] Add JDBC to DataFrameWriter
[SPARK-42731][CONNECT][DOCS] Document Spark Connect configurations
[SPARK-42778][SQL][3.4] QueryStageExec should respect supportsRowBased
this pr is for branch-3.4 https://github.com/apache/spark/pull/40407<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make `QueryStageExec` respect plan.supportsRowBased### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It is a long time issue that if the plan support both columnar and row, then it would add a unnecessary `ColumnarToRow`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-42790][SQL] Abstract the excluded method for better test for JDBC docker tests.
[SPARK-42789][SQL] Rewrite multiple GetJsonObjects to a JsonTuple if their json expressions are the same
[SPARK-42617][PS] Support `isocalendar` from the pandas 2.0.0
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support `isocalendar` from the pandas 2.0.0### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Added new method `DatetimeIndex.isocalendar` and removed two depreceted `DatetimeIndex.week` and `DatetimeIndex.weekofyear````dfs = ps.from_pandas(pd.date_range(start='2019-12-29', freq='D', periods=4).to_series())dfs.dt.isocalendar()                    year  week  day        2019-12-29  2019    52    7        2019-12-30  2020     1    1        2019-12-31  2020     1    2        2020-01-01  2020     1    3dfs.dt.isocalendar().week        2019-12-29    52        2019-12-30     1        2019-12-31     1        2020-01-01     1```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT was updated
[SPARK-42779][SQL] Allow V2 writes to indicate advisory shuffle partition size
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds an API for data sources to indicate the advisory partition size for V2 writes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Data sources have an API to request a particular distribution and ordering of data for V2 writes. If AQE is enabled, the default session advisory partition size (64MB) will be used as target. Unfortunately, this default value is still suboptimal and can lead to small files because the written data can be compressed nicely using columnar file formats. Spark should allow data sources to indicate the advisory shuffle partition size, just like it lets data sources request a particular number of partitions. This feature would allow data sources to estimate the compression ratio and incorporate that in the requested advisory partition size.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. However, the changes are backward compatible.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR extends the existing tests for V2 write distribution and ordering.
[SPARK-42803][CORE][SQL][ML] Use getParameterCount function instead of getParameterTypes.length
[SPARK-41775][PYTHON][FOLLOW-UP] Torch distributor multiple gpus per task
[SPARK-42793][CONNECT] `connect` module requires `build_profile_flags`
[SPARK-42794][SS] Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming
[SPARK-42796][SQL] Support accessing TimestampNTZ columns in CachedBatch
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support accessing TimestampNTZ columns in CachedBatch### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Implement a missing feature for TimestampNTZ type### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, TimestampNTZ type is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
[SPARK-42792][SS] Add support for WRITE_FLUSH_BYTES for RocksDB used in streaming stateful operators
[SPARK-42797][CONNECT][DOCS] Grammatical improvements for Spark Connect content
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Grammatical improvements to the Spark Connect content as a follow-up on https://github.com/apache/spark/pull/40324/<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To improve readability of the pages.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, user-facing documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built the doc website locally and checked the updates.PRODUCTION=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42775][SQL] Throw exception when ApproximatePercentile result doesn't fit into output decimal type.
[SPARK-42798][BUILD] Upgrade protobuf-java to 3.22.3
[SPARK-42799][BUILD] Update SBT build `xercesImpl` version to match with `pom.xml`
[SPARK-42800][CONNECT][PYTHON][ML] Implement ml function `{array_to_vector, vector_to_array}`
[SPARK-42706][SQL][DOCS][3.4] Document the Spark SQL error classes in user-facing documentation
### What changes were proposed in this pull request?Cherry-pick for https://github.com/apache/spark/pull/40336.This PR proposes to document Spark SQL error classes to [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html).- Error Conditions <img width=\"1077\" alt=\"Screen Shot 2023-03-08 at 8 54 43 PM\" src=\"https://user-images.githubusercontent.com/44108233/223706823-7817b57d-c032-4817-a440-7f79119fa0b4.png\">- SQLSTATE Codes <img width=\"1139\" alt=\"Screen Shot 2023-03-08 at 8 54 54 PM\" src=\"https://user-images.githubusercontent.com/44108233/223706860-3f64b00b-fa0d-47e0-b154-0d7be92b8637.png\">- Error Classes that includes sub-error classes (`INVALID_FORMAT` as an example) <img width=\"1045\" alt=\"Screen Shot 2023-03-08 at 9 10 22 PM\" src=\"https://user-images.githubusercontent.com/44108233/223709925-74144f41-8836-45dc-b851-5d96ac8aa38c.png\">### Why are the changes needed?To improve the usability for error messages for Spark SQL.### Does this PR introduce _any_ user-facing change?No API change, but yes, it's user-facing documentation.### How was this patch tested?Manually built docs and check the contents one-by-one compare to [error-classes.json](https://github.com/apache/spark/blob/master/core/src/main/resources/error/error-classes.json).Closes #40336 from itholic/SPARK-42706.Authored-by: itholic <haejoon.lee@databricks.com><!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42801][CONNECT][TESTS] Ignore flaky `write jdbc` test of `ClientE2ETestSuite` on Java 8
[SPARK-42496][CONNECT][DOCS][FOLLOW-UP] Addressing feedback to remove last \">>>\" and adding type(spark) example
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Removing the last \">>>\" in a Python code example based on feedback and adding type(spark) as an example of checking whether a session is Spark Connect.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To help readers determine whether a session is Spark Connect + removing unnecessary extra line for cleaner reading.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, updating user-facing documentation<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built the doc website locally and checked the pages.PRODUCTION=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42619][PS] Add `show_counts` parameter for DataFrame.info
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Added `show_counts` parameter for DataFrame.info ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Changed the name of the parameter `null_counts` to `show_counts` of the method DataFrame.info### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-41259][SQL] SparkSQLDriver use the spark result string that is consistent with that of `df.show`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to 3 points:1) Beautify the output of cast `StructType` to `StringType`, Such as {1, 2} -> {a:1, b:2};2) `SparkSQLDriver` use the spark result string that is consistent with that of `df.show`;3) The spark-sql shell result output of `SHOW TABLES/VIEWS` is not consistent with their schema, Let's say have a case like the following with `spark.sql.cli.print.header true`:    ```shell    spark-sql> create table tbl1 (id string) using parquet;    Response code    Time taken: 1.076 seconds    ```        Before this PR:    ```shell    spark-sql> show tables;    namespace\ttableName\tisTemporary    tbl1    ```    After this PR:    ```shell    spark-sql> show tables;    tableName    tbl1    ```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve and Bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Exist tests and new test.
[SPARK-42806][SPARK-42811][CONNECT] Add `Catalog` support
[SPARK-42807][CORE] Apply custom log URL pattern for yarn-client AM log URL in SHS
[SPARK-42808][CORE] Avoid getting availableProcessors every time in `MapOutputTrackerMaster#getStatistics`
Spark 42809
[SPARK-42809][BUILD] Upgrade scala-maven-plugin from 4.8.0 to 4.8.1
[SPARK-42812][CONNECT] Add client_type to AddArtifactsRequest protobuf message
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The missing `client_type` is added to the `AddArtifactsRequest` protobuf message.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Consistency with the other RPCs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, new field in proto message.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42813][K8S] Print application info when waitAppCompletion is false
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->On K8s cluster mode, 1. when `spark.kubernetes.submission.waitAppCompletion=false`, print the application information on `spark-submit` exit, as it did before [SPARK-35174](https://issues.apache.org/jira/browse/SPARK-35174)2. add `appId` in the output message### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->On K8s cluster mode, when `spark.kubernetes.submission.waitAppCompletion=false`,before [SPARK-35174](https://issues.apache.org/jira/browse/SPARK-35174), the `spark-submit` will exit quickly w/ the basic application information.```logInfo(s\"Deployed Spark application ${conf.appName} with submission ID $sId into Kubernetes\")```After [SPARK-35174](https://issues.apache.org/jira/browse/SPARK-35174), those part of code is unreachable, so nothing is output.This PR also proposes to add `appId` in the output message, to make it consistent w/ the context (if you look at the `LoggingPodStatusWatcherImpl`, this is kind of an exception, `... application $appId ...` is used in other places), and YARN.https://github.com/apache/spark/blob/8860f69455e5a722626194c4797b4b42cccd4510/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1311-L1318### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, changes contain1) when `spark.kubernetes.submission.waitAppCompletion=false`, the user can see the app information when `spark-submit` exit.2) the end of `spark-submit` information contains app id now, which is consistent w/ the context and other resource managers like YARN.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI.
[SPARK-42814][BUILD] Upgrade maven plugins to latest versions
[SPARK-42815][SQL] Subexpression elimination support shortcut expression
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new config to shortcut subexpression elimination for expression `and`, `or`.The subexpression may not need to eval even if it appears more than once.e.g., `if(or(a, and(b, b)))`, the expression `b` would be skipped if `a` is true.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->avoid eval unnecessary expression.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-42816][CONNECT] Support Max Message size up to 128MB
[SPARK-42817][CORE] Logging the shuffle service name once in ApplicationMaster
[SPARK-42791][SQL] Create a new golden file test framework for analysis
[SPARK-42818][CONNECT][PYTHON] Implement DataFrameReader/Writer.jdbc
[SPARK-42818][CONNECT][PYTHON][FOLLOWUP] Add versionchanged
[MINOR] Add comments of `xercesImpl` upgrade precautions in `pom.xml`
[SPARK-42820][BUILD] Update ORC to 1.8.3
[SPARK-42821][SQL] Remove unused parameters in splitFiles methods
[SPARK-42819][SS] Add support for setting max_write_buffer_number and  write_buffer_size for RocksDB used in streaming
[SPARK-42720][PS][SQL] Uses expression for distributed-sequence default index instead of plan
[SPARK-42823][SQL] `spark-sql` shell supports multipart namespaces for initialization
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Currently, we only support initializing spark-sql shell with a single-part schema, which also must be forced to the session catalog.#### case 1, specifying catalog field for v1sessioncatalog```sqlbin/spark-sql --database spark_catalog.defaultException in thread \"main\" org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'spark_catalog.default' not found```#### case 2, setting the default catalog to another one```sqlbin/spark-sql -c spark.sql.defaultCatalog=testcat -c spark.sql.catalog.testcat=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog -c spark.sql.catalog.testcat.url='jdbc:derby:memory:testcat;create=true' -c spark.sql.catalog.testcat.driver=org.apache.derby.jdbc.AutoloadedDriver -c spark.sql.catalogImplementation=in-memory  --database SYS23/03/16 18:40:49 WARN ObjectStore: Failed to get database sys, returning NoSuchObjectExceptionException in thread \"main\" org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'sys' not found```In this PR, we switch to use-statement to support multipart namespaces, which helps us resovleto catalog correctly.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make spark-sql shell better support the v2 catalog framework.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, `--database` option supports multipart namespaces and works for v2 catalogs now. And you will see this behavior on spark web ui.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new ut
[SPARK-42824][CONNECT][PYTHON] Provide a clear error message for unsupported JVM attributes
[SPARK-42826][PS][DOCS] Add migration notes for update to supported pandas version.
[SPARK-42828][PYTHON][SQL] More explicit Python type annotations for GroupedData
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Be more explicit in the `Callable` type annotation for `dfapi` and `df_varargs_api` to explicitly return a `DataFrame`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In PySpark 3.3.x, type hints now infer the return value of something like `df.groupBy(...).count()` to be `Any`, whereas it should be `DataFrame`. This breaks type checking.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No runtime changes introduced, so just relied on CI tests.
[SPARK-42831][SQL] Show result expressions in AggregateExec
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?If the result expressions in AggregateExec are not empty, we should display them. Or we will get confused because some important expressions do not show up in the DAG.### Why are the changes needed?For example, the plan for query `SELECT sum(p) from values(cast(23.4 as decimal(7,2))) t(p)` was incorrect because the result expression `MakeDecimal(sum(UnscaledValue(p#0))#1L,17,2) AS sum(p)#2` is not displayedBefore```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- HashAggregate(keys=[], functions=[sum(UnscaledValue(p#0))], output=[sum(p)#2])   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]      +- HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(p#0))], output=[sum#5L])         +- LocalTableScan [p#0]```After```== Physical Plan ==     AdaptiveSparkPlan isFinalPlan=false+- HashAggregate(keys=[], functions=[sum(UnscaledValue(p#0))], results=[MakeDecimal(sum(UnscaledValue(p#0))#1L,17,2) AS sum(p)#2], output=[sum(p)#2])   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=38]      +- HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(p#0))], results=[sum#13L], output=[sum#13L])         +- LocalTableScan [p#0]```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Local test
[SPARK-42832][SQL] Remove repartition if it is the child of LocalLimit
[SPARK-42557][CONNECT][FOLLOWUP] Remove `broadcast` `ProblemFilters.exclude` rule from mima check
[MINOR] scheduler micro opts
[SPARK-42833][SQL] Refactor `applyExtensions` in `SparkSession`
[SPARK-42835][SQL][TESTS] Add test cases for `Column.explain`
[SPARK-42584][CONNECT] Improve output of `Column.explain`
[SPARK-42838][SQL] changed error class name _LEGACY_ERROR_TEMP_2000
[SPARK-42848][CONNECT][PYTHON] Implement DataFrame.registerTempTable
[SPARK-41818][SPARK-41843][CONNECT][PYTHON][TESTS] Enable more parity tests
[SPARK-42850][SQL] Remove duplicated rule CombineFilters in Optimizer
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The optimizer rule `CombineFilters` is included in `PushDownPredicates`. However, both `PushDownPredicates` and `CombineFilters` shows up in the `defaultBatches` of Optimizer.This PR is to remove the duplicated rule CombineFilters in the Optimizer.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Remove a duplicated rule in the Optimizer.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-42247][CONNECT][PYTHON] Fix UserDefinedFunction to have returnType
[SPARK-42851][SQL] Guard EquivalentExpressions.addExpr() with supportedExpression()
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In `EquivalentExpressions.addExpr()`, add a guard `supportedExpression()` to make it consistent with `addExprTree()` and `getExprState()`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This fixes a regression caused by https://github.com/apache/spark/pull/39010 which added the `supportedExpression()` to `addExprTree()` and `getExprState()` but not `addExpr()`.One example of a use case affected by the inconsistency is the `PhysicalAggregation` pattern in physical planning. There, it calls `addExpr()` to deduplicate the aggregate expressions, and then calls `getExprState()` to deduplicate the result expressions. Guarding inconsistently will cause the aggregate and result expressions go out of sync, eventually resulting in query execution error (or whole-stage codegen error).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This fixes a regression affecting Spark 3.3.2+, where it may manifest as an error running aggregate operators with higher-order functions.Example running the SQL command:```sqlselect max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)```example error message before the fix:```java.lang.IllegalStateException: Couldn't find max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))#4 in [max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))#3]```after the fix this error is gone.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added new test cases to `SubexpressionEliminationSuite` for the immediate issue, and to `DataFrameAggregateSuite` for an example of user-visible symptom.
[SPARK-42849] [WIP] [SQL] Session Variables
[SPARK-42852][SQL] Revert NamedLambdaVariable related changes from EquivalentExpressions
[MINOR][BUILD] Remove unused properties in pom file
[SPARK-42805]`DeduplicateRelations` rule show process `LOGICAL_RDD`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`DeduplicateRelations` rule show process `LOGICAL_RDD`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Spark throw `AnalysisException` when checkout dataset join with origin dataset. Beacause checkout dataset will be skip in `DeduplicateRelations` rule.```    val df = spark.range(10).toDF(\"id\")    val cdf = df.checkpoint()``````Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Failure when resolving conflicting references in Join:'Join Inner:- LogicalRDD [id#5L], false+- Project [id#3L AS id#5L]   +- Range (0, 10, step=1, splits=Some(8))Conflicting attributes: id#5L;'Join Inner:- LogicalRDD [id#5L], false+- Project [id#3L AS id#5L]   +- Range (0, 10, step=1, splits=Some(8))    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:56)    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:55)```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add unit test
[SPARK-42779][SQL][FOLLOWUP] Allow V2 writes to indicate advisory shuffle partition size
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR addresses non-blocking comments for PR #40421.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to make sure the new logic only applies in expected cases.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[CONNECT][ML][WIP] Spark connect ML for scala client
[SPARK-42508][BUILD][FOLLOW-UP] Exlcude org.apache.spark.ml.param.FloatParam$ for Scala 2.13
[SPARK-42827][CONNECT] Support `functions#array_prepend` for Scala connect client
Revert \"[SPARK-42809][BUILD] Upgrade scala-maven-plugin from 4.8.0 to 4.8.1\"
[MINOR][TEST] Fix spelling of 'regex' for RegexFilter
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix RegexFilter's attribute 'regex' spelling issue### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->test-only### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-42868][SQL] Support eliminate sorts in AQE Optimizer
[SPARK-42870][CONNECT] Move `toCatalystValue` to `connect-common`
[SPARK-42340][CONNECT][PYTHON][3.4] Implement Grouped Map API
[SPARK-42891][CONNECT][PYTHON] Implement CoGrouped Map API
[SPARK-42851][SQL] Replace EquivalentExpressions with mutable map in PhysicalAggregation
[SPARK-42871][BUILD] Upgrade slf4j to 2.0.7
[SPARK-42536][BUILD] Upgrade log4j2 to 2.20.0
[SPARK-41006][K8S] Generate new ConfigMap names for each run
[SPARK-42791][SQL][FOLLOWUP] Re-generate golden files for `array_prepend`
modified for SPARK-42839: Assign a name to the error class _LEGACY_ER…
…ROR_TEMP_2003https://issues.apache.org/jira/browse/SPARK-42839modified:1、error-classes.json: \"_LEGACY_ERROR_TEMP_2003\" --> \"CANNOT_ZIP_MAPS\"2、create a test case：named ”LegacyErrorTempSuit.scala“ under package /spark/sql/core/src/test/scala/org/apache/spark/sql/3、use sbt> sql/testOnly LegacyErrorTempSuit, All tests passed<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[MINOR][DOCS] Fix typos 
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix typos in the repo.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve readability.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No tests are needed.
test reading footer within file range
[SPARK-42874][SQL] Enable new golden file test framework for analysis for all input files
[SPARK-42875][CONNECT][PYTHON] Fix toPandas to handle timezone and map types properly
[SPARK-42878][CONNECT] The table API in DataFrameReader could also accept options
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->It turns out that `spark.read.option.table` is a valid call chain and the `table` API does accept options when open a table.Existing Spark Connect implementation does not consider it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Feature parity.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42876][SQL] DataType's physicalDataType should be private[sql]
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`physicalDataType` should not be a public API but be private[sql].### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is to limit API scope to not expose unnecessary API to be public.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No since we have not released Spark 3.4.0 yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42864][ML][3.4] Make `IsotonicRegression.PointsAccumulator` private
[SPARK-42864][ML] Make IsotonicRegression.PointsAccumulator private
[SPARK-42829] [UI] add repeat identifier to cached RDD on stage page
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Adds Repeat Identifier: to the cached RDD node on the Stages page. Made the Repeat Identifier: have bolded text so that it's easier to distinguish from the rest of the text.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Currently there is no way to distinguish which cached RDD is being executed in a particular stage. This aims to fix that.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, on the Stages page in the UI when there is a cached RDD. One example is <img width=\"860\" alt=\"Screen Shot 2023-03-20 at 3 55 40 PM\" src=\"https://user-images.githubusercontent.com/16739760/226527044-4c0719f4-aaf8-4e99-8bce-fd033106379c.png\"><!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Manual test locally in SQL UI.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42830] [UI] Link skipped stages on Spark UI
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Adds text on the UI that shows which executed stage a skipped stage on the UI refers to.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Helps find the execution details, in terms of  figuring out which stages from earlier jobs feed into a later stage in a specific job.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the jobs page can look like the following, where the skipped stage is now clickable and redirects to the stage that was actually executed <img width=\"357\" alt=\"Screen Shot 2023-03-20 at 5 36 14 PM\" src=\"https://user-images.githubusercontent.com/16739760/226529417-a2384904-7e46-48f6-bcd5-c708416e6353.png\"><!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Manual test locally on UI.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42880][DOCS] Update running-on-yarn.md to log4j2 syntax
[MINOR][DOCS] Remove SparkSession constructor invocation in the example
[SPARK-42881][SQL] Codegen Support for get_json_object
[SPARK-42662][CONNECT][PS] Add proto message for pandas API on Spark default index
[SPARK-42924][SQL][CONNECT][PYTHON] Clarify the comment of parameterized SQL args
[SPARK-42885][K8S][BUILD] Upgrade `kubernetes-client` to 6.5.1
[SPARK-42889][CONNECT][PYTHON] Implement cache, persist, unpersist, and storageLevel
[SPARK-42888][BUILD] Upgrade `gcs-connector` to 2.2.11
[SPARK-42892][SQL] Move sameType and relevant methods out of DataType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR moves the following methods from `DataType`:1. equalsIgnoreNullability2. sameType3. equalsIgnoreCaseAndNullabilityThe moved methods are put together into a Util class.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->To make `DataType` become a simpler interface, non-public methods can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No as the moved methods are private within Spark.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT.
[SPARK-42893][PYTHON][3.4] Block Arrow-optimized Python UDFs
[SPARK-41233][CONNECT][PYTHON] Add array_prepend to Spark Connect Python client
[SPARK-42884][CONNECT] Add Ammonite REPL integration
[SPARK-42894][CONNECT] Support `cache`/`persist`/`unpersist`/`storageLevel` for Spark connect jvm client
Revert \"[SPARK-42508][CONNECT][ML] Extract the common .ml classes to `mllib-common`\"
[SPARK-42901][CONNECT][PYTHON] Move `StorageLevel` into a separate file to avoid potential `file recursively imports`
[SPARK-42864][ML] Make `IsotonicRegression.PointsAccumulator` private
[SPARK-42896][SQL][PYTHON] Make `mapInPandas` / `mapInArrow` support barrier mode execution
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make mapInPandas / mapInArrow support barrier mode execution### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is the preparation PR for supporting mapInPandas / mapInArrow barrier execution in spark connect mode. The feature is required by machine learning use cases.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[MINOR][DOCS][PYTHON] Update some urls about deprecated repository pyspark.pandas
[SPARK-42101][SQL][FOLLOWUP] Make QueryStageExec.resultOption and isMeterialized consistent
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/39624 . `QueryStageExec.isMeterialized` should only return true if `resultOption` is assigned. It can be a potential bug to have this inconsistency.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix potential bug### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-42897][SQL] Avoid evaluate variables multiple times for SMJ and SHJ fullOuter join
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?For example:```  val df1 = spark.range(5).select($\"id\".as(\"k1\"))  val df2 = spark.range(10).select($\"id\".as(\"k2\"))  df1.join(df2.hint(\"SHUFFLE_MERGE\"),      $\"k1\" === $\"k2\" % 3 && $\"k1\" + 3 =!= $\"k2\" && $\"k1\" + 5 =!= $\"k2\ \"full_outer\")```the join condition `$\"k1\" + 3 =!= $\"k2\"` and `$\"k1\" + 5 =!= $\"k2\"` will evaluate the variable **k1** twice and caused the codegen failed.```09:43:30.155 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 257, Column 9: Redefinition of local variable \"smj_isNull_9\" org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 257, Column 9: Redefinition of local variable \"smj_isNull_9\" ```Before this PR, we will evaluate multiple times for the variables in the join condition, and throw `Redefinition of local variable`  exception.### Why are the changes needed?Bug fix for codegen issue in FullOuter SMJ.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added UT
[SPARK-42898][SQL] Mark that string/date casts do not need time zone id
[SPARK-42859][CONNECT][PS] Basic support for pandas API on Spark Connect
[SPARK-42899][SQL] Fix DataFrame.to(schema) to handle the case where there is a non-nullable nested field in a nullable field
[SPARK-42900][CONNECT][PYTHON] Fix createDataFrame to respect inference and column names
[SPARK-42584][CONNECT] Improve output of Column.explain
[SPARK-42890] [UI] add repeat identifier on SQL UI
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?On the SQL page in the Web UI, this PR aims to add a repeat identifier to distinguish which InMemoryTableScan is being used at a certain location on the DAG. Ideally this would work in sync with changes from SPARK-42829.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Currently there is no distinction for which InMemoryTableScan is being used at a specific point in the DAG.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, on the SQL page in the UI when there is a InMemoryTableScan. One example is<img width=\"1012\" alt=\"Screen Shot 2023-03-22 at 2 06 14 PM\" src=\"https://user-images.githubusercontent.com/16739760/227093764-618cee51-bd0a-4c27-9945-2cc122819c74.png\"><!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Manual test locally in SQL UI along with a few Unit Tests<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42903][PYTHON][DOCS] Avoid documenting None as as a return value in docstring
[SPARK-42904][SQL] Char/Varchar Support for JDBC Catalog
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add type mapping for spark char/varchar to jdbc types.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The STANDARD JDBC 1.0 and other modern databases define char/varchar normatively.This is currently a kind of bug for DDLs on JDBCCatalogs for encountering errors like```Cause: org.apache.spark.SparkIllegalArgumentException: Can't get JDBC type for varchar(10).[info]   at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGetJdbcTypeError(QueryExecutionErrors.scala:1005)```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, char/varchar are allow for jdbc catalogs### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new ut
[SPARK-42906][K8S] Replace a starting digit with `x` in resource name prefix
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change the generated resource name prefix to meet K8s requirements> DNS-1035 label must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name',  or 'abc-123', regex used for validation is '[a-z]([-a-z0-9]*[a-z0-9])?')### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In current implementation, the following app name causes error```bin/spark-submit \\\t --master k8s://https://*.*.*.*:6443 \\\t --deploy-mode cluster \\\t --name 你好_187609 \\         ...``````Exception in thread \"main\" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing:   POST at: https://*.*.*.*:6443/api/v1/namespaces/spark/services. Message:  Service \"187609-f19020870d12c349-driver-svc\" is invalid: metadata.name: Invalid value: \"187609-f19020870d12c349-driver-svc\": a DNS-1035 label must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name',  or 'abc-123', regex used for validation is '[a-z]([-a-z0-9]*[a-z0-9])?'). ```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT.
[SPARK-42908][PYTHON] Raise RuntimeError when SparkContext is required but not initialized
[SPARK-42907][CONNECT][PYTHON] Implement Avro functions
[SPARK-42895][CONNECT] Improve error messages for stopped Spark sessions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR improves error messages when users attempt to invoke session operations on a stopped Spark session.### Why are the changes needed?To make the error messages more user-friendly.For example:```pythonspark.stop()spark.sql(\"select 1\")```Before this PR, this code will throw two exceptions:```ValueError: Cannot invoke RPC: Channel closed!During handling of the above exception, another exception occurred:Traceback (most recent call last):  ...    return e.code() == grpc.StatusCode.UNAVAILABLEAttributeError: 'ValueError' object has no attribute 'code'```After this PR, it will show this exception:```[NO_ACTIVE_SESSION] No active Spark session found. Please create a new Spark session before running the code.```### Does this PR introduce _any_ user-facing change?Yes. This PR modifies the error messages.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit test.
[SPARK-42202][CONNECT][TEST][FOLLOWUP] Loop around command entry in SimpleSparkConnectService
[SPARK-42911][PYTHON] Introduce more basic exceptions
[SPARK-42891][CONNECT][PYTHON][3.4] Implement CoGrouped Map API
[SPARK-42914][PYTHON] Reuse `transformUnregisteredFunction` for `DistributedSequenceID`.
[SPARK-42861][SQL] Use private[sql] instead of protected[sql] to avoid generating API doc
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is the only issue I found during SQL module API auditing via https://github.com/apache/spark-website/pull/443/commits/615986022c573aedaff8d2b917a0d2d9dc2b67ef . Somehow `protected[sql]` also generates API doc which is unexpected. `private[sql]` solves the problem and I generated doc locally to verify it.Another API issue has been fixed by https://github.com/apache/spark/pull/40499### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix api doc### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42915][SQL] Codegen Support for sentences
[SPARK-42916][SQL] JDBCTableCatalog Keeps Char/Varchar meta on the read-side
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this PR, we make the JDBCTableCatalog mapping the Char/Varchar to the raw implementation to avoid losing meta information.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->For some DDLs related to column updating, the raw types are needed.Otherwise, you may get string->varchar/char casting errors according to the underlying database. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, you can create a table with a varchar column and increase its width. But w/o this PR, you got error### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new unit tests
[SPARK-42917][SQL] Correct getUpdateColumnNullabilityQuery for DerbyDialect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix nullability clause for derby dialect, according to the official derby lang ref guide.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To fix bugs like:```spark-sql ()> create table src2(ID INTEGER NOT NULL, deptno INTEGER NOT NULL);spark-sql ()> alter table src2 ALTER COLUMN ID drop not null;java.sql.SQLSyntaxErrorException: Syntax error: Encountered \"NULL\" at line 1, column 42.```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, but a necessary bugfix### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Test manually.
[SPARK-42918] Generalize handling of metadata attributes in FileSourceStrategy
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change improves the handling of constant and generated metadata fields in `FileSourceStrategy`: instead of relying on hard-wired logic to categorize constant and generated metadata fields and process them, this change embeds the required information when creating them in `FileFormat` and uses this information in `FileSourceStrategy` to process arbitrary metadata fields.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This change is a first step towards allowing file format implementations to inject their own metadata fields into plans.  The second step will be to change `FileScanRdd`/`FileFormat` to be able to populate values of arbitrary constant and generated metadata columns.Once this is done, each file format will be able to declare and populate its own metadata field, e.g. `ParquetFileFormat` can provide the Parquet row index metadata field `ROW_INDEX` without polluting the `FileFormat`.### Does this PR introduce _any_ user-facing change?No, this is strictly a refactor without any functional change.### How was this patch tested?This change is covered by existing tests, e.p. FileMetadataStructSuite.
[SPARK-42899][SQL][FOLLOWUP] Project.reconcileColumnType should use KnownNotNull instead of AssertNotNull
[SPARK-42911][PYTHON][3.4] Introduce more basic exceptions
[Minor][Core] Remove unused variables and method in Spark listeners
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove unused variables and method in Spark listeners### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA tests
[SPARK-42920][CONNECT][PYTHON] Enable tests for UDF with UDT
[SPARK] LogicalPlan.metadataOutput always contains AttributeReference
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Today, `LogicalPlan.metadataOutput` is a `Seq[Attribute]`. However, it always contains `AttributeReference`, because metadata columns are \"pre-resolved\" by nodes implementing `ExposesMetadataColumns`. We can simplify a bunch of code by actually defining `metadataOutput` as a `Seq[AttributeReference]`.* `ExposesMetadataColumns` becomes simpler for a node to implement, because attribute identification and dedup can be factored out in a helper method, and implementing nodes only need a way to copy themselves with updated output. * `AddMetadataColumns` rule can be cleaned up as well with the bonus of eliminating unnecessary metadata column projections it used to impose.### Why are the changes needed?Code cleanup. Easier to reason about, easier to maintain.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing metadata column unit tests.
[SPARK] Project implements ExposesMetadataColumns
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?NOTE: This is a stacked pull request. Ignore the bottom two commits.The work that `AddMetadataColumns` analyzer pass does for `Project` nodes can be expressed directly by `Project` itself by implementing `ExposesMetadataColumns`. ### Why are the changes needed?Simpler analysis rule, logic is more localized.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing metadata column tests.
[SPARK-42921][SQL][TESTS] Split `timestampNTZ/datetime-special.sql` into w/ and w/o `ansi` suffix to pass sql analyzer test in ansi mode
[SPARK-39722] [SQL] getString API for Dataset
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add getString API for Dataset for usecases where string representation needs to be used other than stdout(println)More details in the bug### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->But there are a lot of cases where we might need to get a String representation of the show output. For example logging framework to which we need to push the representation of a df send the string over a REST call from the driversend the string to stderr instead of stdout### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Tested locally using spark-shell
[SPARK-42914][PYTHON][3.4] Reuse `transformUnregisteredFunction` for `DistributedSequenceID`.
[SPARK-42926][BUILD][SQL] Upgrade Parquet to 1.13.0
[SPARK-42927][CORE] Change the access scope of `o.a.spark.util.Iterators#size` to `private[util]`
[SPARK-42928][SQL] Make resolvePersistentFunction synchronized
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes the function `resolvePersistentFunctionInternal` synchronized.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make function resolution thread-safe.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UTs.
[SPARK-42936][SQL] Fix LCA bug when the having clause can be resolved directly by its child Aggregate
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The PR fixes the following bug in LCA + having resolution:```sqlselect sum(value1) as total_1, total_1from values(1, 'name', 100, 50) AS data(id, name, value1, value2)having total_1 > 0SparkException: [INTERNAL_ERROR] Found the unresolved operator: 'UnresolvedHaving (total_1#353L > cast(0 as bigint))```To trigger the issue, the having condition need to be (can be resolved by) an attribute in the select.Without the LCA `total_1`, the query works fine.#### Root cause of the issue`UnresolvedHaving` with `Aggregate` as child can use both the `Aggregate`'s output and the `Aggregate`'s child's output to resolve the having condition. If using the latter, `ResolveReferences` rule will replace the unresolved attribute with a `TempResolvedColumn`.For a `UnresolvedHaving` that actually can be resolved directly by its child `Aggregate`, there will be no `TempResolvedColumn` after the rule `ResolveReferences` applies. This  `UnresolvedHaving` still needs to be transformed to `Filter` by rule `ResolveAggregateFunctions`. This rule recognizes the shape: `UnresolvedHaving - Aggregate`.However, the current condition (the plan should not contain `TempResolvedColumn`) that prevents LCA rule to apply between `ResolveReferences` and `ResolveAggregateFunctions` does not cover the above case. It can insert `Project` in the middle and break the shape can be matched by `ResolveAggregateFunctions`.#### FixThe PR adds another condition for LCA rule to apply: the plan should not contain any `UnresolvedHaving`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->See above reasoning to fix the bug.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing and added tests.
[SPARK-42929][CONNECT] make mapInPandas / mapInArrow support \"is_barrier\"
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->make mapInPandas / mapInArrow support \"is_barrier\"### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->feature parity.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually:`bin/pyspark --remote local`:```from pyspark.sql.functions import pandas_udfdf = spark.createDataFrame([(1, 21), (2, 30)], (\"id\ \"age\"))def filter_func(iterator):    for pdf in iterator:        yield pdf[pdf.id == 1]df.mapInPandas(filter_func, df.schema,  is_barrier=True).collect()def filter_func(iterator):    for batch in iterator:        pdf = batch.to_pandas()        yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])df.mapInArrow(filter_func, df.schema, is_barrier=True).collect()```
[SPARK-42930][CORE][SQL] Change the access scope of `ProtobufSerDe` related implementations to `private[protobuf]`
[SPARK-42931][SS] Introduce dropDuplicatesWithinWatermark
[MINOR][DOCS] Update broken links for pyspark.pandas
[SPARK-41233][FOLLOWUP] Refactor `array_prepend` with `RuntimeReplaceable`
[SPARK-42519][CONNECT][TESTS] Add More WriteTo Tests In Spark Connect Client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add more WriteTo tests for Spark Connect Client<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Improve Test Case, remove same todo<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new tests<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42873][SQL] Define Spark SQL types as keywords
[SPARK-42934][BUILD] Add `spark.hadoop.hadoop.security.key.provider.path` to `scalatest-maven-plugin`
[SPARK-42935] [SQL] Add union required distribution push down
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?We  indroduce a new idea to optimize exchange plan when union spark plan output partitoning can't match parent plan's required distribution.1. First introduce a new RDD, it consists of parent rdds that has the same partition size. The ith parttition corresponds to ith partition of each parent rdd.2. Then push the required distribution to union plan's children. If any child output partitioning matches the required distribution , we can reduce this child shuffle operation.### Why are the changes needed?Union plan does not take full advantage of children plan output partitionings when output partitoning can't match parent plan's required distribution.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?org.apache.spark.sql.execution.PlannerSuiteorg.apache.spark.sql.execution.exchange.UnionZipRDDSuite
[SPARK-42922][SQL] Move from Random to SecureRandom
[SPARK-42937][SQL] `PlanSubqueries` should set `InSubqueryExec#shouldBroadcast` to true
[SPARK-41876][CONNECT][PYTHON] Implement DataFrame.toLocalIterator
[SPARK-42896][SQL][PYTHON][FOLLOW-UP] Rename isBarrier to barrier, and correct docstring
[SPARK-37677][CORE] Unzip could keep file permissions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Just remove comment.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->After https://github.com/apache/hadoop/pull/4036, unzip could keep file permissions.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No need, has been added in hadoop-client side.
[SPARK-42943][SQL] Use LONGTEXT instead of TEXT for StringType for effective length
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Referring to https://dev.mysql.com/doc/refman/8.0/en/string-type-syntax.html, A [TEXT](https://dev.mysql.com/doc/refman/8.0/en/blob.html) column with a maximum length of 65,535 (2^16 − 1) characters.We currently convert our string to MySQL's `text` and jdbc's `CLOB`. The `text` here is insufficient. And `CLOB` is incorrect, which LONGVARCHAR should be replaced instead. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->better compatibility with MySQL and bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, you won't see MysqlDataTruncation if you store a string exceeding 65536 into a column defined by spark' string with MySQL catalog.```javaob aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.221.102.180 executor driver): java.sql.BatchUpdateException: Data truncation: Data too long for column 'c1' at row 1\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:816)\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:418)\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:742)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:893)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:892)\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[SPARK-42942][SQL] Support coalesce table cache stage partitions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new rule `CoalesceCachePartitions` to support coalesce partitions with `TableCacheQueryStageExec`. In order to reuse the code path with `CoalesceShufflePartitions`, this pr also does a small refactor about how we coalesce partitions.RDD cache use the RDD id and partition id as the block id, so it seems not possible to split skewd partitions like shuffle. To reduce complexity, this pr does not allow coalesce partitions with both shuffle and cache stage since shuffle read may contain skewed partition spec.For example, the follow case can not be coalesced by both `CoalesceCachePartitions` and `CoalesceShufflePartitions`.```SMJ  ShuffleQueryStage  TableCacheStage```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make AQE support coalesce table cache stage partitions.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, add a new config to control if coalesce partitions for  table cache stage.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add tests
[SPARK-42945][CONNECT] Support PYSPARK_JVM_STACKTRACE_ENABLED in Spark Connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR supports `spark.sql.pyspark.jvmStacktrace.enabled` in Spark Connect to optionally show the JVM stack trace.It also adds a new Spark Connect config ,`spark.connect.jvmStacktrace.maxSize` (default: 4096), to adjust the stack trace size. This is to prevent the HTTP header size from exceeding the maximum allowed size.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To support an existing config that works with legacy PySpark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit test.
[SPARK-42946][SQL] Redact sensitive data which is nested by variable substitution
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Redact sensitive data which is nested by variable substitution#### Case 1 by SET syntax's key part```sqlspark-sql> set ${spark.ssl.keyPassword};abc    <undefined> ```#### Case 2 by SELECT as String literal ```sqlspark-sql> set spark.ssl.keyPassword;spark.ssl.keyPassword    *********(redacted)Time taken: 0.009 seconds, Fetched 1 row(s)spark-sql> select '${spark.ssl.keyPassword}';abc```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->data security### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, sensitive data can not be extracted by variable substitution### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[SPARK-42947][SQL] Spark Thriftserver LDAP should not use DN pattern if user contains domain
[SPARK-42949][SQL] Simplify code for NAAJ
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Reuse code of `streamSideKeyGenerator()` in NAAJ.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Reuse code and improve code readability.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Passing CI.
[SPARK-42929][CONNECT][FOLLOWUP] Rename isBarrier to barrier
[SPARK-42952][SQL] Simplify the parameter of analyzer rule PreprocessTableCreation and DataSourceAnalysis
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Simplify the parameter of the following analyzer rule:* PreprocessTableCreation: use a SessionCatalog instead of passing SparkSession* DataSourceAnalysis: remove the unused `Analyzer` in the parameter and turn it from class to object.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
[SPARK-42953][Connect] Typed filter, map, flatMap, mapPartitions
[SPARK-42954][PYTHON][CONNECT] Add `YearMonthIntervalType` to PySpark and Spark Connect Python Client
[SPARK-42955][SQL] Skip classifyException and wrap AnalysisException for SparkThrowable
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SparkThrowable is not necessary to be classified by JDBC Dialects and should not be wrapped with a redundantAnalysisException. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->improve error handling for JDBC catalogs### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new unit test.
[SPARK-42956][CONNECT] Support avro functions for Scala client
[SPARK-42957][INFRA] `release-build.sh` should not remove SBOM artifacts
[SPARK-42939][SS][CONNECT] Core streaming Python API for Spark Connect 
[SPARK-42957][INFRA][FOLLOWUP] Use 'cyclonedx' instead of file extensions
[SPARK-42964][SQL] PosgresDialect '42P07' also means table already exists
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR redirects '42P07' SQL state to table not found according to the doc - https://www.postgresql.org/docs/14/errcodes-appendix.html.Otherwise, RENAME TABLE will fail with None.get if the new table name exists.```scala23/03/29 19:33:25 ERROR SparkSQLDriver: Failed in [alter table char3 rename to char2]java.util.NoSuchElementException: None.get\tat scala.None$.get(Option.scala:529)\tat scala.None$.get(Option.scala:527)\tat org.apache.spark.sql.jdbc.PostgresDialect$.classifyException(PostgresDialect.scala:220)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.classifyException(JdbcUtils.scala:1176)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix, avoid `None.get` after regex matching### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no, bugfix### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE query stage optimizer
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `injectQueryStageOptimizerRule` public method in `SparkSessionExtensions`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Provide a entrance for developers to the query stage optimizer phase of adaptive query execution. e.g., they can decide the final rdd partition with different plan.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, after this pr people can inject custom rules into query stage optimizer### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-42631][CONNECT][FOLLOW-UP] Expose Column.expr to extensions
[SPARK-42950][CORE] Add exit code in SparkListenerApplicationEnd
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The exit code is already available in the `stop(exitCode: Int)` function of the SparkContext, it only can be propagated to the event `SparkListenerApplicationEnd` that is emitted in the `postApplicationEnd`Using an `Option[Int]` to reflect that the exit code can be missing if reading an event log of a previous version of spark### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Being able to know if a spark application succeeded or failed when using the spark listener API### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, adding a new field `exitCode` to `SparkListenerApplicationEnd`, which is marked as `@DeveloperApi`### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated the unit tests to make sure the value is correctly propagatedLaunched a spark application from this branch, and looking at the last emitted event ```{\"Event\":\"SparkListenerApplicationEnd\\"Timestamp\":1680108841738,\"Exit Code\":0}```
[SPARK-42967][CORE][3.2][3.3][3.4] Fix SparkListenerTaskStart.stageAttemptId when a task is started after the stage is cancelled
[SPARK-42979][SQL] Define literal constructors as keywords
[SPARK-42970][CONNECT][PYTHON][TESTS] Reuse pyspark.sql.tests.test_arrow test cases
[SPARK-42970][CONNECT][PYTHON][TESTS][3.4] Reuse pyspark.sql.tests.test_arrow test cases
[SPARK-42973][CONNECT][BUILD] Upgrade buf to v1.16.0
[SPARK-42971][CORE] Change to print `workdir` if `appDirs` is null when worker handle `WorkDirCleanup` event
[SPARK-42974][CORE] Restore `Utils#createTempDir` use `ShutdownHookManager#registerShutdownDeleteDir` to cleanup tempDir
[SPARK-42907][TESTS][FOLLOWUP] Avro functions doctest cleanup
[SPARK-42968][SS] Add option to skip commit coordinator as part of StreamingWrite API for DSv2 sources/sinks
[SPARK-42975][SQL] Cast result type to timestamp type for string +/- interval
[SPARK-42978][SQL] Derby&PG: RENAME cannot qualify a new-table-Name with a schema-Name
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix `rename a table` in derby and pg, which schema name is not allowed to qualify the new table name ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->introduce a new error class```json  \"CANNOT_RENAME_ACROSS_SCHEMA\" : {    \"message\" : [      \"Renaming a <type> across schemas is not allowed.\"    ],    \"sqlState\" : \"0AKD0\"  },```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new unit test
[MINOR][CONNECT] Adding Proto Debug String to Job Description.
Revert \"[SPARK-41765][SQL] Pull out v1 write metrics to WriteFiles\"
This reverts commit a111a02de1a814c5f335e0bcac4cffb0515557dc.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SQLMetrics is not only used in the UI, but is also a programming API as users can write a listener, get the physical plan, and read the SQLMetrics values directly.We can ask users to update their code and read SQLMetrics from the new `WriteFiles` node instead. But this is troublesome and sometimes they may need to get both write metrics and commit metrics, then they need to look at two physical plan nodes. Given that https://github.com/apache/spark/pull/39428 is mostly for cleanup and does not have many benefits, reverting is a better idea.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->avoid breaking changes.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, they can programmatically get the write command metrics as before. ### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[SPARK-42958][CONNECT] Refactor `connect-jvm-client-mima-check` to support mima check with avro module
Debugging is awesome
[SPARK-42993][ML][CONNECT] Make PyTorch Distributor compatible with Spark Connect
[SPARK-35198][CONNECT][CORE][PYTHON][SQL] Add support for calling debugCodegen from Python & Java
Add support for calling debugCodegen from Python & Java<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Adds `debugCodegen` to the Dataset APIs core and connector- Adds `debugCodegen` to the pyspark dataframe API- Removes the implicit `debugCodegen` from `sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala` as it is public now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To add a direct method to get debugCodegen state for Java & Python users of Dataframes.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->`debugCodegen` is now accessible from all Scala, python and Java APIs. See usage below.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added tests in:- `sql/core/src/test/scala/org/apache/spark/sql/DebugCodegenSuite.scala`.- `connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/ClientE2ETestSuite.scala`Local testing:```scala> spark.sql(\"SELECT 1\").debugCodegenFound 1 WholeStageCodegen subtrees.== Subtree 1 / 1 (maxMethodCodeSize:55; maxConstantPoolSize:99(0.15% used); numInnerClasses:0) ==*(1) Project [1 AS 1#0]+- *(1) Scan OneRowRelation[]...``````>>> spark.sql(\"select 1\").debugCodegen()Found 1 WholeStageCodegen subtrees.== Subtree 1 / 1 (maxMethodCodeSize:55; maxConstantPoolSize:99(0.15% used); numInnerClasses:0) ==*(1) Project [1 AS 1#0]+- *(1) Scan OneRowRelation[]...```
[SPARK-42316][SQL] Assign name to _LEGACY_ERROR_TEMP_2044
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign name to _LEGACY_ERROR_TEMP_2044, \"BINARY_ARITHMETIC_OVERFLOW\".<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Assign proper name to LEGACY_ERROR_TEMP<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested? ./build/sbt \"testOnly org.apache.spark.sql.errors.QueryExecutionErrorsSuite\"<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42626][CONNECT] Add Destructive Iterator for SparkResult
[SPARK-42981][CONNECT] Add direct arrow serialization
[SPARK-42969][CONNECT][TESTS] Fix the comparison the result with Arrow optimization enabled/disabled
[SPARK-42974][CORE]  Restore `Utils.createTempDir` to use the `ShutdownHookManager` and clean up `JavaUtils.createTempDir` method.
[SPARK-42987][DOCS] Correction of protobuf sql documentation
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Correction of  code highlights in SQL protobuf documentation.old version:![image](https://user-images.githubusercontent.com/8001253/228999196-d39d62f5-992b-429b-8418-efa01300cce4.png)new version:![image](https://user-images.githubusercontent.com/8001253/228999257-54d30e09-0bab-44f6-ae6d-7794e1c4329d.png)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To help spark users to understand python, scala and java code examples.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it improve user-facing documentation### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Comparing visual structure in the old and the new version in vscode markdown preview and github markdown preview.
[SPARK-16484][SQL] Add support for Datasketches HllSketch
[SPARK-42991][SQL] Disable string type +/- interval in ANSI mode
[SPARK-42992][PYTHON] Introduce PySparkRuntimeError
fix typo in StorageLevel __eq__()
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->fix `self.deserialized == self.deserialized` with `self.deserialized == other.deserialized`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The original expression is always True, which is likely to be a typo.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No test added. Use GitHub Actions.
[SPARK-43006][PYSPARK] Fix typo in StorageLevel __eq__()
[SPARK-43005][PYSPARK] Fix typo in pyspark/pandas/config.py
By comparing compute.isin_limit and plotting.max_rows, `v is v` is likely to be a typo.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->fix `v is v >= 0` with `v >= 0`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->By comparing compute.isin_limit and plotting.max_rows, `v is v` is likely to be a typo.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->By GitHub Actions.
Fix ExecutorAllocationManager cannot allocate new instances when all …
[SPARK-43004][CORE] Fix typo in ResourceRequest.equals()
vendor == vendor is always true, this is likely to be a typo.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->fix `vendor == vendor` with `that.vendor == vendor`, and `discoveryScript == discoveryScript` with `that.discoveryScript == discoveryScript`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->vendor == vendor is always true, this is likely to be a typo.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->By GitHub Actions.
[SPARK-43009][SQL] Parameterized `sql()` with `Any` constants
[SPARK-42995][CONNECT][PYTHON] Migrate Spark Connect DataFrame errors into error class
[TDE-88] Access control on spark-3.3.2 
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Implement access control on a database via row level security on metadata<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Implemneting this would bring access control on spark-standalone<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?using doAs for thrift-server / --proxy-user for shell would restrict access<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Created 2 users ( with row level restrictions on metadata db )- To test --proxy-user : Started a shell with --proxy-user flag and ran show tables / count rows on table (with access / without access)- To test doAs : Started a thrift server. Connected as test user via beeline and ran show tables / count rows on table (with access / without access)<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42860][SQL] Add analysed logical mode in org.apache.spark.sql.execution.ExplainMode
[SPARK-42998][CONNECT][PYTHON] Fix DataFrame.collect with null struct
[SPARK-42999][Connect] Dataset#foreach, foreachPartition
[SPARK-42980][CORE] Implement a lightweight SmallBroadcast
[SPARK-42997][SQL] TableOutputResolver must use correct column paths in error messages for arrays and maps
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR fixes `TableOutputResolver` to use correct column paths in error messages for arrays and maps.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to have accurate error messages when there is a type mismatch inside arrays and maps.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
[SPARK-42298][SQL] Assign name to _LEGACY_ERROR_TEMP_2132
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign name to _LEGACY_ERROR_TEMP_2132, \"CANNOT_PARSE_JSON_ARRAYS_AS_STRUCTS\".<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Assign proper name to LEGACY_ERROR_TEMP<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?./build/sbt \"testOnly org.apache.spark.sql.errors.QueryExecutionErrorsSuite\"<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43000][SQL] Do not cast to double type in `PromoteStrings`
[SPARK-42840][SQL] Change `_LEGACY_ERROR_TEMP_2004` error to internal error 
[SPARK-42774][SQL]Expose VectorTypes API for DataSourceV2 Batch Scans
[SPARK-43002][YARN] Modify yarn client application report logging frequency to reduce noise
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?* Added a new config property — `spark.yarn.report.loggingFrequency`* Limit the number of times the yarn application report is logged based on the number of reports processed.   <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Currently, an application report is generated every second, this tends to add a lot of noise especially for long running applications. This bloats the log file and makes it hard to navigate. With this change, we can limit the amount of times the application status report is logged based on the number of reports processed.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?The logs are now ~30s apart```31-03-2023 15:00:08 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:00:08 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:00:38 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:00:38 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:01:08 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:01:08 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:01:38 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:01:38 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:02:09 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:02:09 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:02:31 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:02:31 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: FINISHED)```<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tested locally to ensure the behavior was as expected<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43007][BUILD] Upgrade rocksdbjni to 8.0.0
[SPARK-43008][BUILD] Upgrade joda-time from 2.12.2 to 2.12.5
[SPARK-43011][SQL] `array_insert` should fail with 0 index
[SPARK-43010][PYTHON] Migrate Column errors into error class
[SPARK-43013][PYTHON] Migrate `ValueError` from DataFrame into `PySparkValueError`.
[MINOR][DOCS] Add Java 8 types to value types of Scala/Java APIs
[SPARK-43014][CORE][K8S] Support `spark.kubernetes.setSubmitTimeInDriver`
[WIP][SPARK-42696]Speed up parquet reading with Java Vector API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Parquet has supported vector read speed up with this PR [Vectorized BytePacker decoder using Java Vector API](https://github.com/apache/parquet-mr/pull/1011)The performance gain is 4x ~ 8x according to the parquet microbenchmarkTPC-H(SF100) Q6 has 11% performance increase with Apache Spark integrating parquet vector optimization### Why are the changes needed?This PR used to support parquet vector optimization### Does this PR introduce _any_ user-facing change?Add configuration  spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.### How was this patch tested?For the test case, there are some problems to fix:1. It is necessary to Parquet-mr community release new java version to use the parquet vector optimization. 2. Parquet Vector optimization does not release default, so users have to build parquet with **mvn clean install -P vector-plugins** manually to get the parquet-encoding-vector-{VERSION}.jar and put it on the {SPARK_HOME}/jars path3. github doesn't support select runners with specific instruction set. So it is impossible (a self-hosted runner can do it) to verify the optimization on github runners machine.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42974][CORE][3.4]  Restore `Utils.createTempDir` to use the `ShutdownHookManager` and clean up `JavaUtils.createTempDir` method.
[MINOR][CONNECT][TESTS] Merge two `SparkVersion` related  tests to one
[SPARK-41628][CONNECT][SERVER] The Design for support async query execution
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The Design for support async query execution<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Prepare for code async query execution<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?NO<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43006][PYTHON][TESTS] Fix DataFrameTests.test_cache_dataframe
[SPARK-43019][SQL] Move Ordering to PhysicalDataType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we start to move ordering to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-43018][SQL] Fix bug for INSERT commands with timestamp literals
[SPARK-42963][SQL] Extend SparkSessionExtensions to inject rules into AQE query stage optimizer
[SPARK-43022][CONNECT] Support protobuf functions for Scala client 
[SPARK-42855][SQL] Use runtime null checks in TableOutputResolver
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR migrates `TableOutputResolver` to use runtime NOT NULL checks instead of checking type compatibility during the analysis phase.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed per discussion that happened [here](https://github.com/apache/spark/pull/40308#discussion_r1127081206).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Nullability exceptions will be thrown at runtime (instead of analysis) but there is no API change.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
[SPARK-43023][CONNECT][TESTS] Add switch catalog testing scenario for `CatalogSuite`
[SPARK-42844][SQL] Update the error class `_LEGACY_ERROR_TEMP_2008` to `INVALID_URL`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Update the error_class _LEGACY_ERROR_TEMP_2008 to INVALID_URL.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue SPARK-42844. The original name just a number, update it to a informal name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add a test case in UrlFunctionsSuite to catch the error using sql command.
[SPARK-XXXXX][PS] Matching the behavior of pandas API on Spark to pandas 2.0.0
[SPARK-43026][SQL] Apply AQE with non-exchange table cache
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Apply AQE with non-exchange table cache at `InsertAdaptiveSparkPlan`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`TableCacheQueryStageExec` supports to report runtime statistics, so it's possible that AQE  plans a better executed during re-optimization. Then it has benefits to apply AQE even without shuffle.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-43028][SQL] Add error class SQL_CONF_NOT_FOUND 
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a new error class `SQL_CONF_NOT_FOUND`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error message more user-friendly when getting a non-existing SQL config. For example:```spark.conf.get(\"some.conf\")```Before this PR, it will throw this error:```java.util.NoSuchElementException: some.conf```After this PR:```[SQL_CONF_NOT_FOUND] The SQL config \"some.conf\" cannot be found. Please verify that the config exists.```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. The error message will be changed.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added a new UT.
[SPARK-43025][SQL] Eliminate Union if filters have the same child plan
[SPARK-43030][SQL] Deduplicate relations with metadata columns
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The rule `DeduplicateRelations` finds duplicated relations and renew their output attribute IDs. For performance reasons, it uses object references to find duplicated relations. This is fine as duplicated relations only happen with DataFrame queries. For SQL query, the table look-up always returns the scan relation with fresh attribute IDs.However, with metadata columns, the analyzer will copy a scan relation with new output attributes to include metadata cols. The object references check does not work anymore as two different scan relation instances may have the same attribute IDs.This PR fixes this problem by looking at the output attribute IDs to check duplicated relations.Note that, `CTERelationRef` has the same problem as Spark always creates a new instance of it. The golden file changes are caused by the CTE fix.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new test. I also ran `TPCDSQuerySuite` locally. The runtime of rule `DeduplicateRelations` does not change much.
[SPARK-39696][CORE] Fix data race in access to TaskMetrics.externalAccums 
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR fixes a data race around concurrent access to `TaskMetrics.externalAccums`. The race occurs between the `executor-heartbeater` thread and the thread executing the task. This data race is not known to cause issues on 2.12 but in 2.13 ~due this change https://github.com/scala/scala/pull/9258~ (@LuciferYang bisected this to first cause failures in scala 2.13.7 one possible reason could be https://github.com/scala/scala/pull/9786) leads to an uncaught exception in the `executor-heartbeater` thread, which means that the executor will eventually be terminated due to missing hearbeats.This fix of using of using `CopyOnWriteArrayList` is cherry picked from https://github.com/apache/spark/pull/37206 where is was suggested as a fix by @LuciferYang since `TaskMetrics.externalAccums` is also accessed from outside the class `TaskMetrics`. The old PR was closed because at that point there was no clear understanding of the race condition. @JoshRosen commented here https://github.com/apache/spark/pull/37206#issuecomment-1189930626 saying that there should be no such race based on because all accumulators should be deserialized as part of task deserialization here: https://github.com/apache/spark/blob/0cc96f76d8a4858aee09e1fa32658da3ae76d384/core/src/main/scala/org/apache/spark/executor/Executor.scala#L507-L508 and therefore no writes should occur while the hearbeat thread will read the accumulators. But my understanding is that is incorrect as accumulators will also be deserialized as part of the taskBinary here: https://github.com/apache/spark/blob/169f828b1efe10d7f21e4b71a77f68cdd1d706d6/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala#L87-L88 which will happen while the heartbeater thread is potentially reading the accumulators. This can both due to user code using accumulators (see the new test case) but also when using the Dataframe/Dataset API as  sql metrics will also be `externalAccums`. One way metrics will be sent as part of the taskBinary is when the dep is a `ShuffleDependency`: https://github.com/apache/spark/blob/fbbcf9434ac070dd4ced4fb9efe32899c6db12a9/core/src/main/scala/org/apache/spark/Dependency.scala#L85 with a ShuffleWriteProcessor that comes from https://github.com/apache/spark/blob/fbbcf9434ac070dd4ced4fb9efe32899c6db12a9/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala#L411-L422 <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?The current code has a data race.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?It will fix an uncaught exception in the `executor-hearbeater` thread when using scala 2.13.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?This patch adds a new test case, that before the fix was applied consistently produces the uncaught exception in the heartbeater thread when using scala 2.13.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43024][PS][INFRA] Upgrade pandas to 2.0.0
[SPARK-42621][PS] Add inclusive parameter for pd.date_range
[SPARK-43009][SQL][3.4] Parameterized `sql()` with `Any` constants
Improve IDE build experience against jdk11
spark protobuf: add materializeDefaults option to spark-protobuf
[SPARK-42983][CONNECT][PYTHON] Fix createDataFrame to handle 0-dim numpy array properly
[MINOR][PYTHON][CONNECT][DOCS] Deduplicate versionchanged directive in Catalog
[MINOR][CONNECT][DOCS] Clarify Spark Connect option in Spark scripts
[SPARK-43035][CONNECT] Add error class in Spark Connect server's ErrorInfo
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds error classes, message parameters, and SQL states in the Spark Connect server's ErrorInfo. It also stores the error class and message parameters info in a `SparkConnectException` for the Python client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error class programmatically accessible for python clients.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit tests.
[SPARK-41537][INFRA][CONNECT][FOLLOW-UP] Removes breaking changes within master branch
[MINOR][INFRA] Remove workaround for CVE-2022-24765
[SPARK-42657][CONNECT] Support to find and transfer client-side REPL classfiles to server as artifacts  
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces the concept of a `ClassFinder` that is able to scrape the REPL output (either file-based or in-memory based) for generated class files.  The `ClassFinder` is registered during initialization of the REPL and aids in uploading the generated class files as artifacts to the Spark Connect server.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To run UDFs which are defined on the client side REPL, we require a mechanism that can find the local REPL classfiles and then utilise the mechanism from https://issues.apache.org/jira/browse/SPARK-42653 to transfer them to the server as artifacts.  ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, users can now run UDFs on the default (ammonite) REPL with spark connect.Input (in REPL):```class A(x: Int) { def get = x * 5 + 19 }def dummyUdf(x: Int): Int = new A(x).getval myUdf = udf(dummyUdf _)spark.range(5).select(myUdf(col(\"id\"))).as[Int].collect()```Output:```Array[Int] = Array(19, 24, 29, 34, 39)```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests + E2E tests.
[SPARK-42656][FOLLOWUP] Add BUILD and SCCLASSPATH options to Spark Connect scripts
[SPARK-43039][SQL] Support custom fields in the file source _metadata column.
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Allow `FileFormat` instances to define the schema of the `_metadata` column they expose. ### Why are the changes needed?Today, the schema of the file source `_metadata` column depends on the file format (e.g. parquet file format supports `_metadata.row_index`) but this is hard-wired into the `FileFormat` itself. Not only is this an ugly design, it also prevents custom file formats from adding their own fields to the `_metadata` column.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit tests.
[SPARK-43040][SQL] Improve TimestampNTZ type support in JDBC data source
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->https://github.com/apache/spark/pull/36726 supports TimestampNTZ type in JDBC data source and https://github.com/apache/spark/pull/37013 applies a fix to pass more test cases with H2.The problem is that Java Timestamp is a poorly defined class and different JDBC drivers implement \"getTimestamp\" and \"setTimestamp\" with different expected behaviors in mind. The general conversion implementation would work with some JDBC dialects and their drivers but not others. This issue is discovered when testing with PostgreSQL database.This PR adds a `dialect` parameter to `makeGetter` for applying dialect specific conversions when reading a Java Timestamp into TimestampNTZType. `makeSetter` already has a `dialect` field and we will use that for converting back to Java Timestamp.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix TimestampNTZ support for PostgreSQL. Allows other JDBC dialects to provide dialect specific implementation forconverting between Java Timestamp and Spark TimestampNTZType.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit test.I added new test cases for `PostgresIntegrationSuite` to cover TimestampNTZ read and writes.
[SPARK-43041][SQL] Restore constructors of exceptions for compatibility in connector API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds back old constructors for exceptions used in the public connector API based on Spark 3.3.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to avoid breaking connectors when consuming Spark 3.4.Here is a list of exceptions used in the connector API (`org.apache.spark.sql.connector`):```NoSuchNamespaceExceptionNoSuchTableExceptionNoSuchViewExceptionNoSuchPartitionExceptionNoSuchPartitionsException (not referenced by public Catalog API but I assume it may be related to the exception above, which is referenced)NoSuchFunctionExceptionNoSuchIndexExceptionNamespaceAlreadyExistsExceptionTableAlreadyExistsExceptionViewAlreadyExistsExceptionPartitionAlreadyExistsException (not referenced by public Catalog API but I assume it may be related to the exception below, which is referenced)PartitionsAlreadyExistExceptionIndexAlreadyExistsException```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Adds back previously released constructors.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
Master clone
[MINOR][INFRA] Partially brings workaround for CVE-2022-24765 back.
[SPARK-43044][CONNECT][BUILD] Upgrade buf to v1.17.0
[SPARK-43049][SQL] Use CLOB instead of VARCHAR(255) for StringType for Oracle JDBC
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use CLOB instead of VARCHAR(255) for StringType for Oracle JDBC### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->- Fix insufficient length issue when storing a spark string to oracle.- Make room for Spark VarcharType mapping ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, Using APIs, such as DDL and `df.write.jdbc`, with oracle to store string will result in CLOB columns.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests.
[SPARK-41532][CONNECT][CLIENT] Add check for operations that involve multiple data frames
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add check for operations that involve multiple data frames,  because spark do not support joining for example two data frames from different Spark Connect Sessions.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Spark do not support joining for example two data frames from different Spark Connect Sessions. To avoid exceptions, the client should clearly fail when it tries to construct such a composition.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43050][SQL] Fix construct aggregate expressions by replacing grouping functions
[SPARK-43051][CONNECT] Add option to emit default values
[SPARK-43052][CORE] Handle stacktrace with null file name in event log
[SPARK-43021][SQL] `CoalesceBucketsInJoin` not work when using AQE
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `CoalesceBucketsInJoin` to AQE `preprocessingRules`. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Previously optimized bucket join: 'CoalesceBucketsInJoin'` : https://github.com/apache/spark/pull/28123But when using AQE , `CoalesceBucketsInJoin` can not match beacuse the top of the spark plan is `AdaptiveSparkPlan`.The code : ```  val spark = SparkSession.builder()    .appName(\"BucketJoin\")    .master(\"local[*]\")    .config(\"spark.sql.adaptive.enabled\ true)    .config(\"spark.driver.memory\ \"4\")    .config(\"spark.sql.autoBroadcastJoinThreshold\ \"-1\")    .config(\"spark.sql.bucketing.coalesceBucketsInJoin.enabled\ true)    .enableHiveSupport()    .getOrCreate()    val df1 = (0 until 20).map(i => (i % 5, i % 13, i.toString)).toDF(\"i\ \"j\ \"k\")    val df2 = (0 until 20).map(i => (i % 7, i % 11, i.toString)).toDF(\"i\ \"j\ \"k\")    df1.write.format(\"parquet\").bucketBy(4, \"i\").saveAsTable(\"t1\")    df2.write.format(\"parquet\").bucketBy(2, \"i\").saveAsTable(\"t2\")    val t1 = spark.table(\"t1\")    val t2 = spark.table(\"t2\")    val joined = t1.join(t2, t1(\"i\") === t2(\"i\"))    joined.explain()```Before the PR```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- SortMergeJoin [i#50], [i#56], Inner   :- Sort [i#50 ASC NULLS FIRST], false, 0   :  +- Filter isnotnull(i#50)   :     +- FileScan parquet spark_catalog.default.t1[i#50,j#51,k#52] Batched: true, Bucketed: true, DataFilters: [isnotnull(i#50)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>, SelectedBucketsCount: 4 out of 4   +- Sort [i#56 ASC NULLS FIRST], false, 0      +- Exchange hashpartitioning(i#56, 4), ENSURE_REQUIREMENTS, [plan_id=78]         +- Filter isnotnull(i#56)            +- FileScan parquet spark_catalog.default.t2[i#56,j#57,k#58] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(i#56)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>```After the PR output:```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- SortMergeJoin [i#50], [i#56], Inner   :- Sort [i#50 ASC NULLS FIRST], false, 0   :  +- Filter isnotnull(i#50)   :     +- FileScan parquet spark_catalog.default.t1[i#50,j#51,k#52] Batched: true, Bucketed: true, DataFilters: [isnotnull(i#50)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>, SelectedBucketsCount: 4 out of 4 (Coalesced to 2)   +- Sort [i#56 ASC NULLS FIRST], false, 0      +- Filter isnotnull(i#56)         +- FileScan parquet spark_catalog.default.t2[i#56,j#57,k#58] Batched: true, Bucketed: true, DataFilters: [isnotnull(i#56)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>, SelectedBucketsCount: 2 out of 2```Additional Notes:We don't add CoalesceBucketsInJoin to `AdaptiveSparkPlanExec#queryStageOptimizerRules` because queryStageOptimizerRules is not applied at the beginning of the init plan. Instead, they are applied in the createQueryStages() method. And createQueryStages() is bottom-up, which causes the exchange to be eliminated to be wrapped in a layer of ShuffleQueryStage first, making CoalesceBucketsInJoin unrecognizable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add UT
[SPARK-42951][SS][Connect] DataStreamReader APIs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds the `orc`, `parquet`, and `text` APIs in connect's DataStreamReader### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Part of Streaming Connect project.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, now the three APIs are enabled. But everything is pretty much still under developed so far.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested, unit tests will be added in SPARK-43031 as a follow-up PR https://github.com/apache/spark/pull/40691.
[SPARK-43043][CORE] Improve the performance of MapOutputTracker.updateMapOutput
[SPARK-43031] [SS] [Connect] Enable unit test and doctest for streaming
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Enable unit tests and doctests for streaming queries. A lot are skipped and needs to be un-skipped as the development goes on. Note that I also separated the `{foreach, foreachBatch}` tests from the original test suite. Because currently they are not implemented in connect and it seems unnecessary to manually add a skip for all of them in `StreamingParityTests`. Also it doesn't hurt to separate them I think as these tests are large enough already.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->More tests is always better than less.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->It's test itself.
[SPARK-43055][CONNECT][PYTHON] Support duplicated nested field names
[SPARK-43058][SQL] Move Numeric and Fractional to PhysicalDataType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we  move Numeric and Fractional to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-43057][CONNECT][PYTHON] Migrate Spark Connect Column errors into error class
[SPARK-42994][ML][CONNECT] PyTorch Distributor support Local Mode
[SPARK-43056][SS] RocksDB state store commit should continue background work only if its paused
[SPARK-43061][CORE][SQL] Introduce PartitionEvaluator for SQL operator execution
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a new API `PartitionEvaluator` to define the computing logic and requires the caller side to explicitly list what needs to be serialized and sent to executors via `PartitionEvaluatorFactory`.Two new RDD APIs are added to use `PartitionEvaluator`:```  /**   * Return a new RDD by applying an evaluator to each partition of this RDD. The given evaluator   * factory will be serialized and sent to executors, and each task will create an evaluator with   * the factory, and use the evaluator to transform the data of the input partition.   */  @DeveloperApi  @Since(\"3.5.0\")  def mapPartitionsWithEvaluator[U: ClassTag](      partitionEvaluatorFactory: PartitionEvaluatorFactory[T, U]): RDD[U] = withScope {    new MapPartitionsWithEvaluatorRDD(this, taskEvaluatorFactory)  }  /**   * Zip this RDD's partitions with another RDD and return a new RDD by applying an evaluator to   * the zipped partitions. Assumes that the two RDDs have the *same number of partitions*, but   * does *not* require them to have the same number of elements in each partition.   */  @DeveloperApi  @Since(\"3.5.0\")  def zipPartitionsWithEvaluator[U: ClassTag](      rdd2: RDD[T],      partitionEvaluatorFactory: PartitionEvaluatorFactory[T, U]): RDD[U] = withScope {    new ZippedPartitionsWithEvaluatorRDD(this, rdd2, partitionEvaluatorFactory)  }```Three SQL operators are updated to use the new API to do execution, as a showcase: Project, Filter, WholeStageCodegen. We can migrate more operators later. A config is added to still use the old code path by default.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Using lambda to define the computing logic is a bit tricky:1. it's easy to mistakenly reference objects in the closure, which increases the time to serialize the lambda and sent to executors. `ProjectExec` and `FilterExec` use `child.output` in the lambda which means the entire `child` will be serialized. There are other places trying to avoid this problem, e.g. https://github.com/apache/spark/blob/v3.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala#L90-L922. serializing lambda is strongly discouraged by the [official Java guide](https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#serialization). We should eventually get rid of lambda during distributed execution to make Spark more robust.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-43062][INFRA][PYTHON][TESTS] Add options to lint-python to run each test separately
[SPARK-43063][SQL] `df.show` handle null should print NULL instead of null
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`df.show` handle null should print NULL instead of null to consistent behavior;Like as the following behavior is currently inconsistent:``` shellscala> spark.sql(\"select decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle') as result\").show(false)+------+|result|+------+|null  |+------+`````` shellspark-sql> DESC FUNCTION EXTENDED decode;function_descFunction: decodeClass: org.apache.spark.sql.catalyst.expressions.DecodeUsage:    decode(bin, charset) - Decodes the first argument using the second argument character set.    decode(expr, search, result [, search, result ] ... [, default]) - Compares expr      to each search value in order. If expr is equal to a search value, decode returns      the corresponding result. If no match is found, then it returns default. If default      is omitted, it returns null.Extended Usage:    Examples:      > SELECT decode(encode('abc', 'utf-8'), 'utf-8');       abc      > SELECT decode(2, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');       San Francisco      > SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');       Non domestic      > SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle');       NULL    Since: 3.2.0Time taken: 0.074 seconds, Fetched 4 row(s)`````` shellspark-sql> select decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle');NULL```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`df.show` keep consistent behavior when handle `null` with spark-sql CLI.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, `null` will display NULL instead of null.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
[SPARK-43065][SQL][TESTS] Set job description in `TPCDSQueryBenchmark`
[SPARK-43064][SQL] Spark SQL CLI SQL tab should only show once statement once
[SPARK-43066][SQL] Add test for dropDuplicates in JavaDatasetSuite
[SPARK-43033][SQL] Avoid task retries due to AssertNotNull checks
[SPARK-43038][SQL] Support the CBC mode by `aes_encrypt()`/`aes_decrypt()`
[SPARK-43067][SS] Correct the location of error class resource file in Kafka connector
[SPARK-43059][CONNECT][PYTHON] Migrate TypeError from DataFrame(Reader|Writer) into error class
[SPARK-43069][BUILD] Use `sbt-eclipse` instead of `sbteclipse-plugin`
[SPARK-43070][BUILD] Upgrade `sbt-unidoc` to 0.5.0
[SPARK-43071][SQL] Support SELECT DEFAULT with ORDER BY, LIMIT, OFFSET for INSERT source relation
[SPARK-43072][DOC] Include TIMESTAMP_NTZ type in ANSI Compliance doc
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->There are important syntax rules about Cast/Store assignment/Type precedent list in the [ANSI Compliance doc](https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html)As we are going to release [timestamp_ntz](https://issues.apache.org/jira/browse/SPARK-35662) type in Spark 3.4.0, we should update the doc page as well.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better documentation### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual build and verify<img width=\"1183\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230692965-02ec5a6e-8b8a-48dc-8049-9a87d26b2ce5.png\"><img width=\"1068\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230692988-bd35508c-0577-44c5-8448-f8d3b0aef2ea.png\"><img width=\"764\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230693005-cb61a760-ea11-4e6d-bdcb-2738c7c507c6.png\">
[SPARK-43073][CONNECT] Add proto data types constants
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?#### How to support more subexpressions elimination cases* Get all common expressions from input expressions of the current physical operator to current CodeGenContext. Recursively visits all subexpressions regardless of whether the current expression is a conditional expression.* For each common expression:  * Add a new boolean variable subExprInit to indicate whether it has  already been evaluated.   * Add a new code block in CodeGenSupport trait, and reset those subExprInit variables to false before the physical operators begin to evaluate the input row.  * Add a new wrapper subExpr function for each common subexpression.```private void subExpr_n(${argList}) { if (!subExprInit_n) {   ${eval.code}   subExprInit_n = true;   subExprIsNull_n = ${eval.isNull};   subExprValue_n = ${eval.value}; }}```  * When generating the input expression code,  if the input expression is a common expression, the expression code will be replaced with the corresponding subExpr function. When the subExpr function is called for the first time, subExprInit will be set to true, and the subsequent function calls will do nothing.#### Why should we support whole-stage subexpression eliminationRight now each spark physical operator shares nothing but the input row, so the same expressions may be evaluated multiple times across different operators. For example, the expression udf(c1, c2) in plan Project [udf(c1, c2)] - Filter [udf(c1, c2) > 0] - Relation will be evaluated both in Project and Filter operators.  We can reuse the expression results across different operators such as Project and Filter.#### How to support whole-stage subexpression elimination* Add two properties in CodegenSupport trait, the reusable expressions and the the output attributes, we can reuse the expression results only if the output attributes are the same.* Visit all operators from top to bottom, bound the candidate expressions with the output attributes and add to the current candidate reusable expressions.* Visit all operators from bottom to top, collect all the common expressions to the current operator, and add the initialize code to the current operator if the common expressions have not been initialized.* Replace the common expressions code when generating codes for  the physical operators.### Why are the changes needed?Support more subexpression elimination cases, improve performance.For example, TPCDS q23b query,  we can reuse the result of projection `sum(ss_quantity * ss_sales_price)` in the if expressions:```if (isnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])))   input[0, decimal(28,2), true]else    (input[0, decimal(28,2), true] + cast(knownnotnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])) as decimal(28,2)))```q4, q62, q67 are similar to the above.| Query    | Time with PR | Time without PR | Time diff | Percentage ||----------|--------------|-----------------|-----------|------------|| q1.sql   | 36.97        | 36.623          | -0.347    | 99.06%     || q2.sql   | 42.699       | 41.741          | -0.958    | 97.76%     || q3.sql   | 6.038        | 6.111           | 0.073     | 101.21%    || q4.sql   | 273.849      | 323.799         | 49.95     | 118.24%    || q5.sql   | 76.202       | 76.353          | 0.151     | 100.20%    || q6.sql   | 8.451        | 9.011           | 0.56      | 106.63%    || q7.sql   | 13.017       | 12.954          | -0.063    | 99.52%     || q8.sql   | 10.22        | 11.027          | 0.807     | 107.90%    || q9.sql   | 72.843       | 72.019          | -0.824    | 98.87%     || q10.sql  | 13.233       | 14.028          | 0.795     | 106.01%    || q11.sql  | 112.252      | 112.983         | 0.731     | 100.65%    || q12.sql  | 3.036        | 3.488           | 0.452     | 114.89%    || q13.sql  | 12.471       | 12.911          | 0.44      | 103.53%    || q14a.sql | 210.201      | 220.12          | 9.919     | 104.72%    || q14b.sql | 185.374      | 187.57          | 2.196     | 101.18%    || q15.sql  | 10.189       | 10.338          | 0.149     | 101.46%    || q16.sql  | 80.756       | 82.503          | 1.747     | 102.16%    || q17.sql  | 28.523       | 28.567          | 0.044     | 100.15%    || q18.sql  | 13.417       | 14.271          | 0.854     | 106.37%    || q19.sql  | 6.366        | 6.53            | 0.164     | 102.58%    || q20.sql  | 3.427        | 4.939           | 1.512     | 144.12%    || q21.sql  | 2.096        | 2.16            | 0.064     | 103.05%    || q22.sql  | 14.4         | 14.01           | -0.39     | 97.29%     || q23a.sql | 507.253      | 545.185         | 37.932    | 107.48%    || q23b.sql | 707.054      | 768.148         | 61.094    | 108.64%    || q24a.sql | 193.116      | 193.793         | 0.677     | 100.35%    || q24b.sql | 177.109      | 179.54          | 2.431     | 101.37%    || q25.sql  | 22.264       | 22.949          | 0.685     | 103.08%    || q26.sql  | 8.68         | 8.973           | 0.293     | 103.38%    || q27.sql  | 8.535        | 8.558           | 0.023     | 100.27%    || q28.sql  | 101.953      | 102.713         | 0.76      | 100.75%    || q29.sql  | 75.392       | 76.211          | 0.819     | 101.09%    || q30.sql  | 12.265       | 13.508          | 1.243     | 110.13%    || q31.sql  | 26.477       | 26.965          | 0.488     | 101.84%    || q32.sql  | 3.393        | 3.507           | 0.114     | 103.36%    || q33.sql  | 6.909        | 7.277           | 0.368     | 105.33%    || q34.sql  | 8.41         | 8.572           | 0.162     | 101.93%    || q35.sql  | 34.214       | 36.822          | 2.608     | 107.62%    || q36.sql  | 9.027        | 9.79            | 0.763     | 108.45%    || q37.sql  | 36.076       | 36.753          | 0.677     | 101.88%    || q38.sql  | 71.768       | 74.473          | 2.705     | 103.77%    || q39a.sql | 7.753        | 7.617           | -0.136    | 98.25%     || q39b.sql | 6.365        | 7.229           | 0.864     | 113.57%    || q40.sql  | 16.588       | 17.164          | 0.576     | 103.47%    || q41.sql  | 1.162        | 1.188           | 0.026     | 102.24%    || q42.sql  | 2.3          | 2.561           | 0.261     | 111.35%    || q43.sql  | 7.407        | 7.605           | 0.198     | 102.67%    || q44.sql  | 28.939       | 30.473          | 1.534     | 105.30%    || q45.sql  | 9.796        | 9.634           | -0.162    | 98.35%     || q46.sql  | 9.496        | 9.692           | 0.196     | 102.06%    || q47.sql  | 27.087       | 27.151          | 0.064     | 100.24%    || q48.sql  | 14.524       | 14.889          | 0.365     | 102.51%    || q49.sql  | 21.466       | 21.572          | 0.106     | 100.49%    || q50.sql  | 194.755      | 195.052         | 0.297     | 100.15%    || q51.sql  | 37.493       | 38.56           | 1.067     | 102.85%    || q52.sql  | 2.227        | 2.28            | 0.053     | 102.38%    || q53.sql  | 5.375        | 5.437           | 0.062     | 101.15%    || q54.sql  | 12.556       | 13.015          | 0.459     | 103.66%    || q55.sql  | 2.341        | 2.809           | 0.468     | 119.99%    || q56.sql  | 7.424        | 7.207           | -0.217    | 97.08%     || q57.sql  | 17.606       | 17.797          | 0.191     | 101.08%    || q58.sql  | 6.169        | 6.374           | 0.205     | 103.32%    || q59.sql  | 27.602       | 27.744          | 0.142     | 100.51%    || q60.sql  | 7.04         | 7.459           | 0.419     | 105.95%    || q61.sql  | 7.838        | 7.816           | -0.022    | 99.72%     || q62.sql  | 9.726        | 10.762          | 1.036     | 110.65%    || q63.sql  | 4.816        | 5.176           | 0.36      | 107.48%    || q64.sql  | 253.937      | 261.034         | 7.097     | 102.79%    || q65.sql  | 78.942       | 78.373          | -0.569    | 99.28%     || q66.sql  | 15.199       | 14.73           | -0.469    | 96.91%     || q67.sql  | 926.049      | 1022.971        | 96.922    | 110.47%    || q68.sql  | 7.932        | 7.977           | 0.045     | 100.57%    || q69.sql  | 12.101       | 14.699          | 2.598     | 121.47%    || q70.sql  | 20.7         | 20.872          | 0.172     | 100.83%    || q71.sql  | 14.96        | 15.065          | 0.105     | 100.70%    || q72.sql  | 73.215       | 73.955          | 0.74      | 101.01%    || q73.sql  | 5.973        | 6.126           | 0.153     | 102.56%    || q74.sql  | 97.611       | 99.577          | 1.966     | 102.01%    || q75.sql  | 125.005      | 129.508         | 4.503     | 103.60%    || q76.sql  | 34.812       | 35.34           | 0.528     | 101.52%    || q77.sql  | 7.686        | 8.474           | 0.788     | 110.25%    || q78.sql  | 287.959      | 292.936         | 4.977     | 101.73%    || q79.sql  | 8.401        | 9.616           | 1.215     | 114.46%    || q80.sql  | 59.371       | 60.051          | 0.68      | 101.15%    || q81.sql  | 18.452       | 19.499          | 1.047     | 105.67%    || q82.sql  | 64.093       | 65.032          | 0.939     | 101.47%    || q83.sql  | 4.675        | 4.867           | 0.192     | 104.11%    || q84.sql  | 10.456       | 10.816          | 0.36      | 103.44%    || q85.sql  | 12.347       | 12.77           | 0.423     | 103.43%    || q86.sql  | 6.537        | 6.843           | 0.306     | 104.68%    || q87.sql  | 77.427       | 77.876          | 0.449     | 100.58%    || q88.sql  | 83.082       | 83.385          | 0.303     | 100.36%    || q89.sql  | 6.645        | 6.801           | 0.156     | 102.35%    || q90.sql  | 7.841        | 7.883           | 0.042     | 100.54%    || q91.sql  | 3.88         | 4.129           | 0.249     | 106.42%    || q92.sql  | 3.044        | 3.271           | 0.227     | 107.46%    || q93.sql  | 361.149      | 365.883         | 4.734     | 101.31%    || q94.sql  | 43.929       | 46.667          | 2.738     | 106.23%    || q95.sql  | 196.363      | 197.427         | 1.064     | 100.54%    || q96.sql  | 12.457       | 12.496          | 0.039     | 100.31%    || q97.sql  | 80.131       | 81.821          | 1.69      | 102.11%    || q98.sql  | 6.885        | 7.522           | 0.637     | 109.25%    || q99.sql  | 17.685       | 18.009          | 0.324     | 101.83%    ||          | 6788.707     | 7116.357        | 327.65    | 104.82%    |One of our production query which has 19 case when  expressions, it's query time changed from 1.1 hour to 42 seconds.![image](https://user-images.githubusercontent.com/3626747/227448668-8a58ff33-3296-4a95-984b-292af47532ca.png)A simplify benchmark of the above production query.```    spark.range(1, 2000000, 1, 1)      .selectExpr(        \"cast(id + 1 as decimal) as a\        \"cast(id + 2 as decimal) as b\        \"cast(id + 3 as decimal) as c\        \"cast(id + 4 as decimal) as d\")      .createOrReplaceTempView(\"tab\")    runBenchmark(\"Subexpression elimination in ProjectExec\") {      val benchmark =        new Benchmark(\"Subexpression elimination in ProjectExec\ 2000000, output = output)      benchmark.addCase(s\"Test query\") { _ =>        val query =          s\"\"\"             |SELECT a, b, c, d,             |       a * b / c as s1,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END s2,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |       ELSE 0 END s3,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s4,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                      CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |                 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s5             |FROM tab             |\"\"\".stripMargin        spark.sql(query).noop()      }      benchmark.run()```Local benchmark result:Before this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         9713           9900         263          0.2        4856.7       1.0X```After this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         1238           1307          98          8.1         123.8       1.0X```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Exists UT.
[SPARK-43074] Add the function without constant parameters of `SessionState#executePlan`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the function without constant parameters of `SessionState#executePlan`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Before this change , using pyspark to run the code will get exception because py4j can not support default arguments in scala.```df = spark.sql(\"select 1\") catalyst_plan = df._jdf.queryExecution().logical()print('catalyst_plan: ', catalyst_plan)df_size = spark._jsparkSession.sessionState().executePlan(catalyst_plan)``````py4j.protocol.Py4JError: An error occurred while calling o87.executePlan. Trace:py4j.Py4JException: Method executePlan([class org.apache.spark.sql.catalyst.plans.logical.Project]) does not exist\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\tat py4j.Gateway.invoke(Gateway.java:274)\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\tat java.lang.Thread.run(Thread.java:748)```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->user call  `spark._jsparkSession.sessionState().executePlan(catalyst_plan)`  can get result.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual testing
[SPARK-43007][TESTS][FOLLOWUP] Regenerate benchmark results of `StateStoreBasicOperationsBenchmark`
[SPARK-43075][CONNECT] Change `gRPC` to `grpcio` when it is not installed. 
[MINOR][SQL][TESTS] Tests in `SubquerySuite` should not drop view created in `beforeAll`
[SPARK-43077][SQL] Improve the error message of UNRECOGNIZED_SQL_TYPE
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes UNRECOGNIZED_SQL_TYPE print both the column type name and the id.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->UNRECOGNIZED_SQL_TYPE prints the jdbc type id in the error message currently. This is difficult for spark users to understand the meaning of this kind of error, especially when the type id is from a vendor extension.For example, ```java org.apache.spark.SparkSQLException: Unrecognized SQL type -102```While -102 is nonstandard, it's hard to know what type it is```classOf[java.sql.JDBCType].getEnumConstants.foreach(t => println(t.getName + \"|\" + t.getVendorTypeNumber))BIT|-7TINYINT|-6SMALLINT|5INTEGER|4BIGINT|-5FLOAT|6REAL|7DOUBLE|8NUMERIC|2DECIMAL|3CHAR|1VARCHAR|12LONGVARCHAR|-1DATE|91TIME|92TIMESTAMP|93BINARY|-2VARBINARY|-3LONGVARBINARY|-4NULL|0OTHER|1111JAVA_OBJECT|2000DISTINCT|2001STRUCT|2002ARRAY|2003BLOB|2004CLOB|2005REF|2006DATALINK|70BOOLEAN|16ROWID|-8NCHAR|-15NVARCHAR|-9LONGNVARCHAR|-16NCLOB|2011SQLXML|2009REF_CURSOR|2012TIME_WITH_TIMEZONE|2013TIMESTAMP_WITH_TIMEZONE|2014```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, the unrecognized jdbc type error will also print the type name For example, ```java org.apache.spark.SparkSQLException: Unrecognized SQL type - name: TIMESTAMP WITH LOCAL TIME ZONE, id: -102```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[WIP]Speed up parquet reading with Java Vector API
Master clone2
[SPARK-43080][BUILD] Upgrade `zstd-jni` to 1.5.5-1
[SPARK-43076][PS][CONNECT] Removing the dependency on `grpcio` when remote session is not used.
[SPARK-43090][CONNECT][TESTS] Move `withTable` from `RemoteSparkSession` to `SQLHelper`
[SPARK-43081] [ML] [CONNECT] Add torch distributor data loader that loads data from spark partition data
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Added a `TorchDistributor` method API :```    def _train_on_dataframe(self, train_function, spark_dataframe, *args, **kwargs):        \"\"\"        Runs distributed training using provided spark DataFrame as input data.        You should ensure the input spark DataFrame have evenly divided partitions,        and this method starts a barrier spark job that each spark task in the job        process one partition of the input spark DataFrame.        Parameters        ----------        train_function :            Either a PyTorch function, PyTorch Lightning function that launches distributed            training. Note that inside the function, you can call            `pyspark.ml.torch.distributor.get_spark_partition_data_loader` API to get a torch            data loader, the data loader loads data from the corresponding partition of the            input spark DataFrame.        spark_dataframe :            An input spark DataFrame that can be used in PyTorch `train_function` function.            See `train_function` argument doc for details.        args :            `args` need to be the input parameters to `train_function` function. It would look like            >>> model = distributor.run(train, 1e-3, 64)            where train is a function and 1e-3 and 64 are regular numeric inputs to the function.        kwargs :            `kwargs` need to be the key-work input parameters to `train_function` function.            It would look like            >>> model = distributor.run(train, tol=1e-3, max_iter=64)            where train is a function that has 2 arguments `tol` and `max_iter`.        Returns        -------            Returns the output of `train_function` called with args inside spark rank 0 task.        \"\"\"```Added an API:```def _get_spark_partition_data_loader(num_samples, batch_size, prefetch=2):    \"\"\"    This function must be called inside the `train_function` where `train_function`    is the input argument of `TorchDistributor.train_on_dataframe`.    The function returns a pytorch data loader that loads data from    the corresponding spark partition data.    Parameters    ----------    num_samples :        Number of samples to generate per epoch. If `num_samples` is less than the number of        rows in the spark partition, it generate the first `num_samples` rows of        the spark partition, if `num_samples` is greater than the number of        rows in the spark partition, then after the iterator loaded all rows from the partition,        it wraps round back to the first row.    batch_size:        How many samples per batch to load.    prefetch:        Number of batches loaded in advance.    \"\"\"```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The added APIs are designed for spark 4.0 new ML module.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests.
[SPARK-43082][CONNECT][PYTHON] Arrow-optimized Python UDFs in Spark Connect
[SPARK-42382][BUILD] Upgrade `cyclonedx-maven-plugin` to 2.7.6
[SPARK-43083][SQL][TESTS] Mark `*StateStoreSuite` as `ExtendedSQLTest`
[WIP][SPARK-39634][SQL] Allow file splitting in combination with row index generation
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?- Upgrade the parquet dependency version to `1.13.0` from `1.12.3` (there is already a PR for this apache/spark#40555, rebase this PR once it is merged)- Parquet version `1.13.0` has a fix for [PARQUET-2161](https://issues.apache.org/jira/browse/PARQUET-2161) which allows splitting the parquet files when row index metadata column is selected. Currently the file splitting is disabled. Enable file splitting with row index column.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Splitting parquet files allows better parallelization when row index metadata column is selected.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Uncomment the existing unittests.
[SPARK-43136][CONNECT] Adding groupByKey + mapGroup + coGroup functions
[SPARK-43086][CORE] Support bin pack task scheduling on executors
[SPARK-43087][SQL] Support coalesce buckets in join in AQE
[SPARK-43085][SQL] Support column DEFAULT assignment for multi-part table names
[SPARK-43089][CONNECT] Redact debug string in UI
[SPARK-43088][SQL] Respect RequiresDistributionAndOrdering in CTAS/RTAS
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes sure Spark satisfies distribution and ordering requirements during CTAS/RTAS by carrying the analyzed plan to exec nodes and building `AppendData` for the new table.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed for multiple reasons:- Some data sources may require a specific distribution/ordering/num partitions for **correctness**. All of that information is now being requested from Spark via `RequiresDistributionAndOrdering`, which is ignored by CTAS/RTAS commands. As a result, a data source may ask for a particular distribution or ordering and Spark may not respect it. This can cause correctness issues.- Ignoring `RequiresDistributionAndOrdering` may severely degrade the write **performance** or even fail jobs as the underlying write may produce lots of small files.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[SPARK-43092][CONNECT] Clean up unimplemented `dropDuplicatesWithinWatermark` series functions from `Dataset`
[SPARK-43084] [SS] Add applyInPandasWithState support for spark connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change adds applyInPandasWithState support for Spark connect. Example (try with local mode `./bin/pyspark --remote \"local[*]\"`):```>>> from pyspark.sql.streaming.state import GroupStateTimeout, GroupState>>> from pyspark.sql.types import (...     LongType,...     StringType,...     StructType,...     StructField,...     Row,... )>>> import pandas as pd>>> output_type = StructType(...     [StructField(\"key\ StringType()), StructField(\"countAsString\ StringType())]... )>>> state_type = StructType([StructField(\"c\ LongType())])>>> def func(key, pdf_iter, state):...     total_len = 0...     for pdf in pdf_iter:...         total_len += len(pdf)...     state.update((total_len,))...     yield pd.DataFrame({\"key\": [key[0]], \"countAsString\": [str(total_len)]})...>>>>>> input_path = \"/Users/peng.zhong/tmp/applyInPandasWithState\">>> df = spark.readStream.format(\"text\").load(input_path)>>> q = (...       df.groupBy(df[\"value\"])...       .applyInPandasWithState(...           func, output_type, state_type, \"Update\ GroupStateTimeout.NoTimeout...       )...       .writeStream.queryName(\"this_query\")...       .format(\"memory\")...       .outputMode(\"update\")...       .start()...   )>>>>>> q.status{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}>>>>>> spark.sql(\"select * from this_query\").show()+-----+-------------+|  key|countAsString|+-----+-------------+|hello|            1|| this|            1|+-----+-------------+```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This change adds an API support for spark connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This change adds an API support for spark connect.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested.
[SPARK-43093][SQL][TESTS] Refactor `Add a directory when spark.sql.legacy.addSingleFileInAddFile set to false` to test using tempDir with non-fixed root dir
[SPARK-42380][BUILD] Upgrade Apache Maven to 3.9.1
[SPARK-43302][SQL] Make Python UDAF an AggregateFunction
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Today, `PythonUDF` can be used as an aggregate function according to the eval type. However, this is done in a tricky way, as `PythonUDF` does not extend `AggregateFunction` and we need to add special handling of it here and there. This is pretty error-prone, and we have hit issues such as https://github.com/apache/spark/pull/39824This PR adds a new `PythonUDAF` expression which extends `AggregateFunction`. Now python udaf will be handled the same as normal aggregate functions, except for the places that we need to extract python functions. After this, we can remove most of the special handling of `PythonUDF`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-42656][FOLLOWUP] Rename BUILD parameter to SCBUILD to avoid clashes
[SPARK-41811][CONNECT][CLIENT] Support sql with dataframes and columns
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?1. add columns field on SQL/SQLCommand in connect protobuf to support send expression to server2. parser columns in server to support sql with column3. add string_formatter for connect module to support sql with dataframes.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  4. If you fix some SQL features, you can provide some references of other DBMSes.  5. If there is design documentation, please add the link.  6. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To support  sql with dataframes and columns in spark connect client.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43095][SQL] Avoid Once strategy's idempotence is broken for batch: `Infer Filters`
[SPARK-42597][SQL][FOLLOW-UP] Only rewrite `EqualNullSafe` when the left side is non-nullable
[SPARK-24497][SQL] Support recursive SQL
[Do not merge] Testing repl build on CI
[SPARK-42985][CONNECT][PYTHON] Fix createDataFrame to respect the SQL configs
[SPARK-43099][SQL] Use `getName` instead of `getCanonicalName` to get builder class name when registering udf to FunctionRegistry
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use `getName` instead of `getCanonicalName` to get builder class name when registering udf to FunctionRegistrySince JDK15+, `getCanonicalName` will return null for anonymous classes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This causes the `className` field in `ExpressionInfo` to always be null. This changes the behavior when `DESCRIBE` udfs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
[WIP][SPARK-43097] New pyspark ML logistic regression estimator implemented on top of distributor
[SPARK-43100][CORE] Mismatch of field name in log event writer and parser for push shuffle metrics
[SPARK-43105][CONNECT] Abbreviate Bytes and Strings in proto message
[SPARK-43102][BUILD] Upgrade commons-compress to 1.23.0
[SPARK-43103][SQL] Moving Integral to PhysicalDataType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we move integral to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-43104][BUILD] Set `shadeTestJar` of `protobuf` module to `false`
[SPARK-37099][SQL][FOLLOWUP] Add numOutputRows metric for WindowGroupLimitExec
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `numOutputRows` metric in `WindowGroupLimitExec`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`WindowGroupLimitExec`  is a kind of filter that would drop unused rows in each window group, so `numOutputRows` could help user find if it has benefits.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-37829][SQL] Dataframe.joinWith outer-join should return a null value for unmatched row
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->When doing an outer join with joinWith on DataFrames, unmatched rows return Row objects with null fields instead of a single null value. This is not a expected behavior, and it's a regression introduced in [this commit](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59).This pull request aims to fix the regression, note this is not a full rollback of the commit, do not add back \"schema\" variable.```case class ClassData(a: String, b: Int)val left = Seq(ClassData(\"a\ 1), ClassData(\"b\ 2)).toDFval right = Seq(ClassData(\"x\ 2), ClassData(\"y\ 3)).toDFleft.joinWith(right, left(\"b\") === right(\"b\"), \"left_outer\").collect``````Wrong results (current behavior):    Array(([a,1],[null,null]), ([b,2],[x,2]))Correct results:                     Array(([a,1],null), ([b,2],[x,2]))``` ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We need to address the regression mentioned above. It results in unexpected behavior changes in the Dataframe joinWith API between versions 2.4.8 and 3.0.0+. This could potentially cause data correctness issues for users who expect the old behavior when using Spark 3.0.0+.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added unit test (use the same test in previous [closed pull request](https://github.com/apache/spark/pull/35140), credit to Clément de Groc)Run sql-core and sql-catalyst submodules locally with ./build/mvn clean package -pl sql/core,sql/catalyst
[SPARK-43107][SQL] Coalesce buckets in join applied on broadcast join stream side
[SPARK-42656][FOLLOWUP] `chmod+x` for `connector/connect/bin/spark-connect-scala-client-classpath` script
[SPARK-43110][SQL] Move asIntegral to PhysicalDataType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we move asIntegral to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-43111][PS][CONNECT][PYTHON] Merge nested `if` statements into single `if` statements
[SPARK-42982][CONNECT][PYTHON] Fix createDataFrame to respect the given schema ddl
[MINOR][SQL] Simplify the method resolveExprsAndAddMissingAttrs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The method `resolveExprsAndAddMissingAttrs` contains redundant code: getting the `newExprs` and `newChild` shows up 4 times in different branches.This PR is to simplify the implementation of the method.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code clean up and remove redundant code.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
[SPARK-42953][Connect][Followup] Fix maven test build for Scala client UDF tests
[SPARK-43114][SQL][TESTS] Add interval types to TypeCoercionSuite
[SPARK-43115][CONNECT][PS][TESTS] Split pyspark-pandas-connect from pyspark-connect module
[WIP][SPARK-43112]. Spark may use a column other than the actual specified partitioning column for partitioning, for Hive format tables
[SPARK-43113][SQL] Evaluate stream-side variables when generating code for a bound condition
[SPARK-43118][SS] Remove unnecessary assert for UninterruptibleThread in KafkaMicroBatchStream
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The assert ` assert(Thread.currentThread().isInstanceOf[UninterruptibleThread]) ` found https://github.com/apache/spark/blob/master/connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala#L239  is not needed.  The reason is the following1. This assert was put there due to some issues when the old and deprecated KafkaOffsetReaderConsumer is used.  The default offset reader implementation has been changed to KafkaOffsetReaderAdmin which no longer require it run via UninterruptedThread.2. Even if the deprecated KafkaOffsetReaderConsumer is used, there are already asserts in that impl to check if it is running via UninterruptedThread e.g. https://github.com/apache/spark/blob/master/connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderConsumer.scala#L130 thus the assert in KafkaMicroBatchStream is redundant.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->Remove unnecessary assert.  Clean up code.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->n/a
[SPARK-43119][SQL] Support Get SQL Keywords Dynamically Thru JDBC API and TVF
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces a TVF and makes it support JDBC standard API to support getting SQL Keywords dynamically.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->1. JDBC API Compliance 2. SQL Keywords are helpful for AI-powered BI tools during prompting to generate queries3. Used by Spark SQL Highlighting### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, a new tvf sql_keywords function added### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[WIP][PYTHON][TESTS] Reduce the required resources in PyTorch related tests
[SPARK-43120][SS] Add support for tracking pinned blocks memory usage for RocksDB state store
[SPARK-35723] set k8s pod container request, limit memory separately.
Currently spark driver and exec pod request and limit memory can only be same, not able to set separately when cluster get diff memory limit and request quota. then not able to fully use cluster memory resource.E.g. cluster get total 50G memory request, 200G memory limit, but every spark pod limit memory are same as request memory, then in this cluster total memory spark can use is depends on the smaller one: 50G<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[MINOR][TESTS] Fix DistributedDataParallel model code in TorchDistributor suite
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix DistributedDataParallel model code in TorchDistributor suite### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Testing code fix.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->N/A### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
[SPARK-43121][SQL] Use `BytesWritable.copyBytes` instead of manual copy in `HiveInspectors
[SPARK-41210][K8S] Port executor failure tracker from Spark on YARN to K8s
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fail Spark Application when the number of executor failures reaches the threshold.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Sometimes, the executors can not launch successfully because of the wrong configuration, but in K8s, Driver does not know that, and just keep requesting new executors.This PR ports the window-based executor failure tracking mechanism to K8s(only takes effect when `spark.kubernetes.allocation.pods.allocator` is set to 'direct'), to reduce functionality gap between YARN and K8s.Note that, YARN mode also supports host-based executor allocation failure tracking and application terminating mechanism[2], this PR does not port such functionalities to Kubernetes since it's kind of an independent and big feature, and relies on some YARN features which I'm not sure if K8s has similar one.[1] [SPARK-6735](https://issues.apache.org/jira/browse/SPARK-6735)[2] [SPARK-17675](https://issues.apache.org/jira/browse/SPARK-17675)### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this PR provides two new configurations - `spark.executor.maxNumFailures`- `spark.executor.failuresValidityInterval`which takes effect on YARN, or on Kubernetes when `spark.kubernetes.allocation.pods.allocator` is set to 'direct'.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT added, and manually tested in internal K8s cluster.
[SPARK-42994][TESTS][FOLLOWUP] Skip `TorchDistributorLocalUnitTestsIIOnConnect` for now
[SPARK-43123][SQL] Internal field metadata should not be leaked to catalogs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In Spark, we have defined some internal field metadata to help query resolution and compilation. For example, there are quite some field metadata that are related to metadata columns.However, when we create tables, these internal field metadata can be leaked. This PR updates CTAS/RTAS commands to remove these internal field metadata before creating tables. CREATE/REPLACE TABLE command is fine as users can't generate these internal field metadata via the type string.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to avoid potential issues, like mistakenly treating a data column as metadata column### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new test
[CONNECT] Dump of query cancellation hacking
[WIP][PYTHON][TESTS] Test test_parity_torch_distributor with timeout
init<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43124][SQL] Dataset.show projects CommandResults locally
[SPARK-43125][CONNECT] Fix Connect Server Can't Handle Exception With Null Message
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Fix the bug when Connect Server throw Exception without message.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix bug<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43126][SQL] Mark two Hive UDF expressions as stateful
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Due to the recent refactor, I realized that the two Hive UDF expressions are stateful as they both keep an array to store the input arguments. This PR fix it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to avoid issues in a muti-thread environment.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Too hard to write unit tests and the fix itself is very obvious.
[SPARK-42669][CONNECT] Short circuit local relation RPCs
[SPARK-43129] Scala core API for streaming Spark Connect
[SPARK-43130][SQL] Move InternalType to PhysicalDataType
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR drops `InternalType` from the logical data type and refactors to use the PhysicalDataType and InternalType.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To simplify DataType interface.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42960] [CONNECT] [SS] Add await_termination() and exception() API for Streaming Query in Python
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the `await_termination()` and `exception()` to streaming query class. For `exception`, only pass the `message` the same way in `SparkConnectService`, and construct the error the same way as the `convert_exception` method in `_handle_rpc_error` in `client.py`. For `await_termination`, send the command multiple times instead of waiting to prevent RPC timeout. `<-- I'm definitely open to any discussion on its implementation!`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Add missing APIs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes but part of ongoing developing of Streaming Spark Connect.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests. Note that the unit tests for them are still skipped because of 1. queryManager is not implemented. 2. Allow access to stopped query is not implemented.I was able to test them manually by 1. For `test_stream_await_termination()`, comment out the ```for q in self.spark.streams.active:    q.stop()```2. For `test_stream_exception()`, comment out unregistering terminated query: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L411
[SPARK-43135][INFRA] Remove `branch-3.2` from `publish_snapshot` GitHub Action job
[SPARK-42994][TESTS][FOLLOWUP] Skip `TorchDistributorLocalUnitTestsOnConnect`
[SPARK-42452][BUILD] Remove `hadoop-2` profile from Apache Spark 3.5.0
[SPARK-43137][SQL] Improve ArrayInsert if the position is foldable and equals to zero.
[SPARK-43116][SQL] Fix Cast.forceNullable
[SPARK-43140][SQL][TESTS] Override computeStats in `DummyLeafNode`
[SPARK-43141][BUILD] Ignore generated Java files in checkstyle
[SPARK-43122][CONNECT][PYTHON][ML][TESTS] Reenable TorchDistributorLocalUnitTestsOnConnect and TorchDistributorLocalUnitTestsIIOnConnect
[SPARK-43142] Fix DSL expressions on attributes with special characters
[TEMP] Scala m1 temp
[SPARK-43223][Connect] Typed agg, reduce functions, RelationalGroupedDataset#as
[SPARK-43042] [SS] [Connect] Add table() API support for DataStreamReader
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the table() method support in DataStreamReader in Spark Connect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of building SS Connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes now the table() API is available.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit test
SPARK-43166: name docker users
[SPARK-43145][SQL] Reduce ClassNotFound of hive storage handler table
[SPARK-43146][CONNECT][PYTHON] Implement eager evaluation for __repr__ and _repr_html_
[SPARK-43147] fix flake8 lint for local check
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change flake8 config file### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Before changing this, when doing local python lint `./dev/lint-python`, it checks folder that shouldn't be checked. This doesn't happen in github action.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually testedBefore: ```starting flake8 test.........many more......./python/docs/source/conf.py:112:1: E265 block comment should start with '# '#source_encoding = 'utf-8-sig'^./python/docs/source/conf.py:132:1: E265 block comment should start with '# '#language = None^./python/venv/lib/python3.8/site-packages/pkg_resources/_vendor/more_itertools/more.py:1814:13: E731 do not assign a lambda expression, use a def            key_argument = lambda zipped_items: key(            ^./dev/ansible-for-test-node/roles/jenkins-worker/files/util_scripts/post_github_pr_comment.py:7:1: F401 'urllib.parse' imported but unusedimport urllib.parse^./dev/ansible-for-test-node/roles/jenkins-worker/files/util_scripts/session_lock_resource.py:130:9: F841 local variable 'f' is assigned to but never used        f = _acquire_lock(lock_filename, timeout_secs, lock_message)......many more......```And many moreAfter:```starting flake8 test...flake8 checks passed.```
[SPARK-43150][SQL] Remove workaround for PARQUET-2160
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove workaround(SPARK-41952) for [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-42926](https://issues.apache.org/jira/browse/SPARK-42926) upgraded Parquet to 1.13.0, which includes [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160). So we no longer need [SPARK-41952](https://issues.apache.org/jira/browse/SPARK-41952).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[MINOR][CONNECT][PYTHON] Typo fixes
[SPARK-43151][DOC] Update the prerequisites for generating Python API docs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update the prerequisites for generating Python API docs:* The command should be run under the docs directory so that the input file should be `../dev/requirements.txt`* Remove `sudo` in the command* Remove the version for `torch` and `torchvision`. I was getting ```ERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.0.0)ERROR: No matching distribution found for torch==1.13.1ERROR: Could not find a version that satisfies the requirement torchvision==0.14.1 ```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Update the prerequisites for generating Python API docs, to save troubles from other developers when generating Python Docs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually try the command and new requirements.txt
[SPARK-40609][SQL] Unwrap cast in the join condition to unlock bucketed read
[SPARK-43153][CONNECT] Skip Spark execution when the dataframe is local
[SPARK-43139][SQL][DOCS] Fix incorrect column names in sql-ref-syntax-dml-insert-table.md
[SPARK-43138][CORE] Fix ClassNotFoundException during migration
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR fixes an unhandled ClassNotFoundException during RDD block decommissions migrations.```2023-04-08 04:15:11,791 ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6425687122551756860java.lang.ClassNotFoundException: com.class.from.user.jar.ClassName    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)    at java.base/java.lang.Class.forName0(Native Method)    at java.base/java.lang.Class.forName(Class.java:398)    at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:71)    at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)    at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)    at java.base/java.io.ObjectInputStream.readClass(ObjectInputStream.java:1833)    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1658)    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)    at org.apache.spark.network.netty.NettyBlockRpcServer.deserializeMetadata(NettyBlockRpcServer.scala:180)    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:119)    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)    at java.base/java.lang.Thread.run(Thread.java:829)```The exception occurs if RDD block contains user defined during the serialization of a `ClassTag` for the user defined class. The problem for serialization of the `ClassTag` a instance of `JavaSerializer`(https://github.com/apache/spark/blob/ca2ddf3c2079dda93053e64070ebda1610aa1968/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala#L62) is used, but it never configured to use a class loader including user defined classes. This PR solves the issue by instead use a serializer from the SerializerManager which is configured to use the correct class loader.The reason is this does not occur during normal block replication and only during decommission is that there is a workaround/hack in `BlockManager.doPutIterator` that replaces the `ClassTag` with a `ClassTag[Any]` when replicating that block https://github.com/apache/spark/blob/ca2ddf3c2079dda93053e64070ebda1610aa1968/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L1657-L1664 But during RDD migration (and probably pro-active replication) it will use a different codepath and potentially send the correct ClassTag which leads to the unhandled exception.### Why are the changes needed?The unhandled exception means that block replication does not work properly. Specifically cases where the block contains a user class and it not replicated at creation then the block will never successfully be migrated during decommission.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?It fixes the bug. But also since it changes from a fixed `JavaSerializer` to instead use the `SerializerManager` the `NettyBlockTransferService` might now instead use `KryoSerializer` or some other user configured serializer for the metadata.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?This modifies an existing spec to correctly check that replication happens for repl defined classes while removing the hack that erases the `ClassTag`.  Additionally I tested this manually on a hadoop cluster to check that it also solves the decommission migration issue. If some can point me to some better way to add a spec using user defined classes I would also like to add a unittest for it.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
Master clone3
[SPARK-42317][SQL] Assign name to _LEGACY_ERROR_TEMP_2247: CANNOT_MERGE_SCHEMAS
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign name to _LEGACY_ERROR_TEMP_2247 as \"CANNOT_MERGE_SCHEMAS\".Also proposes to display both left and right schemas in the exception so that one can compare them. Please let me know if you prefer the old error message with a single schema.This is the stack trace after the changes:```scala> spark.read.option(\"mergeSchema\ \"true\").parquet(path)org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:Initial schema:\"STRUCT<id: BIGINT>\"Schema that cannot be merged with the initial schema:\"STRUCT<id: INT>\".  at org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2355)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:104)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5$adapted(SchemaMergeUtils.scala:100)  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:100)  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:496)  at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:78)  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)  at scala.Option.orElse(Option.scala:447)  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)  at scala.Option.getOrElse(Option.scala:189)  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:548)  ... 49 elidedCaused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"BIGINT\" and \"INT\".  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotMergeIncompatibleDataTypesError(QueryExecutionErrors.scala:1326)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:610)  at scala.Option.map(Option.scala:230)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:602)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:599)  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:599)  at org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:647)  at org.apache.spark.sql.types.StructType$.merge(StructType.scala:593)  at org.apache.spark.sql.types.StructType.merge(StructType.scala:498)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:102)  ... 67 more```<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?We should assign proper name to LEGACY_ERROR_TEMP*<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the users will see an improved error message.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Changed an existing test case to test the new error class with `checkError` utility.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43098][SQL] Fix correctness COUNT bug when scalar subquery has group by clause
[SPARK-43157][SQL] Clone InMemoryRelation cached plan to prevent cloned plan from referencing same objects
[SPARK-42475][DOCS][FOLLOW-UP] Fix PySpark connect Quickstart binder link
[SPARK-43158][DOCS] Set upperbound of pandas version for Binder integration
[SPARK-42475][DOCS][FOLLOW-UP] Fix the version string with dev0 to work in Binder integration
[SPARK-42078][PYTHON][FOLLOWUP] Add `CapturedException` to utils
[SPARK-42845][SQL] Update the error class _LEGACY_ERROR_TEMP_2010 to InternalError
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update the error class _LEGACY_ERROR_TEMP_2010 to InternalError.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue [SPARK-42845](https://issues.apache.org/jira/browse/SPARK-42845). The original name just a number, update it to InteralError.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add a test case in QueryExecutionErrorsSuite.
[MINOR][CONNECT][PYTHON] Add missing `super().__init__()` in expressions
[WIP][SPARK-43160][PYTHON]: Removed typing.io deprecated namespace
[MINOR][SQL][DOCS] Improve spark.sql.files.minPartitionNum's doc
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to improve `spark.sql.files.minPartitionNum`'s doc.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve description### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, better config description.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
[SPARK-43152][spark-structured-streaming] Parametrisable output metadata path (_spark_metadata)
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Introduce a new parameter for defining output metadata path. If it's not set then the behavior doesn't change, the output metadata are set to `data_output_path/_spark_metadata`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->1. Separation data from metadata2. Multiple jobs can write to the same directory3. Hive partition discoveryLinks for more description are put in jira ticket: https://issues.apache.org/jira/browse/SPARK-43152### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, there is new parameter in spark-sql config.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests.
[WIP] cancel-in-progress
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->![image](https://user-images.githubusercontent.com/8326978/232464191-444c8e74-be01-4dfb-82d2-f1b763c0a575.png)### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42552][SQL] Fix select without parentheses can't be parsed
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The Sql Parser will try use `SSL`(faster) mode parse sqlText at first, if throw `ParseCancellationException` then try parse sqlText with `LL`(more correcter) mode. But Spark use custom `SparkParserErrorStrategy` will not throw `ParseCancellationException` but `ParseException` in some situation. So we should catch `ParseException` then to make sure parser use `LL` mode try again.Fix sql parser throw `ParseException` in `SSL` mode will not try `LL` mode.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Make some special sql can be parsed. Like `SELECT 1 UNION SELECT 1`.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-32064][SQL] Support temporary table
[SPARK-43165][SQL] Move canWrite to DataTypeUtils
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Move canWrite to DataTypeUtils.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->canWrite access SQLConf so we can move it out from DataType to make DataType as public simpiler API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-43168][SQL] Remove get PhysicalDataType method from Datatype class
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->DataType is public API while we can leave PhysicalDataType as internal API/implementation thus we can remove PhysicalDataType from DataType. So DataType does not need to have a class dependency on PhysicalDataType.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify DataType.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
[SPARK-42585][CONNECT] Streaming of local relations
[SPARK-42984][CONNECT][PYTHON][TESTS] Enable test_createDataFrame_with_single_data_type
[SPARK-41971][SQL] Use deduplicated field names when creating Arrow RecordBatch
[SPARK-43169][INFRA] Bump `previousSparkVersion` to 3.4.0
[SPARK-43171][K8S] Support custom Unix username in Pod
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR allows the users to custom Unix username in Pod by setting env var `SPARK_USER_NAME`, which reduces the gap between Spark on YARN and K8s.Each line in `/etc/passwd` is compose of```username:password:UID:GID:comment:home_directory:shell```This PR simply changes the first item from `$myuid` to `${SPARK_USER_NAME:-$myuid}` to achieve the above ability.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In Spark on YARN, when we launch a Spark application via `spark-submit --proxy-user jack ...`, the YARN will launch containers(usually Linux processes) using Unix user \"jack\ and some components/libraries rely on the login user in default, one example is Alluxiohttps://github.com/Alluxio/alluxio/blob/da77d688bdbb0cf0c6477bed4d3187897fe2a2e1/core/common/src/main/java/alluxio/conf/PropertyKey.java#L6469-L6476```  public static final PropertyKey SECURITY_LOGIN_USERNAME =      stringBuilder(Name.SECURITY_LOGIN_USERNAME)          .setDescription(\"When alluxio.security.authentication.type is set to SIMPLE or \"              + \"CUSTOM, user application uses this property to indicate the user requesting \"              + \"Alluxio service. If it is not set explicitly, the OS login user will be used.\")          .setConsistencyCheckLevel(ConsistencyCheckLevel.ENFORCE)          .setScope(Scope.CLIENT)          .build();```To reduce the difference between Spark on YARN and Spark on K8s, we hope Spark on K8s keeps the same ability to allow to dynamically change login user on submitting Spark application.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it allows the user to custom Pod Unix username by setting env var `SPARK_USER_NAME` in K8s, reducing the gap between Spark on YARN and K8s.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New IT is added.Also manually testing in our internal K8s cluster.```spark-submit --master=k8s://xxxx \\        --conf spark.kubernetes.driverEnv.SPARK_USER_NAME=tom \\\t--conf spark.executorEnv.SPARK_USER_NAME=tom \\\t--proxy-user tom \\        ...```Then login the Pod, verify the Unix username by `id -un` is `tom` instead of `185`
[SPARK-42657][CONNECT][FOLLOWUP] Correct the API version in scaladoc
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SPARK-42657 is for Spark 3.5.0.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix the wrong API version### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Passing GA.
[SPARK-43137][SQL] Improve ArrayInsert if the position is foldable and positive.
[SPARK-43046] [SS] [Connect] Implemented Python API dropDuplicatesWithinWatermark for Spark Connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Implemented `dropDuplicatesWithinWatermark` Python API for Spark Connect. This change is based on a previous [commit](https://github.com/apache/spark/commit/0e9e34c1bd9bd16ad5efca77ce2763eb950f3103) that introduced `dropDuplicatesWithinWatermark` API in Spark.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We recently introduced dropDuplicatesWithinWatermark API in Spark ([commit link](https://github.com/apache/spark/commit/0e9e34c1bd9bd16ad5efca77ce2763eb950f3103)). We want to bring parity to the Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this introduces a new public API, dropDuplicatesWithinWatermark in Spark Connect.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added new test cases in test suites.
[SPARK-42552][SQL] Correct the two-stage parsing strategy of antlr parser
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR follows the https://github.com/antlr/antlr4/issues/192#issuecomment-15238595 to correct the current implementation of the **two-stage parsing strategy** in `AbstractSqlParser`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This should be a long-standing issue, before [SPARK-38385](https://issues.apache.org/jira/browse/SPARK-38385), Spark uses `DefaultErrorStrategy`, and after [SPARK-38385](https://issues.apache.org/jira/browse/SPARK-38385) Spark uses class `SparkParserErrorStrategy() extends DefaultErrorStrategy`. It is not a correct implementation of the \"two-stage parsing strategy\"As mentioned in https://github.com/antlr/antlr4/issues/192#issuecomment-15238595> You can save a great deal of time on correct inputs by using a two-stage parsing strategy.>> 1. Attempt to parse the input using BailErrorStrategy and PredictionMode.SLL.>    If no exception is thrown, you know the answer is correct.> 2. If a ParseCancellationException is thrown, retry the parse using the default>    settings (DefaultErrorStrategy and PredictionMode.LL).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the Spark SQL parser becomes more powerful, SQL like `SELECT 1 UNION SELECT 2` parse succeeded after this change.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT is added.
[SPARK-43172] [CONNECT] Expose host and token from spark connect client
[SPARK-43173][CONNECT][TESTS] Ignore `write jdbc` when test `ClientE2ETestSuite`  without `-Phive`
[SPARK-43174][SQL] Fix SparkSQLCLIDriver completer
[SPARK-43176][CONNECT][PYTHON][TESTS] Deduplicate imports in Connect Tests
SamKenX sync
Bump jetty-server from 9.4.51.v20230217 to 10.0.14
Master clone8
[SPARK-43179][SHUFFLE] Allowing apps to control whether their metadata gets saved in the db by the External Shuffle Service
[SPARK-43181][SQL] Show UI WebURL in `spark-sql` shell
[SPARK-43183][SS] Introduce a new callback \"onQueryIdle\" to StreamingQueryListener
[SPARK-43184][YARN] Resume using enumeration to compare `NodeState.DECOMMISSIONING` state
[SPARK-43185][BUILD] Inline `hadoop-client` related properties in `pom.xml`
[SPARK-43186][SQL][HIVE] Remove workaround for FileSinkDesc
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove `org.apache.spark.sql.hive.HiveShim.ShimFileSinkDesc`, which is used to address serializable issue of `org.apache.hadoop.hive.ql.plan.FileSinkDesc`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[HIVE-6171](https://issues.apache.org/jira/browse/HIVE-6171) changed `FileSinkDesc`'s property from `String dirName` to `Path dirName`, but the `Path` is not serializable until [HADOOP-13519](https://issues.apache.org/jira/browse/HADOOP-13519) (got fixed in Hadoop 3.0.0).Since SPARK-42452 removed support for Hadoop2, we can remove this workaround now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43187][TEST] Remove workaround for MiniKdc's BindException
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR basically reverts the SPARK-31631, which was aimed to address [HADOOP-12656](https://issues.apache.org/jira/browse/HADOOP-12656)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since [HADOOP-12656](https://issues.apache.org/jira/browse/HADOOP-12656) got fixed in Hadoop 2.8.0/3.0.0, and SPARK-42452 removed support for Hadoop2, we can remove this workaround now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43191][CORE] Replace reflection w/ direct calling for Hadoop CallerContext
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Replace reflection w/ direct calling for `org.apache.hadoop.ipc.CallerContext`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`org.apache.hadoop.ipc.CallerContext` was added in [HDFS-9184](https://issues.apache.org/jira/browse/HDFS-9184) (Hadoop 2.8.0/3.0.0), previously, Spark uses reflection to invoke it for compatible w/ Hadoop 2.7, since SPARK-42452 removed support for Hadoop2, we can call it directly instead of using reflection.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43190][SQL] ListQuery.childOutput should be consistent with child output
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update `ListQuery` to only store the number of columns of the original plan, instead of directly storing the original plan output attributes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Storing the plan output attributes is troublesome as we have to maintain them and keep them in sync with the plan. For example, `DeduplicateRelations` may change the plan output, and today we do not update `ListQuery.childOutputs` to keep sync.`ListQuery.childOutputs` was added by https://github.com/apache/spark/pull/18968 . It's only used to track the original plan output attributes as subquery de-correlation may add more columns. We can do the same thing by storing the number of columns of the plan.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, there is no user-facing bug exposed.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->a new plan test
[SPARK-43193][SS] Remove workaround for HADOOP-12074
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SPARK-19718 introduced different code branches for pre-Hadoop 2.8(w/o [HADOOP-12074](https://issues.apache.org/jira/browse/HADOOP-12074)) and Hadoop 2.8+(w/ [HADOOP-12074](https://issues.apache.org/jira/browse/HADOOP-12074))> 1. Check if the message of IOException starts with `java.lang.InterruptedException`. If so, treat it as `InterruptedException`. This is for pre-Hadoop 2.8.> 2. Treat `InterruptedIOException` as `InterruptedException`. This is for Hadoop 2.8+ and other places that may throw `InterruptedIOException` when the thread is interrupted.This PR removes the (1) since Spark no longer supports Hadoop2 now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since [SPARK-42452](https://issues.apache.org/jira/browse/SPARK-42452) removed support for Hadoop2, we can remove the workaround code for pre Hadoop 2.8 now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43192] [CONNECT] Remove user agent charset validation
[SPARK-43195][CORE] Remove unnecessary serializable wrapper in HadoopFSUtils
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove unnecessary serializable wrapper in `HadoopFSUtils`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`Path`, `FileStatus` become serializable in Hadoop3, since SPARK-42452 removed support for Hadoop2, we can remove those wrapper now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI.
[SPARK-43196][YARN] Replace reflection w/ direct calling for `ContainerLaunchContext#setTokensConf`
[SPARK-43199][SQL] Make InlineCTE idempotent
[SPARK-43200][DOCS] Remove Hadoop 2 reference in docs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove Hadoop 2 reference in docs.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-42452 removed support for Hadoop 2.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually review
[SPARK-37829][SQL][3.3] Dataframe.joinWith outer-join should return a null value for unmatched row
[SPARK-42437][CONNECT][PYTHON][FOLLOW-UP] Storage level proto converters
[SPARK-43202][YARN] Replace reflection w/ direct calling for YARN Resource API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Replace reflection w/ direct calling for YARN Resource API, including - `org.apache.hadoop.yarn.api.records.ResourceInformation`, - `org.apache.hadoop.yarn.exceptions.ResourceNotFoundException`which were added in [YARN-4081](https://issues.apache.org/jira/browse/YARN-4081) (Hadoop 2.10.0/3.0.0) ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify code. Since [SPARK-42452](https://issues.apache.org/jira/browse/SPARK-42452) removed support for Hadoop 2, we can call those API directly now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43032][CONNECT][SS] Add Streaming query manager
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add support of `StreamingQueryManager()` to CONNECT PYTHON client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Now users can use typical streaming query manager method by calling `session.streams`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual test and unit test```Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Dec  7 2022 01:11:58)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> q = spark.readStream.format(\"rate\").load().writeStream.format(\"memory\").queryName(\"test\").start()23/04/19 23:10:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-913e48b9-26d8-448f-899f-d9f5ae08707d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.23/04/19 23:10:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.>>> spark.streams.active[<pyspark.sql.connect.streaming.query.StreamingQuery object at 0x7f4590400d90>]>>> q1 = spark.streams.active[0]>>> q1.id == q.idTrue>>> q1.runId == q.runIdTrue>>> q1.runId == q.runIdTrue>>> q.name'test'>>> q1.name'test'>>> q == q1False>>> q1.stop()>>> q<pyspark.sql.connect.streaming.query.StreamingQuery object at 0x7f4590400b20>>>> q1<pyspark.sql.connect.streaming.query.StreamingQuery object at 0x7f4590400ee0>>>> q.isActiveFalse```
[SPARK-43169][INFRA][FOLLOWUP] Add more memory for mima check
[SPARK-43207][CONNECT] Add helper functions to extract value from literal expression
[WIP] Nested DataType compatibility in Arrow-optimized Python UDF and Pandas UDF
[SPARK-43156][SQL] Fix `COUNT(*) is null` bug in correlated scalar subquery
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Example query:```sqlcreate or replace temp view t0 (a, b)as values    (1, 1.0),    (2, 2.0);create or replace temp view t1 (c, d)as values    (2, 3.0); spark.sql(\"select *, (select (count(1)) is null from t1 where t0.a = t1.c) from t0\").collect()res6: Array[org.apache.spark.sql.Row] = Array([1,1.0,null], [2,2.0,false])  ```In this subquery, count(1) always evaluates to a non-null integer value, so count(1) is null is always false. The correct evaluation of the subquery is always false.We incorrectly evaluate it to null for empty groups. The reason is that NullPropagation rewrites `Aggregate [c] [isnull(count(1))]` to `Aggregate [c] [false]`, this rewrite would be correct normally, but in the context of a scalar subquery it breaks our count bug handling in RewriteCorrelatedScalarSubquery.constructLeftJoins . By the time we get there, the query appears to not have the count bug - it looks the same as if the original query had a subquery with `select any_value(false) from r...`, and that case is not subject to the count bug.Postgres comparison show correct always-false result: http://sqlfiddle.com/#!17/67822/5Solution:The reason are `NullPropagation` triggers the bug for `(count(1)) is null`, but the root cause is the wrong handling of literals when dealing with the count bug. The bug can be triggered when the scalar subquery has a global aggregate with a constant, which can be produced by NullPropagation or by the user query directly. We should make sure `resultWithZeroTups` return real value when use scalar subquery and return value are `Literal`. <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix `COUNT(*) is null bug` in correlated scalar subquery<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, some sql query result will be changed.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new tests <!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43178][CONNECT][PYTHON] Migrate UDF errors into PySpark error framework
[SPARK-43208][SQL][HIVE] IsolatedClassLoader should close barrier class InputStream after reading
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As title, close the barrier class `InputStream` after reading in `IsolatedClassLoader`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`IOUtils.toByteArray(inputStream)` is not responsible to close the `inputStream`, the caller should do closing instead.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43210][CONNECT][PYTHON] Introduce `PySparkAssertionError`
[SPARK-43209][CONNECT][PYTHON] Migrate Expression errors into error class
[SPARK-43211][HIVE] Remove Hadoop2 support in IsolatedClientLoader
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove Hadoop2 support in `IsolatedClientLoader`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Clean up Hadoop2 related code since SPARK-42452 removed support for Hadoop2.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated test cases introduced in SPARK-32256, pass GA.
[SPARK-43373][SQL] Revert [SPARK-39203][SQL] Rewrite table location to absolute URI based on database URI
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This reverts https://github.com/apache/spark/pull/36625 and its followup https://github.com/apache/spark/pull/38321 .### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->External table location can be arbitrary and has no connection with the database location. It can be wrong to qualify the external table location based on the database location.If a table written by old Spark versions does not have a qualified location, there is no way to restore it as the information is already lost. People can manually fix the table locations assuming they are under the same HDFS cluster with the database location, by themselves.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
[MINOR][CONNECT][PYTHON][DOCS] Fix the doc of parameter `num` in `DataFrame.offset`
[SPARK-43213][PYTHON] Add `DataFrame.offset` to vanilla PySpark
[SPARK-43231][ML][PYTHON][CONNECT][TESTS] Reduce the memory requirement in torch-related tests
[SPARK-43214][SQL] Post driver-side metrics for LocalTableScanExec/CommandResultExec
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Since `LocalTableScan`/`CommandResultExec` may not trigger a Spark job, post the driver-side metrics even in scenarios where a Spark job is not triggered, so that we can track the metrics in the SQL UI tab.**LocalTableScanExec**before this PR:![截屏2023-04-20 下午6 36 47](https://user-images.githubusercontent.com/8537877/233342293-9d688705-550c-441c-a666-0e88254cd91f.png)after this PR:![截屏2023-04-20 下午6 35 19](https://user-images.githubusercontent.com/8537877/233342319-965f1ee3-3015-4e3b-b70b-25341ffa6090.png)**CommandResultExec**before this PR:![截屏2023-04-20 下午6 20 05](https://user-images.githubusercontent.com/8537877/233342423-3fcc41b8-563b-4d14-a5e7-ee9612abf7be.png)after this PR:![截屏2023-04-20 下午6 18 57](https://user-images.githubusercontent.com/8537877/233342466-c18a4e4c-34ba-46d1-a090-9d83fba63fda.png)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->makes metrics of `LocalTableScanExec`/`CommandResultExec` trackable on the SQL UI tab### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new UT.
[SPARK-43215][YARN] Remove `isYarnResourceTypesAvailable` from  `ResourceRequestHelper`
[SPARK-31733][YARN][TESTS] Make `specify a more specific type for the application` in `ClientSuite` pass in Hadoop 3
[SPARK-42780][BUILD] Upgrade `Tink` to 1.9.0
[SPARK-43217] Correctly recurse in nested maps/arrays in findNestedField
[SPARK-43212][SS][PYTHON] Migrate Structured Streaming errors into error class
[SPARK-43113][SQL][FOLLOWUP] Add comment about copying steam-side variables
[SPARK-43222][CORE][SQL][K8S][TESTS] Remove `isHadoop3` check from tests
[WIP][SPARK-43221][CORE] the BlockManager with the persisted block is preferred
[SPARK-43205] IDENTIFIER() clause (OBSOLETE)
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->We want to allow the parameterization of identifiers in SQL statements without compromising security.Right now this can only be done using ${..}  variable substitution which allows for arbitrary text substitution.The improvement proposed here is to use an IDENTIFIER(<stringliteral>) clause which produces a possibly qualified identifier. That way any text substitution from a client or using ${..} can be safely scoped.Example:SET hivevar:tab = 'mytab';SELECT * FROM IDENTIFIER(${hivevar:tab});==>SELECT * FROM mytab;Note that the IDENTIFIER(<literal) syntax originates from Snowflake.There is no other vendor I am aware of with similar capability.[Link to a more detailed specification](https://docs.google.com/document/d/14BhSZFeDoK-iZa7-66PjlHQ4nYLpdePDWNSW3kkLZAs/edit?usp=sharing)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To protect scripts that substitute table or column names from SQL injection attacks.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this is a new SQL feature.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL tests have been added.
[SPARK-43226] Define extractors for file-constant metadata
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?File-source constant metadata columns are often derived indirectly from file-level metadata values rather than exposing those values directly. Add support for metadata extractors, so that we can express such columns in a generic way.### Why are the changes needed?Allows to express the existing file-source metadata columns in a generic way (previously hard-wired), and also allows to lazily derive expensive metadata values only if the query actually selects them.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit test. Plus, existing file-source metadata unit tests pass.
[SPARK-43144] Scala Client DataStreamReader table() API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the table() API for scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of SS Connect development.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit testI also changed `ProtoToParsedPlanTestSuite` a little to remove the memory addresses, before the change the test for streaming table would fail with:```- streaming_table_API_with_options *** FAILED *** (8 milliseconds)[info]   Expected and actual plans do not match:[info]   [info]   === Expected Plan ===[info]   SubqueryAlias primary.tempdb.myStreamingTable[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@752725d9, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@347d8e2a, tempdb.myStreamingTable[info]   [info]   [info]   === Actual Plan ===[info]   SubqueryAlias primary.tempdb.myStreamingTable[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@a88a5db, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@2c6b362e, tempdb.myStreamingTable```Because the memory address (`InMemoryTable@752725d9`) is different every time it runs. I removed these in the test suite.And verified that memory addresses doesn't exist in existing explain files:```wei.liu:~/oss-spark$ cat connector/connect/common/src/test/resources/query-tests/explain-results/* | grep @wei.liu:~/oss-spark$ ```
[SPARK-43055][CONNECT][PYTHON][FOLLOWUP] Fix deduplicate field names and refactor
[SPARK-41660][SQL][3.3] Only propagate metadata columns if they are used
[SPARK-43219][SQL][DOCS] Add `INSERT INTO REPLACE WHERE` statement into website
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?`INSERT INTO REPLACE` statement be supported in #38404 , but can't be found in website<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add doc for feature we supported.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?![image](https://user-images.githubusercontent.com/32387433/233919892-28131da8-0b71-4203-baaf-f7ef768757d9.png)![image](https://user-images.githubusercontent.com/32387433/233919941-e9e16931-075d-4ba6-85e6-61125fa60eaa.png)<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43191][CORE][FOLLOWUP] Use renamed import statement for Hadoop classes
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Improve code style, use renamed import statement for Hadoop classes### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Address comment from @mridulm https://github.com/apache/spark/pull/40850/files#r1172865732> nit: Instead of using the fully qualified name (which made sense in reflection code earlier), we should have used a renamed import statement.>> Something like:>> ```> import org.apache.hadoop.ipc.{CallerContext => HadoopCallerContext}> import org.apache.hadoop.ipc.CallerContext.{Builder => HadoopCallerContextBuilder}> ```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43128][CONNECT][SS] Make `recentProgress` and `lastProgress` return `StreamingQueryProgress` consistent with the native Scala Api
[SPARK-43225][BUILD][SQL] Remove jackson-core-asl and jackson-mapper-asl from pre-built distribution
[SPARK-43198][CONNECT] Fix \"Could not initialise class ammonite...\" error when using filter
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes the ammonite REPL use the `CodeClassWrapper` mode for classfile generation (make ammonite generate classes instead of objects) and changes the UDF serialization from lazy to eager.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The changes have the following impact:- `CodeClassWrapper` change  - Fixes the `io.grpc.StatusRuntimeException: UNKNOWN: ammonite/repl/ReplBridge$` error when trying to use the `filter` method (see [jira](https://issues.apache.org/jira/browse/SPARK-43198) for reproduction)- Lazy to eager UDF serialization  - With class-based generation, UDFs defined using `def` would hit CNFE because the `ScalarUserDefinedFuntion` class gets captured during serialisation and sent over to the server (the class is a client-only class).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behaviour and the change this PR proposes - provide the console output, description and/or an example to show the behaviour difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. There are two significant changes:- Filter works as expected when using \"in-place\" lambda expressions such as in `spark.range(10).filter(n => n % 2 == 0).collectAsList()`- UDFs defined using a lambda expression which is stored in a `val` fail due to deserialisation issues on the server.  - Root cause is currently unknown but a ticket has been [filed](https://issues.apache.org/jira/browse/SPARK-43227) to address the issue.  - Example: see [this](https://github.com/apache/spark/compare/master...vicennial:spark:SPARK-43198?expand=1#diff-8d8a214eff5d2c8d523b59f2a39758ddfa84912ef7d4e0276f54e979a58f88e0R120-R129) test.  - Currently, it is a compromise to get `filter` working as expected since that bug is a higher-impact due to it impacting the \"general\" way of using the method.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit test.
[SPARK-43128] [SS] [Connect] Implemented StreamingQueryProgress for Spark Connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Implemented StreamingQueryProgress for Spark Connect. Also added related structs under the legacy `StreamingQueryProgress`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since Spark Connect transfers streaming progress as full “json”, implemented `fromJson` method to deserialize the json string to the legacy `StreamingQueryProgress`. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added new unit test suite.
[SPARK-43229][ML][PYTHON][CONNECT] Introduce Barrier Python UDF
[SPARK-43228][SQL] Join keys also match PartitioningCollection in CoalesceBucketsInJoin
[SPARK-43230][CONNECT] Simplify `DataFrameNaFunctions.fillna`
[SPARK-43249][CONNECT] Fix missing stats for SQL Command
[SPARK-43196][YARN][FOLLOWUP] Remove unnecessary Hadoop version check
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->It's not necessary to check Hadoop version 2.9+ or 3.0+ now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify code and docs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43195][BUILD][FOLLOWUP] Fix mima check for Scala 2.13
[WIP][SPARK-NNNNN] Updating AES-CBC support to not use OpenSSL's KDF
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The current implementation of AES-CBC mode called via `aes_encrypt` and `aes_decrypt` uses a key derivation function (KDF) based on OpenSSL's [EVP_BytesToKey](https://www.openssl.org/docs/man3.0/man3/EVP_BytesToKey.html). This is intended for generating keys based on passwords and OpenSSL's documents discourage its use: \"Newer applications should use a more modern algorithm\".`aes_encrypt` and `aes_decrypt` should use the key directly in CBC mode, as it does for both GCM and ECB mode. The output should then be the initialization vector (IV) prepended to the ciphertext – as is done with GCM mode:`[16-byte randomly generated IV | AES-CBC encrypted ciphertext]`### Why are the changes needed?We want to have the ciphertext output similar across different modes. OpenSSL's EVP_BytesToKey is effectively deprecated and their own documentation says not to use it. Instead, CBC mode will generate a random vector.### Does this PR introduce _any_ user-facing change?AES-CBC output generated by the previous format will be incompatible with this change. That change was recently landed and we want to land this before CBC mode is used in practice.### How was this patch tested?A new unit test in `DataFrameFunctionsSuite` was added to test both GCM and CBC modes. Also, a new standalone unit test suite was added in `ExpressionImplUtilsSuite` to test all the modes and various key lengths.CBC values can be verified with `openssl enc` using the following command:```echo -n \"[INPUT]\" | openssl enc -a -e -aes-256-cbc -iv [HEX IV] -K [HEX KEY]echo -n \"Spark\" | openssl enc -a -e -aes-256-cbc -iv f8c832cc9c61bac6151960a58e4edf86 -K 6162636465666768696a6b6c6d6e6f7031323334353637384142434445464748```
[WIP][POC] foreachbatch spark connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->POC for foreachbatch spark connect```>>> def foreach_batch_function(df, epoch_id):...   from pyspark.sql.functions import col, lit...   count = df.count()...   print(\"##### count is \ count)...   df.withColumn('new_column', lit(10)).write.mode('append').saveAsTable('test_foreachbatch_1')...>>> query = (...  spark...  .readStream...  .format(\"rate\")...  .option(\"numPartitions\ \"1\")...  .load()...  .writeStream...  .foreachBatch(foreach_batch_function)...  .start()... )>>>>>> query.status{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}>>>>>>>>> spark.sql(\"select * from test_foreachbatch_1\").show()+--------------------+-----+----------+|           timestamp|value|new_column|+--------------------+-----+----------+|2023-04-21 14:01:...|   29|        10||2023-04-21 14:01:...|    6|        10||2023-04-21 14:01:...|   21|        10||2023-04-21 14:01:...|   16|        10||2023-04-21 14:01:...|   11|        10||2023-04-21 14:01:...|   26|        10||2023-04-21 14:01:...|   32|        10||2023-04-21 14:01:...|   33|        10||2023-04-21 14:01:...|    4|        10||2023-04-21 14:01:...|    5|        10||2023-04-21 14:01:...|   17|        10||2023-04-21 14:01:...|   18|        10||2023-04-21 14:01:...|   24|        10||2023-04-21 14:01:...|   25|        10||2023-04-21 14:01:...|   30|        10||2023-04-21 14:01:...|   31|        10||2023-04-21 14:01:...|    7|        10||2023-04-21 14:01:...|    8|        10||2023-04-21 14:01:...|    0|        10||2023-04-21 14:01:...|    1|        10|+--------------------+-----+----------+only showing top 20 rows```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
 [SPARK-43233] [SS] Add logging for Kafka Batch Reading for topic partition, offset range and task ID
[SPARK-43134] [CONNECT] [SS] JVM client StreamingQuery exception() API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add StreamingQuery exception() API for JVM client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Development of SS Connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual test:```Spark session available as 'spark'.   _____                  __      ______                            __  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/ ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/    /_/@  val q = spark.readStream.format(\"rate\").load().writeStream.option(\"checkpointLocation\ \"/home/wei.liu/ckpt\").toTable(\"my_table\")  q: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.streaming.RemoteStreamingQuery@772f3a3f@ q.exception res1: Option[org.apache.spark.sql.streaming.StreamingQueryException] = None@ q.stop() ```
[SPARK-43270][PYTHON] Implement `__dir__()` in `pyspark.sql.dataframe.DataFrame` to include columns
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Override the parent `__dir__()` method on Python `DataFrame` class to include column names. Main benefit of this is that any autocomplete engine that uses `dir()` to generate autocomplete suggestions (e.g. IPython kernel, Databricks Notebooks) will suggest column names on the completion `df.|`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To keep `__dir__()` consistent with `__getattr__()`. So this is arguably a bug fix. Increases productivity for anyone who uses an autocomplete engine on pyspark code.Example of column attribute completion coming for free after this change:https://user-images.githubusercontent.com/84545946/233747057-56b2589d-d075-4d13-8349-ac5142c38c62.mov### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Will change the output of `dir(df)`. If the user chooses to use the private method `df.__dir__()`, they will also notice an output and docstring difference there.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New doctest with three assertions. Output where I only ran this test:![pyspark test passed](https://user-images.githubusercontent.com/84545946/233744674-b59191a7-08bf-4f3e-a491-945e687727b0.png)To test it in a notebook:```pythonfrom pyspark.sql.dataframe import DataFrameclass DataFrameWithColAttrs(DataFrame):    def __init__(self, df):        super().__init__(df._jdf, df._sql_ctx if df._sql_ctx else df._session)    def __dir__(self):        attrs = super().__dir__()        attrs.extend(attr for attr in self.columns if attr not in attrs)        return attrs```
[SPARK-42750][SQL] Support Insert By Name statement
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?In some use cases, users have incoming dataframes with fixed column names which might differ from the canonical order. Currently there's no way to handle this easily through the INSERT INTO API - the user has to make sure the columns are in the right order as they would when inserting a tuple. We should add an optional BY NAME clause, such that:`INSERT INTO tgt BY NAME <query>`takes each column of <query> and inserts it into the column in `tgt` which has the same name according to the configured `resolver` logic.At now don't support `INSERT OVERWRITE BY NAME`<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add new feature `INSERT INTO BY NAME`<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-42411] [Kubernetes] Add support for istio with strict mtls
[SPARK-43234][CONNECT][PYTHON] Migrate `ValueError` from Conect DataFrame into error class
[SPARK-43237][CORE] Handle null exception message in event log
[SPARK-43238][CORE] Support only decommission idle workers in standalone
[SPARK-43239][PS] Remove `null_counts` from info()
[SPARK-43240][SQL][3.3] Fix the wrong result issue when calling df.describe() method.
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The df.describe() method will cached the RDD.  And if the cached RDD is RDD[Unsaferow], which may be released after the row is used, then the result will be wong. Here we need to copy the RDD before caching as the [TakeOrderedAndProjectExec ](https://github.com/apache/spark/blob/d68d46c9e2cec04541e2457f4778117b570d8cdb/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala#L204)operator does.### Why are the changes needed?bug fix### Does this PR introduce _any_ user-facing change?no### How was this patch tested?
[SPARK-43232][SQL] Improve ObjectHashAggregateExec performance for high cardinality
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The `ObjectHashAggregateExec` has three preformance issues:- heavy overhead of scala sugar in `createNewAggregationBuffer`- unnecessary grouping key comparation after fallback to sort based aggregator- the aggregation buffer in sort based aggregator is not reused for all rest rowsThen the performance is poor with high cardinality grouping keys.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve performance### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The test should be covered by `org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite`Add benchmark for high cardinality case. Note, in this case the performance with no fallback is slower than others.```sql-- 10 * 1000 * 1000 rowsdf.groupBy(\"key1\ \"key2\")  .agg(sum($\"value\"), count($\"value\"), avg($\"value\"), max($\"value\"), min($\"value\"),    collect_set($\"value\"))  .noop()```before```OpenJDK 64-Bit Server VM 1.8.0_322-b06 on Mac OS X 13.2Apple M1 Proobject agg v.s. sort agg with high cardinality:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative----------------------------------------------------------------------------------------------------------------------------------------------sort agg                                                 4997           5099         118          2.0         499.7       1.0Xobject agg w/o fallback                                 12634          12944         311          0.8        1263.4       0.4Xobject agg w/ fallback                                   8792           8871          84          1.1         879.2       0.6X```after```OpenJDK 64-Bit Server VM 1.8.0_322-b06 on Mac OS X 13.2Apple M1 Proobject agg v.s. sort agg with high cardinality:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------------sort agg                                                 4831           5060         199          2.1         483.1       1.0Xobject agg w/o fallback                                  8971           9257         259          1.1         897.1       0.5Xobject agg w/ fallback                                   5612           5706         102          1.8         561.2       0.9X```
[SPARK-43243][PYTHON][CONNECT] Add level param to printSchema for Python
[SPARK-43113][SQL][3.3] Evaluate stream-side variables when generating code for a bound condition
[WIP][CORE] Add shuffle sort merge joins to RDD API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add shuffle sort merge join to RDD API. <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Shuffle sort merge join is the default join strategy in Spark SQL and the RDD API should have an equivalent method for users to use depending on workload. <!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?New methods are exposed to users on RDDs. <!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tests are added to PairRDDFunctionsSuite.scala. And all pass. <!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43204][SQL] Align MERGE assignments with table attributes
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR extends `ResolveRowLevelCommandAssignments` to also cover MERGE assignments.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Similar to SPARK-42151, these changes are needed so that we can rewrite MERGE statements into executable plans for tables that support row-level operations. In particular, our row-level mutation framework assumes Spark is responsible for building an updated version of each affected row and that row is passed back to the data source.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
[SPARK-43248][SQL] Unnecessary serialize/deserialize of Path on parallel gather partition stats
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove unnecessary serialize/deserialize of `Path` on parallel gather partition stats.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify code, since `Path` is serializable in Hadoop3.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43242] fix throw 'Unexpected type of BlockId' in diagnose when…
[SPARK-43063][SQL][FOLLOWUP] Add ToPrettyString expression for Dataset.show
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/40699 to avoid changing the Cast behavior. It pulls out the cast-to-string code into a base trait, and add a new Expression `ToPrettyString` to extend this trait with a little customization.It also handles binary value inside array/struct/map to also print hex format, for `df.show` only, not `Cast`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->avoid behavior change### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->change back the behavior of casting array/map/struct to string regarding null elements. It was `null`, then changed to `NULL` in #40699 , and is `null` again after this PR.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[Draft] State API (FlatMapGroupsWithState) in Scala for Spark Connect
[SPARK-43260][PYTHON] Migrate the Spark SQL pandas arrow type errors into error class.
[SPARK-43246][BUILD] Ignore `privateClasses` and `privateMembers` from connect mima check as default
[SPARK-43261][PYTHON] Migrate `TypeError` from Spark SQL types into error class.
[SPARK-42419][FOLLOWUP][CONNECT][PYTHON] Remove unused exception
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is follow-up for https://github.com/apache/spark/pull/39991 to remove unused exception.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`PySparkTypeError` never raises because we checked the type already in the codes above `if type(startPos) != type(length):` and raise `PySparkTypeError` for unsupported type for `length`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No. it's minor code cleanup### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass.
[SPARK-43262][CONNECT][SS][PYTHON] Migrate Spark Connect Structured Streaming errors into error class
[SPARK-43264][SQL] Avoid allocation of unwritten ColumnVector in Spark Vectorized Reader
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds lazy allocation support for the backing array of ColumnVector used in Spark VectorizedReader.  The scope of this change includes:- Simplify `OnHeapColumnVector` to only use a single-byte array for various data types.- Introduced lazy loading to the `data` array, e.g.: allocating the array only on the first write. - Changed `nulls` byte array to use `BitSet` for a smaller memory footprint and faster batch operations.Out of scope of this PR:- Lazy allocation support for `OffHeapColumnVector`.### Why are the changes needed?This PR is added as a memory optimization for addressing the memory utilization issue when reading a Parquet file with large but sparse columns.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?Existing tests:* ColumnarBatchSuite* ColumnVectorSuite* ColumnVectorUtilsSuite* ConstantColumnVectorSuiteManual tests:Tested on reading a Parquet file with a large nested struct with an array with 16GB executors. This patch fixed the OutOfMemory exception.
[DO NOT MERGE] File constant metadata extractors split
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Experimental PR in response to https://github.com/apache/spark/pull/40885#discussion_r1174277575, so that reviewers can visualize the difference between the two approaches discussed there. NOT FOR MERGE -- If this approach is chosen, I will update the other PR accordingly. Only the last commit differs from the other PR.### Why are the changes needed?N/A### Does this PR introduce _any_ user-facing change?No### How was this patch tested?N/A
[SPARK-43265] Move Error framework to a common utils module
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Move Error framework to a common utils module so that we can share it between Spark and Spark Connect without introducing heavy dependencies on Spark Connect module.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Reduce Dependencies on Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Error framework is internally API so this should be fine.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT.
[SPARK-43266][SQL] Move MergeScalarSubqueries to spark-sql
[SPARK-43263][BUILD] Upgrade `FasterXML jackson` to 2.15.0
[SPARK-43268][SQL] Use proper error classes when exceptions are constructed with a message
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes sure each exception affected in PR #40679 has a proper error class when constructed with an explicit message.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed per [this discussion](https://github.com/apache/spark/pull/40679#discussion_r1159264585).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[SPARK-43206] [SS] [CONNECT] [DRAFT] [DO-NOT-REVIEW] StreamingQuery exception() include stack trace
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add stack trace to streamingQuery's `exception()` method. Following https://github.com/apache/spark/commit/a5c8a3c976889f33595ac18f82e73e6b9fd29b57#diff-98baf452f0352c75a39f39351c5f9e656675810b6d4cfd178f1b0bae9751495bAdd to both python client and scala client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Including stack trace is helpful in debugging### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested:1. Python:```JVM stacktrace:org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (ip-10-110-19-234.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):  File \"/home/wei.liu/oss-spark/python/lib/pyspark.zip/pyspark/worker.py\ line 850, in main    process()```2. Scala: TODO
[MINOR][CONNECT] Remove unnecessary creation of `planner` in `handleWriteOperation` and `handleWriteOperationV2`
[SPARK-42940][SS][CONNECT] Improve session management for streaming queries
[SPARK-43274][SPARK-43275][PYTHON][CONNECT] Introduce `PySparkNotImplementedError`
[SPARK-43276][CONNECT][PYTHON] Migrate Spark Connect Window errors into error class
[SPARK-43277][YARN] Clean up deprecation hadoop api usage in `yarn` module
[MINOR][BUILD] Correct the error message in `dev/connect-check-protos.py`
[SPARK-43279][CORE] Cleanup unused members from `SparkHadoopUtil`
[SPARK-43280][BUILD] Reimplement the protobuf breaking change checker
[SPARK-40708][SQL][WIP] Auto update partition statistics based on write metrics
[SPARK-43272][CORE] Directly call `createFile` instead of reflection
[SPARK-43156][SPARK-43098][SQL] Extend scalar subquery count bug test with decorrelateInnerQuery disabled
[SPARK-43284] Switch back to url-encoded strings
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Update `_metadata.file_path` and `_metadata.file_name` to return url-encoded strings, rather than un-encoded strings. This was a regression introduced in Spark 3.4.0.### Why are the changes needed?This was an inadvertent behavior change.### Does this PR introduce _any_ user-facing change?Yes, fix regression!### How was this patch tested?New test added to validate that the `file_path` and `path_name` are returned as encoded strings. 
[SPARK-43285] Fix ReplE2ESuite consistently failing with JDK 17
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The timeout duration for the REPL has been increased from 10 -> 30 seconds (to address slow start on JDK 17 tests) and the semaphore permits are drained after each test (to avoid cascading failures, [context](https://github.com/apache/spark/pull/40675#discussion_r1174917132)). ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The GA JDK 17 tests consistently fails as described in the [jira issue](https://issues.apache.org/jira/browse/SPARK-43285).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Locally verified by installing and running tests with JDK 17 (both the failure and the subsequent fix).
[DRAFT][SPARK-23607][CORE] Use HDFS extended attributes to store application summary information in SHS
[SPARK-43250][SQL] Replace the error class `_LEGACY_ERROR_TEMP_2014` with an internal error
[SPARK-43281][SQL] Fix concurrent writer does not update file metrics
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`DynamicPartitionDataConcurrentWriter` it uses temp file path to get file status after commit task. However, the temp file has already moved to new path during commit task.This pr calls `closeFile` before commit task.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix bug### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, after this pr the metrics is correct### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-43267][JDBC] Handle postgres unknown user-defined column as string in array
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Spark SQL now doesn’t support creating data frame from a Postgres table that contains user-defined array column.This PR support it as string.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Support handle user-defined array column in SPARK SQL with Postgres<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tested in local. Can't find TestSuite for PG.```sqlCREATE DOMAIN not_null_text    AS TEXT    DEFAULT '';create table films(    code         char(5 char)     not null        constraint firstkey            primary key,    title        varchar(40 char) not null,    did          bigint           not null,    date_prod    date,    kind         varchar(10 char),    tz           timestamp with time zone,    int_arr      integer[],    column_name  not_null_text[],    column_name2 not_null_text);INSERT INTO public.films (code, title, did, date_prod, kind, tz, int_arr, column_name, column_name2) VALUES (e'2   ', 'fdas', 1, '2023-04-07 16:05:48', '2', null, null, null, null);INSERT INTO public.films (code, title, did, date_prod, kind, tz, int_arr, column_name, column_name2) VALUES (e'4   ', 'fdsa', 1, '2023-04-07 16:05:48', '4', null, null, null, null);INSERT INTO public.films (code, title, did, date_prod, kind, tz, int_arr, column_name, column_name2) VALUES ('1    ', 'dafsdf', 1, '2023-04-04 14:43:51', '1', '2023-04-25 18:53:17.467000 +00:00', '{1,2,3}', '{1,fds,fdsa}', 'fdasfasdf');```Test Case```scala  test(\"jdbc array\") {    val connectionProperties = new Properties()    connectionProperties.put(\"user\ \"system\")    connectionProperties.put(\"password\ \"system\")    spark.read.jdbc(      url = \"jdbc:postgresql://localhost:54321/test?useSSL=false&serverTimezone=UTC\" +        \"&useUnicode=true&characterEncoding=utf-8\      table = \"TEST.public.films\      connectionProperties    ).show()  }```Result<img width=\"1444\" alt=\"image\" src=\"https://user-images.githubusercontent.com/32387433/234458027-e67e410b-c417-400d-be7e-431768afc0ef.png\"><!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[PYSPARK] [CONNECT] [ML] PySpark UDF supports python package dependencies
[SPARK-42843][SQL] Update the error class _LEGACY_ERROR_TEMP_2007 to REGEX_GROUP_INDEX
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update the error class _LEGACY_ERROR_TEMP_2007 to REGEX_GROUP_INDEX.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue [SPARK-42843](https://issues.apache.org/jira/browse/SPARK-42843). The original name just a number, update it to an informal name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Current tests already covered it.
[SPARK-43292][CORE][CONNECT] Move `ExecutorClassLoader` to `core` module and simplify `Executor#addReplClassLoaderIfNeeded`
[SPARK-43257][SQL] Replace the error class _LEGACY_ERROR_TEMP_2022 by an internal error
[SPARK-43240][SQL][3.3] Fix the wrong result issue when calling df.describe() method. #40914
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Refer [PR#40914](https://github.com/apache/spark/pull/40914)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43511][CONNECT][SS]Implemented MapGroupsWithState and FlatMapGroupsWithState APIs for Spark Connect
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Implemented MapGroupsWithState and FlatMapGroupsWithState APIs for Spark Connect### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To support streaming state APIs in Spark Connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added unit test
[SPARK-43180][PYTHON-INFRA]: Upgrade mypy and pytest-mypypplugins packages
[SPARK-43293][SQL] `__qualified_access_only` should be ignored in normal columns
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/39596 to fix more corner cases. It ignores the special column flag that requires qualified access for normal output attributes, as the flag should be effective only to metadata columns.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It's very hard to make sure that we don't leak the special column flag. Since the bug has been in the Spark release for a while, there may be tables created with CTAS and the table schema contains the special flag.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new analysis test
[SPARK-43294][BUILD] Upgrade zstd-jni to 1.5.5-2
[SPARK-43288][SQL] DataSourceV2: CREATE TABLE LIKE
[SPARK-43296][CONNECT][PYTHON] Migrate Spark Connect session errors into error class
[SPARK-42192][FOLLOWUP][PYTHON] Refine improper error class and error type
[SPARK-43206] [SS] [CONNECT] StreamingQuery exception() include stack trace
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add stack trace to streamingQuery's `exception()` method. Following https://github.com/apache/spark/commit/a5c8a3c976889f33595ac18f82e73e6b9fd29b57#diff-98baf452f0352c75a39f39351c5f9e656675810b6d4cfd178f1b0bae9751495bAdd to both python client and scala client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Including stack trace is helpful in debugging### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual test:Python:```Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Dec  7 2022 01:11:58)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> from pyspark.sql.functions import col, udf>>> from pyspark.errors import StreamingQueryException>>> sdf = spark.readStream.format(\"text\").load(\"python/test_support/sql/streaming\")>>> bad_udf = udf(lambda x: 1 / 0)>>> sq = sdf.select(bad_udf(col(\"value\"))).writeStream.format(\"memory\").queryName(\"this_query\").start()>>> sq.exception()StreamingQueryException('Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-110-19-234.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\  File \"<stdin>\ line 1, in <lambda>\ZeroDivisionError: division by zero\\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:438)\\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1554)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:483)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:422)\\\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:488)\\\tat org.apache.spark.sql.execution.datasources.v2.V...\\JVM stacktrace:\org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-110-19-234.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\  File \"<stdin>\ line 1, in <lambda>\ZeroDivisionError: division by zero\\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:438)\\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1554)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:483)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:422)\\\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:488)\\\tat org.apache.spark.sql.execution.datasources.v2.V...')```2. Scala:```Spark session available as 'spark'.   _____                  __      ______                            __  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/ ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/    /_/@ import org.apache.spark.sql.functions._ import org.apache.spark.sql.functions._@ val sdf = spark.readStream.format(\"text\").load(\"python/test_support/sql/streaming\") sdf: org.apache.spark.sql.package.DataFrame = [value: string]@ val badUdf = udf((x: String) => 1 / 0) badUdf: org.apache.spark.sql.expressions.UserDefinedFunction = ScalarUserDefinedFunction(  ammonite.$sess.cmd2$Helper$$Lambda$1913/745186412@239d9cb7,  ArrayBuffer(StringEncoder),  PrimitiveIntEncoder,  None,  true,  true)@ val sq = sdf.select(badUdf(col(\"value\"))).writeStream.format(\"memory\").queryName(\"this_query\").start() sq: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.streaming.RemoteStreamingQuery@13866865@ sq.isActive res4: Boolean = false@ sq.exception.get.toString res5: String = \"\"\"org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (ip-10-110-19-234.us-west-2.compute.internal executor driver): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.ScalaUDF.f of type scala.Function1 in instance of org.apache.spark.sql.catalyst.expressions.ScalaUDF\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.jav...JVM stacktrace: org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (ip-10-110-19-234.us-west-2.compute.internal executor driver): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.ScalaUDF.f of type scala.Function1 in instance of org.apache.spark.sql.catalyst.expressions.ScalaUDF\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)...```
[SPARK-43298][PYTHON][ML] predict_batch_udf with scalar input fails with batch size of one
[SPARK-43143] [SS] [CONNECT] Scala StreamingQuery awaitTermination()
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the awaitTermination() API to scala client query. Please note that currently it won't throw the exception as it would do in original method. Because the JVM client side error handling is not ready yet. Details in SPARK-43299### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SS Connect development### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes they can use that now### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->```Spark session available as 'spark'.   _____                  __      ______                            __  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/ ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/    /_/@ val q = spark.readStream.format(\"rate\").load().writeStream.format(\"memory\").queryName(\"test\").start() q: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.streaming.RemoteStreamingQuery@34eaf9c1@ q.awaitTermination(500) res1: Boolean = false```
[SPARK-43286][SQL] Updates aes_encrypt CBC mode to generate random IVs
[SPARK-43290][SQL] Adds AES IV and AAD support to ExpressionImplUtils
Test Apache ORC 1.7.9-SNAPSHOT
[SPARK-43301][CORE][SHUFFLE] BlockStoreClient getHostLocalDirs RPC supports IOException retry
[SPARK-43304][CONNECT][PYTHON] Migrate `NotImplementedError` into `PySparkNotImplementedError`
[CORE] Clear the bitmap for tracking free pages when invoking cleanUp…
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Invoke allocatedPages.clear() to reset the bitmap after cleaning up all pages.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The status of free pages in `allocatedPages` mismatch with `pageTable` if don't add this change.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Running UT
[SPARK-43306][PYTHON] Migrate `ValueError` from Spark SQL types into error class
[SPARK-43307][PYTHON] Migrate PandasUDF value errors into error class
[SPARK-43156][SQL][3.4] Fix `COUNT(*) is null` bug in correlated scalar subquery
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Cherry pick fix COUNT(*) is null bug in correlated scalar subquerycherry pick from #40865 and #40946<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix COUNT(*) is null bug in correlated scalar subquery in branch 3.4<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43309][SPARK-38461][CORE] Extend INTERNAL_ERROR with categories and add error class INTERNAL_ERROR_BROADCAST
[SPARK-43308][SQL] Improve scalar subquery logic plan when result are literal
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When use scalar subquery, sometimes we can get result before physical plan execute. Like `select (select (count(1)) is null from r where a = c ) from l where a < 4`.The result alway be false. So we can skip unnecessary aggregate.Only work when subquery without `group by` statement.eg:```sqlcreate or replace temp view l (a, b)as values    (1, 1.0),    (2, 2.0);create or replace temp view r (c, d)as values    (2, 3.0); select a, (select (count(1)) is null from r where a = c) from l where a < 4```The plan before this PR:```== Parsed Logical Plan =='Project ['a, unresolvedalias(scalar-subquery#240 [], None)]:  +- 'Project [unresolvedalias(isnull('count(1)), None)]:     +- 'Filter ('a = 'c):        +- 'UnresolvedRelation [r], [], false+- 'Filter ('a < 4)   +- 'UnresolvedRelation [l], [], false== Analyzed Logical Plan ==a: int, scalarsubquery(a): booleanProject [a#225, scalar-subquery#240 [a#225] AS scalarsubquery(a)#243]:  +- Aggregate [isnull(count(1)) AS (count(1) IS NULL)#242]:     +- Filter (outer(a#225) = c#236):        +- SubqueryAlias r:           +- View (`r`, [c#236,d#237]):              +- Project [_1#231 AS c#236, _2#232 AS d#237]:                 +- LocalRelation [_1#231, _2#232]+- Filter (a#225 < 4)   +- SubqueryAlias l      +- View (`l`, [a#225,b#226])         +- Project [_1#220 AS a#225, _2#221 AS b#226]            +- LocalRelation [_1#220, _2#221]== Optimized Logical Plan ==Project [_1#220 AS a#225, if (isnull(alwaysTrue#246)) false else (count(1) IS NULL)#242 AS scalarsubquery(a)#243]+- Join LeftOuter, (_1#220 = c#236)   :- Project [_1#220]   :  +- Filter (isnotnull(_1#220) AND (_1#220 < 4))   :     +- LocalRelation [_1#220, _2#221]   +- Aggregate [c#236], [false AS (count(1) IS NULL)#242, c#236, true AS alwaysTrue#246]      +- Project [_1#231 AS c#236]         +- Filter ((_1#231 < 4) AND isnotnull(_1#231))            +- LocalRelation [_1#231, _2#232]== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- Project [_1#220 AS a#225, if (isnull(alwaysTrue#246)) false else (count(1) IS NULL)#242 AS scalarsubquery(a)#243]   +- BroadcastHashJoin [_1#220], [c#236], LeftOuter, BuildRight, false      :- Project [_1#220]      :  +- Filter (isnotnull(_1#220) AND (_1#220 < 4))      :     +- LocalTableScan [_1#220, _2#221]      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=120]         +- HashAggregate(keys=[c#236], functions=[], output=[(count(1) IS NULL)#242, c#236, alwaysTrue#246])            +- Exchange hashpartitioning(c#236, 5), ENSURE_REQUIREMENTS, [plan_id=117]               +- HashAggregate(keys=[c#236], functions=[], output=[c#236])                  +- Project [_1#231 AS c#236]                     +- Filter ((_1#231 < 4) AND isnotnull(_1#231))                        +- LocalTableScan [_1#231, _2#232]```The plan after this PR:```== Parsed Logical Plan =='Project ['a, unresolvedalias(scalar-subquery#240 [], None)]:  +- 'Project [unresolvedalias(isnull('count(1)), None)]:     +- 'Filter ('a = 'c):        +- 'UnresolvedRelation [r], [], false+- 'Filter ('a < 4)   +- 'UnresolvedRelation [l], [], false== Analyzed Logical Plan ==a: int, scalarsubquery(a): booleanProject [a#225, scalar-subquery#240 [a#225] AS scalarsubquery(a)#243]:  +- Aggregate [isnull(count(1)) AS (count(1) IS NULL)#242]:     +- Filter (outer(a#225) = c#236):        +- SubqueryAlias r:           +- View (`r`, [c#236,d#237]):              +- Project [_1#231 AS c#236, _2#232 AS d#237]:                 +- LocalRelation [_1#231, _2#232]+- Filter (a#225 < 4)   +- SubqueryAlias l      +- View (`l`, [a#225,b#226])         +- Project [_1#220 AS a#225, _2#221 AS b#226]            +- LocalRelation [_1#220, _2#221]== Optimized Logical Plan ==Project [_1#220 AS a#225, false AS scalarsubquery(a)#243]+- Filter (isnotnull(_1#220) AND (_1#220 < 4))   +- LocalRelation [_1#220, _2#221]== Physical Plan ==*(1) Project [_1#220 AS a#225, false AS scalarsubquery(a)#243]+- *(1) Filter (isnotnull(_1#220) AND (_1#220 < 4))   +- *(1) LocalTableScan [_1#220, _2#221]```<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve logic plan on scalar subquery### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Test local.The current test case is enough<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43136][CONNECT][Followup] Adding tests for KeyAs
[SPARK-43311][SS] Add RocksDB state store provider memory management enhancements
[SPARK-43300][CORE] NonFateSharingCache wrapper for Guava Cache
[SPARK-43312][PROTOBUF] Option to convert Any fields into JSON
[SPARK-43252][SQL] Rename the error class _LEGACY_ERROR_TEMP_2017 to UNRESOLVED_CUSTOM_CLASS
[SPARK-43314][CONNECT][PYTHON] Migrate Spark Connect client errors into error class
[SPARK-43315][CONNECT][PYTHON][SS] Migrate remaining errors from DataFrame(Reader|Writer) into error class
[SPARK-40448][CONNECT][FOLLOWUP] Remove `InputValidationError` and turn into error class.
[SPARK-41971][SQL][PYTHON] Add a config for pandas conversion how to handle struct types
[SPARK-43316][SQL][TESTS] Add more CTE SQL tests
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR borrows CTE SQL test coverage from other databases (Postgres, ZetaSQL, DuckDB).<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?CTE is a hot area in terms of regression and needs more test coverage.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No user-facing change.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?This PR adds SQL tests only.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43317][SQL] Support combine adjacent aggregation
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new rule in `queryStagePreparationRules`. If there have adjacent aggregation with Partial and Final mode, we can combine them to Complete mode, so that we do not need to merge aggregation buffer.For example:```HashAggregate (Final)         HashAggregate (Complete)       |                             |HashAggregate (Partial)    =>    Exchange       |   Exchange```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve performance. According to the benchmark result, it has more benefits with high cardinality case.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test```OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1036-azureIntel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHzCombine adjacent aggregation cardinality: 5:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative---------------------------------------------------------------------------------------------------------------------------no adjacent aggregation                              16383          16539         228          1.2         819.1       1.0Xcombine adjacent aggregation - disable               11224          11345         187          1.8         561.2       1.5Xcombine adjacent aggregation - enable                 9769           9836          82          2.0         488.4       1.7XCombine adjacent aggregation cardinality: 50:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative----------------------------------------------------------------------------------------------------------------------------no adjacent aggregation                                5197           5295         116          3.8         259.8       1.0Xcombine adjacent aggregation - disable                 8514           8811         258          2.3         425.7       0.6Xcombine adjacent aggregation - enable                  8500           8542          38          2.4         425.0       0.6X```
[Spark-42330] Assign name to _LEGACY_ERROR_TEMP_2175: RULE_ID_NOT_FOUND
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Rename _LEGACY_ERROR_TEMP_2175 as RULE_ID_NOT_FOUND- Add a test case for the error class.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We are migrating onto error classes### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the error message will include the error class name### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->`testOnly *RuleIdCollectionSuite` and Github Actions@MaxGekk @itholic 
[SPARK-42769][TEST][FOLLOWUP] Add missing `assert` in integration test
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add missing `assert` in integration test.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The test does not make sense w/o `assert`, it won't fail even executors don't have the env var `SPARK_DRIVER_POD_IP`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->```build/sbt -Pkubernetes -Pvolcano -Pkubernetes-integration-tests \\  -Dtest.exclude.tags=local,r \\  \"kubernetes-integration-tests/testOnly *Suite -- -z SPARK-42769\"``````...[info] KubernetesSuite:[info] - SPARK-42769: All executor pods have SPARK_DRIVER_POD_IP env variable (19 seconds, 554 milliseconds)[info] VolcanoSuite:[info] - SPARK-42769: All executor pods have SPARK_DRIVER_POD_IP env variable (27 seconds, 697 milliseconds)[info] YuniKornSuite:[info] Run completed in 1 minute, 4 seconds.[info] Total number of tests run: 2[info] Suites: completed 3, aborted 0[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.[success] Total time: 102 s (01:42), completed Apr 28, 2023 11:48:26 PM```
[SPARK-43332][CONNECT][PYTHON] Make it possible to extend ChannelBuilder for SparkConnectClient
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds an ability to extend `ChannelBuilder` with extra logic.### Why are the changes needed?So that it's possible to extend GRPC options like in the example below:```pythonfrom pyspark.sql.connect import SparkSession, ChannelBuilderclass CustomChannelBuilder(ChannelBuilder):    ...custom_channel_builder = CustomChannelBuilder(...)spark = SparkSession.builder().channelBuilder(custom_channel_builder).getOrCreate()```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43319][K8S][TEST] Remove usage of deprecated DefaultKubernetesClient
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Migrate from deprecated `DefaultKubernetesClient` to suggested `KubernetesClient`.Note: The fabric8io/kubernetes-client changes rapidly, there are still bunches of deprecated API usages in the codebase, would like to migrate them in separated PRs.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->```/** * Class for Default Kubernetes Client implementing KubernetesClient interface. * It is thread safe. * * @deprecated direct usage should no longer be needed. Please use the {@link KubernetesClientBuilder} instead. */@Deprecatedpublic class DefaultKubernetesClient ...``````public interface StorageAPIGroupDSL extends Client {  /**   * DSL entrypoint for storage.k8s.io/v1 StorageClass   *   * @deprecated Use <code>client.storage().v1().storageClasses()</code> instead   * @return {@link NonNamespaceOperation} for StorageClass   */  @Deprecated  NonNamespaceOperation<StorageClass, StorageClassList, Resource<StorageClass>> storageClasses();  ...}```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43320][SQL][HIVE] Directly call Hive 2.3.9 API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Call Hive 2.3.9 API directly instead of reflection, basically reverts SPARK-37446.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Switch to direct calling to achieve compile time check.Spark does not officially support building against Hive other than 2.3.9, for cases listed in SPARK-37446, it's the vendor's responsibility to port HIVE-21563 into their maintained Hive 2.3.8-[vender-custom-version].See full discussion in https://github.com/apache/spark/pull/40893.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43313][SQL] Adding missing column DEFAULT values for MERGE INSERT actions
[SPARK-43321][Connect] Dataset#Joinwith
[SPARK-43323][SQL][PYTHON] Fix DataFrame.toPandas with Arrow enabled to handle exceptions properly
[SPARK-43325] regexp_extract_all DataFrame API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Supporting the `regexp_extract_all` DataFrame API as well as it supported in the `Spark SQL`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Using this function also in the DataFrame API### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->It tested in the `StringFunctionsSuite`
[SPARK-43327] Trigger `committer.setupJob` before plan execute in `FileFormatWriter#write`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Trigger `committer.setupJob` before plan execute in `FileFormatWriter#write`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In this issue, the case where `outputOrdering` might not work if AQE is enabled has been resolved.https://github.com/apache/spark/pull/38358However, since it materializes the AQE plan in advance (triggers getFinalPhysicalPlan) , it may cause the committer.setupJob(job) to not execute When `AdaptiveSparkPlanExec#getFinalPhysicalPlan()` is executed with an error.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add UT
[SPARK-43328][SS] Add latest timestamp on no-execution trigger for Idle event in streaming query listener
Update ExecutorAllocationManager.scala
[SPARK-43330] FIX typo (StructsToJosn -> StructsToJson)
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->There is a type in [`docs/sql-migration-guide.md`](https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md?plain=1#L154).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->local test
[SPARK-43330][DOCS] FIX typo (StructsToJosn -> StructsToJson)
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->There is a typo in [docs/sql-migration-guide.md](https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md?plain=1#L154).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->local test
[SPARK-43331][CONNECT] Add Spark Connect SparkSession.interruptAll 
[SPARK-43206][SS][CONNECT][FOLLOWUP] Remove unintended change on `StreamingQueryManager.scala`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->https://github.com/apache/spark/pull/40966 introduced a unneeded change in `StreamingQueryManager` by error. This fix removes it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
[SPARK-43205][SQL] IDENTIFIER clause
[SPARK-432265][CORE][FOLLOW-UP] Add Mima excludes of ErrorInfo and ErrorSubInfo for Scala 2.13
[SPARK-43270][PYTHON][CONNECT][FOLLOW-UP] Implement `__dir__` in PySpark Connect DataFrame
[SPARK-43336][SQL] Casting between Timestamp and TimestampNTZ requires timezone
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Casting between Timestamp and TimestampNTZ requires a timezone since the timezone id is used in the evaluation.This PR updates the method `Cast.needsTimeZone` to include the conversion between Timestamp and TimestampNTZ. As a result:* Casting between Timestamp and TimestampNTZ is considered as unresolved unless the timezone is defined* Canonicalizing cast will include the time zone### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Timezone id is used in the evaluation between Timestamp and TimestampNTZ, thus we should mark such conversion as \"needsTimeZone\"### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No. It is more like a fix for potential bugs that the casting between Timestamp and TimestampNTZ is marked as resolved before resolving the timezone.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
[SPARK-43337][UI][3.4] Asc/desc arrow icons for sorting column does not get displayed in the table column
[SPARK-43337][UI][3.3] Asc/desc arrow icons for sorting column does not get displayed in the table column
[SPARK-43509][CONNECT] Support Creating multiple Spark Connect sessions
[SPARK-42945][CONNECT][FOLLOW-UP] Add user_id and session_id when logging errors
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a follow-up PR for SPARK-42945 to include `user_id` and `session_id` when logging errors on the Spark Connect server side. It is to address [this comment](https://github.com/apache/spark/pull/40575#discussion_r1150223876).### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To improve debuggability.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[MINOR][CONNECT][DOC] Add information on how to regenerate proto for python client
[SPARK-43341][SQL] Patch StructType.toDDL not picking up on non-nullability of nested column
[SPARK-43334] [UI] Fix error while serializing ExecutorPeakMetricsDistributions into API response
[SPARK-37942][CORE][SQL] Migrate error classes
[SPARK-43344][BUILD] Upgrade `mlflow` to 2.3.1
[SPARK-43345][SPARK-43346][SQL] Rename the error classes _LEGACY_ERROR_TEMP_[0041|1206]
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign the proper names to the following `_LEGACY_ERROR_TEMP*` error classes:* `_LEGACY_ERROR_TEMP_0041` -> `DUPLICATE_CLAUSES`* `_LEGACY_ERROR_TEMP_1206` -> `COLUMN_NOT_DEFINED_IN_TABLE`### Why are the changes needed?Proper name improves user experience w/ Spark SQL.### Does this PR introduce _any_ user-facing change?Yes, the PR changes an user-facing error message.### How was this patch tested?By running modified test suties.
[SPARK-16484][SQL] Use 8-bit registers for representing DataSketches
[SPARK-43343][SS] FileStreamSource should disable an extra file glob check when creating DataSource
[SPARK-43347][PYTHON] Remove Python 3.7 Support
[SPARK-43348][PYTHON] Support `Python 3.8` in PyPy3
[SPARK-43349][PS][TEST] Fix flaky test for `DataFrame` creation
[SPARK-43132] [SS] [CONNECT] Python Client DataStreamWriter foreach() API
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the `foreach()` API in `DataStreamWriter` to python client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of SS CONNECT development.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests, added parity test file
[WIP] Nested ArrayType, MapType support in Arrow-optimized UDFs
[SPARK-43324][SQL] Handle UPDATE commands for delta-based sources
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds `RewriteUpdateTable`, similar to `RewriteDeleteFromTable`, to handle UPDATE commands for delta-based sources. Support for group-based sources will come in a follow-up PR.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed per SPIP SPARK-35801.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests. There are more tests in `AlignUpdateAssignmentsSuite`, which was merged earlier.
[SPARK-43350][BUILD] Upgrade `scalafmt` to 3.7.3
[SPARK-43352][K8S][TEST] Inline `DepsTestsSuite#setPythonSparkConfProperties`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Inline `DepsTestsSuite#setPythonSparkConfProperties`, which was introduced in SPARK-27936.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The `setPythonSparkConfProperties` is only invoked in one place, doesn't look helping the readability much. Besides, it should use `conf` instead of `sparkAppConf`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No. Since the caller always passes the `sparkAppConf`, this bug affects nothing actually.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43353][PYTHON] Migrate remaining session errors into error class
[SPARK-43057][FOLLOWUP][CONNECT][PYTHON] Add & refine errors
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is follow-up for https://github.com/apache/spark/pull/40694 to redeem some errors.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make errors clear### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass
Update bufbuild plugin references
[SPARK-43355][K8S][BUILD] Upgrade `kubernetes-client` to 6.6.0
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The release notes are available athttps://github.com/fabric8io/kubernetes-client/releases/tag/v6.6.0### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It's basically a routine work, to keep the third-party libs up-to-date.And https://github.com/fabric8io/kubernetes-client/pull/5073 simplifies SPARK-43356### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43357][SQL] Filter date type quote date
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this PR the date value is quoted before being sent to the hive metastore### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Glue for example expects the dates to be quoted, without quotes we get errors.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Changed the result of unit tests, also tested by running the fix against glue
[SPARK-43351] [CONNECT] Add Spark Connect Go prototype code and example
[SPARK-43032] [SS] [CONNECT] Python SQM bug fix
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Some bug fix for streaming ***connect*** python SQMNote that I also changed ***non-connect***'s StreamingQueryManager `get()` API to return an `Optional[StreamingQuery]`.Before it looks like this when you get a non-exist query:```>>> a = spark.streams.get(\"00000000-0000-0001-0000-000000000001\") >>> a<pyspark.sql.streaming.query.StreamingQuery object at 0x7f86465702b0>>>> a.idTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/home/wei.liu/oss-spark/python/pyspark/sql/streaming/query.py\ line 78, in id    return self._jsq.id().toString()AttributeError: 'NoneType' object has no attribute 'id'```But now it looks like:```>>> a = spark.streams.get(\"00000000-0000-0001-0000-000000000001\") >>> a.idTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>AttributeError: 'NoneType' object has no attribute 'id'```The only difference is the return type, which is not typically honored in Python... But not very sure if that's a breaking change### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested. Also verified that it won't throw even without this fix so it's not that urgent
[SPARK-43386][SQL] Improve list of suggested column/attributes in `UNRESOLVED_COLUMN.WITH_SUGGESTION` error class
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this change we determine whether unresolved identifier is multipart or not and remap list of suggested columns to fit the same pattern.- Main change is in `StringUtils.scala`. The rest is caused by method rename and test fixes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When determining a list of suggested columns/attributes for `UNRESOLVED_COLUMN.WITH_SUGGESTION` we sort by Levenshtein distance between unresolved identifier and list of fully qualified column names from target table. In case of a single-part identifier this might lead to poor experience especially for short(ish) identifiers, e.g. `a` and table with columns `m` and `aa` in default spark catalog => suggested list will be (`spark_catalog.default.m`, `spark_catalog.default.aa`) ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, since we don't document internals of how we generate suggestion list for this error.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[SPARK-43360][SS][CONNECT] Scala client StreamingQueryManager
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add support for scala client `StreamingQueryManager`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Development of scala connect client### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes they can use SQM in scala client### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested, also unit test
[SPARK-43362][SQL] Special handling of JSON type for MySQL connector
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->MySQL JSON type is converted into JDBC VARCHAR type with precision of -1 on some MariaDB drivers.When receiving VARCHAR with negative precision, Spark will throw an error.This PR special cases this scenario by directly converting JSON type into StringType in MySQLDialect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Enable reading MySQL tables that has a JSON column.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Update existing integration test
[SPARK-43363][SQL][PYTHON] Make to call `astype` to the category type only when the arrow type is not provided
[SPARK-43364][SS][DOCS] Add docs for RocksDB state store memory management
[SPARK-43374][INFRA] Move protobuf-java to BSD 3-clause group and update the license copy
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->protobuf-java is licensed under the BSD 3-clause, not the 2 we claimed.And the copy should be updated via https://github.com/protocolbuffers/protobuf/commit/9e080f7ac007b75dacbd233b214e5c0cb2e48e0f### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix license issue### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[SPARK-43375][CONNECT] Improve the error messages for INVALID_CONNECT_URL
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR updates the error messages for the INVALID_CONNECT_URL error.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error messages more user-friendly.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[SPARK-42843][SQL][FOLLOWUP] Remove the old error class _LEGACY_ERROR_TEMP_2007
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove the old error class _LEGACY_ERROR_TEMP_2007 since new one is already created.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To fix JIRA issue 42843 completely.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Current tests covered it.
[SPARK-43376][SQL] Improve reuse subquery with table cache
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->AQE can not reuse subquery if it is pushed into `InMemoryTableScan`. There are two issues:- `ReuseAdaptiveSubquery` can not support reuse subquery if two subquery have the same exprId-  `InMemoryTableScan` miss apply `ReuseAdaptiveSubquery` when wrap `TableCacheQueryStageExec`For example:```Seq(1).toDF(\"c1\").cache().createOrReplaceTempView(\"t1\")Seq(2).toDF(\"c2\").createOrReplaceTempView(\"t2\")spark.sql(\"SELECT * FROM t1 WHERE c1 < (SELECT c2 FROM t2)\")```There are two `subquery#27` but have no `ReusedSubquery` ```AdaptiveSparkPlan isFinalPlan=true+- == Final Plan ==   *(1) Filter (c1#14 < Subquery subquery#27, [id=#20])   :  +- Subquery subquery#27, [id=#20]   :     +- AdaptiveSparkPlan isFinalPlan=true   :        +- LocalTableScan [c2#25]   +- TableCacheQueryStage 0      +- InMemoryTableScan [c1#14], [(c1#14 < Subquery subquery#27, [id=#20])]            :- InMemoryRelation [c1#14], StorageLevel(disk, memory, deserialized, 1 replicas)            :     +- LocalTableScan [c1#14]            +- Subquery subquery#27, [id=#20]               +- AdaptiveSparkPlan isFinalPlan=true                  +- LocalTableScan [c2#25]```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve the coverage of reuse subquery.Note that, it is not a real perf issue because the subquery has been already reused (the same Java object). This pr just makes the plan clearer about subquery reuse.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-43377][SQL] Enable `spark.sql.thriftServer.interruptOnCancel` by default
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr enables `spark.sql.thriftServer.interruptOnCancel` by default### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Address the comment https://github.com/apache/spark/pull/30481#discussion_r1181684437### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI
[SPARK-38462][CORE] Add error class INTERNAL_ERROR_EXECUTOR
[SPARK-43378][CORE] Properly close stream objects in deserializeFromChunkedBuffer
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Fixes a that SerializerHelper.deserializeFromChunkedBuffer does not call close on the deserialization stream. For some serializers like Kryo this creates a performance regressions as the kryo instances are not returned to the pool.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?This causes a performance regression in Spark 3.4.0 for some workloads.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Existing tests.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43340][CORE] Handle missing stack-trace field in eventlogs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR fixes a regression introduced by #36885 which broke JsonProtocol's ability to handle missing fields from exception field. old eventlogs missing a `Stack Trace` will raise a NPE.  As a result, SHS misinterprets  failed-jobs/SQLs as `Active/Incomplete` This PR solves this problem by checking the JsonNode for null. If it is null, an empty array of `StackTraceElements`### Why are the changes needed?Fix a case which prevents the history server from identifying failed jobs if the stacktrace was not set.Example eventlog```{   \"Event\":\"SparkListenerJobEnd\   \"Job ID\":31,   \"Completion Time\":1616171909785,   \"Job Result\":{      \"Result\":\"JobFailed\      \"Exception\":{         \"Message\":\"Job aborted\"      }   }} ```**Original behavior:**The job is marked as `incomplete`Error from the SHS logs:```23/05/01 21:57:16 INFO FsHistoryProvider: Parsing file:/tmp/nds_q86_fail_test to re-build UI...23/05/01 21:57:17 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/tmp/nds_q86_fail_testjava.lang.NullPointerException    at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractElements(JsonProtocol.scala:1589)    at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1558)    at org.apache.spark.util.JsonProtocol$.exceptionFromJson(JsonProtocol.scala:1569)    at org.apache.spark.util.JsonProtocol$.jobResultFromJson(JsonProtocol.scala:1423)    at org.apache.spark.util.JsonProtocol$.jobEndFromJson(JsonProtocol.scala:967)    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:878)    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)....23/05/01 21:57:17 ERROR ReplayListenerBus: Malformed line #24368: {\"Event\":\"SparkListenerJobEnd\\"Job ID\":31,\"Completion Time\":1616171909785,\"Job Result\":{\"Result\":\"JobFailed\\"Exception\":{\"Message\":\"Job aborted\"}}}```**After the fix:**Job 31 is marked as `failedJob`### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added new unit test in JsonProtocolSuite.
[SPARK-43379][DOCS] Deprecate old Java 8 versions prior to 8u371
[SPARK-43380][SQL] Fix Avro data type conversion issues to avoid producing incorrect results
[SPARK-43412][PYTHON][CONNECT] Introduce `SQL_ARROW_BATCHED_UDF` EvalType for Arrow-optimized Python UDFs
[SPARK-43284][SQL][FOLLOWUP] Return URI encoded path, and add a test
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Return URI encoded path, and add a test### Why are the changes needed?Fix regression in spark 3.4.### Does this PR introduce _any_ user-facing change?Yes, fixes a regression in `_metadata.file_path`.### How was this patch tested?New explicit test.
[SPARK-43381][CONNECT] Make 'transformStatCov' lazy
[SPARK-43346][SQL] Change error class _LEGACY_ERROR_TEMP_1206 to COLUMN_NOT_DEFINED
[SPARK-43342][K8S] Support driver and executors to shared same Kubernetes PVC 
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR reverts [SPARK-39006](https://github.com/apache/spark/pull/36374) and add a case of sharing the same PVC between the driver and multiple executors in PV testing of integration testing.  Some PV types (such as NFS) support data sharing via the same PVC for multiple Pods. However, SPARK-39006 broke this scenario, causing multiple Pods unable to share the same PVC.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is regression caused by [SPARK-39006](https://github.com/apache/spark/pull/36374).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests, and added test scenarios
[SPARK-43040][SQL][FOLLOWUP] Avoid duplicated conversion
[SPARK-42842][SQL] Merge the error class _LEGACY_ERROR_TEMP_2006 into REGEX_GROUP_INDEX
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Merge the error class _LEGACY_ERROR_TEMP_2006 into REGEX_GROUP_INDEX.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue [SPARK-42842](https://issues.apache.org/jira/browse/SPARK-42842). The original name just a number, update it to an informal name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Current tests covered it.
[SPARK-43313][SQL][FOLLOWUP] Improvement for DSv2 API SupportsCustomSchemaWrite
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Change `SupportsCustomSchemaWrite` to extend `Table`- Replace `StructType` w/ `Column[]`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-42398 introduces `Column`, which should be used in the new DSv2 API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, unreleased feature### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT is changed as well.
[SPARK-43405][SQL] Remove useless `getRowFormatSQL`, `inputRowFormatSQL`, `outputRowFormatSQL` method
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Remove useless code in class `ScriptInputOutputSchema`. Include `getRowFormatSQL`, `inputRowFormatSQL`, `outputRowFormatSQL` method. It unused when #16869<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Clear code.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43383][SQL] Add `rowCount` statistics to LocalRelation
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to add statistics rowCount for `LocalRelation`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Previously, statistics in `LocalRelation` were missing rowCount.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new UT
[SPARK-43384][SQL] Make `df.show` print a nice string for `MapType`.
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to make `df.show` print a nice string for `MapType`.Let's say have an example like this:```scalaspark.sql(\"SELECT map(1,1.1, 2,2.2) AS col\").show(false)```Before, it print```log+--------------------+|col                 |+--------------------+|{1 -> 1.1, 2 -> 2.2}|+--------------------+```Now, it prints as follows, that's consistent with `spark-sql` CLI.```log+-------------+|col          |+-------------+|{1:1.1,2:2.2}|+-------------+```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make `df.show` print a nice string for `MapType`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, They will face better nice strings representation for MapType.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Exist tests.
[SPARK-43385][SQL] The Generator's statistics should be ratio times greater than the child nodes
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Generator is an expression that produces zero or more rows given a single input row. If UserDefinedGenerator and HiveUDTF were used, the output could be N times that of the child node, resulting in a statistical error.Because of incorrect statistics, Spark may select an incorrect execution plan. For example, if `BroadcastHashJoinExec` is selected, the Job maybe fails to broadcast buildSide.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The `Generator`'s statistics should be ratio times greater than the child nodes. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, Added a configuration to determine that the udtf output data is ratio times the input data### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add UT.
[SPARK-43496][KUBERNETES] Add configuration for pod memory limits
[SPARK-43387][SQL] Provide a human readable error code for _LEGACY_ERROR_TEMP_1168.
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change the error code of `_LEGACY_ERROR_TEMP_1168` -> `INSERT_COLUMN_ARITY_MISMATCH`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Cleaning up legacy error codes### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes - changes error code for legacy error.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests suffice
[SPARK-43342][K8S] Revert SPARK-39006 Show a directional error message for executor PVC dynamic allocation failure
[SPARK-43390][SQL] DSv2 allows CTAS/RTAS to reserve schema nullability
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new method `useNullableQuerySchema` in `Table`, to allow the DataSource implementation to declare whether they need to reserve schema nullability on CTAS/RTAS.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-28837 forcibly uses the nullable schema on CTAS/RTAS, which seems too aggressive:1. The existing matured RDBMSs have different behaviors for reserving schema nullability on CTAS/RTAS, as mentioned in #25536, PostgreSQL forcibly uses the nullable schema, but MySQL respects the query's output schema nullability.2. Some OLAP systems(e.g. ClickHouse) are perf-sensitive for nullable, and have strict restrictions on table schema, e.g. the primary keys are not allowed to be nullable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this PR adds a new DSv2 API, but the default implementation reserves backward compatibility.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UTs are updated.
[SPARK-43391][CORE] Idle connection should be kept when closeIdleConnection is disabled
[SPARK-43393][SQL] Address sequence expression overflow bug.
[SPARK-43394][BUILD] Upgrade maven to 3.8.8
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Upgrade Maven from 3.8.7 to 3.8.8.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Maven 3.8.8 is the latest patched version of 3.8.xhttps://maven.apache.org/docs/3.8.8/release-notes.html### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43395][BUILD] Exclude macOS tar extended metadata in make-distribution.sh
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add args `--no-mac-metadata --no-xattrs --no-fflags` to `tar` on macOS in `dev/make-distribution.sh` to exclude macOS-specific extended metadata.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The binary tarball created on macOS includes extended macOS-specific metadata and xattrs, which causes warnings when unarchiving it on Linux.Step to reproduce1. create tarball on macOS (13.3.1)```➜  apache-spark git:(master) tar --versionbsdtar 3.5.3 - libarchive 3.5.3 zlib/1.2.11 liblzma/5.0.5 bz2lib/1.0.8``````➜  apache-spark git:(master) dev/make-distribution.sh --tgz```2. unarchive the binary tarball on Linux (CentOS-7)```➜  ~ tar --versiontar (GNU tar) 1.26Copyright (C) 2011 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Written by John Gilmore and Jay Fenlason.``````➜  ~ tar -xzf spark-3.5.0-SNAPSHOT-bin-3.3.5.tgztar: Ignoring unknown extended header keyword `SCHILY.fflags'tar: Ignoring unknown extended header keyword `LIBARCHIVE.xattr.com.apple.FinderInfo'```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, dev only.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Create binary tarball on macOS then unarchive on Linux, warnings disappear after this change.
[SPARK-43361][PROTOBUF] spark-protobuf: allow serde with enum as ints
[SPARK-43396][CORE] Add config to control max ratio of decommissioning executors
[SPARK-43397][CORE] Log executor decommission duration in `executorLost` method
[SPARK-39280][SQL] Speed up Timestamp type inference with user-provided format in JSON/CSV data source
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Follow up #36562 , performance improvement when Timestamp type inference with user-provided format.In the current implementation of CSV/JSON data source, the schema inference with user-provided format relies on methods that will throw exceptions if the fields can't convert as some data types . Throwing and catching exceptions can be slow. We can improve it by creating methods that return optional results instead.The optimization of `DefaultTimestampFormatter` has been implemented in #36562 , this PR adds the optimization of user-provided format. The basic logic is to prevent the formatter from throwing exceptions, and then use catch to determine whether the parsing is successful.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Performance improvement when Timestamp type inference with user-provided format.When use JSON datasource, the speed up `88%`. CSV datasource speed also up, but not obvious.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43347][PYTHON][FOLLOWUP] Change black min target-version to py38
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change `black` min `target-version` to py38### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Spark drops support for Python 3.7.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[WIP] Initial go client
[MINOR][INFRA] Correct the GitHub PR label for DSTREAM
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Correct the GitHub PR label for DSTREAM.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-38569 renamed the folder from external to connector.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual review.
[SPARK-43398][CORE] Executor timeout should be max of idle shuffle and rdd timeout
[SPARK-43399][CORE] Add config to control threshold of unregister map ouput when fetch failed
[MINOR][PYTHON] Remove deprecated use of typing.io
[SPARK-43348][PYTHON][TESTS][FOLLOW-UP] Skip the day-time-interval test only with PyPy 3.8
[MINOR][PYTHON] Fix MyPy linter failure
[SPARK-43401][CONNECT][BUILD] Upgrade buf to v1.18.0
[SPARK-43402][SQL] FileSourceScanExec supports push down data filter with scalar subquery
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Scalar subquery can be pushed down as data filter at runtime, since we always execute subquery first. Ideally, we can rewrite `ScalarSubquery` to `Literal` before pushing down filter. The main issue before we do not support that is `ReuseSubquery` is ineffective, see https://github.com/apache/spark/pull/22518. It is not a issue now.For example:```sqlSELECT * FROM t1 WHERE c1 > (SELECT min(c2) FROM t2)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve peformance if data filter have scalar subquery.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-43404][SS] Skip reusing sst file for same version of RocksDB state store to avoid id mismatch error
[SPARK-43406][SQL] enable spark sql to drop multiple partitions in on…
[SPARK-39281][SQL] Speed up Timestamp type inference with legacy format  in JSON/CSV data source
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Follow up https://github.com/apache/spark/pull/36562 , performance improvement when Timestamp type inference with legacy format.In the current implementation of CSV/JSON data source, the schema inference with legacy format relies on methods that will throw exceptions if the fields can't convert as some data types .Throwing and catching exceptions can be slow. We can improve it by creating methods that return optional results instead.The optimization of DefaultTimestampFormatter has been implemented in https://github.com/apache/spark/pull/36562 , this PR adds the optimization of legacy format. The basic logic is to prevent the formatter from throwing exceptions, and then use catch to determine whether the parsing is successful.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Performance improvement when Timestamp type inference with legacy format.When use JSON datasource, the speed up `67%`. CSV datasource speed also up, but not obvious.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43410][SQL] Improve vectorized loop for Packed skipValues
Update build_and_test.yml
[SPARK-43413][SQL] Fix IN subquery ListQuery nullability
[SPARK-43414][TESTS] Fix flakiness in Kafka RDD suites due to port binding configuration issue
[SPARK-42941][SS][CONNECT][DRAFT][DO-NOT-REVIEW] Python StreamingQueryListener POC
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add streaming query listener for python client. In this version, the code runs on the driver.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation development of streaming connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->For now manually:client:```Using Python version 3.9.16 (main, Dec  7 2022 01:11:58)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> from pyspark.sql.connect.streaming.listener import QueryStartedEvent;from pyspark.sql.connect.streaming.listener import StreamingQueryListener;from pyspark.sql.streaming.listener import (QueryProgressEvent, QueryTerminatedEvent, QueryIdleEvent)>>> class MyListener(StreamingQueryListener):...     def onQueryStarted(self, event: QueryStartedEvent) -> None:...             print(\"hi, event query id is: \" +  event.id)...     def onQueryProgress(self, event: QueryProgressEvent) -> None:...             pass...     def onQueryTerminated(self, event: QueryTerminatedEvent) -> None:...             pass...     def onQueryIdle(self, event: QueryIdleEvent) -> None:...             pass... >>> spark.streams.addListener(MyListener())>>> q = spark.readStream.format(\"rate\").load().writeStream.format(\"console\").start()>>> q.stop()```server(driver):```23/05/08 21:27:49 INFO MicroBatchExecution: Starting [id = 6042353a-dc77-436b-9ee5-d4d0653ec0a2, runId = 269288ae-3463-408c-bbb3-a72934a4bc1d]. Use file:/tmp/temporary-9ec9765b-b6d8-427e-bfe9-fa758b9f87b7 to store the query checkpoint.##### Python out for query start event is: out=###### Start running onQueryStarted ######hi, event query id is: 6042353a-dc77-436b-9ee5-d4d0653ec0a2##### Python error for query start event is: error=##### End processing query start event exitCode=0```
[SPARK-43418][CONNECT] Add SparkSession.Builder.getOrCreate
[SPARK-42668][SS][FOLLOW-UP] Do not add file scheme to the Windows path
[SPARK-43421][SS] Implement Changelog based Checkpointing for RocksDB State Store Provider
[SPARK-43420][SQL] Make DisableUnnecessaryBucketedScan smart with table cache
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->If a bucket scan has no interesting partition or contains shuffle exchange, then we would disable bucket scan. But If the bucket scan is inside table cache, the cached plan would be accessed multi-times, then we should not disable bucket scan as the bucket scan could preserve output partitioning and more likely be reused.This pr insert `TableCache` at the top of cached plan, so that it can be aware of whether it's inside table cache or not.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make `DisableUnnecessaryBucketedScan` smart with table cache.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
[SPARK-43423][PYTHON][ML][TESTS] Retry when `test_gmm` fails
[SPARK-43424][SQL] Support vanila JDBC CHAR/VARCHAR through STS
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Add getColumnDisplaySize API support for varchar & char- make data type mapping consistent between SQL and MetaOperation(GetColumns)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->better JDBC API support### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, java.sql.ResultSetMetaData.getColumnDisplaySize now offers the exact max length for char varchar instead of Int.Max### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
[SPARK-43425][SQL] Add `TimestampNTZType` to `ColumnarBatchRow`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Noticed this one was missing when implementing `TimestampNTZType` in Iceberg.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Able to read `TimestampNTZType` using the `ColumnarBatchRow`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I checked for `TestColumnarBatchRow.java` but couldn't find it. So I figured that this would be okay.
[SPARK-43422][SQL] Keep tags on LogicalRelation with metadata output
[SPARK-43403][UI] Ensure old SparkUI in HistoryServer has been detached before loading new one
[WIP]IN expression behavior is inconsistent with equalTo expression
[SPARK-42585][CONNECT][FOLLOWUP] Store cached local relations as proto
[SPARK-43427] spark protobuf: modify serde behavior of unsigned integer types
https://issues.apache.org/jira/browse/SPARK-43427<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?**Explanation**Protobuf supports unsigned integer types, including `uint32` and `uint64`. When deserializing protobuf values with fields of these types, the `from_protobuf` library currently transforms them to the spark type of:uint32 => `IntegerType`uint64 => `LongType``IntegerType` and `LongType` are [signed](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) integer types, so this can lead to confusing results. Namely, if a uint32 value in a stored proto is above 2^31 or a uint64 value is above 2^63, their representation in binary will contain a 1 in the highest bit, which when interpreted as a signed integer will come out as negative (I.e. overflow).I propose that we deserialize unsigned integer types into a type that can contain them correctly, e.g.uint32 => `LongType`uint64 => `Decimal(20, 0)`**Precedent**I believe that unsigned integer types in **parquet** are deserialized in a similar manner, i.e. put into a larger type so that the unsigned representation natively fits. https://issues.apache.org/jira/browse/SPARK-34817 and https://github.com/apache/spark/pull/31921** Example to reproduce **Consider a protobuf message like:```syntax = \"proto3\";message Test {  uint64 val = 1;}```Generate a protobuf with a value above 2^63. I did this in python with a small script like:```import test_pb2s = test_pb2.Test()s.val = 9223372036854775809 # 2**63 + 1serialized = s.SerializeToString()print(serialized)```This generates the binary representation:```b'\\x08\\x81\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x01'```Then, deserialize this using `from_protobuf`. I did this in a notebook so its easier to see, but could reproduce in a scala test as well:<img width=\"597\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1002986/a6c58c19-b9d3-44d4-8c2a-605991d3d5de\">**Backwards Compatibility / Default Behavior****Should we maintain backwards compatibility and add an option that allows deserializing these types differently? OR should we change change the default behavior (with an option to go back to the old way)? Would love some thoughts here!**I think by default it makes more sense to deserialize them as the larger types so that it's semantically more correct. However, there may be existing users of this library that would be affected by this behavior change. Though, maybe we can justify the change since the function is tagged as `Experimental` (and spark 3.4.0 was only released very recently).### Why are the changes needed?Improve unsigned integer deserialization behavior.### Does this PR introduce _any_ user-facing change?Yes, as written it would change the deserialization behavior of unsigned integer field types. However, please see the above section about whether we should or should not maintain backwards compatibility.### How was this patch tested?Unit Testing
[SPARK-43428] Move some class utils to common/utils
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR moves some commonly used class loader/reflection utils to common/utils.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Reduce the required dependency on Spark core.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-43430][CONNECT][PROTO] ExecutePlanRequest supports arbitrary request options.
[SPARK-39420][SQL] Support `ANALYZE TABLE` on Datasource V2 tables
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Support ANALYZE TABLE on v2 tables.Through this PR, users can use the `ANALYZE TABLE` statement on Datasourcev2 to analyze the table. Since the API of Datasourcev2 does not support the `NOSCAN` and `PARTITION` features, currently using the Analyze table with `PARTITION`, `COLUMNS` and `NOSCAN` statements will report an error.The statistics obtained through the analysis will be stored in the `SessionState` to be used by statements of the `DESC EXTENDED`. In the future, the data in the `SessionState` can be provided to `DataSourceV2Relation` to reduce repeated statistics.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?`ANALYZE TABLE` syntax for aligning Datasourcev1 and v2<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43425][SQL][3.4] Add `TimestampNTZType` to `ColumnarBatchRow`
[SPARK-43400][SQL] Add Primary Key syntax support
[SPARK-43434][CONNECT][PYTHON][TESTS] Disable flaky doctest `pyspark.sql.connect.dataframe.DataFrame.writeStream`
Initial go client workflow
Initial go client workflow v2
Initial go client workflow v3
Initial go client workflow v4
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?How to support more subexpressions elimination cases* Get all common expressions from input expressions of the current physical operator to current CodeGenContext. Recursively visits all subexpressions regardless of whether the current expression is a conditional expression.* For each common expression:  * Add a new boolean variable subExprInit to indicate whether it has  already been evaluated.   * Add a new code block in CodeGenSupport trait, and reset those subExprInit variables to false before the physical operators begin to evaluate the input row.  * Add a new wrapper subExpr function for each common subexpression.```private void subExpr_n(${argList}) { if (!subExprInit_n) {   ${eval.code}   subExprInit_n = true;   subExprIsNull_n = ${eval.isNull};   subExprValue_n = ${eval.value}; }}```  * When generating the input expression code,  if the input expression is a common expression, the expression code will be replaced with the corresponding subExpr function. When the subExpr function is called for the first time, subExprInit will be set to true, and the subsequent function calls will do nothing.### Why are the changes needed?Support more subexpression elimination cases, improve performance.For example, TPCDS q23b query,  we can reuse the result of projection `sum(ss_quantity * ss_sales_price)` in the if expressions:```if (isnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])))   input[0, decimal(28,2), true]else    (input[0, decimal(28,2), true] + cast(knownnotnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])) as decimal(28,2)))```q4, q62, q67 are similar to the above.| Query    | Time with PR | Time without PR | Time diff | Percentage ||----------|--------------|-----------------|-----------|------------|| q1.sql   | 36.97        | 36.623          | -0.347    | 99.06%     || q2.sql   | 42.699       | 41.741          | -0.958    | 97.76%     || q3.sql   | 6.038        | 6.111           | 0.073     | 101.21%    || q4.sql   | 273.849      | 323.799         | 49.95     | 118.24%    || q5.sql   | 76.202       | 76.353          | 0.151     | 100.20%    || q6.sql   | 8.451        | 9.011           | 0.56      | 106.63%    || q7.sql   | 13.017       | 12.954          | -0.063    | 99.52%     || q8.sql   | 10.22        | 11.027          | 0.807     | 107.90%    || q9.sql   | 72.843       | 72.019          | -0.824    | 98.87%     || q10.sql  | 13.233       | 14.028          | 0.795     | 106.01%    || q11.sql  | 112.252      | 112.983         | 0.731     | 100.65%    || q12.sql  | 3.036        | 3.488           | 0.452     | 114.89%    || q13.sql  | 12.471       | 12.911          | 0.44      | 103.53%    || q14a.sql | 210.201      | 220.12          | 9.919     | 104.72%    || q14b.sql | 185.374      | 187.57          | 2.196     | 101.18%    || q15.sql  | 10.189       | 10.338          | 0.149     | 101.46%    || q16.sql  | 80.756       | 82.503          | 1.747     | 102.16%    || q17.sql  | 28.523       | 28.567          | 0.044     | 100.15%    || q18.sql  | 13.417       | 14.271          | 0.854     | 106.37%    || q19.sql  | 6.366        | 6.53            | 0.164     | 102.58%    || q20.sql  | 3.427        | 4.939           | 1.512     | 144.12%    || q21.sql  | 2.096        | 2.16            | 0.064     | 103.05%    || q22.sql  | 14.4         | 14.01           | -0.39     | 97.29%     || q23a.sql | 507.253      | 545.185         | 37.932    | 107.48%    || q23b.sql | 707.054      | 768.148         | 61.094    | 108.64%    || q24a.sql | 193.116      | 193.793         | 0.677     | 100.35%    || q24b.sql | 177.109      | 179.54          | 2.431     | 101.37%    || q25.sql  | 22.264       | 22.949          | 0.685     | 103.08%    || q26.sql  | 8.68         | 8.973           | 0.293     | 103.38%    || q27.sql  | 8.535        | 8.558           | 0.023     | 100.27%    || q28.sql  | 101.953      | 102.713         | 0.76      | 100.75%    || q29.sql  | 75.392       | 76.211          | 0.819     | 101.09%    || q30.sql  | 12.265       | 13.508          | 1.243     | 110.13%    || q31.sql  | 26.477       | 26.965          | 0.488     | 101.84%    || q32.sql  | 3.393        | 3.507           | 0.114     | 103.36%    || q33.sql  | 6.909        | 7.277           | 0.368     | 105.33%    || q34.sql  | 8.41         | 8.572           | 0.162     | 101.93%    || q35.sql  | 34.214       | 36.822          | 2.608     | 107.62%    || q36.sql  | 9.027        | 9.79            | 0.763     | 108.45%    || q37.sql  | 36.076       | 36.753          | 0.677     | 101.88%    || q38.sql  | 71.768       | 74.473          | 2.705     | 103.77%    || q39a.sql | 7.753        | 7.617           | -0.136    | 98.25%     || q39b.sql | 6.365        | 7.229           | 0.864     | 113.57%    || q40.sql  | 16.588       | 17.164          | 0.576     | 103.47%    || q41.sql  | 1.162        | 1.188           | 0.026     | 102.24%    || q42.sql  | 2.3          | 2.561           | 0.261     | 111.35%    || q43.sql  | 7.407        | 7.605           | 0.198     | 102.67%    || q44.sql  | 28.939       | 30.473          | 1.534     | 105.30%    || q45.sql  | 9.796        | 9.634           | -0.162    | 98.35%     || q46.sql  | 9.496        | 9.692           | 0.196     | 102.06%    || q47.sql  | 27.087       | 27.151          | 0.064     | 100.24%    || q48.sql  | 14.524       | 14.889          | 0.365     | 102.51%    || q49.sql  | 21.466       | 21.572          | 0.106     | 100.49%    || q50.sql  | 194.755      | 195.052         | 0.297     | 100.15%    || q51.sql  | 37.493       | 38.56           | 1.067     | 102.85%    || q52.sql  | 2.227        | 2.28            | 0.053     | 102.38%    || q53.sql  | 5.375        | 5.437           | 0.062     | 101.15%    || q54.sql  | 12.556       | 13.015          | 0.459     | 103.66%    || q55.sql  | 2.341        | 2.809           | 0.468     | 119.99%    || q56.sql  | 7.424        | 7.207           | -0.217    | 97.08%     || q57.sql  | 17.606       | 17.797          | 0.191     | 101.08%    || q58.sql  | 6.169        | 6.374           | 0.205     | 103.32%    || q59.sql  | 27.602       | 27.744          | 0.142     | 100.51%    || q60.sql  | 7.04         | 7.459           | 0.419     | 105.95%    || q61.sql  | 7.838        | 7.816           | -0.022    | 99.72%     || q62.sql  | 9.726        | 10.762          | 1.036     | 110.65%    || q63.sql  | 4.816        | 5.176           | 0.36      | 107.48%    || q64.sql  | 253.937      | 261.034         | 7.097     | 102.79%    || q65.sql  | 78.942       | 78.373          | -0.569    | 99.28%     || q66.sql  | 15.199       | 14.73           | -0.469    | 96.91%     || q67.sql  | 926.049      | 1022.971        | 96.922    | 110.47%    || q68.sql  | 7.932        | 7.977           | 0.045     | 100.57%    || q69.sql  | 12.101       | 14.699          | 2.598     | 121.47%    || q70.sql  | 20.7         | 20.872          | 0.172     | 100.83%    || q71.sql  | 14.96        | 15.065          | 0.105     | 100.70%    || q72.sql  | 73.215       | 73.955          | 0.74      | 101.01%    || q73.sql  | 5.973        | 6.126           | 0.153     | 102.56%    || q74.sql  | 97.611       | 99.577          | 1.966     | 102.01%    || q75.sql  | 125.005      | 129.508         | 4.503     | 103.60%    || q76.sql  | 34.812       | 35.34           | 0.528     | 101.52%    || q77.sql  | 7.686        | 8.474           | 0.788     | 110.25%    || q78.sql  | 287.959      | 292.936         | 4.977     | 101.73%    || q79.sql  | 8.401        | 9.616           | 1.215     | 114.46%    || q80.sql  | 59.371       | 60.051          | 0.68      | 101.15%    || q81.sql  | 18.452       | 19.499          | 1.047     | 105.67%    || q82.sql  | 64.093       | 65.032          | 0.939     | 101.47%    || q83.sql  | 4.675        | 4.867           | 0.192     | 104.11%    || q84.sql  | 10.456       | 10.816          | 0.36      | 103.44%    || q85.sql  | 12.347       | 12.77           | 0.423     | 103.43%    || q86.sql  | 6.537        | 6.843           | 0.306     | 104.68%    || q87.sql  | 77.427       | 77.876          | 0.449     | 100.58%    || q88.sql  | 83.082       | 83.385          | 0.303     | 100.36%    || q89.sql  | 6.645        | 6.801           | 0.156     | 102.35%    || q90.sql  | 7.841        | 7.883           | 0.042     | 100.54%    || q91.sql  | 3.88         | 4.129           | 0.249     | 106.42%    || q92.sql  | 3.044        | 3.271           | 0.227     | 107.46%    || q93.sql  | 361.149      | 365.883         | 4.734     | 101.31%    || q94.sql  | 43.929       | 46.667          | 2.738     | 106.23%    || q95.sql  | 196.363      | 197.427         | 1.064     | 100.54%    || q96.sql  | 12.457       | 12.496          | 0.039     | 100.31%    || q97.sql  | 80.131       | 81.821          | 1.69      | 102.11%    || q98.sql  | 6.885        | 7.522           | 0.637     | 109.25%    || q99.sql  | 17.685       | 18.009          | 0.324     | 101.83%    ||          | 6788.707     | 7116.357        | 327.65    | 104.82%    |One of our production query which has 19 case when  expressions, it's query time changed from 1.1 hour to 42 seconds.![image](https://user-images.githubusercontent.com/3626747/227448668-8a58ff33-3296-4a95-984b-292af47532ca.png)A simplify benchmark of the above production query.```    spark.range(1, 2000000, 1, 1)      .selectExpr(        \"cast(id + 1 as decimal) as a\        \"cast(id + 2 as decimal) as b\        \"cast(id + 3 as decimal) as c\        \"cast(id + 4 as decimal) as d\")      .createOrReplaceTempView(\"tab\")    runBenchmark(\"Subexpression elimination in ProjectExec\") {      val benchmark =        new Benchmark(\"Subexpression elimination in ProjectExec\ 2000000, output = output)      benchmark.addCase(s\"Test query\") { _ =>        val query =          s\"\"\"             |SELECT a, b, c, d,             |       a * b / c as s1,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END s2,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |       ELSE 0 END s3,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s4,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                      CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |                 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s5             |FROM tab             |\"\"\".stripMargin        spark.sql(query).noop()      }      benchmark.run()```Local benchmark result:Before this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         9713           9900         263          0.2        4856.7       1.0X```After this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         1238           1307          98          8.1         123.8       1.0X```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Exists UT.
[SPARK-21195][CORE] Dynamically register metrics from sources as they are reported
[SPARK-38467][CORE] Use error classes in org.apache.spark.memory
[SPARK-43436][BUILD] Upgrade rocksdbjni to 8.1.1.1
[SPARK-43428][CORE][FOLLOWUP] Utils and JSON Utils refactoring
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->1. We can use define trait for the `SparkClassUtils` and let Spark Utils to extend that to avoid duplicate code.2. We can also define trait for the `JsonUtil` and JsonProtocol can also extend that to re-use code.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->Share more between common/utils module and Spark core.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
[SPARK-43441][CORE] `makeDotNode` should not fail when DeterministicLevel is absent
[SPARK-43440][PYTHON][CONNECT] Support registration of an Arrow-optimized Python UDF
[SPARK-43383][SQL][FOLLOWUP] Use the same method to compute LocalRelation's data length.
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As comment https://github.com/apache/spark/pull/41064#discussion_r1190332445, This PR amins to use the same method to compute LocalRelation's data length.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->code improve.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
[SPARK-43442][PS][CONNECT][TESTS] Split test module `pyspark_pandas_connect`
[SPARK-43286][SQL][TESTS][FOLLOWUP] Moving `ExpressionImplUtilsSuite` to `catalyst` package
[SPARK-43133] Scala Client DataStreamWriter Foreach support
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the scala client side `foreach` support.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of SS Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. Now they can use foreach in scala client exactly as how they used it in non-connect mode### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit testManually tested it works for both `Row` and `Int````import org.apache.spark.sql.{ForeachWriter, Row} import java.io._ val filePath = \"/home/wei.liu/test_foreach/output-row\" val writer = new ForeachWriter[Row] {    var fileWriter: FileWriter = _      def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)  // true to append      true    }      def process(row: Row): Unit = {      fileWriter.write(row.mkString(\ \"))  // separate fields with commas      fileWriter.write(\"\\")  // newline for each row    }      def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }  } val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load() val query = df .writeStream .foreach(writer) .outputMode(\"update\") .start() assert(query.exception.isEmpty)``````import org.apache.spark.sql.{ForeachWriter, Row} import java.io._ val filePath = \"/home/wei.liu/test_foreach/output-int\" val writer = new ForeachWriter[Int] {    var fileWriter: FileWriter = _      def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)  // true to append      true    }      def process(value: Int): Unit = {      fileWriter.write(value.toString)      fileWriter.write(\"\\")  // newline for each value    }      def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }  } val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load() val query = df .selectExpr(\"CAST(value AS INT)\") .as[Int] .writeStream .foreach(writer) .outputMode(\"update\") .start() ```NOTE: below __DOESN'T__ work as of now. A ticket is filed regarding this: SPARK-43796```import org.apache.spark.sql.ForeachWriter import java.io._ val filePath = \"/home/wei.liu/test_foreach/output-custom\" case class MyTestClass(value: Int) {      override def toString: String = value.toString}val writer = new ForeachWriter[MyTestClass] {    var fileWriter: FileWriter = _    def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)      true    }    def process(row: MyTestClass): Unit = {      fileWriter.write(row.toString)      fileWriter.write(\"\\")    }    def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }}val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load()val query = df .selectExpr(\"CAST(value AS INT)\") .as[MyTestClass] .writeStream .foreach(writer) .outputMode(\"update\") .start()```
[SPARK-43446][BUILD] Upgrade Apache Arrow to 12.0.0
[SPARK-43443][SQL] Add benchmark for Timestamp type inference when use invalid value
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When we try to speed up Timestamp type inference with format (PR: #36562 #41078 #41091). There is no way to judge whether the change has improved the speed for Timestamp type inference.So we need a benchmark to measure whether our optimization of Timestamp type inference is useful, we have valid Timestamp value benchmark at now, but don't have invalid Timestamp value benchmark when use Timestamp type inference.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add new benchmark for Timestamp type inference when use invalid value, to make sure our speed up PR work normally.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?benchmarks already are test code.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43447][R] Support R 4.3.0
[SPARK-43448][BUILD] Remove dummy dependency `hadoop-openstack`
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove the dummy dependency `hadoop-openstack` from Spark binary artifacts.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[HADOOP-18442](https://issues.apache.org/jira/browse/HADOOP-18442) removed the `hadoop-openstack` and temporarily retained a dummy jar for the downstream project which consumes it.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43449][INFRA] Remove branch-3.2 daily GitHub Action job and conditions
[SPARK-43356][K8S] Migrate deprecated createOrReplace to serverSideApply
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The deprecation message of `createOrReplace` indicates that we should change `createOrReplace` to `serverSideApply` instead.```@deprecated please use {@link ServerSideApplicable#serverSideApply()} or attempt a create and edit/patch operation.```The change is not fully equivalent, but I believe it's reasonable.> With the caveat that the user may choose not to use forcing if they want to know when there are conflicting changes.> > Also unlike createOrReplace if the resourceVersion is set on the resource and a replace is attempted, it will be optimistically locked.See more details at https://github.com/fabric8io/kubernetes-client/pull/5073### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Remove usage of deprecated API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43455][BUILD][K8S] Bump kubernetes-client 6.6.1
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Release Notes: https://github.com/fabric8io/kubernetes-client/releases/tag/v6.6.1### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It's basically a routine to keep the third-party libs up-to-date.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43457][CONNECT][PYTHON] Augument user agent with OS, Python and Spark versions
[SPARK-40887][K8S] Make `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` work for Spark on K8S
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As title, Make `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` works for Spark on K8S.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Now seems `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` only work for yarn cluster mode.We need to make it works for Spark on K8s.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` works for Spark on K8S after this PR.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT.
[SPARK-38469][CORE] Use error class in org.apache.spark.network
[SPARK-43461][BUILD] Skip compiling useless files when making distribution
[SPARK-43302][SQL][FOLLOWUP] Code cleanup for PythonUDAF
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/40739 to do some code cleanup1. remove the pattern `PYTHON_UDAF` as it's not used by any rule.2. add `PythonFuncExpression.evalType` for convenience: catalyst rules (including third-party extensions) may want to get the eval type of a python function, no matter it's UDF or UDAF.3. update the python profile to use `PythonUDAF.resultId` instead of `AggregateExpression.resultId`, to be consistent with `PythonUDF`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  4. If you fix a bug, you can clarify why it is a bug.-->code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-43485][SQL] Fix the error message for the `unit` argument of the datetime add/diff functions
[SPARK-43470][CORE] Add operating system ,Java, Python version information to application log
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.--> ### What changes were proposed in this pull request?In a heterogeneous cluster, it is hard to debug issues from the Operating system and Java/python versions incompatibilities. Currently, the Operating system and Java/python versions details are missing in the spark application log, this PR adds that information to the application info log, which will help in troubleshooting and debugging any issues that may arise### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To troubleshoot host-specific issues in the Spark application ran on heterogeneous cluster### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually tested
[SPARK-43471][CORE] Handle missing hadoopProperties and metricsProperties
[SPARK-43474] [SS] [CONNECT] Add a spark connect function to create DataFrame reference 
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change adds a new spark connect relation type `CachedDataFrame`, which can represent a DataFrame that's been cached on the server side.On the server side, each (userId, sessionId) has a map to cache DataFrame. DataFrame will be removed from cache when the corresponding session expires. (The caller can also evict the DataFrame from cache earlier, depending on the logic.)On the client side, a new relation type and function is added. The new function can create a DataFrame reference given a key. The key is the id of a cached DataFrame, which is usually passed from server to the client. When transforming the DataFrame reference, the server finds the actual DataFrame from the cache and replace it.One use case of this function will be streaming foreachBatch(). Server needs to call user function for every batch which takes a DataFrame as argument. With the new function, we can cache the DataFrame on the server. Pass the id back to client which can creates the DataFrame reference. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This change is needed to support streaming foreachBatch() in Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Scala unit test.Manual test.(More end to end test will be added when foreachBatch() is supported. Currently there is no way to add a dataframe to the server cache using Python.)
[SPARK-43543][PYTHON] Fix nested MapType behavior in Pandas UDF
[SPARK-42945][CONNECT][FOLLOWUP] Disable JVM stack trace by default
[SPARK-43473][PYTHON] Support struct type in createDataFrame from pandas DataFrame
[SPARK-43482][SS] Expand QueryTerminatedEvent to contain error class if it exists in exception
[SPARK-43483][SQL][DOCS] Adds SQL references for OFFSET clause.
[SPARK-43484][BUILD][DSTREAM] Kafka/Kinesis Assembly should not package hadoop-client-runtime
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change `hadoop-client-runtime`'s scope to `provided` in kafka/kinesis assembly modules.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`hadoop-client-runtime` is already included in Spark binary tgz, we should not package it again into the optional assembly jar.This issue exists since SPARK-33212.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual review.
[SPARK-43489][BUILD] Remove protobuf 2.5.0
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Spark does not use protobuf 2.5.0 directly, instead, it comes from other dependencies, with the following changes, now, Spark does not require protobuf 2.5.0 (please let me know if I miss something),- SPARK-40323 upgraded ORC 1.8.0, which moved from protobuf 2.5.0 to a shaded protobuf 3- SPARK-33212 switched from Hadoop vanilla client to Hadoop shaded client, also removed the protobuf 2 dependency. SPARK-42452 removed the support for Hadoop 2.- SPARK-14421 shaded and relocated protobuf 2.6.1, which is required by the kinesis client, into the kinesis assembly jar- Spark itself's core/connect/protobuf modules use protobuf 3, also shaded and relocated all protobuf 3 deps.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Remove the obsolete dependency, which is EOL long ago, and has CVEs [CVE-2022-3510](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3510) [CVE-2022-3509](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3509) [CVE-2022-3171](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3171) [CVE-2021-22569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22569)### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
[SPARK-43327][CORE][3.3] Trigger `committer.setupJob` before plan execute in `FileFormatWriter#write`
[SPARK-43487][SQL] Fix Nested CTE error message
[SPARK-40129][SQL] Fix Decimal multiply can produce the wrong answer
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The example here is multiplying Decimal(38, 10) by another Decimal(38, 10), but I think it can be reproduced with other number combinations, and possibly with divide too.```scalaSeq(\"9173594185998001607642838421.5479932913\").toDF.selectExpr(\"CAST(value as DECIMAL(38,10)) as a\").selectExpr(\"a * CAST(-12 as DECIMAL(38,10))\").show(truncate=false)```This produces an answer in Spark of `-110083130231976019291714061058.575920` But if I do the calculation in regular java BigDecimal I get `-110083130231976019291714061058.575919````javaBigDecimal l = new BigDecimal(\"9173594185998001607642838421.5479932913\");BigDecimal r = new BigDecimal(\"-12.0000000000\");BigDecimal prod = l.multiply(r);BigDecimal rounded_prod = prod.setScale(6, RoundingMode.HALF_UP);```Spark does essentially all of the same operations, but it used Decimal to do it instead of java's BigDecimal directly. Spark, by way of Decimal, will set a `MathContext` for the multiply operation that has a max precision of 38 and will do half up rounding. That means that the result of the multiply operation in Spark is `-110083130231976019291714061058.57591950`, but for the java BigDecimal code the result is `-110083130231976019291714061058.57591949560000000000`. Then Spark will call `toPrecision` to round up again. So Spark round up result twice.This PR change the code-gen and `nullSafeEval` of `Arithmetic`. To make sure when use multiply method will set custom `MathContext` with precision of 39 (meaning one more scale). Then round up twice will not affect result.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix the bug Decimal multiply produce wrong answer<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43492][SQL] Add 3-args function aliases `DATE_ADD` and `DATE_DIFF`
[MINOR][CONNECT][SS][TESTS] Increase timeout to 30s for tests in StreamingQuerySuite
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Increase timeout to 30s for tests in StreamingQuerySuite### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the tests more stable### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This is a test only change.
[SPARK-43490][BUILD] Upgrade sbt to 1.8.3
[SPARK-41971][PYTHON][FOLLOWUP] Fix toPandas to support empty columns
[SPARK-43487][SQL] Fix wrong error message used for `ambiguousRelationAliasNameInNestedCTEError`
[SPARK-43491][SQL] In expression should act as same as EqualTo when elements in IN expression have same DataType.
[SPARK-43475][SQL] Generalize expression lineage logic to enable optimizations
[SPARK-43494][CORE] Directly call `replicate()` for `HdfsDataOutputStreamBuilder` instead of reflection in `SparkHadoopUtil#createFile`
[SPARK-43495][BUILD] Upgrade RoaringBitmap to 0.9.44 
[SPARK-40189][SQL] Add `json_array_get` function
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?At the moment we do not have any function to get value of JSON array by use specified index.I add a `json_array_get` function which will return the value of JSON array by use specified index.- This function will return value of JSON array, if JSON array is valid.```sqlselect json_array_get('[{\"a\":123},{\"b\":\"hello\"}]', 1);+--------------------------------------------+|json_array_get([{\"a\":123},{\"b\":\"hello\"}], 1)|+--------------------------------------------+|                               {\"b\":\"hello\"}|+--------------------------------------------+```- In case of any other valid JSON string, the specified index does not exist, invalid JSON string or null array or NULL input , NULL will be returned.```sqlselect json_array_get(\"[[1],[2,3],[]]\ 4);+---------------------------------+|json_array_get([[1],[2,3],[]], 4)|+---------------------------------+|                             NULL|+---------------------------------+```<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?- As mentioned in JIRA, this function is supported by presto- For better user experience and ease of use.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, now users can get value of a json array by using `json_array_get`.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43500][PYTHON][TESTS] Test `DataFrame.drop` with empty column list and names containing dot
[SPARK-43454][CORE] support substitution for SparkConf's get and getAllWithPrefix
[SPARK-43493][SQL] Add a max distance argument to the levenshtein() function
[MINOR] Remove redundant character escape \"\\\\\" and add UT
[SPARK-43508][DOC] Replace the link related to hadoop version 2 with hadoop version 3
[SPARK-43359][SQL] Delete from Hive table should throw \"UNSUPPORTED_FEATURE.TABLE_OPERATION\"
[SPARK-43510][YARN] Fix YarnAllocator internal state when adding running executor after processing completed containers
[SPARK-43415][CONNECT] Adding mapValues func before the agg exprs
[SPARK-43512][SS][TESTS] Update StateStoreOperationsBenchmark to reflect updates to RocksDB usage as state store provider
[SPARK-43516][ML][PYTHON][CONNECT] Base interfaces of sparkML for spark3.5: estimator/transformer/model/evaluator
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request? - Defines basic interfaces of Evaluator / Transformer / Model / Evaluator, these interfaces are designed to support both spark connect and legacy spark mode, and are designed to support train / transform / evaluate over either spark dataframe or pandas dataframe - Implement a feature transformer `MaxAbsScaler`, `ScalerScaler` - Implement a regressor evaluator `RegressorEvaluator` that supports MSE and R2 metric evaluation - Implement a summarizer via pure python code that can summarize array type columns on spark dataframe. - Some utility methods.### Why are the changes needed?Project: Distributed ML <> spark connect### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Unit tests.
[SPARK-43517][PYTHON][DOCS] Add a migration guide for namedtuple monkey patch
[SPARK-43519][BUILD][SQL] Bump Parquet to 1.13.1
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Upgrade Parquet from 1.13.0 to 1.13.1### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[Apache Parquet 1.13.1](https://parquet.apache.org/blog/2023/05/18/1.13.1/) is available, the release notes are> ### Version 1.13.1 ###> > Release Notes - Parquet - Version 1.13.1> > #### Improvement> > *   [PARQUET-2276](https://issues.apache.org/jira/browse/PARQUET-2276) - Bring back support > for Hadoop 2.7.3> *   [PARQUET-2297](https://issues.apache.org/jira/browse/PARQUET-2297) - Skip delta problem > check> *   [PARQUET-2292](https://issues.apache.org/jira/browse/PARQUET-2292) - Improve default > SpecificRecord model selection for Avro `{Write,Read}`Support> *   [PARQUET-2290](https://issues.apache.org/jira/browse/PARQUET-2290) - Add CI for Hadoop 2> *   [PARQUET-2282](https://issues.apache.org/jira/browse/PARQUET-2282) - Don't initialize HadoopCodec> *   [PARQUET-2283](https://issues.apache.org/jira/browse/PARQUET-2283) - Remove Hadoop HiddenFileFilter> *   [PARQUET-2081](https://issues.apache.org/jira/browse/PARQUET-2081) - Fix support for rewriting files without ColumnIndexes### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA
[SPARK-43518][SQL] Convert `_LEGACY_ERROR_TEMP_2029` to INTERNAL_ERROR
[SPARK-43502][PYTHON][CONNECT] `DataFrame.drop` should accept empty column
[SPARK-43504][K8S] Mounts the hadoop config map on the executor pod
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this pr, for spark on k8s, the hadoop config map will be mounted in executor side as well.Before, the  hadoop config map is only mounted in driver side.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since [SPARK-25815](https://issues.apache.org/jira/browse/SPARK-25815) [,](https://github.com/apache/spark/pull/22911,) the hadoop config map will not be mounted in executor side.Per the  https://github.com/apache/spark/pull/22911 description:> The main two things that don't need to happen in executors anymore are:> 1. adding the Hadoop config to the executor pods: this is not needed> since the Spark driver will serialize the Hadoop config and send> it to executors when running tasks. But in fact, the executor still need the hadoop configuration.![image](https://github.com/apache/spark/assets/6757692/ff6374c9-7ebd-4472-a85c-99c75a737e2a)As shown in above picture, the driver can resolve `hdfs://zeus`, but the executor can not.so we still need to mount the hadoop config map in executor side.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, users do not need to take workarounds to make executors load the hadoop configuration.Such as:- including hadoop conf in executor image- placing hadoop conf files under `SPARK_CONF_DIR`.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
[SPARK-43520][BUILD][TESTS] Upgrade `mysql-connector-java` to 8.0.33
[SPARK-42604][CONNECT][FOLLOWUP] Remove `typedlit/typedLit` `ProblemFilters.exclude` rule from mima check
[SPARK-43535][BUILD] Adjust the ImportOrderChecker rule to resolve long-standing import order issues
[SPARK-43525][BUILD] Import `scala.collection` instead of `collection`
[SPARK-43527][PYTHON] Fix `catalog.listCatalogs` in PySpark
[SPARK-43522][SQL] Fix creating struct column name with index of array
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When creating a struct column in Dataframe, the code that ran without problems in version 3.3.1 does not work in version 3.4.0.In 3.3.1```scalaval testDF = Seq(\"a=b,c=d,d=f\").toDF.withColumn(\"key_value\ split('value, \\")).withColumn(\"map_entry\ transform(col(\"key_value\"), x => struct(split(x, \"=\").getItem(0), split(x, \"=\").getItem(1) ) ))testDF.show()+-----------+---------------+--------------------+ |      value|      key_value|           map_entry| +-----------+---------------+--------------------+ |a=b,c=d,d=f|[a=b, c=d, d=f]|[{a, b}, {c, d}, ...| +-----------+---------------+--------------------+```In 3.4.0```org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING] Cannot resolve \"struct(split(namedlambdavariable(), =, -1)[0], split(namedlambdavariable(), =, -1)[1])\" due to data type mismatch: Only foldable `STRING` expressions are allowed to appear at odd position, but they are [\"0\ \"1\"].;'Project [value#41, key_value#45, transform(key_value#45, lambdafunction(struct(0, split(lambda x_3#49, =, -1)[0], 1, split(lambda x_3#49, =, -1)[1]), lambda x_3#49, false)) AS map_entry#48]+- Project [value#41, split(value#41, ,, -1) AS key_value#45]   +- LocalRelation [value#41]  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)  at scala.collection.Iterator.foreach(Iterator.scala:943)  at scala.collection.Iterator.foreach$(Iterator.scala:943)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)  at scala.collection.IterableLike.foreach(IterableLike.scala:74)  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)  at scala.collection.Iterator.foreach(Iterator.scala:943)  at scala.collection.Iterator.foreach$(Iterator.scala:943)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)....```The reason is `CreateNamedStruct` will use last expr of value `Expression` as column name. And will check it must are `String`. But array `Expression`'s last expr are `Integer`. The check will failed. So we can skip match with `UnresolvedExtractValue` when last expr not `String`. Then it will when fall back to the default name.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix the bug when creating struct column name with index of array<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43361][PROTOBUF] update documentation for errors related to enum serialization
[DO NOT MERGE] [POC] run foreachBatch() on client
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->POC to run foreachBatch() user function on client side```>>> def foreach_batch_function(df, epoch_id):...   count = df.count()...   print(\"##### The count for batch_id {} is {}\".format(epoch_id, count))...>>> query = (...  spark...  .readStream...  .format(\"rate\")...  .load()...  .writeStream...  .foreachBatch(foreach_batch_function)...  .start()... )##### client: start foreach_batch_callback thread>>> ##### The count for batch_id 0 is 0##### The count for batch_id 1 is 1##### The count for batch_id 2 is 1##### The count for batch_id 3 is 1```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43528][SQL][PYTHON] Support duplicated field names in createDataFrame with pandas DataFrame
[SPARK-43529][SQL] Support general constant expressions as CREATE/REPLACE TABLE OPTIONS values
[SPARK-43530][PROTOBUF] Read descriptor file only once\t
[SPARK-43531][CONNECT][PYTHON][TESTS] Enable more parity tests for Pandas UDFs
[SPARK-43532][BUILD][TESTS] Upgrade `jdbc` related test dependencies
[SPARK-43534][BUILD] Add log4j-1.2-api and log4j-slf4j2-impl to classpath if active hadoop-provided
[SPARK-43505][K8S] support env variables substitution and executor library path
[MINOR][DOC] Refine doc for `Column.over`
[SPARK-43537][INFA][BUILD] Upgrading the ASM dependencies used in the `tools` module to 9.4
Spark-43536 Fixing statsd sink reporter
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The metrics type for spark counter metrics exported to statsD are mapped to gauge instead of counter.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Due to spark counter metrics being cumulative and statsD counter metrics being derive, there is a contract mismatch between them. This leads to aggregation both on client(spark) and server(statsD) side, which results in the value o the metrics being wrong. Since StatsD does not do aggregation on gauge metric, spark should send it cumulative metrics as gauge too, to obtain correct value of the metric.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->The counter metrics that are exported by statsD to any backends will be changed to gauge metrics instead of counter metrics. Any application that processes gauge and counter metrics differently wil need to account for this change.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT has been modified.
[SPARK-43539][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_0003
[SPARK-43540][K8S][CORE] Add working directory into classpath on the driver/executor in K8S cluster mode
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Adding working directory into classpath on the driver in K8S cluster mode.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->After #37417, the spark.files, spark.jars are placed  in the working directory.But seems that the spark context classloader can not access them because they are not in the classpath by default.This pr adds the current working directory into classpath, so that the spark.files, spark.jars placed in the working directory can be accessible by the classloader.For example, the `hive-site.xml` uploaded by `spark.files`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, users do not need to add the working directory into spark classpath manually.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
[SPARK-43413][SQL][FOLLOWUP] Show a directional message in ListQuery nullability assertion
[SPARK-16484][SQL] Update hll function type checks to also check for non-foldable inputs
[SPARK-43541][SQL] Propagate all `Project` tags in resolving of expressions and missing columns
[SPARK-43542][SS] Define a new error class and apply for the case where streaming query fails due to concurrent run of streaming query with same checkpoint
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->We are migrating to a new error framework in order to surface errors in a friendlier way to customers. This PR defines a new error class specifically for when there are concurrent updates to the log for the same batch ID### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This gives more information to customers, and allows them to filter in a better way### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->There is an existing test to check the error message upon this condition. Because we are only changing the error type, and not the error message, this test is sufficient.
[SPARK-43509][PYTHON][CONNECT][FOLLOW-UP] Set SPARK_CONNECT_MODE_ENABLED when running pyspark shell with remote is local
[3.4][SPARK-42826][FOLLOWUP][PS][DOCS] Update migration notes for pandas API on Spark.
[3.4][SPARK-43547][PS][DOCS] Update \"Supported Pandas API\" page to point out the proper pandas docs
[SPARK-43548][SS] Remove workaround for HADOOP-16255
[SPARK-43569][SQL] Remove workaround for HADOOP-14067
[SPARK-43024][PYTHON] Upgrade pandas to 2.0.0
[SPARK-43573][BUILD] Make SparkBuilder could config the heap size of test JVM.
[SPARK-43572][SQL][TEST] Add a test for scrollable result set through thrift server
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new test for scrollable result set support, which is uncovered yet through jdbc APIs### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->test improvement### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new test
[SPARK-43549][SQL] Convert `_LEGACY_ERROR_TEMP_0036` to INVALID_SQL_SYNTAX
[SPARK-43574][PYTHON][SQL] Support to set Python executable for UDF and pandas function APIs in workers during runtime
[SPARK-43383][SQL][FOLLOWUP] LocalRelation should not report row count in tests
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a follow-up of https://github.com/apache/spark/pull/41064 . `LocalRelaton` is heavily used in tests and it's better to not report row count in tests to avoid the query being optimized too well which may hurt test coverage.This PR updates `LocalRelaton` to not report row count in tests, and adds a test-only config to still enable it in tests.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->keep test coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
[SPARK-43575][BUILD][SS] Exclude duplicated classes from kafka assembly jar
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Exclude `javax.activation:activation:jar:1.1.1` and `org.apache.logging.log4j:log4j-slf4j2-impl:jar:2.20.0` from `spark-streaming-kafka-0-10-assembly_2.12-<version>.jar`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We should not include the jar which already exists in Spark binary artifact into the assembly jar of optional component.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->```build/mvn dependency:list -pl :spark-streaming-kafka-0-10-assembly_2.12```before```[INFO] --- maven-dependency-plugin:3.5.0:list (default-cli) @ spark-streaming-kafka-0-10-assembly_2.12 ---[INFO][INFO] The following files have been resolved:[INFO]    org.apache.spark:spark-streaming-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.spark:spark-token-provider-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.kafka:kafka-clients:jar:3.4.0:compile[INFO]    org.apache.spark:spark-tags_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    javax.activation:activation:jar:1.1.1:compile[INFO]    org.apache.logging.log4j:log4j-slf4j2-impl:jar:2.20.0:compile[INFO]    org.spark-project.spark:unused:jar:1.0.0:compile```after```[INFO] --- maven-dependency-plugin:3.5.0:list (default-cli) @ spark-streaming-kafka-0-10-assembly_2.12 ---[INFO][INFO] The following files have been resolved:[INFO]    org.apache.spark:spark-streaming-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.spark:spark-token-provider-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.kafka:kafka-clients:jar:3.4.0:compile[INFO]    org.apache.spark:spark-tags_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.spark-project.spark:unused:jar:1.0.0:compile```
[SPARK-43576][CORE] Remove unused declarations from Core module
[SPARK-43577][BUILD] Upgrade cyclonedx-maven-plugin to 2.7.9
[SPARK-43541][SQL][3.4] Propagate all `Project` tags in resolving of expressions and missing columns
[SPARK-43541][SQL][3.3] Propagate all `Project` tags in resolving of expressions and missing columns
[SPARK-43580][PYTHON][TESTS] Add `https://dlcdn.apache.org/` to default_sites of get_preferred_mirrors
[SPARK-43581][BUILD][K8S] Upgrade `kubernetes-client` to 6.6.2
[SPARK-43582][BUILD] Upgrade `sbt-pom-reader` to 2.4.0
[SPARK-43583][CORE] get MergedBlockedMetaReqHandler from the delegate instead of the SaslRpcHandler instance
[SPARK-43584][BUILD] Update `sbt-assembly`, `sbt-revolver`, `sbt-mima-plugin` plugins
[CONNECT] Refactor test case in UserDefinedFunctionE2ETestSuite to reproduce NPE
[SPARK-43595][BUILD] Update some maven plugins to newest version
[SPARK-43587][CORE][TESTS] Run `HealthTrackerIntegrationSuite` in a dedicated JVM
[SPARK-43586][SQL] Use the smaller value of `Range.numElements` and `Range.numSlices` as `numSlices` of `RangeExec`
[SPARK-43588][BUILD] Upgrade ASM to 9.5
[SPARK-43589][SQL] Fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString`
[SPARK-42958][CONNECT][FOLLOWUP] Correct the parameter passed to `checkMiMaCompatibilityWithAvroModule` to `avroJar`
[SPARK-43589][SQL][3.3] Fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString`
[SPARK-43590][CONNECT] Make `connect-jvm-client-mima-check` to support mima check with `protobuf` module
[SPARK-43591][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_0013
[WIP][SPARK-43593][SQL] Support the minimum number of range shuffle partitions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?If there are few distinct values in the RangePartitioner, there will be very few partitions that could be very large. We can append a random expression to increase the number of partitions.### Why are the changes needed?To optimize RangePartitioner, for example, the following query can be optimized from 2.9 hours to 3.2 mins.![range_partitioner](https://github.com/apache/spark/assets/3626747/a99f7092-d38f-43e4-8881-96ee6d6ed75d)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added UT
[SPARK-43594][SQL] Add LocalDateTime to anyToMicros
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Small change to `anyToMicros` to also accept `LocalDateTime` that's being returned when working with `TIMESTAMP_NTZ`. This simplifies some code at the Iceberg side.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
Span array function
[SPARK-43545][SQL][PYTHON] Support nested timestamp type
[SPARK-43597][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_0017
[SPARK-43598][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_2400
[MINOR][INFRA] Deduplicate `scikit-learn` in Dockerfile
[WIP] fix trim bug
[MINOR][DOCS] Fix wrong reference
[SPARK-43599][CONNECT][BUILD] Upgrade buf to v1.19.0
[SPARK-43600][K8S][DOCS] Update K8s doc to recommend K8s 1.24+
[SPARK-43601][INFRA] Remove the upper bound of `matplotlib` in requirements
[SPARK-43609][BUILD] Upgrade Netty from 4.1.89 to 4.1.92
[SPARK-43612][CONNECT][PYTHON] Implement SparkSession.addArtifact(s) in Python client
[SPARK-43521][SQL] Add `CREATE TABLE LIKE FILE` statement
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Hive support CREATE TABLE LIKE FILE statement in https://issues.apache.org/jira/browse/HIVE-26395 .So this PR bring `CREATE TABLE LIKE FILE` to spark.This statement will read file schema before create table, the schema will be used for new table schema.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add new feature `CREATE TABLE LIKE FILE` statement supported.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43649][SPARK-43650][SPARK-43651][SQL] Assign names to the error class _LEGACY_ERROR_TEMP_240[1-3]
[CONNECT] Add independent maven testing GA task for connect modules
[SPARK-43604][SQL] Refactor `INVALID_SQL_SYNTAX ` for avoiding to embed error's text in source code
[SPARK-42996][CONNECT][PS][ML] Create & assign proper JIRA tickets for all failing tests.
[SPARK-43714][SQL][TESTS] When formatting `error-classes.json` file with `SparkThrowableSuite` , the last line of the file should be empty line
[SPARK-43657][K8S]: reuse config map for executor when running on k8s-cluster mode
[WIP][SPARK-43603][PS][CONNECT][TEST] Rebalance `pyspark.pandas.DataFrame` Unit Tests
[SPARK-41532][CONNECT][FOLLOWUP] add error class `SESSION_NOT_SAME` into error_classes.py
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This is a follow up PR for #40684 . Add error class `SESSION_NOT_SAME` define into `error_classes.py` with a template error message.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Unified error message<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43331][CONNECT][FOLLOWUP] Fix typo in test
[SPARK-43716][BUILD] Revert scala-maven-plugin to 4.8.0 
[SPARK-43742][SQL] Refactor default column value resolution
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR refactors the default column value resolution so that we don't need an extra DS v2 API for external v2 sources. The general idea is to split the default column value resolution into two parts:1. resolve the column \"DEFAULT\" to the column default expression. This applies to `Project`/`UnresolvedInlineTable` under `InsertIntoStatement`, and assignment expressions in `UpdateTable`/`MergeIntoTable`.2. fill missing columns with column default values for the input query. This does not apply to UPDATE and non-INSERT action of MERGE as they use the column from the target table as the default value.The first part should be done for all the data sources, as it's part of column resolution. The second part should not be applied to v2 data sources with `ACCEPT_ANY_SCHEMA`, as they are free to define how to handle missing columns.More concretely, this PR:1. put the column \"DEFAULT\" resolution logic in the rule `ResolveReferences`, with two new virtual rules. This is to follow https://github.com/apache/spark/pull/388882. put the missing column handling in `TableOutputResolver`, which is shared by both the v1 and v2 insertion resolution rule. External v2 data sources can add custom catalyst rules to deal with missing columns for themselves.3. Remove the old rule `ResolveDefaultColumns`. Note that, with the refactor, we no long need to manually look up the table. We will deal with column default values after the target table of INSERT/UPDATE/MERGE is resolved.4. Remove the rule `ResolveUserSpecifiedColumns` and merge it to `PreprocessTableInsertion`. These two rules are both to resolve v1 insertion, and it's tricky to reason about their interactions. It's clearer to resolve the insertion with one pass.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->code cleanup and remove unneeded DS v2 API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->updated tests
[SPARK-43333][SQL] Allow Avro to convert union type to SQL with field name stable with type
[SPARK-43717][CONNECT] Scala client reduce agg cannot handle null partitions for scala primitive inputs
[SPARK-43596][SQL] Handle IsNull predicate in rewriteDomainJoins
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Handle IsNull predicate when doing rewriteDomainJoins. This predicate is a result of constant folding that occurred at some point earlier in the query plan### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug reported in https://issues.apache.org/jira/browse/SPARK-43596### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Query test
[SPARK-43719][WEBUI] Handle `missing row.excludedInStages` field
[SPARK-43718][SQL] Set nullable correctly for keys in USING joins
[SPARK-43546][PYTHON][CONNECT][TESTS] Complete parity tests of Pandas UDF
[SPARK-43737][BUILD] Upgrade zstd-jni to 1.5.5-3
[SPARK-43738][BUILD] Upgrade dropwizard metrics 4.2.18
[SPARK-43739][BUILD] Upgrade commons-io to 2.12.0
[SPARK-43740][PYTHON][CONNECT] Hide unsupported `session` methods from auto-completion
[SPARK-43625][DOCS] Document the difference between `Drop(column)` and `Drop(columnName)`
[MINOR][PS][TESTS] Fix `SeriesDateTimeTests.test_quarter` to work properly
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes to fix `SeriesDateTimeTests.test_quarter` to work properly.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Test has not been properly testing### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, test-only.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested, and the existing CI should pass
[SPARK-43741][BUILD] Upgrade maven-checkstyle-plugin from 3.2.2 to 3.3.0
[SPARK-43743][SQL] Port HIVE-12188(DoAs does not work properly in non-kerberos secured HS2)
[SPARK-38464][CORE] Use error classes in org.apache.spark.io
[SPARK-43747][PYTHON][CONNECT] Implement the pyfile support in SparkSession.addArtifacts
[SPARK-43749][SPARK-43750][SQL] Assign names to the error class _LEGACY_ERROR_TEMP_240[4-5]
[SPARK-43751][SQL][DOC] Document `unbase64` behavior change
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->After SPARK-37820, `select unbase64(\"abcs==\")`(malformed input) always throws an exception.So, `unbase64()`'s behavior for malformed input changed silently after SPARK-37820:- before: return a best-effort result, because it uses [LENIENT](https://github.com/apache/commons-codec/blob/rel/commons-codec-1.15/src/main/java/org/apache/commons/codec/binary/Base64InputStream.java#L46) policy: any trailing bits are composed into 8-bit bytes where possible. The remainder are discarded.- after: throw an exceptionAnd there is no way to restore the previous behavior. To tolerate the malformed input, the user should migrate `unbase64(<input>)` to `try_to_binary(<input>, 'base64')` to get NULL instead of interrupting by exception. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Add the behavior change to migration guide.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manuelly review.
[SPARK-43774][BUILD] Upgrade FasterXML jackson to 2.15.1
[SPARK-43647][CONNECT][TESTS] Clean up hive classes dir when test `connect-client-jvm` without -Phive
[WIP] Remove outdated assumptions on nested struct types
[SPARK-43757][Connect] Change client compatibility from allow list to deny list
[SPARK-43758][BUILD] Upgrade snappy-java to 1.1.10.0
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR upgrades `snappy-java` version to 1.1.10.0 from 1.1.9.1.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The new `snappy-java` version fixes a potential issue for Graviton support when used with old GLIBC versions. See https://github.com/xerial/snappy-java/issues/417.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
[SPARK-43759][SQL][PYTHON] Expose TimestampNTZType in pyspark.sql.types
[SPARK-43760][SQL] Nullability of scalar subquery results
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Makes sure that the results of scalar subqueries are declared as nullable. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is an existing correctness bug, see https://issues.apache.org/jira/browse/SPARK-43760### Does this PR introduce _any_ user-facing change?Fixes a correctness issue, so it is user-facing.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Query tests.
[MINOR][CORE] Fix default value of daemonExitValue in eofExceptionWhileReadPortNumberError
[SPARK-43767][SQL][TESTS] Fix bug in AvroSuite for 'reading from invalid path throws exception'
[SPARK-43762][SPARK-43763][SPARK-43764][SPARK-43765][SPARK-43766][SQL] Assign names to the error class _LEGACY_ERROR_TEMP_24[06-10]
[SPARK-43758][BUILD][FOLLOWUP][3.4] Update Hadoop 2 dependency manifest
[SPARK-43768][PYTHON][CONNECT] Python dependency management support in Python Spark Connect
[SPARK-43769][CONNECT] Implement 'levenshtein(str1, str2[, threshold])' functions
[SPARK-43771][BUILD][CONNECT] Upgrade mima-core from 1.1.0 to 1.1.2
[SPARK-43772][BUILD][CONNECT] Move version configuration in `connect` module to parent
[SPARK-43773][CONNECT][PYTHON] Implement 'levenshtein(str1, str2[, threshold])' functions in python client
[SPARK-43549][SQL] Convert _LEGACY_ERROR_TEMP_0036 to INVALID_SQL_SYNTAX.ANALYZE_TABLE_UNEXPECTED_NOSCAN
[SPARK-43779][SQL] ParseToDate should load the EvalMode in main thread
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->ParseToDate should load the EvalMode in main thread instead of loading it in a lazy val. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is because it is sometimes hard to estimate when the lazy val is executed while the SQLConf where we load the EvalMode is thread local. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT 
[DO NOT MERGE] scala Foreach test code
[SPARK-43775][SQL] DataSource V2: Allow representing updates as deletes and inserts
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a way for data sources to request Spark to represent updates as deletes and inserts. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It may be beneficial for data sources to represent updates as deletes and inserts for delta-based implementations. Specifically, it may help to properly distribute and order records before writing.Delete records set only row ID and metadata attributes. Update records set data, row ID, metadata attributes. Insert records set only data attributes.For instance, a data source may rely on a metadata column `_row_id` (synthetic internally generated) to identify the row and may be partitioned by `bucket(product_id)`. Splitting updates into inserts and deletes would allow data sources to cluster all update and insert records in MERGE for the same partition into a single task. Otherwise, the clustering key for updates and inserts will be different (inserts will always have `_row_id` as null as it is a metadata column). The new functionality is critical to reduce the number of generated files. It also makes sense in UPDATE operations if the original and new partition of a record do not match.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This PR adds a new method to `SupportsDelta` but the change is backward compatible. ### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
[SPARK-43780][SQL] Support correlated references in join predicates
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds support to subqueries that involve joins with correlated references in join predicates, e.g.```select * from t0 join lateral (select * from t1 join t2 on t1a = t2a and t1a = t0a);```(full example in https://issues.apache.org/jira/browse/SPARK-43780)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is a valid SQL that is not yet supported by Spark SQL.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, previously unsupported queries become supported.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Query and unit tests
[SPARK-43782][CORE] Support log level configuration with static Spark conf
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes to add a static Spark conf to configure an override to the Log4j logging level. It’s a config version of the `SparkContext.setLogLevel()` semantics, when set it’ll trigger a log level override at the beginning of `SparkContext` startup.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is particularly helpful in the Spark Connect scenario where there’s no way for the client to call `setLogLevel` because `SparkContext` is not yet supported in its API, and when it connects to a platform where it can't change the Log4j config file.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it adds a new public configuration `\"spark.log.level\"`, and by default it's unset which means there's no override.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->A new unit test in `SparkContextSuite`.
[SPARK-43786][SQL][TESTS] Add a test for nullability about 'levenshtein' function
[SPARK-43785][SQL][DOC] Improve the document of GenTPCDSData, so that developers could easy to generate TPCDS table data.
[SPARK-43666][SPARK-43667][SPARK-43668][SPARK-43669][PS] Fix `BinaryOps` for Spark Connect
[SPARK-43676][SPARK-43677][SPARK-43678][SPARK-43679][PS] Fix `DatetimeOps` for Spark Connect
[SPARK-43789][R] Uses 'spark.sql.execution.arrow.maxRecordsPerBatch' in R createDataFrame with Arrow by default
[SPARK-43692][SPARK-43693][SPARK-43694][SPARK-43695][PS] Fix `StringOps` for Spark Connect
[SPARK-43791][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_1336
[SPARK-43671][SPARK-43672][SPARK-43673][SPARK-43674][PS] Fix `CategoricalOps` for Spark Connect
[SPARK-43792][SQL][PYTHON][CONNECT] Add optional pattern for Catalog.listCatalogs
[MINOR][DOCS][PS] Move a few `Frame` functions to correct categories
[SPARK-43795][CONNECT] Remove parameters not used for SparkConnectPlanner
[SPARK-43794][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_1335
[SPARK-43755][CONNECT] Move execution out of SparkExecutePlanStreamHandler and to a different thread
[SPARK-43798][SQL][PYTHON] Support Python user-defined table functions
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds the initial support for Python user-defined table functions. It allows users to create UDTFs in PySpark and use them in PySpark and SQL.Here are examples of creating and using Python UDTFs:```python# Implement the UDTF classclass TestUDTF:  def __init__(self):    ...  def eval(self, *args):    yield \"hello\ \"world\"      def terminate(self):    ...# Create the UDTFfrom pyspark.sql.functions import udtftest_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")# Invoke the UDTFtest_udtf().show()+-----+-----+|   c1|   c2|+-----+-----+|hello|world|+-----+-----+# Register the UDTFspark.udtf.register(name=\"test_udtf\ f=test_udtf)# Invoke the UDTF in SQLspark.sql(\"SELECT * FROM test_udtf()\").show()+-----+-----+|   c1|   c2|+-----+-----+|hello|world|+-----+-----+```Please note that this is the initial PR, and there will be subsequent follow-up work to make it more user-friendly and performant.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To support another type of user-defined function in PySpark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. After this PR, users can create Python user-defined table functions.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit tests.
[SPARK-43802][SQL] Fix codegen for unhex and unbase64 with failOnError=true
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fixes an error with codegen for unhex and unbase64 expression when failOnError is enabled introduced in https://github.com/apache/spark/pull/37483.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Codegen fails and Spark falls back to interpreted evaluation:```Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type \"BASE64\"```in the code block:```/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),/* 110 */             project_value_1,/* 111 */             BASE64,/* 112 */             \"try_to_binary\");/* 113 */         }```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Bug fix.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added to the existing tests so evaluate an expression with failOnError enabled to test that path of the codegen.
[SPARK-43803] [SS] [CONNECT] Improve awaitTermination() to handle client disconnects
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Streaming awaitTermination() is a long running API. Currently, it keeps running on server even if client disconnects. This change periodically checks if client has disconnected. If so, we can stop the operation and release resources.We use gRPC Context.isCancelled() to determine if client has disconnected and response cannot be returned to the client. [Here is the reference of of isCancelled()](https://grpc.github.io/grpc/cpp/classgrpc_1_1_server_context.html#af2d0f087805b4b475d01b12d73508f09): > Return whether this RPC failed before the server could provide its status back to the client.> This could be because of explicit API cancellation from the client-side or server-side, because of deadline exceeded, network connection reset, HTTP/2 parameter configuration (e.g., max message size, max connection age), etc. It does NOT include failure due to a non-OK status return from the server application's request handler, including [Status::CANCELLED](https://grpc.github.io/grpc/cpp/classgrpc_1_1_status.html#a9994ffe95a0495915d82481c2ec594ab).> IsCancelled is always safe to call when using sync or callback API. When using async API, it is only safe to call IsCancelled after the AsyncNotifyWhenDone tag has been delivered. Thread-safe.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The change improves handling of awaitTermination(). It avoids resource waste of server side.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests.Manually tested on local with:1. start spark connect 2. create a streaming query3. call query.awaitTermination()4. exit() the client to disconnect it5. check that an error (RPC context is cancelled when executing awaitTermination()) is logged on server. It proves that awaitTermination() is exited on the server side when client disconnects```>>> query = (...  spark...  .readStream...  .format(\"rate\")...  .option(\"numPartitions\ \"1\")...  .load()...  .writeStream...  .format(\"memory\")...  .queryName(\"tableName_35\")...  .start()... )>>>>>> query.awaitTermination()...>>> exit()```
[SPARK-43815][SQL] Add `to_varchar` alias for `to_char`
[SPARK-43804][PYTHON] Test on nested structs support in Pandas UDF
[WIP] Support StructType input in Arrow-optimized Python UDFs
[SPARK-42859][PS][TESTS][FOLLOW-UPS] Delete unused file `test_parity_template.py`
[WIP][SPARK-42298] Assign name to _LEGACY_ERROR_TEMP_2132
[WIP][SPARK-43603][PS][CONNECT][TEST] Reorganize `ps.DataFrame` unit tests
[SPARK-43807][SQL] Migrate _LEGACY_ERROR_TEMP_1269 to PARTITION_SCHEMA_IS_EMPTY
[SPARK-43671][PS][FOLLOWUP] Refine `CategoricalOps` functions
[SPARK-43516][PYTHON][FOLLOWUP] Use pyspark cloudpickle instead of cloudpickle
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use `pyspark.cloudpickle` instead of `cloudpickle`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->All pyspark code should use pyspark.cloudpickle instead of `cloudpickle`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
[WIP][SPARK-40586][CONNECT] Decouple plan transformation and validation on server side
[SPARK-16484][SPARK-42527][CONNECT][FOLLOWUP] Simplify functions API by reuse existent functions.
[SPARK-43603][PS][CONNECT][TEST] Reorganize ps.DataFrame unit tests
[SPARK-43808][SQL][TESTS] Use `checkError()` to check `Exception` in `SQLViewTestSuite`
[SPARK-43801][SQL] Support unwrap date type to string type in UnwrapCastInBinaryComparison
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support unwrap date type to string type in UnwrapCastInBinaryComparison. This is similar idea to other instances in UnwrapCastInBinaryComparison. And there is an implementation already but closed later on https://github.com/apache/spark/pull/40294.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Such that filter such as `dt = date '2023-01-01'` where dt is a string column can be pushed down to file scan operator.This will help fix issue such as https://github.com/apache/iceberg/issues/4997### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit test.
[SPARK-43817][SPARK-43702][PYTHON] Support UserDefinedType in createDataFrame from pandas DataFrame and toPandas
[SPARK-43802][SQL][3.4] Fix codegen for unhex and unbase64 with failOnError=true
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a backport of https://github.com/apache/spark/pull/41317.Fixes an error with codegen for unhex and unbase64 expression when failOnError is enabled introduced in https://github.com/apache/spark/pull/37483.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Codegen fails and Spark falls back to interpreted evaluation:```Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type \"BASE64\"```in the code block:```/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),/* 110 */             project_value_1,/* 111 */             BASE64,/* 112 */             \"try_to_binary\");/* 113 */         }```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Bug fix.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added to the existing tests so evaluate an expression with failOnError enabled to test that path of the codegen.
[SPARK-43205][DOCS][FOLLOWUP] IDENTIFIER clause docs
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This is a follow-up of #41007 .This pull request documents the IDENTIFIER clause### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->If we don't document it, it doesn't exist...### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Acceptance test.
[SPARK-43821][CONNECT][TESTS] Make the prompt for `findJar` method in IntegrationTestUtils clearer
[SPARK-41775][PYTHON][FOLLOWUP] Use pyspark.cloudpickle instead of `cloudpickle` in torch distributor
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use pyspark.cloudpickle instead of `cloudpickle` in torch distributor### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make ser and deser code consistent### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43824][SPARK-43825] [SQL] Assign names to the error class _LEGACY_ERROR_TEMP_128[1-2]
[SPARK-43820][SPARK-43822][SPARK-43823][SPARK-43826][SPARK-43827] Assign names to the error class _LEGACY_ERROR_TEMP_241[1-7]
[WIP] Remove some dependency in pom
[SPARK-43830][BUILD] Update scalatest and scalatestplus related dependencies to newest version
[WIP][SPARK-43829][CONNECT] Improve SparkConnectPlanner by reuse Dataset and avoid construct new Dataset
[SPARK-43799][PYTHON] Add descriptor binary option to Pyspark Protobuf API
[SPARK-43836][BUILD] Make Scala 2.13 as default in Spark 3.5
[SPARK-43834][SQL] Use error classes in the compilation errors of `ResolveDefaultColumns`
[SPARK-43837][SQL] Assign a name to the error class _LEGACY_ERROR_TEMP_103[1-2]
[SPARK-43838][SQL] Fix subquery on single table with having clause can't be optimized
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Eg:```scalasql(\"create view t(c1, c2) as values (0, 1), (0, 2), (1, 2)\")sql(\"select c1, c2, (select count(*) cnt from t t2 where t1.c1 = t2.c1 \" +\"having cnt = 0) from t t1\").show() ```The error will throw:``` [PLAN_VALIDATION_FAILED_RULE_IN_BATCH] Rule org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery in batch Operator Optimization before Inferring Filters generated an invalid plan: The plan becomes unresolved: 'Project [toprettystring(c1#224, Some(America/Los_Angeles)) AS toprettystring(c1)#238, toprettystring(c2#225, Some(America/Los_Angeles)) AS toprettystring(c2)#239, toprettystring(cnt#246L, Some(America/Los_Angeles)) AS toprettystring(scalarsubquery(c1))#240]+- 'Project [c1#224, c2#225, CASE WHEN isnull(alwaysTrue#245) THEN 0 WHEN NOT (cnt#222L = 0) THEN null ELSE cnt#222L END AS cnt#246L]   +- 'Join LeftOuter, (c1#224 = c1#224#244)      :- Project [col1#226 AS c1#224, col2#227 AS c2#225]      :  +- LocalRelation [col1#226, col2#227]      +- Project [cnt#222L, c1#224#244, cnt#222L, c1#224, true AS alwaysTrue#245]         +- Project [cnt#222L, c1#224 AS c1#224#244, cnt#222L, c1#224]            +- Aggregate [c1#224], [count(1) AS cnt#222L, c1#224]               +- Project [col1#228 AS c1#224]                  +- LocalRelation [col1#228, col2#229]The previous plan: Project [toprettystring(c1#224, Some(America/Los_Angeles)) AS toprettystring(c1)#238, toprettystring(c2#225, Some(America/Los_Angeles)) AS toprettystring(c2)#239, toprettystring(scalar-subquery#223 [c1#224 && (c1#224 = c1#224#244)], Some(America/Los_Angeles)) AS toprettystring(scalarsubquery(c1))#240]:  +- Project [cnt#222L, c1#224 AS c1#224#244]:     +- Filter (cnt#222L = 0):        +- Aggregate [c1#224], [count(1) AS cnt#222L, c1#224]:           +- Project [col1#228 AS c1#224]:              +- LocalRelation [col1#228, col2#229]+- Project [col1#226 AS c1#224, col2#227 AS c2#225]   +- LocalRelation [col1#226, col2#227] ```The reason of error is the unresolved expression in `Join` node which generate by subquery decorrelation. The `duplicateResolved` in `Join` node are false. That's meaning the `Join` left and right have same `Attribute`, in this eg is `c1#224`. The right `c1#224` `Attribute` generated by having Inputs, because there are wrong having Inputs. This problem only occurs when there contain having clause.also do some code format fix.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix subquery bug on single table when use having clause<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43203][SQL] Move all Drop Table case to DataSource V2
<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?In order to fix DROP table behavior in session catalog cause by #37879. Because we always invoke V1 drop logic if the identifier looks like a V1 identifier. This is a big blocker for external data sources that provide custom session catalogs.So this PR move all Drop Table case to DataSource V2 (use drop table to drop view not include). More information please check https://github.com/apache/spark/pull/37879/files#r1170501180<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  3. If you fix some SQL features, you can provide some references of other DBMSes.  4. If there is design documentation, please add the link.  5. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Move Drop Table case to DataSource V2 to fix bug and prepare for remove drop table v1.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tested by:- V2 table catalog tests: `org.apache.spark.sql.execution.command.v2.DropTableSuite`- V1 table catalog tests: `org.apache.spark.sql.execution.command.v1.DropTableSuiteBase`<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
[SPARK-43839][SQL] Convert `_LEGACY_ERROR_TEMP_1337` to `UNSUPPORTED_FEATURE.TIME_TRAVEL`
Bump scala-library from 2.13.8 to 2.13.9
[SPARK-43840][INFRA] Switch `scala-213` GitHub Action Job to `scala-212`
[SPARK-43842][BUILD] Upgrade `gcs-connector` to 2.2.14
[SPARK-43841][SQL] Handle candidate attributes with no prefix in `StringUtils#orderSuggestedIdentifiersBySimilarity`
[SPARK-43845][INFRA] Setup Scala 2.12 Daily GitHub Action Job
[SPARK-43846][SQL][TESTS] Use checkError() to check Exception in SessionCatalogSuite
