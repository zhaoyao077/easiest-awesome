Positive	Negative	Text
2	-3	### What changes were proposed in this pull request?Fixes `DataFrameWriter.saveAsTable` to find the default source.### Why are the changes needed?Currently `DataFrameWriter.saveAsTable` fails when `format` is not specified because protobuf defines `source` as required and it will be an empty string instead of `null`, then `DataFrameWriter` tries to find the data source `\"\"`.The `source` field should be optional to let Spark decide the default source.### Does this PR introduce _any_ user-facing change?Users can call `saveAsTable` without `format`.### How was this patch tested?Enabled related tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In ANSI SQL mode, function Conv() should return an error if the internal conversion overflowsFor example, before the change:```> select conv('fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 16, 10)18446744073709551615```After the change```> select conv('fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 16, 10)org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select conv('fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 16, 10)       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Similar to the other SQL functions, this PR shows the overflow errors of `conv()` to users under ANSI SQL mode, instead of returning an unexpected number.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, function `conv()` will return an error if the internal conversion overflows### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UTs
1	-2	### What changes were proposed in this pull request?Fixes `DataFrameWriter.bucketBy` and `sortBy` to porperly use the first column name.### Why are the changes needed?Currently `DataFrameWriter.bucketBy` and `sortBy` mistakenly drop the first column name, which ends with `NoSuchElementException`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled related tests.
2	-1	### What changes were proposed in this pull request?Standardize __repr__ of CommonInlineUserDefinedFunction.### Why are the changes needed?To reach parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?The column representation reaches parity with the vanilla PySpark's now, as shown below.Before```>>> udf(lambda x : x + 1)(df.id)Column<'<lambda>(id), True, \"string\ 100, b'\\x80\\x05\\x95\\xe1\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x1fpyspark.cloudpickle.cloudpickle\\x94\\x8c\\x0e_make_function\\x94\\x93\\x94(h\\x00\\x8c\_builtin_type\\x94\\x93\\x94\\x8c\\x08CodeType\\x94\\x85\\x94R\\x94(K\\x01K\\x00K\\x00K\\x01K\\x02KCC\\x08|\\x00d\\x01\\x17\\x00S\\x00\\x94NK\\x01\\x86\\x94)\\x8c\\x01x\\x94\\x85\\x94\\x8c\\x07<stdin>\\x94\\x8c\\x08<lambda>\\x94K\\x01C\\x00\\x94))t\\x94R\\x94}\\x94(\\x8c\\x0b__package__\\x94N\\x8c\\x08__name__\\x94\\x8c\\x08__main__\\x94uNNNt\\x94R\\x94\\x8c$pyspark.cloudpickle.cloudpickle_fast\\x94\\x8c\\x12_function_setstate\\x94\\x93\\x94h\\x16}\\x94}\\x94(h\\x13h\\\x8c\\x0c__qualname__\\x94h\\\x8c\\x0f__annotations__\\x94}\\x94\\x8c\\x0e__kwdefaults__\\x94N\\x8c\\x0c__defaults__\\x94N\\x8c\__module__\\x94h\\x14\\x8c\\x07__doc__\\x94N\\x8c\\x0b__closure__\\x94N\\x8c\\x17_cloudpickle_submodules\\x94]\\x94\\x8c\\x0b__globals__\\x94}\\x94u\\x86\\x94\\x86R0\\x8c\\x11pyspark.sql.types\\x94\\x8c\StringType\\x94\\x93\\x94)\\x81\\x94\\x86\\x94.', f3.9'>```Now```>>> udf(lambda x : x + 1)(df.id)Column<'<lambda>(id)'>```### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When build with IntelliJ, I hit the following error from time to time:```spark/sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala:149:18value getArgument is not a member of org.mockito.invocation.InvocationOnMock      invocation.getArgument[Identifier](0).name match {```Sometimes the error can be solved by rebuilt with sbt or maven. But the best might be just avoid using the method that causes this compilation error.### Why are the changes needed?Make the life easier for IDE users.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual and existing tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add documentation for TimestampNTZ type### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Add documentation for the new data type TimestampNTZ.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Build docs and preview:<img width=\"782\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/218656254-096df429-851d-4046-8a6f-f368819c405b.png\"><img width=\"777\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/218656277-e8cfe747-2c45-476d-b70f-83c654e0b0f2.png\">
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->After https://github.com/apache/spark/pull/40000, the connect framework make format as optional but miss updating some test case.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->recover CI<img width=\"1052\" alt=\"image\" src=\"https://user-images.githubusercontent.com/12025282/218662255-fd8de2f9-f64e-43c6-8e19-0f2268882b44.png\">### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->pass CI
1	-3	### What changes were proposed in this pull request?```[info] - Write to Path with invalid input *** FAILED *** (298 milliseconds)[info]   Expected exception org.apache.spark.SparkClassNotFoundException to be thrown, but no exception was thrown (SparkConnectProtoSuite.scala:605)[info]   org.scalatest.exceptions.TestFailedException:[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)[info]   at org.scalatest.Assertions.assertThrows(Assertions.scala:825)[info]   at org.scalatest.Assertions.assertThrows$(Assertions.scala:804)[info]   at org.scalatest.funsuite.AnyFunSuite.assertThrows(AnyFunSuite.scala:1564)```### Why are the changes needed?master was broken### Does this PR introduce _any_ user-facing change?no, test-only### How was this patch tested?manually test
1	-2	### What changes were proposed in this pull request?Avoid calling `output` before analysis;Avoid applying optimizer rules before analysis;Remove the usage of optimizer `CombineUnions`, since it may discard the `PLAN_ID_TAG`### Why are the changes needed?it is not expected to calling `output` and apply optimizer rules before analysis### Does this PR introduce _any_ user-facing change?No### How was this patch tested?updated UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support scaleUpFactor and initialNumPartition in pySpark RDD take API, follow-up issue [SPARK-40211](https://issues.apache.org/jira/browse/SPARK-40211) and pr #37661 .### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->By setting this new configuration to a high value we could effectively mitigate the “run multiple jobs” overhead in pySpark RDD take behavior.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add a new test case.
1	-1	### What changes were proposed in this pull request?1, make `array_insert` accept int `pos` and `Any` value2, add it to connect### Why are the changes needed?to be consistent with other pyspark functions### Does this PR introduce _any_ user-facing change?yes### How was this patch tested?added tests
1	-2	### What changes were proposed in this pull request?This fixes how setting for limiting recursive depth is handled in Protobuf functions (`recursive.fields.max.depth`). Original documentation says as setting it to '0' removes the recursive field. But we never did that. We allow at least once. E.g. schema for recursive message 'EventPerson' does not change between the settings '0' or '1'. This fixes it by requiring the max depth to be at least 1. It also fixes how the recursion enfored. Updated the tests and added an extra test with new protobuf 'EventPersonWrapper'. I will annotate the diff inline pointing to main fixes. ### Why are the changes needed?This fixes a bug with enforcing `recursive.fields.max.depth` and clarifies more in the documentation. ### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Unit tests
2	-1	### What changes were proposed in this pull request?`array_append` should accept `Any` value### Why are the changes needed?make `array_append` consistent with other array functions in pyspark### Does this PR introduce _any_ user-facing change?yes### How was this patch tested?added UT
1	-2	### What changes were proposed in this pull request?match https://github.com/apache/spark/pull/40135### Why are the changes needed?`DataFrame.drop` should handle duplicated columns properly.we can not always convert column names to columns when there are multi columns with the same name.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?enabled tests
2	-3	### What changes were proposed in this pull request?Update javascript library DataTables, used in the UI, to 1.13.2.### Why are the changes needed?The 1.10.25 version of DataTables, that Spark currently uses, seems vulerable: https://security.snyk.io/package/npm/datatables.net/1.10.25. The vulnerability may not affect Spark but it is safer to update.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
2	-2	Currently PySpark version of `catalog.cacheTable` function does not support to specify storage level. This is to add that.After changes:## Spark Connect```bin/pyspark --remote \"local[*]\"Python 3.9.5 (default, Nov 23 2021, 15:27:38) [GCC 9.3.0] on linuxType \"help\ \"copyright\ \"credits\" or \"license\" for more information.Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/02/17 20:41:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.5 (default, Nov 23 2021 15:27:38)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> spark.range(1).write.saveAsTable(\"tab1\ format=\"csv\ mode=\"overwrite\")>>> spark.catalog.listTables()[Table(name='tab1', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]>>> spark.catalog.cacheTable(\"tab1\")>>> spark.catalog.isCached(\"tab1\")True>>> spark.catalog.clearCache()>>> spark.catalog.isCached(\"tab1\")False>>> from pyspark.storagelevel import StorageLevel>>> spark.catalog.cacheTable(\"tab1\ StorageLevel.OFF_HEAP)>>> spark.catalog.isCached(\"tab1\")True```## PySpark```/home/spark# bin/pysparkPython 3.9.5 (default, Nov 23 2021, 15:27:38) [GCC 9.3.0] on linuxType \"help\ \"copyright\ \"credits\" or \"license\" for more information.Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/02/17 20:43:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0-SNAPSHOT      /_/Using Python version 3.9.5 (default, Nov 23 2021 15:27:38)Spark context Web UI available at http://localhost:4040Spark context available as 'sc' (master = local[*], app id = local-1676666626670).SparkSession available as 'spark'.>>> spark.range(1).write.saveAsTable(\"tab2\ format=\"csv\ mode=\"overwrite\")>>> spark.catalog.listTables()[Table(name='tab2', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]>>> spark.catalog.cacheTable(\"tab2\")>>> >>> spark.catalog.isCached(\"tab2\")True>>> spark.catalog.clearCache()>>> spark.catalog.isCached(\"tab2\")False>>> from pyspark.storagelevel import StorageLevel>>> spark.catalog.cacheTable(\"tab2\ StorageLevel.OFF_HEAP)>>> spark.catalog.isCached(\"tab2\")True```### What changes were proposed in this pull request?Add extra parameter to catalog.cacheTable### Why are the changes needed?To allow users specify which storage level to use in cache in PySpark/Connect code### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?Updated existing test cases 
2	-1	### What changes were proposed in this pull request?This PR improves `TreeNode.multiTransform()` to generate the alternative sequences only if needed and fully dynamically. Consider the following simplified example:```(a + b).multiTransform {  case a => Stream(1, 2)  case b => Stream(10, 20)}```the result is the cartesian product: `Stream(1 + 10, 2 + 10, 1 + 20, 2 + 20)`.Currently `multiTransform` calculates the 2 alternative streams for `a` and `b` **before** start building building the cartesian product stream using `+`. So kind of caches the \"inner\" `Stream(1, 2)` in the beginning and when the \"outer\" stream (`Stream(10, 20)`) iterates from `10` to `20` reuses the cache. Although this caching is sometimes useful it has 2 drawbacks:- If the \"outer\" (`b` alternatives) stream returns `Seq.emtpy` (to indicate pruning) the alternatives for the `a` are unecessary calculated and will be discarded.- The \"inner\" stream transformation can't depend on the current \"outer\" stream alternative.   E.g. let's see the above `a + b` example but we want to transform both `a` and `b` to `1` and `2`, and we want to have only those alternatives where these 2 are transformed equal (`Stream(1 + 1, 2 + 2)`). This is currently it is not possible with a single `multiTransform` call due to the inner stream alternatives are calculated in advance and cached. But, if `multiTransform` would be dynamic and the \"inner\" alternatives stream would be recalculated when the \"outer\" alternatives stream iterates then this would be possible:  ```  // Cache  var a_or_b = None  (a + b).multiTransform {    case a | b =>      // Return alternatives from cache if this is not the first encounter      a_or_b.getOrElse(        // Besides returning the alternatives for the first encounter, also set up a mechanism to        // update the cache when the new alternatives are requested.        Stream(Literal(1), Literal(2)).map { x =>          a_or_b = Some(Seq(x))          x        }.append {          a_or_b = None          Seq.empty        })  }  ```Please note:- that this is a simplified example and we could have run 2 simple `transforms` to get the exprected 2 expressions, but `multiTransform` can do other orthogonal transformations in the same run (e.g. `c` -> `Seq(100, 200)`) and `multiTransform` has the advantage of returning the results lazlily as a stream.- the original behaviour of caching \"inner\" alternative streams is still doable and actually our current usecases in `AliasAwareOutputExpression` and in `BroadcastHashJoinExec` still do it as they store the alternatives in advance in maps and the `multiTransform` call just gets the alternatives from those maps when needed. ### Why are the changes needed?Improvement to make `multiTransform` more versatile.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added new UTs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In v2 writes, make createJobDescription in FileWrite.toBatch not lazy### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->There is a difference in behavior between v1 writes and v2 writes in the order of events happening when configuring the file writer and the committer.v1:writer.prepareWrite()committer.setupJob()v2:committer.setupJob()writer.prepareWrite() This is because the `prepareWrite()` call (that is the one performing the call `job.setOutputFormatClass(classOf[ParquetOutputFormat[Row]])`)happens as part of the `createWriteJobDescription` which is `lazy val` in the `toBatch` call and therefore is evaluated after the `committer.setupJob` at the end of the `toBatch`This causes issues when evaluating the committer as some elements might be missing, for example the aforementioned output format class not being set, causing the committer being set up as generic write instead of parquet write. The fix is very simple and it is to make the `createJobDescription` call non-lazy### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing test.
1	-1	### What changes were proposed in this pull request?Add a lot of the existing Dataframe APIs to the Spark Connect Scala Client.This PR does not contain:- Typed APIs- Aggregation- Streaming (not supported by connect just yet)- NA/Stats functions- TempView registration.### Why are the changes needed?We want the Scala Client Dataset to reach parity with the existing Dataset.### How was this patch tested?Added a lot of golden tests.Added a number of test cases to the E2E suite for the functionality that requires server interaction.
1	-1	### What changes were proposed in this pull request?Added some changes to the docstrings to make the docs more readable.<img width=\"752\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/218838122-c28fcba7-dff8-498d-9be2-57b8efc498cd.png\"><img width=\"725\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/218882816-729508fb-afc7-49b5-8ff8-92d42390fa2d.png\">### Why are the changes needed?Just for readability's sake.### Does this PR introduce _any_ user-facing change?No?### How was this patch tested?Not Needed.
1	-1	### What changes were proposed in this pull request?This is a follow-up of #39882.Fixes `FunctionsParityTests.test_raise_error` to call the proper test.### Why are the changes needed?`FunctionsParityTests.test_raise_error` should've called `check_raise_error`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?The fixed test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->With the configuration `spark.sql.timestampType`,  TIMESTAMP in Spark is a user-specified alias associated with one of the TIMESTAMP_LTZ and TIMESTAMP_NTZ variations. This is quite complicated to Spark users.There is another option `spark.sql.sources.timestampNTZTypeInference.enabled` for schema inference. I would like to introduce it in https://github.com/apache/spark/pull/40005 but having two flags seems too much. After thoughts, I decide to merge `spark.sql.sources.timestampNTZTypeInference.enabled` into `spark.sql.timestampType` and let  `spark.sql.timestampType` control the schema inference behavior.We can have followups to add data source options \"inferTimestampNTZType\" for CSV/JSON/partiton column like JDBC data source did.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make the new feature simpler.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the feature is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UTI also tried ```git grep spark.sql.sources.timestampNTZTypeInference.enabledgit grep INFER_TIMESTAMP_NTZ_IN_DATA_SOURCES```to make sure the flag INFER_TIMESTAMP_NTZ_IN_DATA_SOURCES is removed.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove constructed but never used variable in test case.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Codebase cleanup.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?Fixes `DataFrameWriter.insertInto` to call the corresponding method instead of `saveAsTable`.### Why are the changes needed?Currently `SparkConnectPlanner` calls `saveAsTable` instead of `insertInto` even for `DataFrameWriter.insertInto` in Spark Connect, but they have different logic internally, so we should use the corresponding method.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled related tests.
2	-1	### What changes were proposed in this pull request?Add SparkSession Read API to read data into Spark via Scala Client:```DataFrameReader.format(…).option(“key”, “value”).schema(…).load()```The following methods are skipped by the Scala Client on purpose:```[info]   deprecated method json(org.apache.spark.api.java.JavaRDD)org.apache.spark.sql.Dataset in class org.apache.spark.sql.DataFrameReader does not have a correspondent in client version[info]   deprecated method json(org.apache.spark.rdd.RDD)org.apache.spark.sql.Dataset in class org.apache.spark.sql.DataFrameReader does not have a correspondent in client version[info]   method json(org.apache.spark.sql.Dataset)org.apache.spark.sql.Dataset in class org.apache.spark.sql.DataFrameReader does not have a correspondent in client version[info]   method csv(org.apache.spark.sql.Dataset)org.apache.spark.sql.Dataset in class org.apache.spark.sql.DataFrameReader does not have a correspondent in client version```### Why are the changes needed?To read data from csv etc. format.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?E2E, Golden tests.
2	-2	### What changes were proposed in this pull request?Always set `containsNull=true` in the data type returned by `ArrayInsert#dataType`.### Why are the changes needed?PR #39970 fixed an issue where the data type for `array_insert` did not always have `containsNull=true` when the user was explicitly inserting a nullable value into the array. However, that fix does not handle the case where `array_insert` implicitly inserts null values into the array (e.g., when the insertion position is out-of-range):```spark-sql> select array_insert(array('1', '2', '3', '4'), -6, '5');23/02/14 16:10:19 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)java.lang.NullPointerException\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)```Because we can't know at planning time whether the insertion position will be out of range, we should always set `containsNull=true` on the data type for `array_insert`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit tests.
1	-1	### What changes were proposed in this pull request?This PR adds most Column APIs for the Spark Connect Scala Client.### Why are the changes needed?We want the Scala Client to have API parity with the existing SparkSession/Dataset APIs.### How was this patch tested?Golden files, and I added a test for local behavior.
2	-3	### What changes were proposed in this pull request?Throw an error when the second value in day(hour, minute) to second interval out of range [0, 59]### Why are the changes needed?Currently an invalid second value will not get an error>>> spark.sql(\"select INTERVAL '10 01:01:99' DAY TO SECOND\")DataFrame[INTERVAL '10 01:02:39' DAY TO SECOND: interval day to second]{}But minute range check is ok>>> spark.sql(\"select INTERVAL '10 01:60:01' DAY TO SECOND\")requirement failed: minute 60 outside range [0, 59](line 1, pos 16)We need check second value too### Does this PR introduce _any_ user-facing change?no### How was this patch tested?New unit tests.
1	-2	### What changes were proposed in this pull request?use `Distinct` instead of `Deduplicate`### Why are the changes needed?to delay analysis, see https://github.com/apache/spark/pull/40008#discussion_r1106529796### Does this PR introduce _any_ user-facing change?plan shown in `explain` may change### How was this patch tested?updated UT
1	-1	### What changes were proposed in this pull request?Remove unused imports### Why are the changes needed?clean up### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing UT
2	-3	### What changes were proposed in this pull request?This PR fixes `SparkR` `install.spark` method.```$ curl -LO https://dist.apache.org/repos/dist/dev/spark/v3.3.2-rc1-bin/SparkR_3.3.2.tar.gz$ R CMD INSTALL SparkR_3.3.2.tar.gz$ RR version 4.2.1 (2022-06-23) -- \"Funny-Looking Kid\"Copyright (C) 2022 The R Foundation for Statistical ComputingPlatform: aarch64-apple-darwin20 (64-bit)R is free software and comes with ABSOLUTELY NO WARRANTY.You are welcome to redistribute it under certain conditions.Type 'license()' or 'licence()' for distribution details.  Natural language support but running in an English localeR is a collaborative project with many contributors.Type 'contributors()' for more information and'citation()' on how to cite R or R packages in publications.Type 'demo()' for some demos, 'help()' for on-line help, or'help.start()' for an HTML browser interface to help.Type 'q()' to quit R.> library(SparkR)Attaching package: ‘SparkR’The following objects are masked from ‘package:stats’:    cov, filter, lag, na.omit, predict, sd, var, windowThe following objects are masked from ‘package:base’:    as.data.frame, colnames, colnames<-, drop, endsWith, intersect,    rank, rbind, sample, startsWith, subset, summary, transform, union> install.spark()Spark not found in the cache directory. Installation will start.MirrorUrl not provided.Looking for preferred site from apache website...Preferred mirror site found: https://dlcdn.apache.org/sparkDownloading spark-3.3.2 for Hadoop 2.7 from:- https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgztrying URL 'https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.7.tgz'simpleWarning in download.file(remotePath, localPath): downloaded length 0 != reported length 196> install.spark(hadoopVersion=\"3\")Spark not found in the cache directory. Installation will start.MirrorUrl not provided.Looking for preferred site from apache website...Preferred mirror site found: https://dlcdn.apache.org/sparkDownloading spark-3.3.2 for Hadoop 3 from:- https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-3.tgztrying URL 'https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-3.tgz'simpleWarning in download.file(remotePath, localPath): downloaded length 0 != reported length 196```Note that this is a regression at Spark 3.3.0 and not a blocker for on-going Spark 3.3.2 RC vote.### Why are the changes needed?https://spark.apache.org/docs/latest/api/R/reference/install.spark.html#ref-usage![Screenshot 2023-02-14 at 10 07 49 PM](https://user-images.githubusercontent.com/9700541/218946460-ab7eab1b-65ae-4cb2-bc7c-5810ad359ac9.png)First, the existing Spark 2.0.0 link is broken.- https://spark.apache.org/docs/latest/api/R/reference/install.spark.html#details- http://apache.osuosl.org/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz (Broken)Second, Spark 3.3.0 changed the Hadoop postfix pattern from the distribution files so that the function raises errors as described before.- http://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz (Old Pattern)- http://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop2.tgz (New Pattern)### Does this PR introduce _any_ user-facing change?No, this fixes a bug like Spark 3.2.3 and older versions.### How was this patch tested?Pass the CI and manual testing. Please note that the link pattern is correct although it fails because 3.5.0 is not published yet.```$ NO_MANUAL=1 ./dev/make-distribution.sh --r$ R CMD INSTALL R/SparkR_3.5.0-SNAPSHOT.tar.gz$ R> library(SparkR)> install.spark()Spark not found in the cache directory. Installation will start.MirrorUrl not provided.Looking for preferred site from apache website...Preferred mirror site found: https://dlcdn.apache.org/sparkDownloading spark-3.5.0 for Hadoop 3 from:- https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgztrying URL 'https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz'simpleWarning in download.file(remotePath, localPath): downloaded length 0 != reported length 196```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Updates to the PySpark documentation web site:- Fixing typo on the Getting Started page (Version => Versions)- Capitalizing \"In/Out\" in the DataFrame Quick Start notebook- Adding \"(Legacy)\" to the Spark Streaming heading on the Spark Streaming page- Reorganizing the User Guide page to list PySpark guides first, minor language updates, and removing links to legacy streaming and RDD programming guides to not promote these as prominently and focus on the recommended APIs<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To improve usability of the PySpark doc website by adding guidance (calling out legacy APIs), fixing a few language issues, and making PySpark content more prominent.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing PySpark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built and manually reviewed/tested the PySpark documentation web site locally.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-3	### What changes were proposed in this pull request?Throw an error when the second value in day(hour, minute) to second interval out of range [0, 59]### Why are the changes needed?Currently an invalid second value will not get an error```spark.sql(\"select INTERVAL '10 01:01:99' DAY TO SECOND\")DataFrame[INTERVAL '10 01:02:39' DAY TO SECOND: interval day to second]{}```But minute range check is ok```spark.sql(\"select INTERVAL '10 01:60:01' DAY TO SECOND\")requirement failed: minute 60 outside range [0, 59](line 1, pos 16)```### We need check second value tooDoes this PR introduce any user-facing change?no### How was this patch tested?New unit tests.
2	-2	### What changes were proposed in this pull request?This PR aims to remove `Hadoop 2` GitHub Action job for Apache Spark 3.5+### Why are the changes needed?We are removing Hadoop 2 support and the job has been broken for three weeks already.- https://github.com/apache/spark/actions/workflows/build_hadoop2.yml![Screenshot 2023-02-15 at 12 41 42 AM](https://user-images.githubusercontent.com/9700541/218976613-5dcdf21f-5660-4b5b-b1bc-1697915da663.png)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual review.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add a doc of how `_metadata` nullability is implemented for generated metadata columns.### Why are the changes needed?Improve readability### Does this PR introduce _any_ user-facing change?No### How was this patch tested?N/A
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The CliSessionState does not contain the current database info, we shall use spark's `catalog.currentDatabase` ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, when users use spark-sql and switch database, the prompt now show the correct one instead of `default`### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally tested```textmatespark-sql (default)> create database abc;Time taken: 0.24 secondsspark-sql (default)> use abc;Time taken: 0.027 secondsspark-sql (ABC)>````
1	-1	### What changes were proposed in this pull request?Add `applyInPandasWithState` to API references### Why are the changes needed?It's missing in doc### Does this PR introduce _any_ user-facing change?yes### How was this patch tested?CI
2	-2	### What changes were proposed in this pull request?Spark 3.1.x already EOL and has been deleted from https://dist.apache.org/repos/dist/release/spark, this pr remove the filter condition `!(v.startsWith(\"3.1\") && SystemUtils.isJavaVersionAtLeast(JavaVersion.JAVA_17))`  from `testingVersions` check, all version already support test with Java 17.### Why are the changes needed?Delete unnecessary test conditions.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-2	### What changes were proposed in this pull request?This PR proposes to use `BigInt` instead of `Long` datatype for  conversion of numbers between different radixes, because of this we will get rid of overflows.This change allows to convert big numbers, like:```spark-sql> SELECT CONV(SUBSTRING('0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff', 3), 16, 10);```will give correct result:`115792089237316195423570985008687907853269984665640564039457584007913129639935`**P.S. According to the NumberConverterBenchmark it work faster with Java 8 and slower with Java 11 and 17.You can check benchmark results.**### Why are the changes needed?Changes allow to convert big numbers among different radixes.### Does this PR introduce _any_ user-facing change?Users could convert big values by using `conv` function, without an overflow.### How was this patch tested?By existing tests + an additional test cases were added to:`sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/MathExpressionsSuite.scala``sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberConverterSuite.scala`
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR tries to implement functions.max for Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing scala client testing framework.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Similar with https://github.com/apache/spark/pull/37327, this PR renames the JDBC data source option `inferTimestampNTZType` as `preferTimestampNTZ`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It is simpler and more straightforward. Also, it is consistent with the CSV data source option introduced in https://github.com/apache/spark/pull/37327,### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the TimestampNTZ project is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Rename the CSV data source option `prefersDate` as `preferDate`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->All the CSV data source options doesn't have a `s` on the verb in the naming. For example, `inferSchema`, `ignoreLeadingWhiteSpace` and `ignoreTrailingWhiteSpace`. The renaming makes the naming consistent.Also, the title of JIRA https://issues.apache.org/jira/browse/SPARK-39904 uses `preferDate` as well.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the data source option is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Creating a new PySpark migration guide sub page and consolidating the existing 9 separate pages into this one new page. This makes it easier to take a look across multiple version upgrades by simply scrolling on the page instead of having to navigate back and forth. Note that this is similar to the Spark Core Migration Guide page here:https://spark.apache.org/docs/latest/core-migration-guide.htmlAlso updating the existing main Migration Guide page to point to this new sub page and making some minor language updates to help readers.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To improve usability of the PySpark doc site.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing PySpark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built and tested the PySpark documentation web site locally.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-3	### What changes were proposed in this pull request?Removing redundant check for whether GPUs exist on the driver node.### Why are the changes needed?For slightly cleaner code. Could close PR if we don't need to merge it in.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?As long as normal tests work, we don't expect any other failures.
1	-1	### What changes were proposed in this pull request?Enables the doctest `pyspark.sql.connect.readwriter.DataFrameReader.option`.### Why are the changes needed?The issue described [SPARK-41817](https://issues.apache.org/jira/browse/SPARK-41817) was already fixed at #39545.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?The enabled test.
1	-1	### What changes were proposed in this pull request?This PR proposes to `pyspark.sql.connect.utils` to keep common codes, especially about dependnecies.### Why are the changes needed?For example, [SPARK-41457](https://issues.apache.org/jira/browse/SPARK-41457) added `require_minimum_grpc_version` in `pyspark.sql.pandas.utils` which is actually unrelated from the connect module. we should move all to a separate utils directory for better readability and maintenance. ### Does this PR introduce _any_ user-facing change?No, dev-only.### How was this patch tested?Existing tests should cover this.
1	-3	### What changes were proposed in this pull request?Clean-up results in `ClientE2ETestSuite`.### Why are the changes needed?`ClientE2ETestSuite` is very noisy because we do not clean-up results. This makes testing a bit annoying.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?It is a test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The current default value DS V2 API is a bit inconsistent. The `createTable` API only takes `StructType`, so implementations must know the special metadata key of the default value to access it. The `TableChange` API has the default value as an individual field.This API adds a new `Column` interface, which holds both current default (as a SQL string) and exist default (as a v2 literal). `createTable` API now takes `Column`. This avoids the need of special metadata key and is also more extensible when adding more special cols like generated cols. This is also type-safe and makes sure the exist default is literal. The implementation is free to decide how to encode and store default values. Note: backward compatibility is taken care of.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->better DS v2 API for default value### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
1	-1	### What changes were proposed in this pull request?This PR adds the following functions to Spark Connect Scala Client:- Sort Functions- Aggregate Functions- Misc Functions- Math Functions### Why are the changes needed?We want to the Spark Connect Scala Client to reach parity with the original functions API.### Does this PR introduce _any_ user-facing change?Yes, it adds a lot of functions.### How was this patch tested?Added test for all functions and their significant variations.
2	-2	### What changes were proposed in this pull request?This is found during Apache Spark 3.3.2 docker image publishing. It's not an Apache Spark but important for `docker-image-tool.sh` to provide backward compatibility during cross-building. This PR targets for all **future releases**, Apache Spark 3.4.0/3.3.3/3.2.4.### Why are the changes needed?Docker `buildx` v0.10.0 publishes OCI Manifests by default which is not supported by `docker manifest` command like the following.https://github.com/docker/buildx/issues/1509```$ docker manifest inspect apache/spark:v3.3.2no such manifest: docker.io/apache/spark:v3.3.2```Note that the published images are working on both AMD64/ARM64 machines, but `docker manifest` cannot be used. For example, we cannot create `latest` tag.### Does this PR introduce _any_ user-facing change?This will fix the regression of Docker `buildx`.### How was this patch tested?Manually builds the multi-arch image and check `manifest`.```$ docker manifest inspect apache/spark:v3.3.2{   \"schemaVersion\": 2,   \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\   \"manifests\": [      {         \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\         \"size\": 3444,         \"digest\": \"sha256:30ae5023fc384ae3b68d2fb83adde44b1ece05f926cfceecac44204cdc9e79cb\         \"platform\": {            \"architecture\": \"amd64\            \"os\": \"linux\"         }      },      {         \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\         \"size\": 3444,         \"digest\": \"sha256:aac13b5b5a681aefa91036d2acae91d30a743c2e78087c6df79af4de46a16e1b\         \"platform\": {            \"architecture\": \"arm64\            \"os\": \"linux\"         }      }   ]}```
1	-3	### What changes were proposed in this pull request?SPARK-27180 introduced some third-party Java source code  to solve Yarn module test failure,  but maven and sbt can also test pass without them, so this pr remove these files.### Why are the changes needed?Clean up the third-party Java source code copy in Spark.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- manual check:**Maven** ```build/mvn cleanbuild/mvn clean install -DskipTestes -pl resource-managers/yarn -am -Pyarnbuild/mvn -Dtest=none -DwildcardSuites=org.apache.spark.deploy.yarn.YarnClusterSuite -pl resource-managers/yarn test -Pyarnbuild/mvn test -pl resource-managers/yarn -Pyarn -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest ```Both `YarnClusterSuite` and full module test passed.**SBT**```build/sbt clean yarn/test -Pyarn -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest ```All tests passed.
2	-1	### What changes were proposed in this pull request?This pr cleans up unused declarations in the Hive module:- Input parameter `dataTypes` of `HiveInspectors#wrap` method: the input parameter `dataTypes` introduced by SPARK-9354, but after SPARK-17509, the implementation of `HiveInspectors#wrap` no longer needs to explicitly pass `dataTypes` and it becomes a unused, and `inputDataTypes` in `HiveSimpleUDF` becomes a unused after this pr- `UNLIMITED_DECIMAL_PRECISION` and `UNLIMITED_DECIMAL_SCALE` in `HiveShim`: these two `val` introduced by SPARK-6909 for unlimited decimals, but SPARK-9069 remove unlimited precision support for DecimalType and  SPARK-14877 deleted `object HiveMetastoreTypes` and used `.catalogString` instead, these two `val` are not used anymore.### Why are the changes needed?Code clean up.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?Currently, the Spark Connect service's `client_type` attribute (which is really [useragent]) is set to `_SPARK_CONNECT_PYTHON` to signify PySpark.With this change, the connection for the Spark Connect remote accepts an optional`user_agent` parameter which is then passed down to the service.[user agent]: https://www.w3.org/WAI/UA/work/wiki/Definition_of_User_Agent### Why are the changes needed?This enables partners using Spark Connect to set their application as the user agent,which then allows visibility and measurement of integrations and usages of sparkconnect.### Does this PR introduce _any_ user-facing change?A new optional `user_agent` parameter is now recognized as part of the Spark Connectconnection string.### How was this patch tested?- unit tests attached- manually running the `pyspark` binary with the `user_agent` connection string set and   verifying the payload sent to the server. Similar testing for the default.
2	-3	### What changes were proposed in this pull request?The `ProtoToPlanTestSuite` were broken for Scala 2.13 . This was caused by the following two problems:- Explain output between 2.12 and 2.13 is not stable because we render collection implementations as well. For this I changed the rendering of the offending classes to be version agnostic.- UDF code had deserialization issues. This was always the risk. I have removed those tests, we will work on improving UDF coverage in a follow-up.### Why are the changes needed?We want to test Scala 2.13 as well.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Test only change.
2	-1	### What changes were proposed in this pull request?This makes `ProtoToPlanTestSuite` use analyzed plans instead of parsed plans.### Why are the changes needed?This is to increase the fidelity of the `ProtoToPlanTestSuite`, especially since we are going to be adding functions. Functions are special because the spark connect planner leaves them unresolved, the actual binding only happens in the analyzer. Without running the analyzer we would not know if the bindings are correct.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?It is a test change.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Starting to support basic aggregation in Scala client. The first step is to support aggregation by strings.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?Support v2 DESCRIBE TABLE EXTENDED for columns### Why are the changes needed?DS v1/v2 command parity### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR is a follow-up for https://issues.apache.org/jira/browse/SPARK-42131. It adds parentheses in TOP clause in MSSQL dialect as they are only omitted for backward compatibility and required otherwise: https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql?view=sql-server-ver16#compatibility-support.I also added tests to check Limit clause translation for Oracle and MSSQL dialects.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Updates TOP to include round brackets.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I added unit tests.
1	-1	### What changes were proposed in this pull request?Enables tests in `ReadwriterV2ParityTests`.### Why are the changes needed?Now that `DataFrameWriterV2` for Spark Connect is implemented, we can enable tests in `ReadwriterV2ParityTests`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled tests.
1	-1	### What changes were proposed in this pull request?Implemented the basic Dataset#write API to allow users to write the df into tables, csv etc. files.### Why are the changes needed?Basic write operation.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Integration tests.
1	-2	### What changes were proposed in this pull request?This PR aims to add JVM GC option test coverage to K8s Integration Suite.To reuse the existing code, `isG1GC` variable is moved from `MemoryManager` to `Utils`.### Why are the changes needed?To provide more test coverage for JVM Options.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs```[info] KubernetesSuite:[info] - SPARK-42190: Run SparkPi with local[*] (4 seconds, 990 milliseconds)[info] - Run SparkPi with no resources (7 seconds, 101 milliseconds)[info] - Run SparkPi with no resources & statefulset allocation (7 seconds, 27 milliseconds)[info] - Run SparkPi with a very long application name. (7 seconds, 100 milliseconds)[info] - Use SparkLauncher.NO_RESOURCE (7 seconds, 947 milliseconds)[info] - Run SparkPi with a master URL without a scheme. (6 seconds, 932 milliseconds)[info] - Run SparkPi with an argument. (9 seconds, 47 milliseconds)[info] - Run SparkPi with custom labels, annotations, and environment variables. (6 seconds, 969 milliseconds)[info] - All pods have the same service account by default (6 seconds, 916 milliseconds)[info] - Run extraJVMOptions check on driver (3 seconds, 964 milliseconds)[info] - Run extraJVMOptions JVM GC option check - G1GC (3 seconds, 948 milliseconds)[info] - Run extraJVMOptions JVM GC option check - Other GC (4 seconds, 51 milliseconds)...```
2	-2	### What changes were proposed in this pull request?This patch allows for eager execution of SQL statements using the Spark Connect Data Frame API. When `spark.sql` is called it will send the query to the server where a DataFrame is created and cached with the Session. If the query is a command this transformation will immediately kick of the execution.### Why are the changes needed?Compatibility### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit and e2e testing- [x] Add unit tests- [x] Add e2e python tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-41448](https://issues.apache.org/jira/browse/SPARK-41448) make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
1	-2	### What changes were proposed in this pull request?This pr aims upgrade `cyclonedx-maven-plugin` from 2.7.3 to 2.7.5### Why are the changes needed?The release notes as follows:- https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.4- https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.5On the other hand, we can upgrade to use maven 3.9.0 to build Spark after upgrading `cyclonedx-maven-plugin` to 2.7.5, otherwise, the build error described in SPARK-42380 will occur.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions.- Manual check the `cyclonedx.xml` file can be generated normally.
2	-3	### What changes were proposed in this pull request?* Reduce the number of retries when sending a request to spark connect service.* Slightly improve debug logging### Why are the changes needed?Currently, 15 retries with the current backoff strategy result in the client sitting inthe retry loop for ~400 seconds in the worst case. This means, applications andusers using the spark connect client will hang for >6 minutes with no response.Instead, simply reduce the retry to 4, which reduces the retry loop in the worst caseto a reasonable ~4 seconds.### Does this PR introduce _any_ user-facing change?Reduces the number of retries to the service and improves error response timewhen the spark connect service is unresponsive or unreachable.### How was this patch tested?Manual testing
2	-1	### What changes were proposed in this pull request?This PR proposes to complete missing API Reference for Spark Connect.Built API docs should include \"Changed in version\" for Spark Connect when it's implemented as below:<img width=\"814\" alt=\"Screen Shot 2023-02-20 at 9 49 09 AM\" src=\"https://user-images.githubusercontent.com/44108233/219986313-374e0959-b8c5-44f6-942c-bba1c0407909.png\">### Why are the changes needed?Improving usability for Spark Connect.### Does this PR introduce _any_ user-facing change?No, it's documentation.### How was this patch tested?Manually built docs, confirmed each function and class one by one.
1	-2	### What changes were proposed in this pull request?1. Change to get matching partition names rather than partition objects when drop partitions### Why are the changes needed?1. Partition names are enough to drop partitions2. It can reduce the time overhead and driver memory overhead. ### Does this PR introduce _any_ user-facing change?Yes, we have add a new sql conf to enable this feature: `spark.sql.hive.dropPartitionByName.enabled`### How was this patch tested?Add new tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Adding more API to `agg` including max,min,mean,count,avg,sum.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?Enables a doctest for `DataFrame.write`.### Why are the changes needed?Now that `DataFrame.write.saveAsTable` was fixed, we can enabled the doctest.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled the doctest.
2	-1	### What changes were proposed in this pull request?This aims to regenerate benchmark results on `master` branch as a baseline for Spark 3.5.0 and a way to comparing Apache Spark 3.4.0 branch.### Why are the changes needed?These are reference values with minor changes.```- OpenJDK 64-Bit Server VM 1.8.0_352-b08 on Linux 5.15.0-1023-azure+ OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1031-azure- OpenJDK 64-Bit Server VM 11.0.17+8 on Linux 5.15.0-1023-azure+ OpenJDK 64-Bit Server VM 11.0.18+10 on Linux 5.15.0-1031-azure- OpenJDK 64-Bit Server VM 17.0.5+8 on Linux 5.15.0-1023-azure+ OpenJDK 64-Bit Server VM 17.0.6+10 on Linux 5.15.0-1031-azure```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual review.
3	-4	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Showing the essential information when throwing `InvalidUnsafeRowException`. Including where the check failed, and status of the `unsafeRow` and `expctedSchema`Example output:```[UnsafeRowStatus] expectedSchema: StructType(StructField(key1,IntegerType,false),StructField(key2,IntegerType,false),StructField(sum(key1),IntegerType,false),StructField(sum(key2),IntegerType,false)), expectedSchemaNumFields: 4, numFields: 4, bitSetWidthInBytes: 8, rowSizeInBytes: 40 fieldStatus: [UnsafeRowFieldStatus] index: 0, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1[UnsafeRowFieldStatus] index: 1, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1[UnsafeRowFieldStatus] index: 2, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1[UnsafeRowFieldStatus] index: 3, expectedFieldType: IntegerType, isNull: false, isFixedLength: true, offset: -1, size: -1```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Right now if such error happens, it's hard to track where it errored, and what the misbehaved row & schema looks like. With this change these information are more clear.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As https://github.com/apache/spark/pull/40005#pullrequestreview-1299089504 pointed out, the java doc for data type recommends using factory methods provided in org.apache.spark.sql.types.DataTypes. Since the ANSI interval types missed the `DataTypes` as well, this PR also revise their doc.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Unify the data type doc### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Local preview<img width=\"826\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/219821685-321c2fd1-6248-4930-9c61-eec68f0dcb50.png\">
1	-1	### What changes were proposed in this pull request?Adding DataFrameWriterV2. This allows users to use the Dataset#writeTo API.### Why are the changes needed?Impls Dataset#writeTo### Does this PR introduce _any_ user-facing change?No### How was this patch tested?E2EThis is based on https://github.com/apache/spark/pull/40061
2	-1	### What changes were proposed in this pull request?Fixes the alias name for numpy literals.Also fixes `F.lit` in Spark Connect to support `np.bool_` objects.### Why are the changes needed?Currently the alias name for literals created from numpy scalars contains something like `CAST(` ... `AS <type>)`, but it should be removed and return only the value string as same as literals from Python numbers.### Does this PR introduce _any_ user-facing change?The alias name will be changed.### How was this patch tested?Modifed/enabled related tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?SPIP: https://docs.google.com/document/d/1_MVEpGxz6U_CNqKArR1M1l2oP-3I7O67grfwPtniLaA/edit?usp=sharingPOC of scaling Spark Driver via parallel schedulers.Uses multiple groups of `CoarseGrainedSchedulerBackend, TaskSchedulerImpl`<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Low performance of Spark Driver with multiple large jobs.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Configs for enabling parallel schedulers:`spark.driver.schedulers.parallelism` - number of parallel schedulers, no value or <= 1 will disable parallelism<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Comparison tests with spark-sql processes of same parallelism level <!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?Upgrade ZooKeeper from 3.6.3 to 3.6.4[Release notes](https://zookeeper.apache.org/doc/r3.6.4/releasenotes.html)### Why are the changes needed?[ZooKeeper 3.6 is EoL since 30th December, 2022](https://zookeeper.apache.org/releases.html)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA
1	-2	### What changes were proposed in this pull request?Protobuf serializer (used in `to_protobuf()`) should error if non-nullable fields (i.e. protobuf `required` fields) are present in the schema of the catalyst record being converted to a protobuf.But `isNullable()` method used for this check returns opposite (see PR comment in the diff).  As a result, Serializer incorrectly requires the fields that are optional. This PR fixes this check (see PR comment in the diff).This also requires corresponding fix for couple of unit tests. In order use a Protobuf message with a `required` field, Protobuf version 2 file `proto2_messages.proto` is added.Two tests are updated to verify missing required fields results in an error.  ### Why are the changes needed?This is need to fix a bug where we were incorrectly enforcing a schema check on optional fields rather than on required fields. ### Does this PR introduce _any_ user-facing change?It fixes a bug, and gives more flexibility for user queries. ### How was this patch tested? - Updated unit tests
3	-2	### What changes were proposed in this pull request?This pr aims upgrade netty from 4.1.88 to 4.1.89.### Why are the changes needed?This is a bug fix version, this release contains a fix for two regressions that were introduced by 4.1.88.Final:- Don't fail on HttpObjectDecoder's maxHeaderSize greater then (Integer.MAX_VALUE - 2) ([#13216](https://github.com/netty/netty/pull/13216))- dyld: Symbol not found: _netty_jni_util_JNI_OnLoad when upgrading from 4.1.87.Final to 4.1.88.Final ([#13214](https://github.com/netty/netty/pull/13214))All changes between 4.1.88 and 4.1.89 as follows:- https://github.com/netty/netty/compare/netty-4.1.88.Final...netty-4.1.89.Final### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass Github Actions
2	-1	### What changes were proposed in this pull request?This pr aims upgrade commons-crypto from 1.1.0 to 1.2.0.### Why are the changes needed?The new features in 1.2.0 as follows:https://github.com/apache/commons-crypto/blob/1ebfddd0e77585884872416a0dff2dd114a88864/RELEASE-NOTES.txt#L12-L21<img width=\"1111\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/220000261-0c59d7a1-3c88-4ea4-a8c5-39da88ead4f7.png\">### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This pr aims upgrade `scala-parser-combinators from` from 2.1.1 to 2.2.0.### Why are the changes needed?https://github.com/scala/scala-parser-combinators/pull/496 add `NoSuccess.I` to helps users avoid exhaustiveness warnings in their pattern matches, especially on Scala 2.13 and 3. The full release note as follows:- https://github.com/scala/scala-parser-combinators/releases/tag/v2.2.0### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-2	### What changes were proposed in this pull request?This pr aims upgrade `protobuf-java` from 3.21.12 to 3.22.0 ### Why are the changes needed?The new version bring some improvements like:- Use bit-field int values in buildPartial to skip work on unset groups of fields. (https://github.com/protocolbuffers/protobuf/commit/2326aef1a454a4eea363cc6ed8b8def8b88365f5)- Fix serialization warnings in generated code when compiling with Java 18 and above (https://github.com/protocolbuffers/protobuf/pull/10561)- Enable Text format parser to skip unknown short-formed repeated fields. (https://github.com/protocolbuffers/protobuf/commit/6dbd4131fa6b2ad29b2b1b827f21fc61b160aeeb)- Add serialVersionUID to ByteString and subclasses (https://github.com/protocolbuffers/protobuf/pull/10718)and some bug fix like:- Mark default instance as immutable first to avoid race during static initialization of default instances. (https://github.com/protocolbuffers/protobuf/pull/10770)- Fix Timestamps fromDate for negative 'exact second' java.sql.Timestamps (https://github.com/protocolbuffers/protobuf/pull/10321)- Fix Timestamps.fromDate to correctly handle java.sql.Timestamps before unix epoch (https://github.com/protocolbuffers/protobuf/pull/10126)- Fix bug in nested builder caching logic where cleared sub-field builders would remain dirty after a clear and build in a parent layer. https://github.com/protocolbuffers/protobuf/issues/10624The release notes as follows:- https://github.com/protocolbuffers/protobuf/releases/tag/v22.0### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new higher order function `filter_value` which takes a column to validate and a function that takes the result of that column and returns a boolean indicating whether to keep the value or return null. This is semantically the same as a `when` expression passing the column into a validation function, except it guarantees to only evaluate the initial column once. The idea was taken from the Scala `Option.filter`, open to other names if anyone has a better idea.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Conditionally evaluated expressions are currently not candidates for subexpression elimination. This can lead to a lot of duplicate evaluations of expressions when doing common data cleaning tasks, such as only keeping a value if it matches some validation checks. It gets worse when multiple different checks are chained together, and you can end up with a single expensive expression being evaluated numerous times.https://github.com/apache/spark/pull/32987 attempts to solve this by allowing conditionally evaluated expressions to be candidates for subexpression elimination, however I have not been able to get that merged in the past 1.5 years. I still think that is valuable and useful, especially as an opt-in behavior, but this is an alternative option to help improve performance of these kinds of data validation tasks.A custom implementation of `NullIf` could help as well, however it would only support exact equals checks, where this can support any logic you need to do validation.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Adds a new function.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UTs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Fix `SIMPLIFIEID` -> `SIMPLIFIED`- Fix indentation and whitespaces around a few `val` definitions### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix typo and code formatting### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests pass
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Making Python the first tab for code examples in the Spark SQL, DataFrames and Datasets Guide.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Python is the easiest approachable and most popular language so this change moves it to the first tab (showing by default) for code examples.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing Spark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?I built the website locally and manually tested the pages.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-3	### What changes were proposed in this pull request?This PR proposes to remove duplicate test (see https://github.com/apache/spark/commit/cb463fb40e8f663b7e3019c8d8560a3490c241d0).### Why are the changes needed?This test fails with ANSI mode on https://github.com/apache/spark/actions/runs/4213931226/jobs/7314033662, and it's a duplicate. Should better remove.### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?Manually tested.
1	-1	### What changes were proposed in this pull request?This PR adds the following functions to the scala client:- Misc functions.- String functions.- Date/Time functions.### Why are the changes needed?We want to be able the same APIs in the scala client as in the original Dataset API.### Does this PR introduce _any_ user-facing change?Yes, it adds new for functions to the Spark Connect Scala client.### How was this patch tested?Added tests to `PlanGenerationTestSuite` (and indirectly to `ProtoToPlanTestSuite`). Overloads are tested in `FunctionTestSuite`.
1	-2	### What changes were proposed in this pull request?This PR makes it encode the string using the `UTF_8` charset in `ParquetFilters`.### Why are the changes needed?Fix data issue where the default charset is not `UTF_8`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
3	-4	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SPARK-41952 was raised for a while, but unfortunately, the Parquet community does not publish the patched version yet, as a workaround, we can fix the issue on the Spark side first.We encountered this memory issue when migrating data from parquet/snappy to parquet/zstd, Spark executors always occupy unreasonable off-heap memory and have a high risk of being killed by NM.See more discussions at https://github.com/apache/parquet-mr/pull/982 and https://github.com/apache/iceberg/pull/5681### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The issue is fixed in the parquet community [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160), but the patched version is not available yet.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it's bug fix.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing UT should cover the correctness check, I also verified this patch by scanning a large parquet/zstd table.```spark-shell --executor-cores 4 --executor-memory 6g --conf spark.executor.memoryOverhead=2g``````spark.sql(\"select sum(hash(*)) from parquet_zstd_table \").show(false)```- before this patchAll executors get killed by NM quickly.```ERROR YarnScheduler: Lost executor 1 on hadoop-xxxx.****.org: Container killed by YARN for exceeding physical memory limits. 8.2 GB of 8 GB physical memory used. Consider boosting spark.executor.memoryOverhead.```<img width=\"1872\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26535726/220031678-e9060244-5586-4f0c-8fe7-55bb4e20a580.png\">- after this patchQuery runs well, no executor gets killed.<img width=\"1881\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26535726/220031917-4fe38c07-b38f-49c6-a982-2091a6c2a8ed.png\">
3	-2	### What changes were proposed in this pull request?This PR proposes to add \"Live Notebook: DataFrame with Spark Connect\" at [Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/index.html) documents as below:<img width=\"794\" alt=\"Screen Shot 2023-02-23 at 1 15 41 PM\" src=\"https://user-images.githubusercontent.com/44108233/220820191-ca0e5705-1694-4eaa-8658-67d522af1bf8.png\">Basically, the notebook copied the contents of 'Live Notebook: DataFrame', and updated the contents related to Spark Connect.The Notebook looks like the below:<img width=\"814\" alt=\"Screen Shot 2023-02-23 at 1 15 54 PM\" src=\"https://user-images.githubusercontent.com/44108233/220820218-bbfb6a58-7009-4327-aea4-72ed6496d77c.png\">### Why are the changes needed?To help quick start using DataFrame with Spark Connect for those who new to Spark Connect.### Does this PR introduce _any_ user-facing change?No, it's documentation.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually built the docs, and run the CI.
2	-2	### What changes were proposed in this pull request?This PR enhances `ConstantPropagation` to support more cases. 1. Propagated through other binary comparisons.2. Propagated across equality comparisons. This can be further optimized to `false`.For example:```sqla = 1 and b > a + 2 ==> a = 1 && b > 3a = 1 and a = 2 ==> 2 = 1 and 1 = 2 ==> false```### Why are the changes needed?Improve query performance. [Denodo](https://community.denodo.com/docs/html/browse/latest/en/vdp/administration/optimizing_queries/automatic_simplification_of_queries/removing_redundant_branches_of_queries_partitioned_unions) also has a similar optimization. For example:```sqlCREATE TABLE t1(a int, b int) using parquet;CREATE TABLE t2(x int, y int) using parquet;CREATE TEMP VIEW v1 AS                                        SELECT * FROM t1 JOIN t2 WHERE a = x AND a = 0                UNION ALL                                                     SELECT * FROM t1 JOIN t2 WHERE a = x AND (a IS NULL OR a <> 0);SELECT * FROM v1 WHERE x > 1;```Before this PR:```== Optimized Logical Plan ==Union false, false:- Project [a#0 AS a#12, b#1 AS b#13, x#2 AS x#14, y#3 AS y#15]:  +- Join Inner:     :- Filter (isnotnull(a#0) AND (a#0 = 0)):     :  +- Relation spark_catalog.default.t1[a#0,b#1] parquet:     +- Filter (isnotnull(x#2) AND ((0 = x#2) AND (x#2 > 1))):        +- Relation spark_catalog.default.t2[x#2,y#3] parquet+- Join Inner, (a#16 = x#18)   :- Filter ((isnull(a#16) OR NOT (a#16 = 0)) AND ((a#16 > 1) AND isnotnull(a#16)))   :  +- Relation spark_catalog.default.t1[a#16,b#17] parquet   +- Filter ((isnotnull(x#18) AND (x#18 > 1)) AND (isnull(x#18) OR NOT (x#18 = 0)))      +- Relation spark_catalog.default.t2[x#18,y#19] parquet```After this PR:```== Optimized Logical Plan ==Join Inner, (a#16 = x#18):- Filter ((isnull(a#16) OR NOT (a#16 = 0)) AND ((a#16 > 1) AND isnotnull(a#16))):  +- Relation spark_catalog.default.t1[a#16,b#17] parquet+- Filter ((isnotnull(x#18) AND (x#18 > 1)) AND (isnull(x#18) OR NOT (x#18 = 0)))   +- Relation spark_catalog.default.t2[x#18,y#19] parquet```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
2	-2	### What changes were proposed in this pull request?To support ambiguous join columns in the Scala client, this patch adds the unique plan ID to the Scala client and updates the generated test files accordingly.### Why are the changes needed?Compatibility### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
3	-2	### What changes were proposed in this pull request?This PR aims to reduce the degree of concurrency during ORC schema merging testing.### Why are the changes needed?#40049 seems to expose a few flaky tests in `branch-3.4` CI . Note that `master` branch looks fine.- https://github.com/apache/spark/runs/11463120795- https://github.com/apache/spark/runs/11463886897- https://github.com/apache/spark/runs/11467827738- https://github.com/apache/spark/runs/11471484144- https://github.com/apache/spark/runs/11471507531- https://github.com/apache/spark/runs/11474764316![Screenshot 2023-02-20 at 12 30 19 PM](https://user-images.githubusercontent.com/9700541/220193503-6d6ce2ce-3fd6-4b01-b91c-bc1ec1f41c03.png)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass the CIs.
2	-1	### What changes were proposed in this pull request?## Common classesThis PR move following packages/classes into new module `spark-mllib-common`:1, `org.apache.spark.ml.attribute.*`2, `org.apache.spark.ml.linalg.*`3, `org.apache.spark.ml.param.*` except:  - `SharedParamsCodeGen` due to it depends on `spark-core`, and it is not a runtime dependency;  - `getExecutionContext` method in `HasParallelism` due to it depends on `spark-core`;4, `org.apache.spark.ml.PredictorParams`5, `org.apache.spark.ml.classification.ClassifierParams`6, `org.apache.spark.ml.classification.ProbabilisticClassifierParams`7, `org.apache.spark.ml.util.Identifiable`8, `org.apache.spark.ml.util.SchemaUtils`9, `org.apache.spark.ml.feature.{LabeledPoint, Instance}`When implementing the algorithms, we should also move corresponding Params traits into `spark-mllib-common`, e.g. `DecisionTreeParams`, `DecisionTreeClassifierParams`, etc## TestsuitesThis PR moves following testsuites to `spark-mllib-common`:1, `org.apache.spark.ml.attribute.*`2, `org.apache.spark.ml.linalg.*` except `test(\"JavaTypeInference with VectorUDT\")` in `VectorUDTSuite` due to cyclical dependency;3, `org.apache.spark.ml.param.*` except `test(\"Filtering ParamMap\")` in `ParamsSuite` due to cyclical dependency;4, `org.apache.spark.ml.util.IdentifiableSuite`## DependencyThe new module `spark-mllib-common` depends on following packages:1, `json4s-jackson`2, `spark-tags`3, `spark-mllib-local`4, `spark-catalyst`:- `org.apache.spark.sql.types.*`- `org.apache.spark.sql.catalyst.InternalRow` in `VectorUDT` and `MatrixUDT`- `org.apache.spark.sql.catalyst.expressions.{GenericInternalRow, UnsafeArrayData}` in `VectorUDT` and `MatrixUDT`## Dependency changesThis PR make `spark-mllib` depend on the `spark-mllib-common`;This PR make Spark Connect Scala Client `spark-connect-client-jvm` depend on the `spark-mllib-common`;This PR make Spark Connect Server `spark-connect` depend on the `spark-mllib`;## Reorg mllib modulesThis PR also reorg the mllib modules:```mllib  | --- core: spark-mllib  | --- local: spark-mllib-local  | --- common: spark-mllib-common```### Why are the changes needed?to support ML APIs atop Spark-Connect Scala Client### Does this PR introduce _any_ user-facing change?no### How was this patch tested?updated tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr improves the `NestedColumnAliasing` to support prune the adjacent project nodes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->CollapseProject won't combine adjacent projects into one, e.g. non-cheap expression has been accessed more than once with the below project. Then there would be possible to appear some adjacent project nodes that `NestedColumnAliasing` does not support pruning.It's a common case with lateral column alias that it would push down an extra project, e.g.```sqlCREATE TABLE t (c struct<c1: int, c2 string>) USING PARQUET;SELECT c.c1, c.c1 + 1 as x, x + 1 FROM t;-- before*(1) Project [c#15.c1 AS c1#53, x#47, (x#47 + 1) AS (lateralAliasReference(x) + 1)#55]+- *(1) Project [c#15, (c#15.c1 + 1) AS x#47]   +- *(1) ColumnarToRow      +- FileScan parquet spark_catalog.default.t[c#15] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c:struct<c1:int,c2:string>>-- after*(1) Project [_extract_c1#38 AS c1#34, x#27, (x#27 + 1) AS (lateralAliasReference(x) + 1)#37]+- *(1) Project [c#33.c1 AS _extract_c1#38, (c#33.c1 + 1) AS x#27]   +- *(1) ColumnarToRow      +- FileScan parquet spark_catalog.default.t[c#33] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<c:struct<c1:int>>```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add new test
2	-1	### What changes were proposed in this pull request? Fix `Sort`'s maxRowsPerPartition if maxRows does not exist.### Why are the changes needed?`LimitPushDown` may be use this value.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
2	-2	### What changes were proposed in this pull request?This PR aims to simplify ORC schema merging conflict error check.### Why are the changes needed?Currently, `branch-3.4` CI is broken because the order of partitions.- https://github.com/apache/spark/runs/11463120795- https://github.com/apache/spark/runs/11463886897- https://github.com/apache/spark/runs/11467827738- https://github.com/apache/spark/runs/11471484144- https://github.com/apache/spark/runs/11471507531- https://github.com/apache/spark/runs/11474764316![Screenshot 2023-02-20 at 12 30 19 PM](https://user-images.githubusercontent.com/9700541/220193503-6d6ce2ce-3fd6-4b01-b91c-bc1ec1f41c03.png)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass the CIs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Avoid the following NPE in an anonym SparkListener in DataFrameReaderWriterSuite, as job desc may be absent```java.lang.NullPointerException\tat java.util.concurrent.ConcurrentLinkedQueue.checkNotNull(ConcurrentLinkedQueue.java:920)\tat java.util.concurrent.ConcurrentLinkedQueue.offer(ConcurrentLinkedQueue.java:327)\tat java.util.concurrent.ConcurrentLinkedQueue.add(ConcurrentLinkedQueue.java:297)\tat org.apache.spark.sql.test.DataFrameReaderWriterSuite$$anon$2.onJobStart(DataFrameReaderWriterSuite.scala:1151)\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1462)\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Test Improvement### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
2	-2	### What changes were proposed in this pull request?In the PR, I propose to capture the session time zone config (`spark.sql.session.timeZone`) as a view property, and use it while re-parsing/analysing the view. If the SQL config is not set while creating a view, use the default value of the config.### Why are the changes needed?To improve user experience with Spark SQL. The current behaviour might confuse users because query results depends on whether or not the session time zone was set explicitly while creating a view.### Does this PR introduce _any_ user-facing change?Yes. Before the changes, the current value of the session time zone is used in view analysis but this behaviour can be restored via another SQL config `spark.sql.legacy.useCurrentConfigsForView`. ### How was this patch tested?By running the new test via:```$ build/sbt \"test:testOnly *.PersistedViewTestSuite\"```
2	-1	### What changes were proposed in this pull request?Implement `DataFrame.mapInPandas` and enable parity tests to vanilla PySpark.A proto message `FrameMap` is intorudced for `mapInPandas` and `mapInArrow`(to implement next).### Why are the changes needed?To reach parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. `DataFrame.mapInPandas` is supported. An example is as shown below.```py>>> df = spark.range(2)>>> def filter_func(iterator):...   for pdf in iterator:...     yield pdf[pdf.id == 1]... >>> df.mapInPandas(filter_func, df.schema)DataFrame[id: bigint]>>> df.mapInPandas(filter_func, df.schema).show()+---+                                                                           | id|+---+|  1|+---+```### How was this patch tested?Unit tests.
1	-2	### What changes were proposed in this pull request?This PR aims add the partition transforms functions to the Scala spark connect client.### Why are the changes needed?Provide same APIs in the Scala spark connect client as in the original Dataset API.### Does this PR introduce _any_ user-facing change?Yes, it adds new for functions to the Spark Connect Scala client.### How was this patch tested?- Add new test- Manual checked `connect-client-jvm` and `connect` with Scala-2.13
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Follow existing proto style guide, we should always add `Required/Optional` to proto documentation.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve documentation.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support Window orderby, partitionby, rowsbetween/rangebetween.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-2	### What changes were proposed in this pull request?Add NULLs for INSERTs with user-specified lists of fewer columns than the target table.This is done by changing the `USE_NULLS_FOR_MISSING_DEFAULT_COLUMN_VALUES` SQLConf default value from false to true.### Why are the changes needed?This behavior is consistent with other query engines.### Does this PR introduce _any_ user-facing change?Yes, per above.### How was this patch tested?Unit test coverage in `InsertSuite`.
2	-2	### What changes were proposed in this pull request?Fixes `DataFrameWriterV2` to find the default source.### Why are the changes needed?Currently `DataFrameWriterV2` in Spark Connect doesn't work without the provider with a weird error:For example:```pydf.writeTo(\"test_table\").create()``````pyspark.errors.exceptions.connect.SparkConnectGrpcException: (org.apache.spark.SparkClassNotFoundException) [DATA_SOURCE_NOT_FOUND] Failed to find the data source: . Please find packages at `https://spark.apache.org/third-party-projects.html`.```### Does this PR introduce _any_ user-facing change?Users will be able to use `DataFrameWriterV2` without provider as same as PySpark.### How was this patch tested?Added some tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Added minor UI fixes.<img width=\"732\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/220488925-eda62d80-d54d-41e9-a9ec-53d02b6fb94d.png\"><img width=\"725\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/220488948-929b1c35-4da7-4317-9883-078c2a57896a.png\"><img width=\"693\" alt=\"image\" src=\"https://user-images.githubusercontent.com/81988348/220488975-fdc34ae5-a539-4557-993c-d740232b29b5.png\">### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->For easy to read documentation.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
2	-3	### What changes were proposed in this pull request?Upgrade pandas from 1.1.5 to 1.5.3, numpy from 1.19.4 to 1.20.3 in the Dockerfile used for Spark releases.They are also what we use to cut `v3.4.0-rc1`.### Why are the changes needed?Otherwise, errors are raised as shown below when building release docs.```ImportError: Warning: Latest version of pandas (1.5.3) is required to generate the documentation; however, your version was 1.1.5ImportError: this version of pandas is incompatible with numpy < 1.20.3your numpy version is 1.19.4.Please upgrade numpy to >= 1.20.3 to use this pandas version```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual tests.
1	-2	### What changes were proposed in this pull request?This PR follow-ups for https://github.com/apache/spark/pull/39441 to fix the wrong error message.### Why are the changes needed?Error message correction.### Does this PR introduce _any_ user-facing change?No, but it's just about error message.### How was this patch tested?The existing CI should pass
1	-1	### What changes were proposed in this pull request?### Why are the changes needed?`WindowGroupLimitExec` should supports codegen### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?N/A
2	-2	### What changes were proposed in this pull request?This PR enhance `LimitPushDown` to support push down topK through join. The key idea is that if the order expressions comes from one join side and pushing down topK does not affect query result:1. Left outer join and order expressions come from the left side.2. Right outer join and order expressions come from the right side.3. Inner join, cross join, left outer join, right outer join, full outer join and join condition is empty and order expressions from one side.4. Left anti join, left semi join and join condition is empty and order expressions from left side.### Why are the changes needed?Improve query performance. [TiDB](https://docs.pingcap.com/tidb/dev/topn-limit-push-down) has this optimization.Case 1:```scalaspark.range(100000000).selectExpr(\"id as a\ \"id as b\").write.saveAsTable(\"t1\")spark.range(100000000).selectExpr(\"id as x\ \"id as y\").write.saveAsTable(\"t2\")sql(\"select * from t1 left join t2 on a = x order by b limit 5\").collect()spark.sql(\"set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.LimitPushDown\")sql(\"select * from t1 left join t2 on a = x order by b limit 5\").collect()```Disable push down | Enable push down-- | --<img src=\"https://user-images.githubusercontent.com/5399861/220509675-b5bb96d2-c4cd-464f-b464-ff74c28a473c.png\" width=\"300\" height=\"630\"> | <img src=\"https://user-images.githubusercontent.com/5399861/220509756-66b1e981-25a9-4abd-8359-d0a8cb9924e9.png\" width=\"300\" height=\"630\">Case 2:```scalaspark.range(100000000).selectExpr(\"id % 10000 as a\ \"id as b\").write.saveAsTable(\"t1\")spark.range(100000000).selectExpr(\"id % 10000 as x\ \"id as y\").write.saveAsTable(\"t2\")sql(\"select * from t1 left join t2 on a = x order by b limit 5\").collect()spark.sql(\"set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.LimitPushDown\")sql(\"select * from t1 left join t2 on a = x order by b limit 5\").collect()```Disable push down | Enable push down-- | --<img src=\"https://user-images.githubusercontent.com/5399861/220572967-53de205e-6a6e-4455-bded-a718659449df.png\" width=\"300\" height=\"630\"> | <img src=\"https://user-images.githubusercontent.com/5399861/220510582-f02045b1-93b8-4849-90a4-7c2ae42c765f.png\" width=\"300\" height=\"630\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
2	-2	### What changes were proposed in this pull request?Extend the CollapseWindow rule to collapse Window nodes, when one window in subquery.### Why are the changes needed?```select a, b, c, row_number() over (partition by a order by b) as d from( select a, b, rank() over (partition by a order by b) as c from t1) t2== Optimized Logical Plan ==beforeWindow [row_number() windowspecdefinition(a#11, b#12 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS d#26], [a#11], [b#12 ASC NULLS FIRST]+- Window [rank(b#12) windowspecdefinition(a#11, b#12 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS c#25], [a#11], [b#12 ASC NULLS FIRST]   +- InMemoryRelation [a#11, b#12], StorageLevel(disk, memory, deserialized, 1 replicas)         +- *(1) Project [_1#6 AS a#11, _2#7 AS b#12]            +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#6, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#7]               +- *(1) MapElements org.apache.spark.sql.DataFrameSuite$$Lambda$1517/1628848368@3a479fda, obj#5: scala.Tuple2                  +- *(1) DeserializeToObject staticinvoke(class java.lang.Long, ObjectType(class java.lang.Long), valueOf, id#0L, true, false, true), obj#4: java.lang.Long                     +- *(1) Range (0, 10, step=1, splits=2)afterWindow [rank(b#12) windowspecdefinition(a#11, b#12 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS c#25, row_number() windowspecdefinition(a#11, b#12 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS d#26], [a#11], [b#12 ASC NULLS FIRST]+- InMemoryRelation [a#11, b#12], StorageLevel(disk, memory, deserialized, 1 replicas)      +- *(1) Project [_1#6 AS a#11, _2#7 AS b#12]         +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#6, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#7]            +- *(1) MapElements org.apache.spark.sql.DataFrameSuite$$Lambda$1518/1928028672@4d7a64ca, obj#5: scala.Tuple2               +- *(1) DeserializeToObject staticinvoke(class java.lang.Long, ObjectType(class java.lang.Long), valueOf, id#0L, true, false, true), obj#4: java.lang.Long                  +- *(1) Range (0, 10, step=1, splits=2)```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
3	-2	### What changes were proposed in this pull request?correct the output column name of groupBy.agg(count_distinct),  so the \"*\" is expanded correctly into column names and the output column has the distinct keyword.### Why are the changes needed?Output column name for groupBy.agg(count_distinct)  is incorrect . However similar queries in spark sql return correct output column. For groupBy.agg queries on dataframe \"*\" is not expanded correctly in the output column  and the distinct keyword is missing from output column.```// initializing datascala> val df = spark.range(1, 10).withColumn(\"value\ lit(1))df: org.apache.spark.sql.DataFrame = [id: bigint, value: int]scala> df.createOrReplaceTempView(\"table\")// Dataframe  aggregate queries with incorrect output columnscala> df.groupBy(\"id\").agg(count_distinct($\"*\"))res3: org.apache.spark.sql.DataFrame = [id: bigint, count(unresolvedstar()): bigint]scala> df.groupBy(\"id\").agg(count_distinct($\"value\"))res1: org.apache.spark.sql.DataFrame = [id: bigint, count(value): bigint]// Spark Sql aggregate queries with correct output columnscala> spark.sql(\" SELECT id, COUNT(DISTINCT *) FROM table GROUP BY id \")res4: org.apache.spark.sql.DataFrame = [id: bigint, count(DISTINCT id, value): bigint]scala> spark.sql(\" SELECT id, COUNT(DISTINCT value) FROM table GROUP BY id \")res2: org.apache.spark.sql.DataFrame = [id: bigint, count(DISTINCT value): bigint]```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added UT
1	-3	### What changes were proposed in this pull request?This PR proposes to disable ANSI for several conv test cases in `MathFunctionsSuite`. They are intentionally testing the behaviours when ANSI is disabled. Exception cases are already handled in https://github.com/apache/spark/commit/cb463fb40e8f663b7e3019c8d8560a3490c241d0 I believe.### Why are the changes needed?To make the ANSI tests pass. It currently fails (https://github.com/apache/spark/actions/runs/4228390267/jobs/7343793692):```2023-02-21T03:03:20.3799795Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-33428 conv function should trim input string (177 milliseconds)\u001B[0m\u001B[0m2023-02-21T03:03:20.4252604Z 03:03:20.424 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 138.0 (TID 256)2023-02-21T03:03:20.4253602Z org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.2023-02-21T03:03:20.4254440Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.arithmeticOverflowError(QueryExecutionErrors.scala:643)2023-02-21T03:03:20.4255265Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.overflowInConvError(QueryExecutionErrors.scala:315)2023-02-21T03:03:20.4256001Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.encode(NumberConverter.scala:68)2023-02-21T03:03:20.4256888Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:158)2023-02-21T03:03:20.4257450Z \tat org.apache.spark.sql.catalyst.util.NumberConverter.convert(NumberConverter.scala)2023-02-21T03:03:20.4258084Z \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:38)2023-02-21T03:03:20.4258720Z \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)2023-02-21T03:03:20.4259293Z \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)2023-02-21T03:03:20.4259769Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)2023-02-21T03:03:20.4260157Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)2023-02-21T03:03:20.4260535Z \tat org.apache.spark.util.Iterators$.size(Iterators.scala:29)2023-02-21T03:03:20.4260918Z \tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1944)2023-02-21T03:03:20.4261283Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1266)2023-02-21T03:03:20.4261649Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1266)2023-02-21T03:03:20.4262050Z \tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)2023-02-21T03:03:20.4262726Z \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)2023-02-21T03:03:20.4263206Z \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)2023-02-21T03:03:20.4263628Z \tat org.apache.spark.scheduler.Task.run(Task.scala:139)2023-02-21T03:03:20.4264227Z \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)2023-02-21T03:03:20.4265048Z \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)2023-02-21T03:03:20.4266209Z \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)2023-02-21T03:03:20.4266805Z \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2023-02-21T03:03:20.4267369Z \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2023-02-21T03:03:20.4267799Z \tat java.lang.Thread.run(Thread.java:750)```### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?Fixed unittests.
3	-4	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Find spark driver container exit code and set as spark submit exit code.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Many users face this problem [SPARK-26365](https://issues.apache.org/jira/browse/SPARK-26365)> When launching apps using spark-submit in a kubernetes cluster, if the Spark applications fails (returns exit code = 1 for example), spark-submit will still exit gracefully and return exit code = 0.As a comparison, in yarn cluster mode, spark submit will throw exception when application be failed or killed.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->YesBefore, when application (driver container) on kubernetes fail, spark-submit still return 0 as exit code.After this, spark submit will use driver's exit code or throw exception when not found driver's exit code.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->In local; If need unit test, willing to add
2	-1	### What changes were proposed in this pull request?Add Classifier.getNumClasses back### Why are the changes needed?some famous libraries like `xgboost` happen to depend on this method, even though it is not a public APIso it should be nice to make xgboost integration better.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?update mima
1	-1	### What changes were proposed in this pull request?This PR aims add the window functions to the Scala spark connect client.### Why are the changes needed?Provide same APIs in the Scala spark connect client as in the original Dataset API.### Does this PR introduce _any_ user-facing change?Yes, it adds new for functions to the Spark Connect Scala client.### How was this patch tested?- Add new test- Manual checked connect-client-jvm and connect with Scala-2.13
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Reimplement `PercentileHeap` such that:- the percentile value is always in the `topHeap`, this speeds up `percentile` access- rebalance the heaps more efficiently by checking which heap should grow due to the new insertion and doing a rebalance based on target heap sizes- the heaps are java PriorityQueue's *without* comparators. Comparator call overhead slows down `poll`/`offer` by more than 2x. Instead implement a max-heap by `poll`/`offer` on the negated domain of numbers.### Why are the changes needed?`PercentileHeap` is heavy weight enough to cause scheduling delays if inserted inside the scheduler loop.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added more extensive unittests.
3	-2	### What changes were proposed in this pull request?Alternative approach to #39902. This PR removes conditional presence of first arg as grouping key and makes it mandatory. @EnricoMi suggested this could be one potential approach. This approach doubles down on \"Explicit better than implicit\" and that leads to reduction in cognitive overload.All the remaining details remain more or less the same. Note: this is a breaking change to this experimental API.Pandas cogroup UDF with applyInPandas currently support two dataframes. This is already very useful but limits us both API wise and efficiency wise when we have to use multiple DFs with cogroup.applyInPandas. The PR here is to support multiple DFs in cogroup.```pythondf1 = spark.createDataFrame(    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],    (\"time\ \"id\ \"v1\"))df2 = spark.createDataFrame(    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],    (\"time\ \"id\ \"v2\"))df3 = spark.createDataFrame(    [(20000101, 1, \"asd\"), (20000101, 2, \"d\")],    (\"time\ \"id\ \"v3\"))df4 = spark.createDataFrame(    [(20000101, 1, \"v\"), (20000101, 2, \"g\")],    (\"time\ \"id\ \"v4\"))def asof_join(key, df1, df2, df3, df4):    df12 = pd.merge_asof(df1, df2, on=\"time\ by=\"id\")    df123 = pd.merge_asof(df12, df3, on=\"time\ by=\"id\")    df1234 = pd.merge_asof(df123, df4, on=\"time\ by=\"id\")    return df1234df1.groupby(\"id\").cogroup(df2.groupby(\"id\"), df3.groupby(\"id\"), df4.groupby(\"id\")).applyInPandas(    asof_join, schema=\"time int, id int, v1 double, v2 string, v3 string, v4 string\").show()+--------+---+---+---+---+---+|    time| id| v1| v2| v3| v4|+--------+---+---+---+---+---+|20000101|  1|1.0|  x|asd|  v||20000102|  1|3.0|  x|asd|  v||20000101|  2|2.0|  y|  d|  g||20000102|  2|4.0|  y|  d|  g|+--------+---+---+---+---+---+```<img width=\"527\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3813695/221379159-f30925b0-76e4-4465-85c9-03054e2a7e31.png\">### Why are the changes needed?Currently pyspark support `cogroup.applyInPandas` with only 2 dataframes. The improvement request is to support multiple dataframes with variable arity. We have cases where we want to pass multiple(20 to 30) data frames to `cogroup.applyInPandas` function which then are used to either combine them together for further processing in spark or emit excel files or combined graphs. Workarounds are possible but they are very inefficient and error-prone.### Does this PR introduce _any_ user-facing change?The previous experimental implementation is awkward. The UDF can accept either 2 or 3 args. If 2 args are passed in the UDF then UDF receives 2 cogrouped DFs, if 3 args are passed then UDF receives 2 cogrouped DFs with grouping key as the first arg. he previous API is limiting and has implications on new proposed change. There is no clear way of distinguishing whether 3 args are for 3 DFs or 2 DFs + 1 grouping key. This change removes this behaviour. In this change, we always pass key as first arg to the UDF. Breaking change:```pythondef test_udf(df1, df2):    return df1df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(    test_udf, \"time int, id int, v1 double\").show()```BEFORE:```+--------+---+---+|    time| id| v1|+--------+---+---+|20000101|  1|1.0||20000102|  1|3.0||20000101|  2|2.0||20000102|  2|4.0|+--------+---+---+```AFTER:```Traceback (most recent call last):  File \"previous.py\ line 64, in <module>    df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(  File \"/workspace/spark/python/lib/pyspark.zip/pyspark/sql/pandas/group_ops.py\ line 459, in applyInPandas  File \"/workspace/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py\ line 387, in pandas_udf  File \"/workspace/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py\ line 452, in _create_pandas_udfValueError: Invalid function: the function in cogroup.applyInPandas must contain vararg or must take three or more argument with key being first```To make it work, change the UDF:```pythondef test_udf(key, df1, df2):    return df1df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(    test_udf, \"time int, id int, v1 double\").show()```
1	-3	### What changes were proposed in this pull request?This PR proposes to do not cache local port.### Why are the changes needed?When Spark Context is stopped, and started again, the Spark Connect server shuts down and starts up again too (while JVM itself is alive). So, we should not cache the local port but have the new local port.For example, in https://github.com/apache/spark/pull/40109, the Spark Connect server at `ReadwriterTestsMixin` stops after the tests. And then, `ReadwriterV2TestsMixin` starts the new Spark Connect server which causes failures on any actual protobuf message exchanges to the server.### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?I tested it on the top of https://github.com/apache/spark/pull/40109. That PR should validate it.
1	-1	### What changes were proposed in this pull request?Use '_metadata.row_index' in tests when that is possible, given that this is the preferred way to access Row Index.### Why are the changes needed?To add test coverage for reading '_metadata.row_index'### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Just test changes here
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `.agg` in Dataset in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
3	-2	### What changes were proposed in this pull request?In the PR, I propose to change auto-generation of column aliases (the case when an user doesn't assign any alias explicitly). Before the changes, Spark SQL generates such alias from `Expression` but this PR proposes to take the parse tree (output of lexer), and generate an alias using the term tokens from the tree.New helper function `ParserUtils.toExprAlias` takes a `ParseTree` from `Antlr4`, and converts it to a `String` using the following simple rule:- Concatenate all terminal nodes of the lexer tree without any gaps.- Upper case keywords and numeric literals.For example, the sequence of tokens \"1\ \"in\ \"(\ \"1.0d\" \")\" is converted to the alias \"1IN(1.0D)\".By default, the feature is off, and the SQL config `spark.sql.stableDerivedColumnAlias.enabled` allows to enable it.Closes #39332 ### Why are the changes needed?To improve user experience with Spark SQL. It is always best practice to name the result of any expressions in a queries select list,  if one plans to reference them later. This yields the most readable results and stability. However, sometimes queries are generated or we’re just lazy and trust in the auto generated names. The problem is that the auto-generated names are produced by pretty printing the expression tree which is, while “generally” readable, not meant to be stable across long durations of time. For example:```sqlspark-sql> DESC SELECT substring('hello', 5);substring(hello, 5, 2147483647)\tstring```the auto-generated column alias `substring(hello, 5, 2147483647)` contains not-obvious elements.### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?By running new test:```$ build/sbt \"test:testOnly *.ResolveAliasesSuite\"```
1	-1	### What changes were proposed in this pull request?This PR aims to remove `Hadoop 2` from PySpark installation guide.### Why are the changes needed?From Apache Spark 3.4.0, we don't provide Hadoop 2 binaries.### Does this PR introduce _any_ user-facing change?This is a documentation fix to be consistent with the new availability.### How was this patch tested?Manual review.
3	-1	### What changes were proposed in this pull request?1. Instead of creating multiple sub directories in k8s upload directory (spark.kubernetes.file.upload.path) for each file to be uploaded, create a single subdirectory to be used for all file uploads of a specific application. This directory will be named using the spark application id.2. Delete the sub directory and it's content when job terminates### Why are the changes needed?The change is required to cleanup the files and directories which are created under the k8s upload path to prevent space getting full. without this change, user needs to manually clean up these files.### Does this PR introduce _any_ user-facing change?Yes, Users submitting spark on k8s job, won't need to manually cleanup the files under upload directory.### How was this patch tested?Through git action and manually by running jobs in local k8s cluster.Also, added unit test for testing the change in behaviour of sub directory creation under upload path.Some logs for verification:23/02/22 18:41:16 INFO SparkContext: Successfully stopped SparkContext23/02/22 18:41:16 INFO ShutdownHookManager: Shutdown hook called23/02/22 18:41:16 INFO ShutdownHookManager: Deleting directory /spark-local2/spark-f58ecb91-0bb6-4b10-a372-405f69780c1523/02/22 18:41:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-c10b27ca-fe68-4382-913f-4a535833f64e23/02/22 18:41:16 INFO ShutdownHookManager: Deleting directory /spark-local1/spark-8c4ef9d8-7688-4f51-b587-16dee1d264c723/02/22 18:41:16 INFO UploadDirManager: Shutdown hook called23/02/22 18:41:17 INFO UploadDirManager: Upload dir deleted successfully.: hdfs://****:8020/user/*****/k8s/spark-upload-spark-b6609808729a49e9b6f53f20ed50d05b
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support Cube and Rollup in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API Coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
1	-1	### What changes were proposed in this pull request?This PR adds all the collections functions to `functions.scala` for Scala client. This is the last PR large functions PR, there are a few functions missing, these will be added later.### Why are the changes needed?We want the Scala client to have API parity with the existing API### Does this PR introduce _any_ user-facing change?Yes, it adds functions to the Spark Connect Scala Client.### How was this patch tested?Added tests to `PlanGenerationTestSuite` and to `ProtoToPlanTestSuite`. I have added a few tests to `ClientE2ETestSuite` for lambda functions (to make sure name scoping works) and the array shuffle function (non-deterministic, hard to test with golden files).
1	-1	### What changes were proposed in this pull request?This is a follow-up of #39690.### Why are the changes needed?To be consistent across multiple docs.### Does this PR introduce _any_ user-facing change?No, this is a doc-only change.### How was this patch tested?Manual review.
3	-3	### What changes were proposed in this pull request?This PR aims to update `YuniKorn` documentation with the latest v1.2.0 and fix codify issues in doc.### Why are the changes needed?- https://yunikorn.apache.org/release-announce/1.2.0### Does this PR introduce _any_ user-facing change?This is a documentation-only change.**BEFORE**- https://dist.apache.org/repos/dist/dev/spark/v3.4.0-rc1-docs/_site/running-on-kubernetes.html#using-apache-yunikorn-as-customized-scheduler-for-spark-on-kubernetes**AFTER**<img width=\"927\" alt=\"Screenshot 2023-02-22 at 2 27 50 PM\" src=\"https://user-images.githubusercontent.com/9700541/220775386-90268ecb-facf-4701-bcb7-4f6b3e847e70.png\">### How was this patch tested?Manually test with YuniKorn v1.2.0.```$ helm list -n yunikornNAME    \tNAMESPACE\tREVISION\tUPDATED                             \tSTATUS  \tCHART         \tAPP VERSIONyunikorn\tyunikorn \t1       \t2023-02-22 14:01:11.728926 -0800 PST\tdeployed\tyunikorn-1.2.0``````$ build/sbt -Psparkr -Pkubernetes -Pkubernetes-integration-tests -Dspark.kubernetes.test.deployMode=docker-desktop \"kubernetes-integration-tests/test\" -Dtest.exclude.tags=minikube,local,decom -Dtest.default.exclude.tags=''[info] KubernetesSuite:[info] - SPARK-42190: Run SparkPi with local[*] (10 seconds, 832 milliseconds)[info] - Run SparkPi with no resources (12 seconds, 421 milliseconds)[info] - Run SparkPi with no resources & statefulset allocation (17 seconds, 861 milliseconds)[info] - Run SparkPi with a very long application name. (12 seconds, 531 milliseconds)[info] - Use SparkLauncher.NO_RESOURCE (17 seconds, 697 milliseconds)[info] - Run SparkPi with a master URL without a scheme. (12 seconds, 499 milliseconds)[info] - Run SparkPi with an argument. (18 seconds, 734 milliseconds)[info] - Run SparkPi with custom labels, annotations, and environment variables. (12 seconds, 520 milliseconds)[info] - All pods have the same service account by default (17 seconds, 504 milliseconds)[info] - Run extraJVMOptions check on driver (9 seconds, 402 milliseconds)[info] - SPARK-42474: Run extraJVMOptions JVM GC option check - G1GC (9 seconds, 389 milliseconds)[info] - SPARK-42474: Run extraJVMOptions JVM GC option check - Other GC (9 seconds, 330 milliseconds)[info] - Verify logging configuration is picked from the provided SPARK_CONF_DIR/log4j2.properties (17 seconds, 710 milliseconds)[info] - Run SparkPi with env and mount secrets. (19 seconds, 797 milliseconds)[info] - Run PySpark on simple pi.py example (18 seconds, 568 milliseconds)[info] - Run PySpark to test a pyfiles example (15 seconds, 622 milliseconds)[info] - Run PySpark with memory customization (18 seconds, 507 milliseconds)[info] - Run in client mode. (6 seconds, 185 milliseconds)[info] - Start pod creation from template (17 seconds, 696 milliseconds)[info] - SPARK-38398: Schedule pod creation from template (12 seconds, 585 milliseconds)[info] - Run SparkR on simple dataframe.R example (19 seconds, 639 milliseconds)[info] YuniKornSuite:[info] - SPARK-42190: Run SparkPi with local[*] (12 seconds, 421 milliseconds)[info] - Run SparkPi with no resources (20 seconds, 465 milliseconds)[info] - Run SparkPi with no resources & statefulset allocation (15 seconds, 516 milliseconds)[info] - Run SparkPi with a very long application name. (20 seconds, 532 milliseconds)[info] - Use SparkLauncher.NO_RESOURCE (15 seconds, 545 milliseconds)[info] - Run SparkPi with a master URL without a scheme. (20 seconds, 575 milliseconds)[info] - Run SparkPi with an argument. (16 seconds, 462 milliseconds)[info] - Run SparkPi with custom labels, annotations, and environment variables. (20 seconds, 568 milliseconds)[info] - All pods have the same service account by default (15 seconds, 630 milliseconds)[info] - Run extraJVMOptions check on driver (12 seconds, 483 milliseconds)[info] - SPARK-42474: Run extraJVMOptions JVM GC option check - G1GC (12 seconds, 665 milliseconds)[info] - SPARK-42474: Run extraJVMOptions JVM GC option check - Other GC (11 seconds, 615 milliseconds)[info] - Verify logging configuration is picked from the provided SPARK_CONF_DIR/log4j2.properties (20 seconds, 810 milliseconds)[info] - Run SparkPi with env and mount secrets. (24 seconds, 622 milliseconds)[info] - Run PySpark on simple pi.py example (16 seconds, 650 milliseconds)[info] - Run PySpark to test a pyfiles example (23 seconds, 662 milliseconds)[info] - Run PySpark with memory customization (15 seconds, 450 milliseconds)[info] - Run in client mode. (5 seconds, 121 milliseconds)[info] - Start pod creation from template (20 seconds, 552 milliseconds)[info] - SPARK-38398: Schedule pod creation from template (15 seconds, 847 milliseconds)[info] - Run SparkR on simple dataframe.R example (22 seconds, 739 milliseconds)[info] Run completed in 15 minutes, 41 seconds.[info] Total number of tests run: 42[info] Suites: completed 2, aborted 0[info] Tests: succeeded 42, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.[success] Total time: 1306 s (21:46), completed Feb 22, 2023, 2:28:18 PM```
2	-1	### What changes were proposed in this pull request?Adding SSL encryption and access token support for Scala client### Why are the changes needed?To support basic client side encryption to protect data sent over the network.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit tests. Manual tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The PR fixes DB2 Limit clause syntax. Although DB2 supports LIMIT keyword, it seems that this support varies across databases and versions and the recommended way is to use `FETCH FIRST x ROWS ONLY`. In fact, some versions don't support LIMIT at all. Doc: https://www.ibm.com/docs/en/db2/11.5?topic=subselect-fetch-clause, usage example: https://www.mullinsconsulting.com/dbu_0502.htm.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fixes the incorrect Limit clause which could cause errors when using against DB2 versions that don't support LIMIT.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I added a unit test and an integration test to cover this functionality.
1	-3	### What changes were proposed in this pull request?Existing implementation always convert inputs (maybe column or column name) to columns, this cause `AMBIGUOUS_REFERENCE` issue since there maybe several columns with the same name.In the JVM side, the logics of drop(column: Column) and drop(columnName: String) are different, we can not simply always convert a column name to column via col() method.When there are multi-column with the same name (e.g, `name`), users can:1, `drop('name')` --- drop all the columns;2, `drop(df1.name)` --- drop the column from the specific dataframe `df1`;But if users call `drop(col('name'))`, it will fail due to ambiguous issue.In Pyspark, it is a bit complex, that the user can input both column names with columns. This PR drops the columns first, and then the column names.### Why are the changes needed?bug fix```>>> from pyspark.sql import Row>>> df1 = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\ \"name\"])>>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])>>> df3 = df1.join(df2, df1.name == df2.name, 'inner')>>> df3.show()+---+----+------+----+|age|name|height|name|+---+----+------+----+| 16| Bob|    85| Bob|| 14| Tom|    80| Tom|+---+----+------+----+```BEFORE```>>> df3.drop(\"name\ \"age\").columnsTraceback (most recent call last):...pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].```AFTER```>>> df3.drop(\"name\ \"age\").columns['height']```### Does this PR introduce _any_ user-facing change?no### How was this patch tested?added tests
3	-4	### What changes were proposed in this pull request?This pr use `LocalProject(\"assembly\") / Compile / Keys.package` instead of `buildTestDeps` to ensure `${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION/jars` is available for testing for `connect-client-jvm` module.On the other hand, this pr also add similar options support for `testOnly`### Why are the changes needed?Make `write table` in `ClientE2ETestSuite` sbt local test pass ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual test：run `test````build/sbt clean \"connect-client-jvm/test\"```**Before**```[info] - write table *** FAILED *** (34 milliseconds)[info]   io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport[info]   at io.grpc.Status.asRuntimeException(Status.java:535)[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)[info]   at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943)[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)[info]   at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:169)[info]   at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:255)[info]   at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:338)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$13(ClientE2ETestSuite.scala:145)[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.withTable(RemoteSparkSession.scala:169)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.withTable$(RemoteSparkSession.scala:167)[info]   at org.apache.spark.sql.ClientE2ETestSuite.withTable(ClientE2ETestSuite.scala:33)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$12(ClientE2ETestSuite.scala:143)[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)[info]   at scala.collection.immutable.List.foreach(List.scala:431)[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)[info]   at org.scalatest.Suite.run(Suite.scala:1114)[info]   at org.scalatest.Suite.run$(Suite.scala:1096)[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)[info]   at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)[info]   at java.base/java.lang.Thread.run(Thread.java:833)Warning: Unable to serialize throwable of type io.grpc.StatusRuntimeException for TestFailed(Ordinal(0, 15),UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport,ClientE2ETestSuite,org.apache.spark.sql.ClientE2ETestSuite,Some(org.apache.spark.sql.ClientE2ETestSuite),write table,write table,Vector(),Vector(),Some(io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport),Some(34),Some(IndentedText(- write table,write table,0)),Some(SeeStackDepthException),Some(org.apache.spark.sql.ClientE2ETestSuite),None,pool-1-thread-1-ScalaTest-running-ClientE2ETestSuite,1677123932064), setting it as NotSerializableWrapperException.Warning: Unable to read from client, please check on client for futher details of the problem.[info] - writeTo with create and using *** FAILED *** (27 milliseconds)[info]   io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport[info]   at io.grpc.Status.asRuntimeException(Status.java:535)[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)[info]   at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943)[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)[info]   at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:169)[info]   at org.apache.spark.sql.DataFrameWriterV2.executeWriteOperation(DataFrameWriterV2.scala:160)[info]   at org.apache.spark.sql.DataFrameWriterV2.create(DataFrameWriterV2.scala:81)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$15(ClientE2ETestSuite.scala:162)[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.withTable(RemoteSparkSession.scala:169)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.withTable$(RemoteSparkSession.scala:167)[info]   at org.apache.spark.sql.ClientE2ETestSuite.withTable(ClientE2ETestSuite.scala:33)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$14(ClientE2ETestSuite.scala:161)[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)Warning: Unable to serialize throwable of type io.grpc.StatusRuntimeException for TestFailed(Ordinal(0, 17),UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport,ClientE2ETestSuite,org.apache.spark.sql.ClientE2ETestSuite,Some(org.apache.spark.sql.ClientE2ETestSuite),writeTo with create and using,writeTo with create and using,Vector(),Vector(),Some(io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport),Some(27),Some(IndentedText(- writeTo with create and using,writeTo with create and using,0)),Some(SeeStackDepthException),Some(org.apache.spark.sql.ClientE2ETestSuite),None,pool-1-thread-1-ScalaTest-running-ClientE2ETestSuite,1677123932096), setting it as NotSerializableWrapperException.[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)[info]   at scala.collection.immutable.List.foreach(List.scala:431)[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)[info]   at org.scalatest.Suite.run(Suite.scala:1114)[info]   at org.scalatest.Suite.run$(Suite.scala:1096)[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)[info]   at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)Warning: Unable to read from client, please check on client for futher details of the problem.[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)[info]   at java.base/java.lang.Thread.run(Thread.java:833)[info] - writeTo with create and append *** FAILED *** (20 milliseconds)[info]   io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport[info]   at io.grpc.Status.asRuntimeException(Status.java:535)[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)[info]   at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943)[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)[info]   at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:169)[info]   at org.apache.spark.sql.DataFrameWriterV2.executeWriteOperation(DataFrameWriterV2.scala:160)[info]   at org.apache.spark.sql.DataFrameWriterV2.create(DataFrameWriterV2.scala:81)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$17(ClientE2ETestSuite.scala:175)[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.withTable(RemoteSparkSession.scala:169)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.withTable$(RemoteSparkSession.scala:167)[info]   at org.apache.spark.sql.ClientE2ETestSuite.withTable(ClientE2ETestSuite.scala:33)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$16(ClientE2ETestSuite.scala:174)[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)[info]   at scala.collection.immutable.List.foreach(List.scala:431)[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)Warning: Unable to read from client, please check on client for futher details of the problem.[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)Warning: Unable to serialize throwable of type io.grpc.StatusRuntimeException for TestFailed(Ordinal(0, 19),UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport,ClientE2ETestSuite,org.apache.spark.sql.ClientE2ETestSuite,Some(org.apache.spark.sql.ClientE2ETestSuite),writeTo with create and append,writeTo with create and append,Vector(),Vector(),Some(io.grpc.StatusRuntimeException: UNKNOWN: org/apache/parquet/hadoop/api/ReadSupport),Some(20),Some(IndentedText(- writeTo with create and append,writeTo with create and append,0)),Some(SeeStackDepthException),Some(org.apache.spark.sql.ClientE2ETestSuite),None,pool-1-thread-1-ScalaTest-running-ClientE2ETestSuite,1677123932118), setting it as NotSerializableWrapperException.[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)Fatal: Existing as unable to continue running tests, after 3 failing attempts to read event from server socket. [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)[info]   at org.scalatest.Suite.run(Suite.scala:1114)[info]   at org.scalatest.Suite.run$(Suite.scala:1096)[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)[info]   at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)[info]   at java.base/java.lang.Thread.run(Thread.java:833)```**After**```[info] Run completed in 12 seconds, 629 milliseconds.[info] Total number of tests run: 505[info] Suites: completed 9, aborted 0[info] Tests: succeeded 505, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```run `testOnly````build/sbt clean \"connect-client-jvm/testOnly *ClientE2ETestSuite\"build/sbt clean \"connect-client-jvm/testOnly *CompatibilitySuite\"   ```**Before**```[info] org.apache.spark.sql.ClientE2ETestSuite *** ABORTED *** (27 milliseconds)[info]   java.lang.AssertionError: assertion failed: Failed to find the jar inside folder: /spark/connector/connect/server/target[info]   at scala.Predef$.assert(Predef.scala:223)[info]   at org.apache.spark.sql.connect.client.util.IntegrationTestUtils$.findJar(IntegrationTestUtils.scala:67)[info]   at org.apache.spark.sql.connect.client.util.SparkConnectServerUtils$.sparkConnect$lzycompute(RemoteSparkSession.scala:64)[info]   at org.apache.spark.sql.connect.client.util.SparkConnectServerUtils$.sparkConnect(RemoteSparkSession.scala:59)[info]   at org.apache.spark.sql.connect.client.util.SparkConnectServerUtils$.start(RemoteSparkSession.scala:90)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll(RemoteSparkSession.scala:120)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll$(RemoteSparkSession.scala:118)[info]   at org.apache.spark.sql.ClientE2ETestSuite.beforeAll(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:33)[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[info]   at java.lang.Thread.run(Thread.java:750)[info] - compatibility MiMa tests *** FAILED *** (27 milliseconds)[info]   java.lang.AssertionError: assertion failed: Failed to find the jar inside folder: /spark/connector/connect/client/jvm/target[info]   at scala.Predef$.assert(Predef.scala:223)[info]   at org.apache.spark.sql.connect.client.util.IntegrationTestUtils$.findJar(IntegrationTestUtils.scala:67)[info]   at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar$lzycompute(CompatibilitySuite.scala:57)[info]   at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar(CompatibilitySuite.scala:53)[info]   at org.apache.spark.sql.connect.client.CompatibilitySuite.$anonfun$new$1(CompatibilitySuite.scala:69)[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)```**After**```[info] Run completed in 13 seconds, 572 milliseconds.[info] Total number of tests run: 17[info] Suites: completed 1, aborted 0[info] Tests: succeeded 17, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed[info] Run completed in 1 second, 578 milliseconds.[info] Total number of tests run: 2[info] Suites: completed 1, aborted 0[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a follow-up of https://github.com/apache/spark/pull/37525 . When the project list has aliases, we go to the `projectExpression` branch which filters away invalid partitioning/ordering that reference non-existing attributes in the current plan node. However, this filtering is missing when the project list has no alias, where we directly return the child partitioning/ordering.This PR fixes it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to make sure we always return valid output partitioning/ordering.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use `DecimalAddNoOverflowCheck` instead of `Add` to craete bound ordering for window range frame### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Before 3.4, the `Add` did not check overflow. Instead, we always wrapped `Add` with a `CheckOverflow`. After https://github.com/apache/spark/pull/36698, we make `Add` check overflow by itself. However, the bound ordering of window range frame uses `Add` to calculate the boundary that is used to determine which input row lies within the frame boundaries of an output row. Then the behavior is changed with an extra overflow check. Technically，We could allow an overflowing value if it is just an intermediate result. So this pr use `DecimalAddNoOverflowCheck` to replace the `Add` to restore the previous behavior.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, restore the previous(before 3.4) behavior### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
2	-1	### What changes were proposed in this pull request?get ColStats in `DescribeColumnExec` when `isExtended` is true### Why are the changes needed?To make code cleaner### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing test
3	-4	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR fixes the internal error `Child is not Cast or ExpressionProxy of Cast` for valid `CaseWhen` expr with `Cast` expr in its branches.Specifically, after SPARK-39865, an improved error msg for overflow exception during table insert was introduced. The improvement covers `Cast` expr and `ExpressionProxy` expr, but `CaseWhen` and other complex ones are not covered. An example below hits an internal error today.```create table t1 as select x FROM values (1), (2), (3) as tab(x);create table t2 (x Decimal(9, 0));insert into t2 select 0 - (case when x = 1 then 1 else x end) from t1 where x = 1;```To fix the query failure, we decide to fall back to the previous handling if the expr is not a simple `Cast` expr or `ExpressionProxy` expr.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To fix the query regression introduced in SPARK-39865.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No. We just fall back to the previous error msg if the expression involving `Cast` is not a simple one.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?- Added Unit test.- Removed one test case for the `Child is not Cast or ExpressionProxy of Cast` internal error, as now we do not check if the child has a `Cast` expression and fall back to the previous error message.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?Protobuf deserializer (`from_protobuf()` function()) optionally supports recursive fields up to certain depth. Currently it uses `NullType` to terminate the recursion. But an `ArrayType` containing `NullType` is not really useful and  it does not work delta.This PR fixes this by removing the field to terminate recursion rather than using `NullType`. The following example illustrates the difference. E.g. Consider a recursive Protobuf like this:```message Node {    int value = 1;    repeated Node children = 2  // recursive array}message Tree {    Node root = 1}```Catalyst schama with `from_protobuf()` of `Tree` with max recursive depth set to 2, would be:    - **Before**:  _STRUCT<root: STRUCT<value: int, children: array<STRUCT<value: int, **children: array< void >**>>>>_   - **After**: _STRUCT<root: STRUCT<value: int, children: array<STRUCT<value: int>>>>_Notice that at second level, the `children` array is dropped, rather than being defined as `array<void>`. ### Why are the changes needed? - This improves how Protobuf connector handles recursive fields. It avoids using `void` fields which are problematic in many scenarios and do not add any information.### Does this PR introduce _any_ user-facing change? - This changes the schema in a subtle manner while using with recursive support enabled. Since this only removes an optional field, it is backward compatible. ### How was this patch tested? - Added multiple unit tests and updated existing one. Most of the changes for this PR are in the tests. 
2	-2	### What changes were proposed in this pull request?Sometimes, the SQL exists filter which condition compares the window function `ROW_NUMBER()` with number. For example,```SELECT *,         ROW_NUMBER() OVER(ORDER BY a) AS rnFROM Tab1WHERE rn <= 5```We can create a `Limit` as the parent node of `Window` node. Such as: `Limit(Literal(5), Window)` and the optimizer rule `LimitPushDownThroughWindow` will push down the `Limit` as the child of `Window`. After this optimization, Spark executes top n and reduce the data size before shuffle. We can consider the SQL adjusted as the below.```SELECT *,         ROW_NUMBER() OVER(ORDER BY a) AS rnFROM     (SELECT *    FROM Tab1    ORDER BY  a LIMIT 5) t```In short, it supports following pattern:```SELECT (... row_number()    OVER (ORDER BY  ... ) AS rn)WHERE rn (==|<|<=) k        AND other conditions```### Why are the changes needed?Improve the performance by infer window limit and push down it through window when partitionSpec is empty.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?The top n (`Limit` + `Sort`) have better performance than `WindowGroupLimit` if the window function is `RowNumber` and `Window`'s partitionSpec is empty.Before this PR, the micro benchmark of Top-K show below.```Java HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU  2.60GHzBenchmark Top-K:                                                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative-----------------------------------------------------------------------------------------------------------------------------------------------ROW_NUMBER (PARTITION: , WindowGroupLimit: false)                        10711          11068         363          2.0         510.8       1.0XROW_NUMBER (PARTITION: , WindowGroupLimit: true)                          2611           2929         210          8.0         124.5       4.1X```After this PR, the micro benchmark of Top-K show below.```Java HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzBenchmark Top-K:                                                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative-----------------------------------------------------------------------------------------------------------------------------------------------ROW_NUMBER (PARTITION: , WindowGroupLimit: false)                        11054          11684         406          1.9         527.1       1.0XROW_NUMBER (PARTITION: , WindowGroupLimit: true)                          1737           1772          23         12.1          82.8       6.4X```We can see the change of `ROW_NUMBER (PARTITION: , WindowGroupLimit: true) `: 2929 -> 1772.
2	-1	### What changes were proposed in this pull request?This pr aims add more types support of `sql.functions#lit` function, include:- Decimal- Instant- Timestamp- LocalDateTime- Date- Duration- Period- CalendarInterval### Why are the changes needed?Make ·sql.functions#lit· function support more types### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test- Manual checked new case with Scala-2.13
2	-3	### What changes were proposed in this pull request?When using the 'builtin' Hive version for the Hive metadata client, do not create a separate classloader, and rather continue to use the overall user/application classloader (regardless of Java version). This standardizes the behavior for all Java versions with that of Java 9+. See SPARK-42539 for more details on why this approach was chosen.### Why are the changes needed?Please see a much more detailed description in SPARK-42539. The tl;dr is that user-provided JARs (such as `hive-exec-2.3.8.jar`) take precedence over Spark/system JARs when constructing the classloader used by `IsolatedClientLoader` on Java 8 in 'builtin' mode, which can cause unexpected behavior and/or breakages. This violates the expectation that, unless user-first classloader mode is used, Spark JARs should be prioritized over user JARs. It also seems that this separate classloader was unnecessary from the start, since the intent of 'builtin' mode is to use the JARs already existing on the regular classloader (as alluded to [here](https://github.com/apache/spark/pull/24057#discussion_r265142878)). The isolated clientloader was originally added in #5876 in 2015. This bit in the PR description is the only mention of the behavior for \"builtin\":> attempt to discover the jars that were used to load Spark SQL and use those. This option is only valid when using the execution version of Hive.I can't follow the logic here; the user classloader clearly has all of the necessary Hive JARs, since that's where we're getting the JAR URLs from, so we could just use that directly instead of grabbing the URLs. When this was initially added, it only used the JARs from the user classloader, not any of its parents, which I suspect was the motivating factor (to try to avoid more Spark classes being duplicated inside of the isolated classloader, I guess). But that was changed a month later anyway in #6435 / #6459, so I think this may have basically been deadcode from the start. It has also caused at least one issue over the years, e.g. SPARK-21428, which disables the new-classloader behavior in the case of running inside of a CLI session.### Does this PR introduce _any_ user-facing change?No, except to protect Spark itself from potentially being broken by bad user JARs.### How was this patch tested?This includes a new unit test in `HiveUtilsSuite` which demonstrates the issue and shows that this approach resolves it. It has also been tested on a live cluster running Java 8 and Hive communication functionality continues to work as expected.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support Pivot with provided pivot column values. Not supporting Pivot without providing column values because that requires to do max value check which depends on the implementation of Spark configuration in Spark Connect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds a new builtin table-valued function `json_tuple`.### Why are the changes needed?To improve the usability of table-valued generator functions.### Does this PR introduce _any_ user-facing change?Yes. After this PR, the table-valued generator function `json_tuple` can be used in the FROM clause of a query.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL query tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces a new client-streaming RPC service \"AddArtifacts\" to handle the transfer of artifacts from the client to the server. New message types `AddArtifactsRequest` and `AddArtifactsResponse` are added that specify the format of artifact transfer.An artifact is defined by its `name` and `data` fields.- `name`  - The name of the artifact is expected in the form of a \"Relative Path\" that is made up of a sequence of directories and the final file element.  - Examples of \"Relative Path\"s: `jars/test.jar`,  `classes/xyz.class`, `abc.xyz`, `a/b/X.jar`.  - The server is expected to maintain the hierarchy of files as defined by their name. (i.e The relative path of the file on the server's filesystem will be the same as the name of the provided artifact).- `data`  - The raw data of the artifact.The intention behind the `name` format is to add extensibility to the approach. Through this scheme, the server can maintain the hierarchy/grouping of files in any way the client specifies as well as transfer different \"forms\" of artifacts without needing any updates to the protocol/code itself.The protocol supports batching and chunking (due to gRPC size limits) of artifacts as required.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In the decoupled client-server architecture of Spark Connect, a remote client may use a local JAR or a new class in their UDF that may not be present on the server. To handle these cases of missing \"artifacts\ a protocol for artifact transfer is needed to move the required artifacts from the client side over to the server side.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support parameterized SQL API in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds a new builtin table-valued function `stack`.### Why are the changes needed?To improve the usability of table-valued generator functions.### Does this PR introduce _any_ user-facing change?Yes. After this PR, the table-valued generator function `stack` can be used in the FROM clause of a query.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL query tests.
1	-1	### What changes were proposed in this pull request?Implements `SparkSession.conf`.Took #39995 over.### Why are the changes needed?`SparkSession.conf` is a missing feature.### Does this PR introduce _any_ user-facing change?Yes, `SparkSession.conf` will be available.### How was this patch tested?Added/enabled related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds new builtin table-valued functions `posexplode`, `posexplode_outer`, `json_tuple` and `stack`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To improve the usability of table-valued generator functions. Now all generate functions can be used as table value functions.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. After this PR, 4 new table-valued generator functions can be used in the FROM clause of a query.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL query tests
1	-1	### What changes were proposed in this pull request?This PR aims to remove `experimental` notes from `Volcano` docs.### Why are the changes needed?Apache Spark 3.3.0 added `Volcano` as an experimental module. Now, we can remove it from Apache Spark 3.4.0 because we don't expect breaking future behavior changes.### Does this PR introduce _any_ user-facing change?No, this is a documentation only change.### How was this patch tested?Manual review.
2	-3	### What changes were proposed in this pull request?This PR proposes to avoid new Python typing syntax that causes the test failure in lower Python version.### Why are the changes needed?Python 3.7 support is broken:```+ ./python/run-tests --python-executables=python3Running PySpark tests. Output is in /home/ec2-user/spark/python/unit-tests.logWill test against the following Python executables: ['python3']Will test the following Python modules: ['pyspark-connect', 'pyspark-core', 'pyspark-errors', 'pyspark-ml', 'pyspark-mllib', 'pyspark-pandas', 'pyspark-pandas-slow', 'pyspark-resource', 'pyspark-sql', 'pyspark-streaming']python3 python_implementation is CPythonpython3 version is: Python 3.7.16Starting test(python3): pyspark.ml.tests.test_feature (temp output: /home/ec2-user/spark/python/target/8ca9ab1a-05cc-4845-bf89-30d9001510bc/python3__pyspark.ml.tests.test_feature__kg6sseie.log)Starting test(python3): pyspark.ml.tests.test_base (temp output: /home/ec2-user/spark/python/target/f2264f3b-6b26-4e61-9452-8d6ddd7eb002/python3__pyspark.ml.tests.test_base__0902zf9_.log)Starting test(python3): pyspark.ml.tests.test_algorithms (temp output: /home/ec2-user/spark/python/target/d1dc4e07-e58c-4c03-abe5-09d8fab22e6a/python3__pyspark.ml.tests.test_algorithms__lh3wb2u8.log)Starting test(python3): pyspark.ml.tests.test_evaluation (temp output: /home/ec2-user/spark/python/target/3f42dc79-c945-4cf2-a1eb-83e72b40a9ee/python3__pyspark.ml.tests.test_evaluation__89idc7fa.log)Finished test(python3): pyspark.ml.tests.test_base (16s)Starting test(python3): pyspark.ml.tests.test_functions (temp output: /home/ec2-user/spark/python/target/5a3b90f0-216b-4edd-9d15-6619d3e03300/python3__pyspark.ml.tests.test_functions__g5u1290s.log)Traceback (most recent call last):  File \"/usr/lib64/python3.7/runpy.py\ line 193, in _run_module_as_main    \"__main__\ mod_spec)  File \"/usr/lib64/python3.7/runpy.py\ line 85, in _run_code    exec(code, run_globals)  File \"/home/ec2-user/spark/python/pyspark/ml/tests/test_functions.py\ line 21, in <module>    from pyspark.ml.functions import predict_batch_udf  File \"/home/ec2-user/spark/python/pyspark/ml/functions.py\ line 38, in <module>    from typing import Any, Callable, Iterator, List, Mapping, Protocol, TYPE_CHECKING, Tuple, UnionImportError: cannot import name 'Protocol' from 'typing' (/usr/lib64/python3.7/typing.py)Had test failures in pyspark.ml.tests.test_functions with python3; see logs.```### Does this PR introduce _any_ user-facing change?This change has not been released out yet so no user-facing change. But this is a release blocker.### How was this patch tested?Manually tested via:```bash./python/run-tests --python-executables=python3.7```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new trait `ReferenceAllColumns ` that overrides `references` using children output. Then we can skip it during rewriting attributes in transformUpWithNewOutput.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->There are two reasons with this new trait:1. it's dangerous to call `references` on an unresolved plan that all of references come from children2. it's unnecessary to rewrite its attributes that all of references come from children### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->prevent potential bug### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test and pass CI
3	-2	### What changes were proposed in this pull request?The PR fixes DB2 Limit clause syntax. Although DB2 supports LIMIT keyword, it seems that this support varies across databases and versions and the recommended way is to use `FETCH FIRST x ROWS ONLY`. In fact, some versions don't support LIMIT at all. Doc: https://www.ibm.com/docs/en/db2/11.5?topic=subselect-fetch-clause, usage example: https://www.mullinsconsulting.com/dbu_0502.htm.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fixes the incorrect Limit clause which could cause errors when using against DB2 versions that don't support LIMIT.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->    I added a unit test and an integration test to cover this functionality.
2	-2	### What changes were proposed in this pull request?This is the scala version of https://github.com/apache/spark/pull/39925.We introduce a plan_id that is both used for each plan created by the scala client, and by the columns created when calling `Dataframe.col(..)` and `Dataframe.apply(..)`. This way we can later properly resolve the columns created for a specific Dataframe.### Why are the changes needed?Joining columns  created using Dataframe.apply(...) does not work when the column names are ambiguous. We should be able to figure out where a column comes from when they are created like this.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Updated golden files. Added test case to ClientE2ETestSuite.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Support more subexpression elimination cases.How to do subexpression elimination:* Get all common expressions from input expressions. Recursively visits all subexpressions regardless of whether the current expression is a conditional expression.* For each common expression:  * Add a new boolean variable subExprInit_n to indicate whether we have  already evaluated the common expression, and reset it to false at the start of operator.consume()  * Add a new wrapper subExpr function for common subexpression.```javaprivate void subExpr_n(${argList}) { if (!subExprInit_n) {   ${eval.code}   subExprInit_n = true;   subExprIsNull_n = ${eval.isNull};   subExprValue_n = ${eval.value}; }}```  * Replace all the common subexpression with the wrapper function `subExpr_n(argList)`.## New support subexpression elimination patterns* Support subexpression elimination with conditional expressions```sqlSELECT case when v + 2 > 1 then 1            when v + 1 > 2 then 2            when v + 1 > 3 then 3 END vvFROM values(1) as t2(v)```We can reuse the result of expression  v + 1```sqlSELECT a, max(if(a > 0, b + c, null)) max_bc, min(if(a > 1, b + c, null)) min_bcFROM values(1, 1, 1) as t(a, b, c)GROUP BY a```We can reuse the result of expression  b + c* Support subexpression elimination in FilterExec```sqlSELECT * FROM (  SELECT v * v + 1 v1 from values(1) as t2(v)) twhere v1 > 5 and v1 < 10```We can reuse the result of expression  v * v + 1* Support subexpression elimination in JoinExec```sqlSELECT * FROM values(1, 1) as t1(a, b) join values(1, 2) as t2(x, y)ON b * y between 2 and 3``` We can reuse the result of expression  b * y* Support subexpression elimination in ExpandExec```sqlSELECT a, count(b),   \tcount(distinct case when b > 1 then b + c else null end) as count_bc_1,   \tcount(distinct case when b < 0 then b + c else null end) as count_bc_2FROM values(1, 1, 1) as t(a, b, c)GROUP BY a```We can reuse the result of expression  b + c### Why are the changes needed?Support more subexpression elimination cases, improve performance.For example, TPCDS q23b query,  we can reuse the result of projection `sum(ss_quantity * ss_sales_price)` in the if expressions:```if (isnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])))   input[0, decimal(28,2), true]else    (input[0, decimal(28,2), true] + cast(knownnotnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])) as decimal(28,2)))```q4, q62, q67 are similar to the above.| Query    | Time with PR | Time without PR | Time diff | Percentage ||----------|--------------|-----------------|-----------|------------|| q1.sql   | 36.97        | 36.623          | -0.347    | 99.06%     || q2.sql   | 42.699       | 41.741          | -0.958    | 97.76%     || q3.sql   | 6.038        | 6.111           | 0.073     | 101.21%    || q4.sql   | 273.849      | 323.799         | 49.95     | 118.24%    || q5.sql   | 76.202       | 76.353          | 0.151     | 100.20%    || q6.sql   | 8.451        | 9.011           | 0.56      | 106.63%    || q7.sql   | 13.017       | 12.954          | -0.063    | 99.52%     || q8.sql   | 10.22        | 11.027          | 0.807     | 107.90%    || q9.sql   | 72.843       | 72.019          | -0.824    | 98.87%     || q10.sql  | 13.233       | 14.028          | 0.795     | 106.01%    || q11.sql  | 112.252      | 112.983         | 0.731     | 100.65%    || q12.sql  | 3.036        | 3.488           | 0.452     | 114.89%    || q13.sql  | 12.471       | 12.911          | 0.44      | 103.53%    || q14a.sql | 210.201      | 220.12          | 9.919     | 104.72%    || q14b.sql | 185.374      | 187.57          | 2.196     | 101.18%    || q15.sql  | 10.189       | 10.338          | 0.149     | 101.46%    || q16.sql  | 80.756       | 82.503          | 1.747     | 102.16%    || q17.sql  | 28.523       | 28.567          | 0.044     | 100.15%    || q18.sql  | 13.417       | 14.271          | 0.854     | 106.37%    || q19.sql  | 6.366        | 6.53            | 0.164     | 102.58%    || q20.sql  | 3.427        | 4.939           | 1.512     | 144.12%    || q21.sql  | 2.096        | 2.16            | 0.064     | 103.05%    || q22.sql  | 14.4         | 14.01           | -0.39     | 97.29%     || q23a.sql | 507.253      | 545.185         | 37.932    | 107.48%    || q23b.sql | 707.054      | 768.148         | 61.094    | 108.64%    || q24a.sql | 193.116      | 193.793         | 0.677     | 100.35%    || q24b.sql | 177.109      | 179.54          | 2.431     | 101.37%    || q25.sql  | 22.264       | 22.949          | 0.685     | 103.08%    || q26.sql  | 8.68         | 8.973           | 0.293     | 103.38%    || q27.sql  | 8.535        | 8.558           | 0.023     | 100.27%    || q28.sql  | 101.953      | 102.713         | 0.76      | 100.75%    || q29.sql  | 75.392       | 76.211          | 0.819     | 101.09%    || q30.sql  | 12.265       | 13.508          | 1.243     | 110.13%    || q31.sql  | 26.477       | 26.965          | 0.488     | 101.84%    || q32.sql  | 3.393        | 3.507           | 0.114     | 103.36%    || q33.sql  | 6.909        | 7.277           | 0.368     | 105.33%    || q34.sql  | 8.41         | 8.572           | 0.162     | 101.93%    || q35.sql  | 34.214       | 36.822          | 2.608     | 107.62%    || q36.sql  | 9.027        | 9.79            | 0.763     | 108.45%    || q37.sql  | 36.076       | 36.753          | 0.677     | 101.88%    || q38.sql  | 71.768       | 74.473          | 2.705     | 103.77%    || q39a.sql | 7.753        | 7.617           | -0.136    | 98.25%     || q39b.sql | 6.365        | 7.229           | 0.864     | 113.57%    || q40.sql  | 16.588       | 17.164          | 0.576     | 103.47%    || q41.sql  | 1.162        | 1.188           | 0.026     | 102.24%    || q42.sql  | 2.3          | 2.561           | 0.261     | 111.35%    || q43.sql  | 7.407        | 7.605           | 0.198     | 102.67%    || q44.sql  | 28.939       | 30.473          | 1.534     | 105.30%    || q45.sql  | 9.796        | 9.634           | -0.162    | 98.35%     || q46.sql  | 9.496        | 9.692           | 0.196     | 102.06%    || q47.sql  | 27.087       | 27.151          | 0.064     | 100.24%    || q48.sql  | 14.524       | 14.889          | 0.365     | 102.51%    || q49.sql  | 21.466       | 21.572          | 0.106     | 100.49%    || q50.sql  | 194.755      | 195.052         | 0.297     | 100.15%    || q51.sql  | 37.493       | 38.56           | 1.067     | 102.85%    || q52.sql  | 2.227        | 2.28            | 0.053     | 102.38%    || q53.sql  | 5.375        | 5.437           | 0.062     | 101.15%    || q54.sql  | 12.556       | 13.015          | 0.459     | 103.66%    || q55.sql  | 2.341        | 2.809           | 0.468     | 119.99%    || q56.sql  | 7.424        | 7.207           | -0.217    | 97.08%     || q57.sql  | 17.606       | 17.797          | 0.191     | 101.08%    || q58.sql  | 6.169        | 6.374           | 0.205     | 103.32%    || q59.sql  | 27.602       | 27.744          | 0.142     | 100.51%    || q60.sql  | 7.04         | 7.459           | 0.419     | 105.95%    || q61.sql  | 7.838        | 7.816           | -0.022    | 99.72%     || q62.sql  | 9.726        | 10.762          | 1.036     | 110.65%    || q63.sql  | 4.816        | 5.176           | 0.36      | 107.48%    || q64.sql  | 253.937      | 261.034         | 7.097     | 102.79%    || q65.sql  | 78.942       | 78.373          | -0.569    | 99.28%     || q66.sql  | 15.199       | 14.73           | -0.469    | 96.91%     || q67.sql  | 926.049      | 1022.971        | 96.922    | 110.47%    || q68.sql  | 7.932        | 7.977           | 0.045     | 100.57%    || q69.sql  | 12.101       | 14.699          | 2.598     | 121.47%    || q70.sql  | 20.7         | 20.872          | 0.172     | 100.83%    || q71.sql  | 14.96        | 15.065          | 0.105     | 100.70%    || q72.sql  | 73.215       | 73.955          | 0.74      | 101.01%    || q73.sql  | 5.973        | 6.126           | 0.153     | 102.56%    || q74.sql  | 97.611       | 99.577          | 1.966     | 102.01%    || q75.sql  | 125.005      | 129.508         | 4.503     | 103.60%    || q76.sql  | 34.812       | 35.34           | 0.528     | 101.52%    || q77.sql  | 7.686        | 8.474           | 0.788     | 110.25%    || q78.sql  | 287.959      | 292.936         | 4.977     | 101.73%    || q79.sql  | 8.401        | 9.616           | 1.215     | 114.46%    || q80.sql  | 59.371       | 60.051          | 0.68      | 101.15%    || q81.sql  | 18.452       | 19.499          | 1.047     | 105.67%    || q82.sql  | 64.093       | 65.032          | 0.939     | 101.47%    || q83.sql  | 4.675        | 4.867           | 0.192     | 104.11%    || q84.sql  | 10.456       | 10.816          | 0.36      | 103.44%    || q85.sql  | 12.347       | 12.77           | 0.423     | 103.43%    || q86.sql  | 6.537        | 6.843           | 0.306     | 104.68%    || q87.sql  | 77.427       | 77.876          | 0.449     | 100.58%    || q88.sql  | 83.082       | 83.385          | 0.303     | 100.36%    || q89.sql  | 6.645        | 6.801           | 0.156     | 102.35%    || q90.sql  | 7.841        | 7.883           | 0.042     | 100.54%    || q91.sql  | 3.88         | 4.129           | 0.249     | 106.42%    || q92.sql  | 3.044        | 3.271           | 0.227     | 107.46%    || q93.sql  | 361.149      | 365.883         | 4.734     | 101.31%    || q94.sql  | 43.929       | 46.667          | 2.738     | 106.23%    || q95.sql  | 196.363      | 197.427         | 1.064     | 100.54%    || q96.sql  | 12.457       | 12.496          | 0.039     | 100.31%    || q97.sql  | 80.131       | 81.821          | 1.69      | 102.11%    || q98.sql  | 6.885        | 7.522           | 0.637     | 109.25%    || q99.sql  | 17.685       | 18.009          | 0.324     | 101.83%    ||          | 6788.707     | 7116.357        | 327.65    | 104.82%    |One of our production query which has 19 case when  expressions, it's query time changed from 1.1 hour to 42 seconds.![image](https://user-images.githubusercontent.com/3626747/227448668-8a58ff33-3296-4a95-984b-292af47532ca.png)A simplify benchmark of the above production query.```    spark.range(1, 2000000, 1, 1)      .selectExpr(        \"cast(id + 1 as decimal) as a\        \"cast(id + 2 as decimal) as b\        \"cast(id + 3 as decimal) as c\        \"cast(id + 4 as decimal) as d\")      .createOrReplaceTempView(\"tab\")    runBenchmark(\"Subexpression elimination in ProjectExec\") {      val benchmark =        new Benchmark(\"Subexpression elimination in ProjectExec\ 2000000, output = output)      benchmark.addCase(s\"Test query\") { _ =>        val query =          s\"\"\"             |SELECT a, b, c, d,             |       a * b / c as s1,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END s2,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |       ELSE 0 END s3,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s4,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                      CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |                 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s5             |FROM tab             |\"\"\".stripMargin        spark.sql(query).noop()      }      benchmark.run()```Local benchmark result:Before this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         9713           9900         263          0.2        4856.7       1.0X```After this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         1238           1307          98          8.1         123.8       1.0X```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Exists UT.
2	-1	### What changes were proposed in this pull request?The pr aims to > 1.Fix typos 'WriteOperaton -> WriteOperation' for `SaveModeConverter`.> 2.Update comment.### Why are the changes needed?Fix typos.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?This patch allows for eager execution of SQL statements using the Spark Connect Data Frame API.  The implementation of the patch is as follows: When `spark.sql` is called, the client sends a command to the server including the SQL statement. The server will evaluate the query and execute the side-effects if necessary. If the query was a command it will return the results as a `Relaiton.LocalRelation` back to the client otherwise it will return a `Relation.SQL` to the client. The client then simply forwards the received payload to the next operator.### Why are the changes needed?Compatibility### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
3	-3	 ```\"23/02/23 23:57:44 INFO Executor: Running task 2.0 in stage 57.1 (TID 363)\"23/02/23 23:58:44 ERROR RocksDB StateStoreId(opId=0,partId=3,name=default): RocksDB instance could not be acquired by [ThreadId: Some(49), task: 3.0 in stage 57, TID 363] as it was not released by [ThreadId: Some(51), task: 3.1 in stage 57, TID 342] after 60002 ms.``` We are seeing those error messages for a testing query. The `taskId != partitionId` but we fail to be clear on this in the error log.It's confusing when we see those logs: the second log entry seems to talk about `task 3.0` (it's actually partition 3 and retry attempt 0), but the `TID 363` is already occupied by `task 2.0 in stage 57.1`. Also, it's unclear at which stage retry attempt, the lock is acquired (or fails to be acquired)### What changes were proposed in this pull request?* add `partition ` after `task: ` in the log message for clarification* add stage attempt to distinguish different stage retries.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->improve the log message for a better debuggability### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->only log message change
3	-4	The current behavior of the `compute` method in both `StateStoreRDD` and `ReadStateStoreRDD` is: we first get the state store instance and then get the input iterator for the inputRDD.For RocksDB state store, the running task will acquire and hold the lock for this instance. The retried task or speculative task will fail to acquire the lock and eventually abort the job if there are some network issues. For example, When we shrink the executors, the alive one will try to fetch data from the killed ones because it doesn't know the target location (prefetched from the driver) is dead until it tries to fetch data. The query might be hanging for a long time as the executor will retry `spark.shuffle.io.maxRetries=3` times and for each retry wait for `spark.shuffle.io.connectionTimeout` (default value is 120s) before timeout. In total, the task could be hanging for about 6 minutes. And the retried or speculative tasks won't be able to acquire the lock in this period.Making lock acquisition happen after retrieving the input iterator should be able to avoid this situation.### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Making lock acquisition happen after retrieving the input iterator.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Avoid the failure like the following when there is a network issue```java.lang.IllegalStateException: StateStoreId(opId=0,partId=3,name=default): RocksDB instance could not be acquired by [ThreadId: Some(47), task: 3.1 in stage 57, TID 793] as it was not released by [ThreadId: Some(51), task: 3.1 in stage 57, TID 342] after 60003 ms. ```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing UT should be good enough
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Track load time for state store provider and log warning if it exceeds threshold### Why are the changes needed?We have seen that the initial state store provider load can be blocked by external factors such as filesystem initialization. This log enables us to track cases where this load takes too long and we log a warning in such cases.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Augmented some of the tests to verify the logging is working as expected.Sample logs:```14:58:51.784 WARN org.apache.spark.sql.execution.streaming.state.StateStore: Loaded state store provider in loadTimeMs=2049 for storeId=StateStoreId[ checkpointRootLocation=file:/Users/anish.shrigondekar/spark/spark/target/tmp/streaming.metadata-1f2ff296-1ece-4a0c-b4b4-48aa0e909b49/state, operatorId=0, partitionId=2, storeName=default ] and queryRunId=a4063603-3929-4340-9920-eca206ebec3614:58:53.838 WARN org.apache.spark.sql.execution.streaming.state.StateStore: Loaded state store provider in loadTimeMs=2046 for storeId=StateStoreId[ checkpointRootLocation=file:/Users/anish.shrigondekar/spark/spark/target/tmp/streaming.metadata-1f2ff296-1ece-4a0c-b4b4-48aa0e909b49/state, operatorId=0, partitionId=3, storeName=default ] and queryRunId=a4063603-3929-4340-9920-eca206ebec3614:58:55.885 WARN org.apache.spark.sql.execution.streaming.state.StateStore: Loaded state store provider in loadTimeMs=2044 for storeId=StateStoreId[ checkpointRootLocation=file:/Users/anish.shrigondekar/spark/spark/target/tmp/streaming.metadata-1f2ff296-1ece-4a0c-b4b4-48aa0e909b49/state, operatorId=0, partitionId=4, storeName=default ] and queryRunId=a4063603-3929-4340-9920-eca206ebec36```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Match https://github.com/apache/spark/blob/6a2433070e60ad02c69ae45706a49cdd0b88a082/python/pyspark/sql/connect/dataframe.py#L1500 to throw unsupported exceptions in Scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better indicating a API is not supported yet.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
2	-2	### What changes were proposed in this pull request?Fixes `SparkConnectStreamHandler` to handle configs properly while planning.The whole process should be done in `session.withActive` to take the proper `SQLConf` into account.### Why are the changes needed?Some components for planning need to check configs in `SQLConf.get` while building the plan, but currently it's unavailable.For example, `spark.sql.legacy.allowNegativeScaleOfDecimal` needs to check when construct `DecimalType` but it's not set while planning, thus it causes an error when trying to cast to `DecimalType(1, -1)` with the config set to `\"true\"`:```[INTERNAL_ERROR] Negative scale is not allowed: -1. Set the config \"spark.sql.legacy.allowNegativeScaleOfDecimal\" to \"true\" to allow it.```### Does this PR introduce _any_ user-facing change?The configs will take effect while planning.### How was this patch tested?Enabled a related test.
1	-2	### What changes were proposed in this pull request?Fixes `DataFrameReader` to use the default source.### Why are the changes needed?```pyspark.read.load(path)```should work and use the default source without specifying the format.### Does this PR introduce _any_ user-facing change?The `format` doesn't need to be specified.### How was this patch tested?Enabled related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add temp view API to Dataset### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?Make binary compatibility check for SparkSession/Dataset/Column/functions etc.### Why are the changes needed?Help us to have a good understanding of the current API coverage of the Scala client.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing tests.
1	-2	### What changes were proposed in this pull request?Make all client tests to extend from ConnectFunSuite to avoid `// scalastyle:ignore funsuite` when extending directly from `AnyFunSuite`### Why are the changes needed?Simple dev work.### Does this PR introduce _any_ user-facing change?No. test only.### How was this patch tested?n/a
1	-3	### What changes were proposed in this pull request?Fixes `DataFrame.toPandas` to handle duplicated column names.### Why are the changes needed?Currently```pyspark.sql(\"select 1 v, 1 v\").toPandas()```fails with the error:```pyTraceback (most recent call last):...  File \".../python/pyspark/sql/connect/dataframe.py\ line 1335, in toPandas    return self._session.client.to_pandas(query)  File \".../python/pyspark/sql/connect/client.py\ line 548, in to_pandas    pdf = table.to_pandas()  File \"pyarrow/array.pxi\ line 830, in pyarrow.lib._PandasConvertible.to_pandas  File \"pyarrow/table.pxi\ line 3908, in pyarrow.lib.Table._to_pandas  File \"/.../lib/python3.9/site-packages/pyarrow/pandas_compat.py\ line 819, in table_to_blockmanager    columns = _deserialize_column_index(table, all_columns, column_indexes)  File \"/.../lib/python3.9/site-packages/pyarrow/pandas_compat.py\ line 938, in _deserialize_column_index    columns = _flatten_single_level_multiindex(columns)  File \"/.../lib/python3.9/site-packages/pyarrow/pandas_compat.py\ line 1186, in _flatten_single_level_multiindex    raise ValueError('Found non-unique column index')ValueError: Found non-unique column index```Simliar to #28210.### Does this PR introduce _any_ user-facing change?Duplicated column names will be available when calling `toPandas()`.### How was this patch tested?Enabled related tests.
1	-1	### What changes were proposed in this pull request?Changes are only in tests files.- Refactored the `TPCHBase` class and created `TPCHSchema` similar to TPCDS.- Created a base class `TPCSchema` which is extended by `TPCDSSchema` and `TPCHSchema`.### Why are the changes needed?This PR just refactors the code. Going forward it will help in making code changes at just one place to reflect in both TPCDS and TPCH. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Ran the PlanStabilitySuite to verify the changes. 
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Follow up https://github.com/apache/spark/pull/40164 to also throw unsupported operation exception for `persist`. Right now we are ok to depends on the `StorageLevel` in core module but in the future that shall be refactored and moved to a common module.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better way to indicate a non-supported API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `groupBy(col1: String, cols: String*)` to Scala client Dataset API.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix BUILD caused by https://github.com/apache/spark/pull/40168. This is not intentional when a PR changes stuff where another PR replies on old code but already pass the tests. In this case if either PR is merged without causing a conflict then another PR can be merged without knowing that it will cause a BUILD break.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix BUILD### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
1	-1	### What changes were proposed in this pull request?This PR adds the client side typed API to the Spark Connect Scala Client.### Why are the changes needed?We want to reach API parity with the existing APIs.### Does this PR introduce _any_ user-facing change?Yes, it adds user API.### How was this patch tested?Added tests to `ClientE2ETestSuite`, and updated existing tests.
1	-1	### What changes were proposed in this pull request?The pr aims to implement SparkSession.version and SparkSession.time.### Why are the changes needed?API coverage.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add new UT.
2	-1	### What changes were proposed in this pull request?Enhance `EliminateOuterJoin` by removing the outer join if they are all distinct aggregate functions. For example:```sqlSELECT t1.c1, sum(DISTINCT t1.c2) FROM t1 LEFT JOIN t2 ON t1.c1 = t2.c1 GROUP BY t1.c1==>SELECT t1.c1, sum(DISTINCT t1.c2) FROM t1 GROUP BY t1.c1```### Why are the changes needed?Improve query performance. TiDB also supports this optimization:<img width=\"1221\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5399861/221417544-cfd13fe1-6210-44b3-91d4-c3da0bd084f0.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
1	-1	### What changes were proposed in this pull request?Remove Jenkins from building-spark web page.### Why are the changes needed?### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?No need.
1	-1	### What changes were proposed in this pull request?This PR adds the ColumnName for the Spark Connect Scala Client. This is a stepping stone to implement the SQLImplicits.### Why are the changes needed?API parity with the current API.### Does this PR introduce _any_ user-facing change?Yes. It adds new API.### How was this patch tested?Added existing tests tot `ColumnTestSuite`.
1	-1	### What changes were proposed in this pull request?This PR aims to use `wrapper versions` for SBT and Maven in `connect` test module's exceptions and comments.### Why are the changes needed?To clarity the versions we used.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
1	-3	### What changes were proposed in this pull request?This PR aims to exclude `RelationalGroupedDataset.apply` from the compatibility test and also apply `ScalaFmt` changes on the comments.https://github.com/apache/spark/blob/24f0c45dc11eb7ac1ee43ebd630cdb325da30326/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala#L692-L697### Why are the changes needed?`org/apache/spark/sql/RelationalGroupedDataset` doesn't have the corresponding one of `static method apply(...)`.- https://github.com/apache/spark/actions/runs/4274237512/jobs/7440712998```[info] - compatibility MiMa tests *** FAILED *** (1 second, 474 milliseconds)[info]   Comparing client jar: /home/runner/work/spark/spark/connector/connect/client/jvm/target/scala-2.13/spark-connect-client-jvm-assembly-3.4.1-SNAPSHOT.jar[info]   and sql jar: /home/runner/work/spark/spark/sql/core/target/scala-2.13/spark-sql_2.13-3.4.1-SNAPSHOT.jar[info]   static method apply(org.apache.spark.sql.Dataset,scala.collection.immutable.Seq,org.apache.spark.sql.RelationalGroupedDataset#GroupType)org.apache.spark.sql.RelationalGroupedDataset in class org.apache.spark.sql.RelationalGroupedDataset does not have a correspondent in client version[info]   method apply(org.apache.spark.sql.Dataset,scala.collection.immutable.Seq,org.apache.spark.sql.RelationalGroupedDataset#GroupType)org.apache.spark.sql.RelationalGroupedDataset in object org.apache.spark.sql.RelationalGroupedDataset does not have a correspondent in client version (CompatibilitySuite.scala:208)```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
2	-2	### What changes were proposed in this pull request?Extend the CollapseWindow rule to collapse Window nodes with the equivalent partition/order expressions in two withColumn() ### Why are the changes needed?```Seq((1, 1), (2, 2)).toDF(\"a\ \"b\")      .withColumn(\"max_b\ expr(\"max(b) OVER (PARTITION BY abs(a))\"))      .withColumn(\"min_b\ expr(\"min(b) OVER (PARTITION BY abs(a))\"))== Optimized Logical Plan ==beforeProject [a#7, b#8, max_b#11, min_b#17]+- Window [min(b#8) windowspecdefinition(_w0#19, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS min_b#17], [_w0#19]   +- Project [a#7, b#8, max_b#11, abs(a#7) AS _w0#19]      +- Window [max(b#8) windowspecdefinition(_w0#13, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS max_b#11], [_w0#13]         +- Project [_1#2 AS a#7, _2#3 AS b#8, abs(_1#2) AS _w0#13]            +- LocalRelation [_1#2, _2#3]afterProject [a#7, b#8, max_b#11, min_b#17]+- Window [max(b#8) windowspecdefinition(_w0#13, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS max_b#11, min(b#8) windowspecdefinition(_w0#13, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS min_b#17], [_w0#13]   +- Project [_1#2 AS a#7, _2#3 AS b#8, abs(_1#2) AS _w0#13]      +- LocalRelation [_1#2, _2#3]```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
1	-2	### What changes were proposed in this pull request?This is a follow-up of https://github.com/apache/spark/pull/40180.### Why are the changes needed?At previous PR, `Scalastyle` is checked but `scalafmt` was missed at the last commit.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass all CI linter jobs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Throw exceptions for unsupported session API:1. newSession2. getActiveSession3. getDefaultSession4. active### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->Better indicate an API is not supported to users.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
1	-1	### What changes were proposed in this pull request?This PR adds the RuntimeConfig class for the Spark Connect Scala Client.### Why are the changes needed?API Parity.### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Added tests to the ClientE2ETestSuite.
1	-1	### What changes were proposed in this pull request?This PR adds the `SQLImplicits` class to Spark Connect. This makes it easier for end users to work with Connect Datasets.The current implementation only contains the column conversions, we will add the encoder implicits in a follow-up.### Why are the changes needed?API Parity.### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?I added a new test suite: `SQLImplicitTestSuite.`
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->https://github.com/apache/spark/pull/40073 accidentally changed the relationship of the two `if` statement in `StateStoreProvider.validateStateRowFormat`. Before they were inclusive, i.e. ```if (a) {  // <code>  if (b) {    // <code>  }}```It was changed to parallel, i.e.```if (a) {  // <code>}if (b) {    // <code>}```This PR change it back to the original behavior and add a unit test to prevent it in the future.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->As above.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit test
1	-2	### What changes were proposed in this pull request?This PR proposes to document how to perform chained time window aggregations. Although it is introduced as a way to perform chained time window aggregations, it can be also used \"generally\" to apply operations which require timestamp column against the time window data.### Why are the changes needed?We didn't document the new functionality in the guide doc in SPARK-40925. There was a doc change SPARK-42105, but it only mentioned the unblock of limitations.### Does this PR introduce _any_ user-facing change?Yes, documentation change.### How was this patch tested?Created a page via `SKIP_API=1 bundle exec jekyll serve --watch` and confirmed. Screenshot:<img width=\"611\" alt=\"스크린샷 2023-02-28 오전 8 32 24\" src=\"https://user-images.githubusercontent.com/1317309/221713232-3ea906ce-23f6-4293-82c0-de1e69ea1ee8.png\">
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Mainly refactoring. Move the logic of resolving StreamingRelation versions to a new rule. Concretely, create a new node named `VersionUnresolvedRelation`, which will be resolved to `StreamingRelation` or `StreamingRelationV2` with rule `ResolveDataSourceVersion` in analyzer. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better Code Structure.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests should be enough.
3	-2	### What changes were proposed in this pull request?This PR enhance `UnwrapCastInBinaryComparison` to support unwrap date type to timestamp type.The way to unwrap date type to timestamp type are:```GreaterThan(Cast(ts, DateType), date) -> GreaterThanOrEqual(ts, Cast(date + 1, TimestampType))GreaterThanOrEqual(Cast(ts, DateType), date) -> GreaterThanOrEqual(ts, Cast(date, TimestampType))Equality(Cast(ts, DateType), date) -> And(GreaterThanOrEqual(ts, Cast(date, TimestampType)), LessThan(ts, Cast(date + 1, TimestampType)))LessThan(Cast(ts, DateType), date) -> LessThan(ts, Cast(date, TimestampType))LessThanOrEqual(Cast(ts, DateType), date) -> LessThan(ts, Cast(date + 1, TimestampType))```### Why are the changes needed?Improve query performance.A common use case. We store cold data in HDFS by partition, store hot data in MySQL, and then union all the results. The filter in the MySQL branch cannot be pushed down, which affects performance:```sqlCREATE TABLE t_cold(id bigint, start timestamp, dt date) using parquet PARTITIONED BY (dt);CREATE TABLE t_hot(id bigint, start timestamp) using org.apache.spark.sql.jdbc OPTIONS (`url` '***', `dbtable` 'db.t2', `user` 'spark', `password` '***');CREATE VIEW all_data AS SELECT * FROM t_cold UNION ALL SELECT *, to_date(start) FROM t_hot;SELECT * FROM all_data WHERE start BETWEEN '2023-02-06' AND '2023-02-07';```Before this PR | After this PR-- | --<img src=\"https://user-images.githubusercontent.com/5399861/221576723-7fc45356-65db-48e2-8d40-88420c21c9f5.png\" width=\"400\" height=\"730\"> | <img src=\"https://user-images.githubusercontent.com/5399861/221575848-5b975ed0-70ab-4527-acfe-796cc20e169b.png\" width=\"400\" height=\"730\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
1	-4	### What changes were proposed in this pull request?The main changes of this pr as follows:- Refactor `CompatibilitySuite` as a new tool `CheckConnectJvmClientCompatibility` and move it into `tools` module- Add a new shell script `dev/connect-jvm-client-mima-check`, it will use `CheckConnectJvmClientCompatibility ` to check the mima compatibility of `connect-jvm-client` module.- Add `dev/connect-jvm-client-mima-check` to github task### Why are the changes needed?For fix test error report in `[VOTE] Release Apache Spark 3.4.0 (RC1)` mail list.Testing `CompatibilitySuite` with maven requires some pre-work: ```build/mvn clean install -DskipTests -pl sql/core -ambuild/mvn clean install -DskipTests -pl connector/connect/client/jvm -am ```So if we run `build/mvn package test` to test whole project as before, `CompatibilitySuite` will failed as follows:```CompatibilitySuite:- compatibility MiMa tests *** FAILED ***  java.lang.AssertionError: assertion failed: Failed to find the jar inside folder: /home/bjorn/spark-3.4.0/connector/connect/client/jvm/target  at scala.Predef$.assert(Predef.scala:223)  at org.apache.spark.sql.connect.client.util.IntegrationTestUtils$.findJar(IntegrationTestUtils.scala:67)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar$lzycompute(CompatibilitySuite.scala:57)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar(CompatibilitySuite.scala:53)  at org.apache.spark.sql.connect.client.CompatibilitySuite.$anonfun$new$1(CompatibilitySuite.scala:69)  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  at org.scalatest.Transformer.apply(Transformer.scala:22)  at org.scalatest.Transformer.apply(Transformer.scala:20)  ...- compatibility API tests: Dataset *** FAILED ***  java.lang.AssertionError: assertion failed: Failed to find the jar inside folder: /home/bjorn/spark-3.4.0/connector/connect/client/jvm/target  at scala.Predef$.assert(Predef.scala:223)  at org.apache.spark.sql.connect.client.util.IntegrationTestUtils$.findJar(IntegrationTestUtils.scala:67)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar$lzycompute(CompatibilitySuite.scala:57)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar(CompatibilitySuite.scala:53)  at org.apache.spark.sql.connect.client.CompatibilitySuite.$anonfun$new$7(CompatibilitySuite.scala:110)  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  at org.scalatest.Transformer.apply(Transformer.scala:22) ```So we need to fix this problem for developers.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->v2 catalog default namespace may be empty```javaException in thread \"main\" org.apache.spark.sql.AnalysisException: Multi-part identifier cannot be empty.\tat org.apache.spark.sql.errors.QueryCompilationErrors$.emptyMultipartIdentifierError(QueryCompilationErrors.scala:1887)\tat org.apache.spark.sql.connector.catalog.CatalogV2Implicits$MultipartIdentifierHelper.<init>(CatalogV2Implicits.scala:152)\tat org.apache.spark.sql.connector.catalog.CatalogV2Implicits$.MultipartIdentifierHelper(CatalogV2Implicits.scala:150)\tat org.apache.spark.sql.internal.CatalogImpl.currentDatabase(CatalogImpl.scala:65)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally verified
2	-1	### What changes were proposed in this pull request?Remove the averaging in the case the number of elements in the heap is even. Always return the number from the large heap which matches what we would do if we sorted an array and returned the N'th element where `N = (percentage * num_elements).toInt`.Both approaches are valid in terms of computing quantiles. For the purpose that PercentileHeap is used today both work just as well, except the version proposed in this PR is 30% faster (or put differently the version in master now is 50% slower than the version in this PR):```before58 ns per op on heaps of size 1000after40 ns per op on heaps of size 1000```Given that scheduler is performance sensitive on scheduling decisions it is best if we pick the fastest implementation.### Why are the changes needed?See above.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests and benchmark.
3	-4	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change adds a `reason: String` to the argument list of `TaskScheduler.cancelTasks`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Currently all tasks killed by `TaskScheduler.cancelTasks` will have a `TaskEndReason` \"TaskKilled (Stage cancelled)\". To better differentiate reasons for stage cancellations (e.g. user-initiated or caused by task failures in the stage), we could add a reason argument in `TaskScheduler.cancelTasks`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated unit tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->THis PR aims to ensure \"at least one time unit should be given for interval literal\" by modifying SqlBaseParser### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->INTERVAL is a Non-Reserved keyword in spark. But when I run```shellscala> spark.sql(\"select interval from mytable\")```I get```org.apache.spark.sql.catalyst.parser.ParseException:at least one time unit should be given for interval literal(line 1, pos 7)== SQL ==select interval from mytable-------^^^  at org.apache.spark.sql.errors.QueryParsingErrors$.invalidIntervalLiteralError(QueryParsingErrors.scala:196)......```It is a bug because \"Non-Reserved keywords\" have a special meaning in particular contexts and can be used as identifiers in other contexts. So by design, INTERVAL can be used as a column name.Currently the interval's grammar is```interval    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)?    ;```There is no need to make the time unit nullable, we can ensure \"at least one time unit should be given for interval literal\" if the interval's grammar is```interval    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)    ;```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->local test```shellscala> val myDF = spark.sparkContext.makeRDD(1 to 5).toDF(\"interval\")myDF: org.apache.spark.sql.DataFrame = [interval: int]scala> myDF.createOrReplaceTempView(\"mytable\")scala> spark.sql(\"select interval from mytable;\").show()+--------+|interval|+--------+|       1||       2||       3||       4||       5|+--------+```
1	-2	### What changes were proposed in this pull request?Set `spark.sql.legacy.createHiveTableByDefault ` to false.### Why are the changes needed?In the spark [documentation](https://github.com/apache/spark/blob/master/docs/sql-ref-syntax-ddl-create-table-datasource.md?plain=1#L121), it is stated that the default creation table is parquet, but you need to set \"spark.sql.legacy.createHiveTableByDefault\" to false, otherwise the default is textfile.### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?tests were added
1	-1	### What changes were proposed in this pull request?This PR adds TypedColumn to the Spark Connect Scala Client. We also add one of the typed select methods for Dataset, and typed count function.### Why are the changes needed?API Parity.### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Added tests to PlanGenerationTestSuite and ClientE2EtestSuite.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR is a follow-up for #39910. It updates the error message of the error class INVALID_TEMP_OBJ_REFERENCE.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error message more user-friendly.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. This PR updates the error message for INVALID_TEMP_OBJ_REFERENCE.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
1	-2	### What changes were proposed in this pull request?The PR fixes a mistake in SPARK-41188 that removed the PythonRunner code setting OMP_NUM_THREADS to number of executor cores by default. That author and reviewers thought it's a duplicate.### Why are the changes needed?SPARK-41188 stopped setting OMP_NUM_THREADS to number of executor cores by default when running Python UDF on YARN.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual testing
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `Pivot` API when pivot column values are not provided. The decision here is that we push everything into server thus does not do max value validation for the pivot column on the client sides (both Scala and Python) now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
1	-1	### What changes were proposed in this pull request?Enables more `DataFrame.mapInPandas` parity tests.### Why are the changes needed?Now that we have `SparkSession.conf`, we can enable some more parity tests for `DataFrame.mapInPandas`### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes `TableOutputResolver` use full names for inner fields in resolution errors.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to avoid confusion when there are multiple inner fields with the same name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
1	-1	### What changes were proposed in this pull request?Enables more parity tests related to `functions`.### Why are the changes needed?There are still some more tests we should enable.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Modified/enabled related tests.
3	-3	### What changes were proposed in this pull request?Introduce a new physical type `Decimal128` for `DecimalType`.### Why are the changes needed?Spark SQL today supports the `Decimal` data type. The implementation of Spark `Decimal` holds a `BigDecimal` or `Long` value. Spark `Decimal` provides some operators like `+`, `-`, `*`, `/`, `%` and so on. These operators rely heavily on the computational power of `BigDecimal` or `Long` itself. For ease of understanding, take the `+` as an example. The implementation shows below.```  def + (that: Decimal): Decimal = {    if (decimalVal.eq(null) && that.decimalVal.eq(null) && scale == that.scale) {      Decimal(longVal + that.longVal, Math.max(precision, that.precision) + 1, scale)    } else {      Decimal(toBigDecimal.bigDecimal.add(that.toBigDecimal.bigDecimal))    }  }```We can see add the two `Long` value if Spark `Decimal` holds a `Long` value. Otherwise,  add the two `BigDecimal` if Spark `Decimal` holds a `BigDecimal` value. The other operators of Spark `Decimal` adopt the similar way. Furthermore, the code shown above calls `Decimal.apply` to construct a new instance of Spark `Decimal`.  As we know, the add operator of `BigDecimal` constructed a new instance of `BigDecimal`. In short, Spark `Decimal` is an immutable physical type. Through rough analysis, we know the calculation operators of Spark `Decimal` create a lot of new instances of `Decimal` and may create a lot of new instances of `BigDecimal`.If a large table has a field called 'colA whose type is Spark `Decimal`, the execution of SUM('colA) will involve the creation of a large number of Spark `Decimal` instances and `BigDecimal` instances. These Spark `Decimal` instances and `BigDecimal` instances will lead to garbage collection frequently.`Int128` is a high-performance data type about 1X~6X more efficient than `BigDecimal` for typical operations. It uses a finite (128 bit) precision and can handle up to decimal(38, X). The implementation of `Int128` just uses two `Long` values to represent the high and low bits of 128 bits respectively. `Int128` is lighter than `BigDecimal`, reduces the cost of new() and garbage collection.Spark could support a new physical type to represent the `DecimalType` with `Int128`, it would improve the performance for calculation operators.Now, let's call the new physical type `Decimal128`.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.Manual test benchmark```Compare decimal addition:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Decimal: s1 + s2                                      6              7           2         17.7          56.4       1.0XDecimal128: s1 + s2                                   2              2           0         43.1          23.2       2.4XDecimal: s1 + s2 + s3 + s4                           14             15           1          7.1         140.6       0.4XDecimal128: s1 + s2 + s3 + s4                         6              8           1         15.6          64.2       0.9XDecimal: l1 + l2                                     13             14           1          7.6         130.7       0.4XDecimal128: l1 + l2                                   4              5           0         23.2          43.2       1.3XDecimal: l1 + l2 + l3 + l4                           34             35           1          2.9         342.3       0.2XDecimal128: l1 + l2 + l3 + l4                        14             15           0          7.2         138.7       0.4XDecimal: s2 + l3 + l1 + s4                           40             41           1          2.5         395.7       0.1XDecimal128: s2 + l3 + l1 + s4                        25             26           1          3.9         253.7       0.2XDecimal: lz1 + lz2                                   13             14           1          7.7         129.5       0.4XDecimal128: lz1 + lz2                                 4              4           0         26.6          37.6       1.5XDecimal: lz1 + lz2 + lz3 + lz4                       37             38           1          2.7         368.4       0.2XDecimal128: lz1 + lz2 + lz3 + lz4                    11             12           0          8.8         113.5       0.5XDecimal: s2 + lz3 + lz1 + s4                         38             39           1          2.6         381.3       0.1XDecimal128: s2 + lz3 + lz1 + s4                      19             20           1          5.2         194.2       0.3X``````Compare decimal multiply:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Decimal: s1 * s2                                      5              5           0         19.5          51.3       1.0XDecimal128: s1 * s2                                   2              2           0         44.1          22.7       2.3XDecimal: s1 * s2 * s5 * s6                           13             15           1          7.5         132.6       0.4XDecimal128: s1 * s2 * s5 * s6                         5              5           0         20.5          48.7       1.1XDecimal: s3 * s4                                      9              9           1         11.4          87.9       0.6XDecimal128: s3 * s4                                   3              3           0         32.5          30.7       1.7XDecimal: l2 * s2                                      9             10           1         11.2          89.2       0.6XDecimal128: l2 * s2                                   3              3           0         30.2          33.1       1.6XDecimal: l2 * s2 * s5 * s6                           23             25           3          4.3         234.9       0.2XDecimal128: l2 * s2 * s5 * s6                        12             12           0          8.6         115.8       0.4XDecimal: s1 * l2                                      9             10           0         10.6          94.1       0.5XDecimal128: s1 * l2                                   3              3           0         31.8          31.5       1.6XDecimal: l1 * l2                                     10             10           0         10.2          97.7       0.5XDecimal128: l1 * l2                                   3              3           0         31.9          31.4       1.6X``````Compare decimal division:                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Decimal: s1 / s2                                     18             18           1          5.7         175.7       1.0XDecimal128: s1 / s2                                   5              5           0         21.1          47.5       3.7XDecimal: s1 / s2 / s2 / s2                           47             49           1          2.1         473.9       0.4XDecimal128: s1 / s2 / s2 / s2                         9              9           0         11.1          90.3       1.9XDecimal: s1 / s3                                     27             28           1          3.7         270.2       0.7XDecimal128: s1 / s3                                   4              4           0         25.0          40.1       4.4XDecimal: s2 / l1                                     25             25           1          4.1         245.8       0.7XDecimal128: s2 / l1                                   4              4           0         23.7          42.3       4.2XDecimal: l1 / s2                                     20             21           1          5.1         197.5       0.9XDecimal128: l1 / s2                                   4              4           0         24.0          41.7       4.2XDecimal: s3 / l1                                     28             29           1          3.5         282.1       0.6XDecimal128: s3 / l1                                   4              4           0         24.1          41.4       4.2XDecimal: l2 / l3                                     34             35           1          2.9         340.1       0.5XDecimal128: l2 / l3                                  21             22           1          4.7         213.9       0.8XDecimal: l2 / l4 / l4 / l4                           68             71           2          1.5         681.1       0.3XDecimal128: l2 / l4 / l4 / l4                        23             24           1          4.3         230.7       0.8XDecimal: l2 / s4 / s4 / s4                           58             60           1          1.7         576.0       0.3XDecimal128: l2 / s4 / s4 / s4                        40             42           1          2.5         399.2       0.4X``````Compare decimal modulo:                   Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Decimal: s1 % s2                                     36             37           1          2.8         356.6       1.0XDecimal128: s1 % s2                                  18             19           1          5.6         178.6       2.0XDecimal: s1 % s2 % s2 % s2                           50             52           1          2.0         501.1       0.7XDecimal128: s1 % s2 % s2 % s2                        50             51           1          2.0         496.4       0.7XDecimal: s2 % l2                                      8              9           1         12.9          77.6       4.6XDecimal128: s2 % l2                                  17             17           1          6.1         165.2       2.2XDecimal: l3 % s3                                     50             52           1          2.0         503.1       0.7XDecimal128: l3 % s3                                  16             16           1          6.4         155.4       2.3XDecimal: s4 % l3                                      8              9           2         12.8          77.9       4.6XDecimal128: s4 % l3                                  13             14           0          7.4         134.8       2.6XDecimal: l2 % l3                                     14             15           0          6.9         144.2       2.5XDecimal128: l2 % l3                                  16             17           1          6.2         162.5       2.2XDecimal: l2 % l3 % l4 % l1                           85             88           2          1.2         854.1       0.4XDecimal128: l2 % l3 % l4 % l1                        51             54           2          1.9         513.4       0.7X```In some cases, division will have performance regression
1	-1	### What changes were proposed in this pull request?Add implicit encoder resolution to `SQLImplicits` class.### Why are the changes needed?API parity.### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Added test to `SQLImplicitsTestSuite`.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds  char/varchar length checks for inner fields during resolution when struct fields are reordered.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These checks are needed to handle nested char/varchar columns correctly. Prior to this change, we would lose the raw type information when constructing nested attributes. As a result, we will not insert proper char/varchar length checks.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests that would previously fail.
2	-1	### What changes were proposed in this pull request?Tiny PR to make most of the scala client classes have a private[sql] constructor.### Why are the changes needed?Consistency, safety for the future.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?No
1	-2	### What changes were proposed in this pull request?This PR addressed missed commit on reflecting review comment (https://github.com/apache/spark/pull/40188#discussion_r1119055652) from previous PR #40188.### Why are the changes needed?Previously pushed commit didn't reflect all review comments.### Does this PR introduce _any_ user-facing change?Yes, documentation.### How was this patch tested?N/A.
2	-3	### What changes were proposed in this pull request?This PR proposes to disable ANSI for one more conv test cases in `MathFunctionsSuite`. They are intentionally testing the behaviours when ANSI is disabled. This is another followup of https://github.com/apache/spark/pull/40117.### Why are the changes needed?To make the ANSI tests pass. It currently fails (https://github.com/apache/spark/actions/runs/4277973597/jobs/7447263656):```2023-02-28T02:34:53.0298317Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-36229 inconsistently behaviour where returned value is above the 64 char threshold (105 milliseconds)\u001B[0m\u001B[0m2023-02-28T02:34:53.0631723Z 02:34:53.062 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 146.0 (TID 268)2023-02-28T02:34:53.0632672Z org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.2023-02-28T02:34:53.0633557Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.arithmeticOverflowError(QueryExecutionErrors.scala:643)2023-02-28T02:34:53.0634361Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.overflowInConvError(QueryExecutionErrors.scala:315)2023-02-28T02:34:53.0635124Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.encode(NumberConverter.scala:68)2023-02-28T02:34:53.0711747Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:158)2023-02-28T02:34:53.0712298Z \tat org.apache.spark.sql.catalyst.util.NumberConverter.convert(NumberConverter.scala)2023-02-28T02:34:53.0712925Z \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:38)2023-02-28T02:34:53.0713547Z \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)2023-02-28T02:34:53.0714098Z \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)2023-02-28T02:34:53.0714552Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)2023-02-28T02:34:53.0715094Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)2023-02-28T02:34:53.0715466Z \tat org.apache.spark.util.Iterators$.size(Iterators.scala:29)2023-02-28T02:34:53.0715829Z \tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1944)2023-02-28T02:34:53.0716195Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1266)2023-02-28T02:34:53.0716555Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1266)2023-02-28T02:34:53.0716963Z \tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)2023-02-28T02:34:53.0717400Z \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)2023-02-28T02:34:53.0717857Z \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)2023-02-28T02:34:53.0718332Z \tat org.apache.spark.scheduler.Task.run(Task.scala:139)2023-02-28T02:34:53.0718743Z \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)2023-02-28T02:34:53.0719152Z \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)2023-02-28T02:34:53.0719548Z \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)2023-02-28T02:34:53.0720001Z \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)2023-02-28T02:34:53.0720481Z \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)2023-02-28T02:34:53.0720848Z \tat java.lang.Thread.run(Thread.java:750)2023-02-28T02:34:53.0721492Z 02:34:53.065 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 146.0 (TID 268) (localhost executor driver): org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.2023-02-28T02:34:53.0722264Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.arithmeticOverflowError(QueryExecutionErrors.scala:643)2023-02-28T02:34:53.0722821Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.overflowInConvError(QueryExecutionErrors.scala:315)2023-02-28T02:34:53.0723337Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.encode(NumberConverter.scala:68)2023-02-28T02:34:53.0723963Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:158)2023-02-28T02:34:53.0724474Z \tat org.apache.spark.sql.catalyst.util.NumberConverter.convert(NumberConverter.scala)2023-02-28T02:34:53.0725128Z \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:38)2023-02-28T02:34:53.0725826Z \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)2023-02-28T02:34:53.0726376Z \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)2023-02-28T02:34:53.0726827Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)2023-02-28T02:34:53.0727189Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)2023-02-28T02:34:53.0727556Z \tat org.apache.spark.util.Iterators$.size(Iterators.scala:29)2023-02-28T02:34:53.0727931Z \tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1944)2023-02-28T02:34:53.0728346Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1266)2023-02-28T02:34:53.0728701Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1266)2023-02-28T02:34:53.0729096Z \tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)2023-02-28T02:34:53.0729523Z \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)2023-02-28T02:34:53.0729966Z \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)```### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?Fixed unittests.
1	-2	### What changes were proposed in this pull request?Refactor the AnalyzePlan RPC and add `session.version`### Why are the changes needed?Existing implementation always return schema, explain string, input files, etc together, but in most cases we only need the  schema, so we should separate them to avoid unnecessary analysis, optimization and IO (required in `input files`).### Does this PR introduce _any_ user-facing change?yes, new session API```>>> spark.version'3.5.0-SNAPSHOT'>>> ```### How was this patch tested?updated tests and added tests
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->the hive sessionState initiated in SparkSQLCLIDriver will be started later in HiveClient during communicating with HMS if necessary. There are some cases that it will not get started:- fail early before reaching HiveClient- HiveClient is not used, e.g., v2 catalog only- ...### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bugfix, an app will end up with unexpected states, e.g.,```javabin/spark-sql -c spark.sql.catalogImplementation=in-memory -e \"select 1\"23/02/28 13:40:22 WARN Utils: Your hostname, hulk.local resolves to a loopback address: 127.0.0.1; using 10.221.102.180 instead (on interface en0)23/02/28 13:40:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another addressSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/02/28 13:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark master: local[*], Application Id: local-16775628240271Time taken: 2.578 seconds, Fetched 1 row(s)23/02/28 13:40:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist23/02/28 13:40:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist23/02/28 13:40:29 WARN Hive: Failed to register all functions.java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3901)\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:395)\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:339)\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:319)\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:288)\tat org.apache.hadoop.hive.ql.session.SessionState.unCacheDataNucleusClassLoaders(SessionState.java:1596)\tat org.apache.hadoop.hive.ql.session.SessionState.close(SessionState.java:1586)\tat org.apache.hadoop.hive.cli.CliSessionState.close(CliSessionState.java:66)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.$anonfun$main$2(SparkSQLCLIDriver.scala:153)\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat scala.util.Try$.apply(Try.scala:213)\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\tat java.lang.Thread.run(Thread.java:750)Caused by: java.lang.reflect.InvocationTargetException\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\t... 31 moreCaused by: MetaException(message:Version information not found in metastore. )\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:83)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6902)\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:162)\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\t... 36 moreCaused by: MetaException(message:Version information not found in metastore. )\tat org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:7810)\tat org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:7788)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)\tat com.sun.proxy.$Proxy37.verifySchema(Unknown Source)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:595)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:588)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)\t... 40 more```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally verified
1	-2	### What changes were proposed in this pull request?Set OMP_NUM_THREADS to `spark.task.cpus` instead of `spark.executor.cores` by default.### Why are the changes needed?If OMP_NUM_THREADS is set to executor cores, we might still have issues when the number of executer cores is large but the number of task cpus is 1.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Run the following PySpark UDF script with these 2 Spark properties:- spark.executor.cores=3- spark.task.cpus=1```pythonimport osfrom pyspark.sql import SparkSessionfrom pyspark.sql.functions import udfspark = SparkSession.builder.getOrCreate()var_name = 'OMP_NUM_THREADS'def get_env_var():  return os.getenv(var_name)udf_get_env_var = udf(get_env_var)spark.range(1).toDF(\"id\").withColumn(f\"env_{var_name}\ udf_get_env_var()).show(truncate=False)```Output with release `3.3.0`:```+---+-------------------+|id |env_OMP_NUM_THREADS|+---+-------------------+|0  |3                  |+---+-------------------+```After the fix:```+---+-------------------+|id |env_OMP_NUM_THREADS|+---+-------------------+|0  |1                  |+---+-------------------+```
1	-4	### What changes were proposed in this pull request?The main changes of this pr as follows:- Refactor `CompatibilitySuite` as a new tool `CheckConnectJvmClientCompatibility`- Add a new shell script `dev/connect-jvm-client-mima-check`, it will use `CheckConnectJvmClientCompatibility` to check the mima compatibility of `connect-jvm-client` module.- Add `dev/connect-jvm-client-mima-check` to github task### Why are the changes needed?For fix test error report in `[VOTE] Release Apache Spark 3.4.0 (RC1)` mail list.Testing `CompatibilitySuite` with maven requires some pre-work: ```build/mvn clean install -DskipTests -pl sql/core -ambuild/mvn clean install -DskipTests -pl connector/connect/client/jvm -am ```So if we run `build/mvn package test` to test whole project as before, `CompatibilitySuite` will failed as follows:```CompatibilitySuite:- compatibility MiMa tests *** FAILED ***  java.lang.AssertionError: assertion failed: Failed to find the jar inside folder: /home/bjorn/spark-3.4.0/connector/connect/client/jvm/target  at scala.Predef$.assert(Predef.scala:223)  at org.apache.spark.sql.connect.client.util.IntegrationTestUtils$.findJar(IntegrationTestUtils.scala:67)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar$lzycompute(CompatibilitySuite.scala:57)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar(CompatibilitySuite.scala:53)  at org.apache.spark.sql.connect.client.CompatibilitySuite.$anonfun$new$1(CompatibilitySuite.scala:69)  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  at org.scalatest.Transformer.apply(Transformer.scala:22)  at org.scalatest.Transformer.apply(Transformer.scala:20)  ...- compatibility API tests: Dataset *** FAILED ***  java.lang.AssertionError: assertion failed: Failed to find the jar inside folder: /home/bjorn/spark-3.4.0/connector/connect/client/jvm/target  at scala.Predef$.assert(Predef.scala:223)  at org.apache.spark.sql.connect.client.util.IntegrationTestUtils$.findJar(IntegrationTestUtils.scala:67)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar$lzycompute(CompatibilitySuite.scala:57)  at org.apache.spark.sql.connect.client.CompatibilitySuite.clientJar(CompatibilitySuite.scala:53)  at org.apache.spark.sql.connect.client.CompatibilitySuite.$anonfun$new$7(CompatibilitySuite.scala:110)  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  at org.scalatest.Transformer.apply(Transformer.scala:22) ```So we need to fix this problem for developers.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-1	### What changes were proposed in this pull request?This pr aims upgrade jetty from 9.4.50.v20221201 to 9.4.51.v20230217.### Why are the changes needed?The main change of this version includes:- https://github.com/eclipse/jetty.project/pull/9352- https://github.com/eclipse/jetty.project/pull/9345The release notes as follows:- https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.51.v20230217### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass Github Actions
2	-1	### What changes were proposed in this pull request?This PR proposes to add examples of unblocked workloads after SPARK-42376, which unblocks stream-stream time interval join followed by stateful operator.### Why are the changes needed?We'd like to remove the description of limitations which no longer exist, as well as provide some code examples so that users can get some sense how to use the functionality.### Does this PR introduce _any_ user-facing change?Yes, documentation change.### How was this patch tested?Created a page via SKIP_API=1 bundle exec jekyll serve --watch and confirmed.Screenshots:> Scala![스크린샷 2023-03-07 오후 9 39 36](https://user-images.githubusercontent.com/1317309/223424683-e7f7e721-a0fa-4e3c-a8f0-139d060dd045.png)> Java![스크린샷 2023-03-07 오후 9 39 28](https://user-images.githubusercontent.com/1317309/223424706-b4da49c1-f088-4513-85d6-8750b89dac56.png)> Python![스크린샷 2023-03-07 오후 9 37 03](https://user-images.githubusercontent.com/1317309/223424412-c12500cc-946f-4e09-8b0c-6ceed5b3aeee.png)
1	-1	### What changes were proposed in this pull request?This PR proposes to mark the APIs as deprecated or remove the APIs that will be deprecated or removed in upcoming pandas 2.0.0 release.See [What's new in 2.0.0](https://pandas.pydata.org/pandas-docs/version/2.0/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes) for more detail.### Why are the changes needed?We should match the behavior to pandas API.### Does this PR introduce _any_ user-facing change?Yes, some APIs will be removed, so they will be no longer available.### How was this patch tested?Fixed UTs when necessary case.
1	-1	### What changes were proposed in this pull request?The pr aims to implement DataFrameNaFunctions.### Why are the changes needed?API coverage.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add new UT.
2	-1	### What changes were proposed in this pull request?This is the first part of SPARK-42579, the pr is aims to support `Array[_]` data type for `function.lit`.### Why are the changes needed?Make `function.lit` support nested dataType### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test- Manually checked Scala 2.13 test```build/sbt \"connect-client-jvm/test\" -Phive -Pscala-2.13build/sbt \"connect/test\" -Phive -Pscala-2.13```
2	-4	https://issues.apache.org/jira/browse/SPARK-42622### What changes were proposed in this pull request?Disable substitution in values for the `StringSubstitutor` used to resolve error messages### Why are the changes needed?when constructing an error message `ErrorClasssesJSONReader`1. reads the error message from core/src/main/resources/error/error-classes.json 2. replaces `<fieldValue>` with `${fieldValue}` in the error message it read3. uses `StringSubstitutor` to replace `${fieldValue}` with the actual valueIf `fieldValue` is defined as `\"${foo}\"` then it will also try and resolve foo. When foo is undefined it will throw an IllegalArgumentExceptionThis is very problematic instance in this scenario. Where parsing json will fail if it does not adhere to the correct schema![image](https://user-images.githubusercontent.com/133639/221866500-99f187a0-8db3-42a7-85ca-b027fdec160d.png)Here the error message produced will be something like `Cannot parse the field name properties and the value ${foo} of the JSON token type string to target Spark data type struct.`And because foo is undefined another error will be triggered, and another, and another.. until you have a stackoverflow.It could be employed as a denial of service attack on data pipelines### Does this PR introduce _any_ user-facing change?No### How was this patch tested?LocallyBefore![image](https://user-images.githubusercontent.com/133639/221988445-9e751898-1038-40c0-96c6-68881d326a36.png)After![image](https://user-images.githubusercontent.com/133639/221988511-3ae586f2-4c96-44b4-a798-88573350a4ed.png)
2	-2	### Problem descriptionNumpy has started changing the alias to some of its data-types. This means that users with the latest version of numpy they will face either warnings or errors according to the type that they are using. This affects all the users using numoy > 1.20.0One of the types was fixed back in September with this [pull](https://github.com/apache/spark/pull/37817) request[numpy 1.24.0](https://github.com/numpy/numpy/pull/22607): The scalar type aliases ending in a 0 bit size: np.object0, np.str0, np.bytes0, np.void0, np.int0, np.uint0 as well as np.bool8 are now deprecated and will eventually be removed.[numpy 1.20.0](https://github.com/numpy/numpy/pull/14882): Using the aliases of builtin types like np.int is deprecated### What changes were proposed in this pull request?From numpy 1.20.0 we receive a deprecattion warning on np.object(https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations) and from numpy 1.24.0 we received an attribute error:```attr = 'object'    def __getattr__(attr):        # Warn for expired attributes, and return a dummy function        # that always raises an exception.        import warnings        try:            msg = __expired_functions__[attr]        except KeyError:            pass        else:            warnings.warn(msg, DeprecationWarning, stacklevel=2)                def _expired(*args, **kwds):                raise RuntimeError(msg)                return _expired            # Emit warnings for deprecated attributes        try:            val, msg = __deprecated_attrs__[attr]        except KeyError:            pass        else:            warnings.warn(msg, DeprecationWarning, stacklevel=2)            return val            if attr in __future_scalars__:            # And future warnings for those that will change, but also give            # the AttributeError            warnings.warn(                f\"In the future `np.{attr}` will be defined as the \"                \"corresponding NumPy scalar.\ FutureWarning, stacklevel=2)            if attr in __former_attrs__:>           raise AttributeError(__former_attrs__[attr])E           AttributeError: module 'numpy' has no attribute 'object'.E           `np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. E           The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:E               https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations```From numpy version 1.24.0 we receive a deprecation warning on np.object0 and every np.datatype0 and np.bool8>>> np.object0(123)<stdin>:1: DeprecationWarning: `np.object0` is a deprecated alias for ``np.object0` is a deprecated alias for `np.object_`. `object` can be used instead.  (Deprecated NumPy 1.24)`.  (Deprecated NumPy 1.24)### Why are the changes needed?The changes are needed so pyspark can be compatible with the latest numpy and avoid - attribute errors on data types being deprecated from version 1.20.0: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations- warnings on deprecated data types from version 1.24.0: https://numpy.org/devdocs/release/1.24.0-notes.html#deprecations### Does this PR introduce _any_ user-facing change?The change will suppress the warning coming from numpy 1.24.0 and the error coming from numpy 1.22.0### How was this patch tested?I assume that the existing tests should catch this. (see all section Extra questions)I found this to be a problem in my work's project where we use for our unit tests the toPandas() function to convert to np.object. Attaching the run result of our test:```_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _/usr/local/lib/python3.9/dist-packages/<my-pkg>/unit/spark_test.py:64: in run_testcase    self.handler.compare_df(result, expected, config=self.compare_config)/usr/local/lib/python3.9/dist-packages/<my-pkg>/spark_test_handler.py:38: in compare_df    actual_pd = actual.toPandas().sort_values(by=sort_columns, ignore_index=True)/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py:232: in toPandas    corrected_dtypes[index] = np.object  # type: ignore[attr-defined]_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _attr = 'object'    def __getattr__(attr):        # Warn for expired attributes, and return a dummy function        # that always raises an exception.        import warnings        try:            msg = __expired_functions__[attr]        except KeyError:            pass        else:            warnings.warn(msg, DeprecationWarning, stacklevel=2)                def _expired(*args, **kwds):                raise RuntimeError(msg)                return _expired            # Emit warnings for deprecated attributes        try:            val, msg = __deprecated_attrs__[attr]        except KeyError:            pass        else:            warnings.warn(msg, DeprecationWarning, stacklevel=2)            return val            if attr in __future_scalars__:            # And future warnings for those that will change, but also give            # the AttributeError            warnings.warn(                f\"In the future `np.{attr}` will be defined as the \"                \"corresponding NumPy scalar.\ FutureWarning, stacklevel=2)            if attr in __former_attrs__:>           raise AttributeError(__former_attrs__[attr])E           AttributeError: module 'numpy' has no attribute 'object'.E           `np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. E           The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:E               https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations/usr/local/lib/python3.9/dist-packages/numpy/__init__.py:305: AttributeError```Although i cannot provide the code doing in python the following should show the problem:```>>> import numpy as np>>> np.object0(123)<stdin>:1: DeprecationWarning: `np.object0` is a deprecated alias for ``np.object0` is a deprecated alias for `np.object_`. `object` can be used instead.  (Deprecated NumPy 1.24)`.  (Deprecated NumPy 1.24)123>>> np.object(123)<stdin>:1: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.Traceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/usr/local/lib/python3.9/dist-packages/numpy/__init__.py\ line 305, in __getattr__    raise AttributeError(__former_attrs__[attr])AttributeError: module 'numpy' has no attribute 'object'.`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations```I do not have a use-case in my tests for np.object0 but I fixed like the suggestion from numpy### Supported Versions:I propose this fix to be included in all pyspark 3.3 and onwards### JIRAI know a JIRA ticket should be created I sent an email and I am waiting for the answer to document the case also there.### Extra questions:By grepping for np.bool and np.object I see that the tests include them. Shall we change them also? Data types with _ I think they are not affected.```git grep np.objectpython/pyspark/ml/functions.py:        return data.dtype == np.object_ and isinstance(data.iloc[0], (np.ndarray, list))python/pyspark/ml/functions.py:        return any(data.dtypes == np.object_) and any(python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[1], np.object)python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[4], np.object)  # datetime.datepython/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[1], np.object)python/pyspark/sql/tests/test_dataframe.py:                self.assertEqual(types[6], np.object)python/pyspark/sql/tests/test_dataframe.py:                self.assertEqual(types[7], np.object)git grep np.boolpython/docs/source/user_guide/pandas_on_spark/types.rst:np.bool       BooleanTypepython/pyspark/pandas/indexing.py:            isinstance(key, np.bool_) for key in cols_selpython/pyspark/pandas/tests/test_typedef.py:            np.bool: (np.bool, BooleanType()),python/pyspark/pandas/tests/test_typedef.py:            bool: (np.bool, BooleanType()),python/pyspark/pandas/typedef/typehints.py:    elif tpe in (bool, np.bool_, \"bool\ \"?\"):python/pyspark/sql/connect/expressions.py:                assert isinstance(value, (bool, np.bool_))python/pyspark/sql/connect/expressions.py:                elif isinstance(value, np.bool_):python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[2], np.bool)python/pyspark/sql/tests/test_functions.py:            (np.bool_, [(\"true\ \"boolean\")]),```If yes concerning bool was merged already should we fix it too?
2	-3	### What changes were proposed in this pull request?Follow on to SPARK-40034, Dynamic/absolute path support in PathOutputCommittersDynamic partitioning though the PathOutputCommitProtocol needs to add the target directories to the superclass's partition list else the partition delete doesn'ttake place -the job extends the dataset, rather than replaces it.Fix:* add an `addPartition()` method subclasses can use* add a `getPartitions` method to return an immutable  copy of the list for testing.* add the test.* Also fix `newTaskTempFileAbsPath(`) to return a path, irrespective of committer type.In dynamic mode, because the parent dir of an absolute path is deleted, there's a safety check to reject any requests for a file in a parent dir. Thisis something which could be pulled up to HadoopMapReduceCommitProtocol -it needs the same check, if the risk is considered realistic.The patch now downgrades from failing on dynamic partitioning if the committer doesn't declare it supports it to printing a warning. Why this? well, it*does* work through the s3a committers, it's just `O(data)`. If someone does want to do `INSERT OVERWRITE` then they can be allowed to, just warned aboutit. The outcome will be correct except in the case of: \"if the driver fails partway through dir rename, only some of the files will be there\"Finally, it update the protocol spec in `HadoopMapReduceCommitProtocol` to cover the dynamic partition job commit in more detail.### Why are the changes needed?* The code in SPARK-40034 was incomplete. The new tests show that it works now.* dynamic partition jobs aren't actually incorrect through the s3 committers, just slow, so if someone really wants it they should be free to.* The `newFileAbsPath()` code is required of all committers, despite its near-total lack of use.### Does this PR introduce _any_ user-facing change?Updates the cloud docs to say that dynamic partition overwrite does work everywhere, just may be really slow.### How was this patch tested?New unit tests in `CommitterBindingSuite`; `New test suite `PathOutputPartitionedWriteSuite extends PartitionedWriteSuite` which runs the `PartitionedWriteSuite` throughthe PathOutputCommitter and with, on hadoop 3.3.5+ the manifest committer chosen.executed with* hadoop 3.3.4* RC2 of hadoop 3.3.5 through `-Dhadoop.version=3.3.5 -Psnapshots-and-staging
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Following up on https://github.com/apache/spark/pull/40210, add correct `version` in the scala client side.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
1	-1	### What changes were proposed in this pull request?Reorganizes imports in `python/pyspark/sql/tests/test_functions.py`.### Why are the changes needed?Currently imports in the file `python/pyspark/sql/tests/test_functions.py` is not organized well.There are individual imports in test functions, or imports by function names or module names, etc.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Modified tests.
2	-3	### What changes were proposed in this pull request?When using the 'builtin' Hive version for the Hive metadata client, do not create a separate classloader, and rather continue to use the overall user/application classloader (regardless of Java version). This standardizes the behavior for all Java versions with that of Java 9+. See SPARK-42539 for more details on why this approach was chosen.Please note that this is a re-submit of #40144. That one introduced test failures, and potentially a real issue, because the PR works by setting `isolationOn = false` for `builtin` mode. In addition to adjusting the classloader, `HiveClientImpl` relies on `isolationOn` to determine if it should use an isolated copy of `SessionState`, so the PR inadvertently switched to using a shared `SessionState` object. I think we do want to continue to have the isolated session state even in `builtin` mode, so this adds a new flag `sessionStateIsolationOn` which controls whether the session state should be isolated, _separately_ from the `isolationOn` flag which controls whether the classloader should be isolated. Default behavior is for `sessionStateIsolationOn` to be set equal to `isolationOn`, but for `builtin` mode, we override it to enable session state isolated even though classloader isolation is turned off.### Why are the changes needed?Please see a much more detailed description in SPARK-42539. The tl;dr is that user-provided JARs (such as `hive-exec-2.3.8.jar`) take precedence over Spark/system JARs when constructing the classloader used by `IsolatedClientLoader` on Java 8 in 'builtin' mode, which can cause unexpected behavior and/or breakages. This violates the expectation that, unless user-first classloader mode is used, Spark JARs should be prioritized over user JARs. It also seems that this separate classloader was unnecessary from the start, since the intent of 'builtin' mode is to use the JARs already existing on the regular classloader (as alluded to [here](https://github.com/apache/spark/pull/24057#discussion_r265142878)). The isolated clientloader was originally added in #5876 in 2015. This bit in the PR description is the only mention of the behavior for \"builtin\":> attempt to discover the jars that were used to load Spark SQL and use those. This option is only valid when using the execution version of Hive.I can't follow the logic here; the user classloader clearly has all of the necessary Hive JARs, since that's where we're getting the JAR URLs from, so we could just use that directly instead of grabbing the URLs. When this was initially added, it only used the JARs from the user classloader, not any of its parents, which I suspect was the motivating factor (to try to avoid more Spark classes being duplicated inside of the isolated classloader, I guess). But that was changed a month later anyway in #6435 / #6459, so I think this may have basically been deadcode from the start. It has also caused at least one issue over the years, e.g. SPARK-21428, which disables the new-classloader behavior in the case of running inside of a CLI session.### Does this PR introduce _any_ user-facing change?No, except to protect Spark itself from potentially being broken by bad user JARs.### How was this patch tested?This includes a new unit test in `HiveUtilsSuite` which demonstrates the issue and shows that this approach resolves it. It has also been tested on a live cluster running Java 8 and Hive communication functionality continues to work as expected.Unit tests failing in #40144 have been locally tested (`HiveUtilsSuite`, `HiveSharedStateSuite`, `HiveCliSessionStateSuite`, `JsonHadoopFsRelationSuite`).
1	-2	### What changes were proposed in this pull request?This PR aims to upgrade `zstd-jni` to 1.5.4-2. I'll update this PR with the benchmark results soon.### Why are the changes needed?To bring the following bug fixes of `v1.5.4-2`,- https://github.com/luben/zstd-jni/releases/tag/v1.5.4-2  - https://github.com/luben/zstd-jni/commit/1317e44493c676b1164c8b0398616d43fa349b5b> This avoids having to lookup the field and fixes a bug where thewrong fieldId was passed causing a segfault ifZstdDecompressCtx#reset() was called before ZstdCompressCtx() was used.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass the CI.
2	-2	### What changes were proposed in this pull request?Fixes `createDataFrame` to support durations.### Why are the changes needed?Currently the following command:```pyspark.createDataFrame(pd.DataFrame({\"a\": [timedelta(microseconds=123)]})) ```raises an error:```[UNSUPPORTED_ARROWTYPE] Unsupported arrow type Duration(NANOSECOND).```because Arrow takes a different type for `timedelta` objects from what Spark expects.### Does this PR introduce _any_ user-facing change?`timedelta` objects will be properly converted to `DayTimeIntervalType`.### How was this patch tested?Enabled the related test.
1	-2	### What changes were proposed in this pull request?Fixes `createDataFrame` to handle duplicated column names.### Why are the changes needed?Currently the following command returns a wrong result:```py>>> spark.createDataFrame([(1, 2)], [\"c\ \"c\"]).show()+---+---+|  c|  c|+---+---+|  2|  2|+---+---+```### Does this PR introduce _any_ user-facing change?Duplicated column names will work:```py>>> spark.createDataFrame([(1, 2)], [\"c\ \"c\"]).show()+---+---+|  c|  c|+---+---+|  1|  2|+---+---+```### How was this patch tested?Enabled the related test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support SameSemantics in Spark Connect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->SameSemantics API calls from users returns result now than throwing an exception.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-2	### What changes were proposed in this pull request?Add NULLs for INSERTs with user-specified lists of fewer columns than the target table.This is done by updating the semantics of the `USE_NULLS_FOR_MISSING_DEFAULT_COLUMN_VALUES` SQLConf to only apply for INSERTs with explicit user-specific column lists, and changing it to true by default.### Why are the changes needed?This behavior is consistent with other query engines.### Does this PR introduce _any_ user-facing change?Yes, per above.### How was this patch tested?Unit test coverage in `InsertSuite`.
1	-2	### What changes were proposed in this pull request?This pr just delete a invalid `TODO(SPARK-36168)` from `dev/test-dependencies.sh` due to SPARK-36168 has been identified as `Not A Problem`.### Why are the changes needed?Delete a invalid TODO.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This PR aims to add a migration note for bloom filter join.### Why are the changes needed?SPARK-38841 enabled bloom filter joins by default in Apache Spark 3.4.0.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual review.
3	-2	### What changes were proposed in this pull request?Update the description of default data source in the document.### Why are the changes needed?Documentation needs to be corrected as discussed in [PR-40196](https://github.com/apache/spark/pull/40196).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?Make `parse_data_type` use new proto message `DDLParse`### Why are the changes needed?when the ddl_string represents an atomic type, existing method based on `DataFrame.schema` (which is always a StructType) has an extra conversion: AtomicType -> StructType -> AtomicTypeThis PR leverages the dedicated proto message### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing UT
1	-2	### What changes were proposed in this pull request?This PR adds public interfaces for creating `Dataset` and `Column` instances, and for executing commands. These interfaces only allow creating `Plan`s and `Expression`s that contain an `extension` to limit what we need to expose.### Why are the changes needed?Required to implement extensions to the Scala Spark Connect client.### Does this PR introduce _any_ user-facing change?Yes, adds new public interfaces (see above).### How was this patch tested?Added unit tests.
2	-2	### What changes were proposed in this pull request?We use the current scala version to figure out which jar to load.### Why are the changes needed?The jar resolution in the connect client tests can resolve the jar for the wrong scala version if you are working with multiple scala versions.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?It is a test.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to add tests for the error class INTERNAL_ERROR to QueryExecutionErrorsSuite### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The changes improve test coverage, and document expected error messages in tests### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->By running the tests
1	-2	### What changes were proposed in this pull request?This PR fixed the counter-intuitive behaviors of the `TimestampAdd` expression mentioned in https://issues.apache.org/jira/browse/SPARK-42635. See the following *user-facing* changes for details.### Does this PR introduce _any_ user-facing change?Yes. This PR fixes the three problems mentioned in SPARK-42635:1. When the time is close to daylight saving time transition, the result may be discontinuous and not monotonic.2. Adding month, quarter, and year silently ignores `Int` overflow during unit conversion.3. Adding sub-month units (week, day, hour, minute, second, millisecond, microsecond)silently ignores `Long` overflow during unit conversion.Some examples of the result changes:Old results:```// In America/Los_Angeles timezone:timestampadd(DAY, 1, 2011-03-12 03:00:00) = 2011-03-13 03:00:00 (this is correct, put it here for comparison)timestampadd(HOUR, 23, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(HOUR, 24, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(SECOND, 86400 - 1, 2011-03-12 03:00:00) = 2011-03-13 03:59:59timestampadd(SECOND, 86400, 2011-03-12 03:00:00) = 2011-03-13 03:00:00// In UTC timezone:timestampadd(quarter, 1431655764, 1970-01-01 00:00:00) = 1969-09-01 00:00:00timestampadd(day, 106751992, 1970-01-01 00:00:00) = -290308-12-22 15:58:10.448384```New results:```// In America/Los_Angeles timezone:timestampadd(DAY, 1, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(HOUR, 23, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(HOUR, 24, 2011-03-12 03:00:00) = 2011-03-13 04:00:00timestampadd(SECOND, 86400 - 1, 2011-03-12 03:00:00) = 2011-03-13 03:59:59timestampadd(SECOND, 86400, 2011-03-12 03:00:00) = 2011-03-13 04:00:00// In UTC timezone:timestampadd(quarter, 1431655764, 1970-01-01 00:00:00) = throw overflow exceptiontimestampadd(day, 106751992, 1970-01-01 00:00:00) = throw overflow exception```### How was this patch tested?Pass existing tests and some new tests.
1	-1	### What changes were proposed in this pull request?This allows LocalRelation to use an actual datatype as its schema.### Why are the changes needed?Passing the actual schema has a higher fidelity than using strings. Metadata for example is not passed when you a DDL string.### Does this PR introduce _any_ user-facing change?No. It is an addition to the connect protocol.### How was this patch tested?Updated a test, and this works. I will do the PlanGenerationTestSuite in a follow-up.
1	-1	### What changes were proposed in this pull request?Add `SparkSession.stop()` to SparkSession.### Why are the changes needed?API parity.### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Manually tested it.
2	-3	### What changes were proposed in this pull request?Fixes `createDataFrame` to support DDL string as schema.### Why are the changes needed?Currently DDL string as schema is ignored when the data is Python objects and the inference fails:```py>>> spark.createDataFrame([(100, None)], \"age INT, name STRING\").show()Traceback (most recent call last):...ValueError: Some of types cannot be determined after inferring, a StructType Schema is required in this case```### Does this PR introduce _any_ user-facing change?The DDL string as schema will not be ignored when the schema can't be inferred from the given data.```py>>> spark.createDataFrame([(100, None)], \"age INT, name STRING\").show()+---+----+|age|name|+---+----+|100|null|+---+----+```### How was this patch tested?Enabled related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove stale entries from the excluding rules for CompatibilitySuite.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Keep API compatibility list in sync.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?This PR adds all the `SparkSession.createDataFrame(..)` and `SparkSession.createDataset(..)` methods we can support in connect. The implicit conversion that uses this is also added.I moved the `ArrowWriter` class from sql/core to sql/catalyst for the arrow writing.### Why are the changes needed?API partity with the existing SQL APIs### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?I have added a number of tests to `ClientE2ETestSuite`.
1	-1	### What changes were proposed in this pull request?Upgrade buf from 1.14.0 to 1.15.0### Why are the changes needed?routine upgrade, I manually test and this upgrade will not change the generated files### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?manually test and CI
2	-1	### What changes were proposed in this pull request?Implement `spark.udf.registerJavaFunction` and `spark.udf.registerJavaUDAF`. A new proto `JavaUDF` is introduced.### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. `spark.udf.registerJavaFunction` and `spark.udf.registerJavaUDAF` are supported now.### How was this patch tested?Parity unit tests.
2	-3	### What changes were proposed in this pull request?This PR proposes to disable ANSI mode in `ProtoToParsedPlanTestSuite`.### Why are the changes needed?The plan suite is independent from ANSI mode as it does not check the result itself, and some tests fail when ANSI mode is on (https://github.com/apache/spark/actions/runs/4299081862/jobs/7493923661):```[info] - function_to_date_with_format *** FAILED *** (12 milliseconds)[info]   Expected and actual plans do not match:[info]   [info]   === Expected Plan ===[info]   Project [cast(gettimestamp(s#0, yyyy-MM-dd, TimestampType, Some(America/Los_Angeles), false) as date) AS to_date(s, yyyy-MM-dd)#0][info]   +- LocalRelation <empty>, [d#0, t#0, s#0, x#0L, wt#0][info]   [info]   [info]   === Actual Plan ===[info]   Project [cast(gettimestamp(s#0, yyyy-MM-dd, TimestampType, Some(America/Los_Angeles), true) as date) AS to_date(s, yyyy-MM-dd)#0][info]   +- LocalRelation <empty>, [d#0, t#0, s#0, x#0L, wt#0] (ProtoToParsedPlanTestSuite.scala:129)[info]   org.scalatest.exceptions.TestFailedException:[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)[info]   at org.scalatest.Assertions.fail(Assertions.scala:933)[info]   at org.scalatest.Assertions.fail$(Assertions.scala:929)[info]   at org.scalatest.funsuite.AnyFunSuite.fail(AnyFunSuite.scala:1564)[info]   at org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite.$anonfun$createTest$2(ProtoToParsedPlanTestSuite.scala:129)[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)```### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?Manually tested.
1	-1	### What changes were proposed in this pull request?This PR aims to update `connect` module dependency from `sql` to hive.### Why are the changes needed?SPARK-41725 added `hive` dependency via test suite.https://github.com/apache/spark/blob/41d3103f4d69a9ec25d9f78f3f94ff5f3b64ef78/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala#L75### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
1	-2	### What changes were proposed in this pull request?The pr aims to upgrade cyclonedx from 2.7.3 to 2.7.5.### Why are the changes needed?> When I run: mvn -DskipTests clean package,During compilation, the following error message appears:<img width=\"1364\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222338040-d7c8d595-be0b-40bb-af49-6b260dc0c425.png\">#### When I upgrade the cyclonedx version to 2.7.5, the above error message disappears.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual check & Pass GA.
3	-1	### What changes were proposed in this pull request?This pr aims upgrade `versions-maven-plugin` to 2.15.0### Why are the changes needed?New version bring some improvements like:- https://github.com/mojohaus/versions/pull/898- https://github.com/mojohaus/versions/pull/883- https://github.com/mojohaus/versions/pull/878- https://github.com/mojohaus/versions/pull/893and some bug fix:- https://github.com/mojohaus/versions/pull/901- https://github.com/mojohaus/versions/pull/897- https://github.com/mojohaus/versions/pull/891The full release notes as follows:- https://github.com/mojohaus/versions/releases/tag/2.15.0### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- GA `Dependencies test` should work normally- Manually check `./dev/test-dependencies.sh --replace-manifest`, run successful
1	-2	### What changes were proposed in this pull request?This PR aims to remove the standard Apache License header from the top of third-party source files.According to LICENSE file, I found two files.- https://github.com/apache/spark/blob/master/LICENSE### Why are the changes needed?This was requested via `dev@spark` mailing list.- https://lists.apache.org/thread/wfy9sykncw2znhzlvyd18bkyjr7l9x43Here is the ASF legal policy.- https://www.apache.org/legal/src-headers.html#3party> Do not add the standard Apache License header to the top of third-party source files.### Does this PR introduce _any_ user-facing change?No. This is a source code distribution.### How was this patch tested?Manual review.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Making Python the first tab for code examples in the Spark documentation.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?This completes the work started with [SPARK-42493].Python is the most approachable and most popular language and this change moves Python code examples to the first tab (showing by default).<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing Spark documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?I built the website locally and manually tested the pages.SKIP_SCALADOC=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?This PR removes `sql(\"command\").collect()` workaround in PySpark tests codes.### Why are the changes needed?They were added previously to work around within Spark Connect. This is fixed now, so we don't need to call `collect` anymore.### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?CI in this PR should test it out.
1	-1	### What changes were proposed in this pull request?Currently, the connect project have the new `DataFrameReader` API which is corresponding to Spark `DataFrameReader` API. But the connect `DataFrameReader` missing the jdbc API.### Why are the changes needed?This PR try to add JDBC to `DataFrameReader`### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
2	-2	### What changes were proposed in this pull request?This PR aims to ensure \"at least one time unit should be given for interval literal\" by modifying SqlBaseParser.This is a backport of https://github.com/apache/spark/pull/40195### Why are the changes needed?INTERVAL is a Non-Reserved keyword in spark. But when I run```shellscala> spark.sql(\"select interval from mytable\")```I get```org.apache.spark.sql.catalyst.parser.ParseException:at least one time unit should be given for interval literal(line 1, pos 7)== SQL ==select interval from mytable-------^^^  at org.apache.spark.sql.errors.QueryParsingErrors$.invalidIntervalLiteralError(QueryParsingErrors.scala:196)......```It is a bug because \"Non-Reserved keywords\" have a special meaning in particular contexts and can be used as identifiers in other contexts. So by design, INTERVAL can be used as a column name.Currently the interval's grammar is```interval    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)?    ;```There is no need to make the time unit nullable, we can ensure \"at least one time unit should be given for interval literal\" if the interval's grammar is```interval    : INTERVAL (errorCapturingMultiUnitsInterval | errorCapturingUnitToUnitInterval)    ;```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test: PlanParsserSuite.\"SPARK-42553: NonReserved keyword 'interval' can be column name\"Local test```shellscala> val myDF = spark.sparkContext.makeRDD(1 to 5).toDF(\"interval\")myDF: org.apache.spark.sql.DataFrame = [interval: int]scala> myDF.createOrReplaceTempView(\"mytable\")scala> spark.sql(\"select interval from mytable;\").show()+--------+|interval|+--------+|       1||       2||       3||       4||       5|+--------+```
1	-1	### What changes were proposed in this pull request?This pr aims upgrade dropwizard metrics to 4.2.15.### Why are the changes needed?The new version bring some bug fix:- https://github.com/dropwizard/metrics/pull/3125- https://github.com/dropwizard/metrics/pull/3179And the release notes as follows:- https://github.com/dropwizard/metrics/releases/tag/v4.2.16- https://github.com/dropwizard/metrics/releases/tag/v4.2.17### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass Github Actions
1	-2	### What changes were proposed in this pull request?This pr aims partial implement DataFrameNaFunctions includes `approxQuantile`, `cov`, `corr`, `crosstab`, `freqItems`,  `sampleBy` and `countMinSketch`,  `bloomFilter` are not supported in this pr due to lack of protobuf message.### Why are the changes needed?Spark connect Scala client API coverage### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?- Add new test.- checked `connect` and `connect-client-jvm` with Scala 2.13 manually
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces a mechanism to transfer artifacts (currently, local `.jar` + `.class` files) from a Spark Connect JVM/Scala client over to the server side of Spark Connect. The mechanism follows the protocol as defined in https://github.com/apache/spark/pull/40147 and supports batching (for multiple \"small\" artifacts) and chunking (for large artifacts).Note: Server-side artifact handling is not covered in this PR.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In the decoupled client-server architecture of Spark Connect, a remote client may use a local JAR or a new class in their UDF that may not be present on the server. To handle these cases of missing \"artifacts\ we implement a mechanism to transfer artifacts from the client side over to the server side as per the protocol defined in https://github.com/apache/spark/pull/40147.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, users would be able to use the `addArtifact` and `addArtifacts` methods (via a `SparkSession` instance) to transfer local files (`.jar` and `.class` extensions).### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests - located in `ArtifactSuite`. 
1	-1	### What changes were proposed in this pull request?Adding a simple script to start the Scala client in the Scala REPL. As well as a script to start the spark connect server for the client to connect to.### Why are the changes needed?Make the JVM client more easy to be used.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually tested.
3	-3	**What changes were proposed in this pull request?**The result of attribute resolution should consider only unique values for the reference. If it has duplicate values, it will incorrectly result into ambiguous reference error.**Why are the changes needed?**The below query fails incorrectly due to ambiguous reference error.val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(\"id\\"col2\\"col3\\"col4\ \"col5\")val op_cols_mixed_case = List(\"id\\"col2\\"col3\\"col4\ \"col5\ \"ID\")val df3 = df1.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)df3.select(\"id\").show()org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id.df3.explain()== Physical Plan ==*(1) Project [_1#6 AS id#17, _2#7 AS col2#18, _3#8 AS col3#19, _4#9 AS col4#20, _5#10 AS col5#21, _1#6 AS ID#17]Before the fix, attributes matched were:attributes: Vector(id#17, id#17)Thus, it throws ambiguous reference error. But if we consider only unique matches, it will return correct result.unique attributes: Vector(id#17)**Does this PR introduce any user-facing change?**Yes, Users migrating from Spark 2.3 to 3.x will face this error as the scenario used to work fine in Spark 2.3 but fails in Spark 3.2. After the fix, iit will work correctly as it was in Spark 2.3.**How was this patch tested?**Added unit test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add tests for grouping() and grouping_id() functions.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve testing coverage.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
1	-1	### What changes were proposed in this pull request?Introduces `UnparsedDataType` and delays parsing DDL string for Python UDFs until `SparkConnectClient` is available.`UnparsedDataType` carries the DDL string and parse it in the server side.It should not be enclosed in other data types.Also changes `createDataFrame` to use the proto `DDLParse`.### Why are the changes needed?Currently `parse_data_type` depends on `PySparkSession` that creates a local PySpark, but it won't be available in the client side.When `SparkConnectClient` is available, we can use the new proto `DDLParse` to parse the data types as string.### Does this PR introduce _any_ user-facing change?The UDF's `returnType` attribute could be a string in Spark Connect if it is provided as string.### How was this patch tested?Existing tests.
1	-1	### What changes were proposed in this pull request?Fix `SparkConnectAnalyzeHandler` to use `withActive`.### Why are the changes needed?Similar to #40165, `SQLConf.get` is necessary when transforming the proto to plans.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr adds a new physical plan `DriverSortExec`. It represents a special case of global `SortExec`  which is the root node. Then we can save one shuffle.Add a new config `spark.sql.execution.driverSortThreshold` to control if it's safe to do sort at driver side. We should make sure the max rows of plan is small enough to avoid oom and preformance regression.This optimization should work fine since It gets benefits from AQE framework that the max rows of logical plan is accurate and it can be propagated cross most of nodes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve performance### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test and do some benchmarkSort 50, 000 rows for such case `df.sort.collect`:```OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1033-azureIntel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHzDriverSort:                               Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Sort                                               1158           1355         210          0.0       23169.3       1.0XDriverSort                                          636            680          55          0.1       12715.2       1.8X```
1	-2	### What changes were proposed in this pull request?Reimplement `FPGrowthModel.transform` with dataframe operations### Why are the changes needed?delay the `collect()` execution of model dataframe `associationRules`increase the optimization space of SQL optimizer### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing UT
1	-2	This is a backport of #40237.### What changes were proposed in this pull request?This PR fixed the counter-intuitive behaviors of the `TimestampAdd` expression mentioned in https://issues.apache.org/jira/browse/SPARK-42635. See the following *user-facing* changes for details.### Does this PR introduce _any_ user-facing change?Yes. This PR fixes the three problems mentioned in SPARK-42635:1. When the time is close to daylight saving time transition, the result may be discontinuous and not monotonic.2. Adding month, quarter, and year silently ignores `Int` overflow during unit conversion.3. Adding sub-month units (week, day, hour, minute, second, millisecond, microsecond)silently ignores `Long` overflow during unit conversion.Some examples of the result changes:Old results:```// In America/Los_Angeles timezone:timestampadd(DAY, 1, 2011-03-12 03:00:00) = 2011-03-13 03:00:00 (this is correct, put it here for comparison)timestampadd(HOUR, 23, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(HOUR, 24, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(SECOND, 86400 - 1, 2011-03-12 03:00:00) = 2011-03-13 03:59:59timestampadd(SECOND, 86400, 2011-03-12 03:00:00) = 2011-03-13 03:00:00// In UTC timezone:timestampadd(quarter, 1431655764, 1970-01-01 00:00:00) = 1969-09-01 00:00:00timestampadd(day, 106751992, 1970-01-01 00:00:00) = -290308-12-22 15:58:10.448384```New results:```// In America/Los_Angeles timezone:timestampadd(DAY, 1, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(HOUR, 23, 2011-03-12 03:00:00) = 2011-03-13 03:00:00timestampadd(HOUR, 24, 2011-03-12 03:00:00) = 2011-03-13 04:00:00timestampadd(SECOND, 86400 - 1, 2011-03-12 03:00:00) = 2011-03-13 03:59:59timestampadd(SECOND, 86400, 2011-03-12 03:00:00) = 2011-03-13 04:00:00// In UTC timezone:timestampadd(quarter, 1431655764, 1970-01-01 00:00:00) = throw overflow exceptiontimestampadd(day, 106751992, 1970-01-01 00:00:00) = throw overflow exception```### How was this patch tested?Pass existing tests and some new tests.
1	-1	### What changes were proposed in this pull request?When colregex returns a single column it should link the plans plan_id. For reference here is the non-connect Dataset code that does this:https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L1512This also needs to be fixed for the Python client.### Why are the changes needed?Let the `UnresolvedAttribute` link plan_id if it is exist.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?We should run `InferFiltersFromConstraints` again after running `RewritePredicateSubquery` rule. `RewritePredicateSubquery `rewrite IN and EXISTS queries to LEFT SEMI/LEFT ANTI joins. But we don't infer filters for these newly generated joins. We noticed in TPCH 1TB q21 by inferring filter for these new joins, one `lineitem` table scan can be reduced as `ReusedExchange` got introduce. Previously due to mismatch in filter predicates reuse was not happening.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Can improve query performance.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->PlanStability test
1	-3	### What changes were proposed in this pull request?This PR proposes to add `toSeq` to work around Scala 2.13 build at `getCrcValues`.### Why are the changes needed?The build in the master branch fails in Scala 2.13 build:```[error] /home/runner/work/spark/spark/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/ArtifactSuite.scala:87:11: type mismatch;[error]  found   : scala.collection.mutable.Buffer[Long][error]  required: Seq[Long][error]       .map(_.toLong)[error]           ^```https://github.com/apache/spark/actions/runs/4323142317/jobs/7546408812### Does this PR introduce _any_ user-facing change?No, dev-only.### How was this patch tested?CI in this PR should test it out.
2	-2	### What changes were proposed in this pull request?This PR improves `ConstantPropagation` rule, the new implementation:- Collects constants into a map directly, instead of a list that is then transformed into a map.- Supports substituting in conflicting equalities:  e.g. `a = 1 AND a = 2` => `a = 1 AND 1 = 2`  Before this PR this unsatisfiable filter was not reduced:  ```  == Physical Plan ==  *(1) Project [value#219 AS a#222]  +- *(1) Filter ((isnotnull(value#219) AND (value#219 = 1)) AND (value#219 = 2))     +- *(1) LocalTableScan [value#219]  ```  But after this PR it is:  ```  == Physical Plan ==  LocalTableScan <empty>, [a#222]  ```- Allows substitution in other than `EqualTo` and `EqualNullSafe` `BinaryComparison`expressions:  e.g. `a = 5 AND b < a + 3` => `a = 5 AND b < 8`- Gets rid of the `Option` wrapper from the return type of `ConstantPropagation.traverse()` as modern `.mapExpressions()`, `.withNewChildren()` and `.mapChildren()` methods can recognize if there was no change in arguments.### Why are the changes needed?Improve performance.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New UTs are taken from @wangyum's PR https://github.com/apache/spark/pull/40093
2	-2	### What changes were proposed in this pull request?The website was recently released with a new style. This patch adopts the same style for the docs page.#### How was this done?The main website and the docs page are built on bootstrap for styling. This means that particular styles applied to the main page will work nicely on the docs page as well. The biggest chunk of work was to simply copy the stylesheet, the long tail was some manual adjustments.* Copy the main stylesheet from the new website and drop it into the docs build* Remove the old CSS from the layout* Update the layout to use the new bootstrap 5 instead of the old bootstrap* Use bootstrap for tab visibility instead of home-grown JS* Use bootstrap for table striping instead of custom CSS* Update all 139 tables to have a proper `thead` to render nicely* Keep the navigation bar top fixed, with a body top padding.* Copied minor CSS properties from the old stylesheet to the new one to handle the folding side bar menu.* Using bootstrap from CDN instead of local#### Images<img width=\"1266\" alt=\"SCR-20230311-g53\" src=\"https://user-images.githubusercontent.com/3421/224479611-98784918-f5f5-4609-b22c-a36e3e2efe06.png\"><img width=\"989\" alt=\"SCR-20230311-g5a\" src=\"https://user-images.githubusercontent.com/3421/224479613-ba097e3b-7394-4724-9ef5-d55395f54434.png\"><img width=\"970\" alt=\"SCR-20230311-g5f\" src=\"https://user-images.githubusercontent.com/3421/224479817-cbb3ca74-cb41-4b74-8c17-9ae58740192d.png\">### Why are the changes needed?Style### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Browser
2	-2	### What changes were proposed in this pull request?This PR proposes to support `withSequenceColumn` as PySpark DataFrame internal function.### Why are the changes needed?To prepare supporting pandas API on Spark with Spark Connect.We currently use `distributed-sequence` as default index type in pandas API on Spark, but it's not working for Spark Connect as it is because the distributed sequence index is implemented by leveraging the `withSequenceColumn` which is implemented on JVM side as below:```python            # This can't be used in Spark Connect, because Spark Connect client is not depends on JVM.            return SparkDataFrame(                sdf._jdf.toDF().withSequenceColumn(column_name),                sdf.sparkSession,            )```Therefore, in order to operate with the same code in the existing general PySpark DataFrame and Spark Connect DataFrame, this PR proposes to make `withSequenceColumn` can be used as a PySpark internal function as suggested below:```python            # This works both in regular/connect PySpark DataFrame            return sdf._withSequenceColumn(column_name)```This is the first PR in preparation for supporting the pandas API on Spark in Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, this API is added intended to internal usage.### How was this patch tested?Added UTs for Scala and Python codes.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?In the `pyspark.sql.functions`, we replaced `from typing import foo, bar, etc` with `import typing` and alluses of `foo`, `bar` and `etc` in that module were replaced with `typing.foo`, `typing.bar` and `typing.etc`.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Exposing methods from `typing` inside `spark.sql.functions` could lead to confusing errors on the user side, like in this example:```from pyspark.sql import SparkSessionfrom pyspark.sql import functions as fspark = SparkSession.builder.getOrCreate()df = spark.sql(\"\"\"SELECT 1 as a\"\"\")df.withColumn(\"a\ f.cast(\"STRING\ f.col(\"a\"))).printSchema()  ```The code above runs without raising any error but yields an incorrect result:```root|-- a: integer (nullable = false)```This is because `f.cast` is in fact `typing.cast` and we should have used `f.col(\"a\").cast(\"STRING\")` instead.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, if a user has made the mistake described in the example above, their code that did run (with an incorrect behaviour) will instead break after they upgrade to a version containing this change. But one could argue that it would be a good thing, since it will expose a mistake in their code.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?No new test were added.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes an implementation of newSession API. The idea is we reuse user context(e.g. user_id), gRPC channel, etc. But differentiate different Spark Remote Session by client id, which is generated randomly. So this idea has the benefits of:1. reusing gRPC channel to not over too manny connections to the server.2. Each user can has multiple remote sessions, differentiated by client ids (or named session ids in server side).  ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->API coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-3	### What changes were proposed in this pull request?We have seen some cases where the task exits as cancelled/failed which triggers the abort in the task completion listener for HDFSStateStoreProvider. As part of this, we cancel the backing stream and close the compressed stream. However, different stores such as Azure blob store could throw exceptions which are not caught in the current path, leading to job failures. This change proposes to fix this issue by catching all non fatal exceptions thrown by cancel/close.### Why are the changes needed?Changes are required to avoid job failures due to exceptions thrown by output stream handlers on abort with the HDFSStateStoreProvider.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Modified a test and simulated a NPE in the abort path and verified that the task and thereby the job fails before this change.After the change, the test passes fine.
1	-3	### What changes were proposed in this pull request?Make use of the new spark-connect script to make the Scala client test to not directly depends on any other modules.The dependency is still there but hidden by the spark-connect script. When calling the script to start the server, the script performs a `build/sbt package` to ensure all the jars are build and can be found in the correct path. After the change, we can use the following commands to run the Scala client tests:```build/mvn cleanbuild/mvn compile -pl connector/connect/client/jvmbuild/mvn test -pl connector/connect/client/jvm``````build/sbt cleanbuild/sbt \"testOnly org.apache.spark.sql.ClientE2ETestSuite\" ``````build/sbt cleanbuild/sbt \"connect-client-jvm/test\"```**Scala 2.13**```build/mvn cleanbuild/mvn -Pscala-2.13 compile -pl connector/connect/client/jvm -am -DskipTestsbuild/mvn -Pscala-2.13 test -pl connector/connect/client/jvm// These commands failed with errors to find catalyst ArrowWriter. The error seems unrelated to this change.``````build/sbt cleanbuild/sbt \"testOnly org.apache.spark.sql.ClientE2ETestSuite\" -Pscala-2.13``````build/sbt cleanbuild/sbt \"connect-client-jvm/test\" -Pscala-2.13```After the change, the waiting time to run the E2ESuite is ~3min for a clean build. Then ~1min for subsequent runs. The test is slower only because we moved the build time from many commands to this single command. There is no limitations to add more tests as the delay is only caused by the shared server start time. Once the server is started, the tests run fast.### Why are the changes needed?A single command for maven users to mvn clean install to run tests and build.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Build, Manual tests.
1	-1	### What changes were proposed in this pull request?Currently, the connect functions missing the broadcast API. This PR want add this API to connect's functions.### Why are the changes needed?Add the broadcast function to connect's functions.scala.### Does this PR introduce _any_ user-facing change? 'No'.New feature.### How was this patch tested?New test cases.
2	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40252 supported some jdbc API that reuse the proto msg `DataSource`. The `DataFrameReader` also have another kind jdbc API that is unrelated to load data source.### Why are the changes needed?This PR adds the new proto msg `PartitionedJDBC` to support the remaining jdbc API.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
2	-2	### What changes were proposed in this pull request?The pr aims to upgrade maven-surefire-plugin from 3.0.0-M8 to 3.0.0-M9 & eliminate build warnings.### Why are the changes needed?https://github.com/apache/maven-surefire/compare/surefire-3.0.0-M8...surefire-3.0.0-M9[system-properties](https://maven.apache.org/surefire/maven-surefire-plugin/examples/system-properties.html)<img width=\"632\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222911581-56f886ea-31b5-44d1-a155-74af883bf797.png\"><img width=\"1046\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222961111-1cd21386-f141-45d1-9969-be315e93e6dd.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual check & Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to remove unused protobuf imports in 'spark/connect/commands.proto'.### Why are the changes needed?Eliminate build warnings as follow:<img width=\"1051\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222913613-d5448908-7ab1-463c-bdad-502992cce761.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual check & Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to fix bug for createDataFrame from complex type schema.### Why are the changes needed?When I code UT for `DataFrameNaFunctions` as follow:val schema = new StructType()      .add(\"c1\ new StructType()        .add(\"c1-1\ StringType)        .add(\"c1-2\ StringType))val data = Seq(Row(Row(null, \"a2\")), Row(Row(\"b1\ \"b2\")), Row(null))val df = spark.createDataFrame(data.asJava, schema)df.show()I found that the above code does not work. The error message as follow:<img width=\"657\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222938339-77dec8c6-549b-41de-869b-8a191a0f745e.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add new UTPass GA.
2	-1	### What changes were proposed in this pull request?In #39459  we introduced a new config entry `spark.rdd.cache.visibilityTracking.enabled` and mark the support version as 3.4.0. Based on the discussion https://github.com/apache/spark/pull/39459#discussion_r1123978338 we won't backport this to 3.4, so modify the support version for the new added config entry to 3.5.0### Why are the changes needed?Fixing config entry  support version.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing UT.
3	-2	### What changes were proposed in this pull request?This PR proposes to document the PySpark error class list into a part of [Development](https://spark.apache.org/docs/latest/api/python/development/index.html) page.![Screen Shot 2023-03-05 at 5 49 28 PM](https://user-images.githubusercontent.com/44108233/222950913-fbe92c97-0ce6-4270-b827-c5c8080ad37f.png)### Why are the changes needed?To make it easier for developers to find error classes.### Does this PR introduce _any_ user-facing change?No, it's documentation,### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually build docs by running `make clean html`
2	-3	### What changes were proposed in this pull request?`build/mvn` tends to use the new maven version to build Spark now, and GA starts to use 3.9.0 as the default maven version.But there may be some uncertain factors when building Spark with unverified version.For example, `java-11-17` GA build task build with maven 3.9.0 has many error logs in master like follow:```Error: [ERROR] An error occurred attempting to read POMorg.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42)     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke (Method.java:568)    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)```So this pr change the version check condition of `build/mvn` to make it build Spark only with the verified maven version.### Why are the changes needed?Make `build/mvn` build Spark only with the verified maven version### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- `java-11-17` GA build task pass and no error message as above- Manual test:1. Make the system use maven 3.9.0( > 3.8.7 ) by default:run `mvn -version````Apache Maven 3.9.0 (9b58d2bad23a66be161c4664ef21ce219c2c8584)Maven home: /Users/${userName}/Tools/mavenJava version: 1.8.0_362, vendor: Azul Systems, Inc., runtime: /Users/${userName}/Tools/zulu8/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\ version: \"13.2.1\ arch: \"aarch64\ family: \"mac\"```and run `build/mvn -version````Using `mvn` from path: /${basedir}/spark/build/apache-maven-3.8.7/bin/mvnUsing SPARK_LOCAL_IP=localhostApache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29)Maven home: /${basedir}/spark/build/apache-maven-3.8.7Java version: 1.8.0_362, vendor: Azul Systems, Inc., runtime: /Users/${userName}/Tools/zulu8/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\ version: \"13.2.1\ arch: \"aarch64\ family: \"mac\"``` We can see Spark use 3.8.7 in build directory when the system default maven > 3.8.72. Make the system use maven 3.8.7 by default:run `mvn -version````mvn -versionApache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29)Maven home: /Users/${userName}/Tools/mavenJava version: 1.8.0_362, vendor: Azul Systems, Inc., runtime: /Users/${userName}/Tools/zulu8/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\ version: \"13.2.1\ arch: \"aarch64\ family: \"mac\"```and run `build/mvn -version````Using `mvn` from path: /Users/${userName}/Tools/maven/bin/mvnUsing SPARK_LOCAL_IP=localhostApache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29)Maven home: /Users/${userName}/Tools/mavenJava version: 1.8.0_362, vendor: Azul Systems, Inc., runtime: /Users/${userName}/Tools/zulu8/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\ version: \"13.2.1\ arch: \"aarch64\ family: \"mac\"``` We can see Spark use system default maven 3.8.7 when the system default maven is 3.8.7.3. Make the system use maven 3.8.6( < 3.8.7 ) by default:run `mvn -version````mvn -versionApache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63)Maven home: /Users/${userName}/Tools/mavenJava version: 1.8.0_362, vendor: Azul Systems, Inc., runtime: /Users/${userName}/Tools/zulu8/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\ version: \"13.2.1\ arch: \"aarch64\ family: \"mac\"```and run `build/mvn -version````Using `mvn` from path: /Users/${userName}/Tools/maven/bin/mvnUsing SPARK_LOCAL_IP=localhostApache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29)Maven home: /Users/${userName}/Tools/mavenJava version: 1.8.0_362, vendor: Azul Systems, Inc., runtime: /Users/${userName}/Tools/zulu8/zulu-8.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\ version: \"13.2.1\ arch: \"aarch64\ family: \"mac\"``` We can see Spark use 3.8.7 in build directory when the system default maven < 3.8.7.
2	-2	### What changes were proposed in this pull request?The pr aims to upgrade scalafmt from 3.7.1 to 3.7.2### Why are the changes needed?A. Release note:> https://github.com/scalameta/scalafmt/releasesB. V3.7.1 VS V3.7.2> https://github.com/scalameta/scalafmt/compare/v3.7.1...v3.7.2C. Bring bug fix & some improvement:<img width=\"560\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222950202-76fea081-3b0b-458b-b963-248e472107f4.png\"><img width=\"583\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/222950245-47dab2a4-df28-4b31-9a05-0e1ccc8b5acc.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?This pr just drop temp view after test which create by `test temp view` in `ClientE2ETestSuite`.### Why are the changes needed?Drop temp view after test to clean up resource.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-4	### What changes were proposed in this pull request?Currently a stage will be resubmitted in a few scenarios:1. Task failed with `FetchFailed` will trigger stage re-submit;2. Barrier task failed;3. Shuffle data loss due to executor/host decommissioned;For the first 2 scenarios, there is a config `spark.stage.maxConsecutiveAttempts` to limit the retry times. While for the 3rd scenario, there'll be potential risks for inifinite retry if there are always executors hosting the shuffle data from successful tasks got killed/lost, the stage will be re-run again and again.To avoid the potential risk, the proposal in this PR is to add a new config `spark.stage.maxConsecutiveAttempts` to limit the overall max attempts number for each stage, the stage will be aborted once the retry times beyond the limitation.### Why are the changes needed?To avoid the potential risks for stage infinite retry.### Does this PR introduce _any_ user-facing change?Added limitation for stage retry times, so jobs may fail if they need to retry for mutiplte times beyond the limitation.### How was this patch tested?Added new UT.
2	-2	### What changes were proposed in this pull request?`UnresolvedNamedLambdaVariable` do not need unique names in python. We already did this for the scala client, and it is good to have parity between the two implementations.### Why are the changes needed?Try to avoid unique names for `UnresolvedNamedLambdaVariable`.### Does this PR introduce _any_ user-facing change?'No'.New feature### How was this patch tested?N/A
1	-2	### What changes were proposed in this pull request?This PR proposes to add a brief description of Spark Connect to the PySpark main page.<img width=\"1081\" alt=\"Screen Shot 2023-03-06 at 11 35 41 AM\" src=\"https://user-images.githubusercontent.com/44108233/223006571-42fccf6f-cb7b-479c-9f11-5c246b442fac.png\">### Why are the changes needed?Spark Connect is a new and experimental feature of PySpark that enables Spark to run anywhere and work with different data stores and services. Adding a brief description of Spark Connect to the main page will inform users about this feature and how it can benefit their use cases. Additionally, the note about the experimental nature of this feature will help users understand the potential risks of using Spark Connect in production environments.### Does this PR introduce _any_ user-facing change?No for API usage, but this PR introduces a user-facing documents by adding a new section about Spark Connect to the PySpark main page.### How was this patch tested?To ensure that the documentation builds correctly, the `make clean html` command was executed to test the build process.
3	-2	This is a backport of https://github.com/apache/spark/pull/40064 for branch-3.2<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-41448](https://issues.apache.org/jira/browse/SPARK-41448) make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
3	-2	This is a backport of https://github.com/apache/spark/pull/40064 for branch-3.3<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make a serializable jobTrackerId instead of a non-serializable JobID in FileWriterFactory### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-41448](https://issues.apache.org/jira/browse/SPARK-41448) make consistent MR job IDs in FileBatchWriter and FileFormatWriter, but it breaks a serializable issue, JobId is non-serializable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
1	-1	### What changes were proposed in this pull request?Currently, the connect project have the new `DataFrameWriter` API which is corresponding to Spark `DataFrameWriter` API. But the connect's `DataFrameWriter` missing the jdbc API.### Why are the changes needed?This PR try to add JDBC to `DataFrameWriter`.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?@hvanhovell It seems that add test cases no way.
3	-2	### What changes were proposed in this pull request?Write temp checkpoints for streaming queries to local filesystem even if default FS is set differently### Why are the changes needed?We have seen cases where the default FS could be a remote file system and since the path for streaming checkpoints is not specified explcitily, this could cause pileup under 2 cases:- query exits with exception and the flag to force checkpoint removal is not set- driver/cluster terminates without query being terminated gracefully### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Verified that the checkpoint is resolved and written to the local FS```23/03/04 01:42:49 INFO ResolveWriteToStream: Checkpoint root file:/local_disk0/tmp/temporary-c97ab8bd-6b03-4c28-93ea-751d30a2d3f9 resolved to file:/local_disk0/tmp/temporary-c97ab8bd-6b03-4c28-93ea-751d30a2d3f9....23/03/04 01:46:37 INFO MicroBatchExecution: [queryId = 66c4c] Deleting checkpoint file:/local_disk0/tmp/temporary-c97ab8bd-6b03-4c28-93ea-751d30a2d3f9.```
1	-2	### What changes were proposed in this pull request?Currently, there are a lot of test cases for broadcast hint is invalid. Because the data size is smaller than broadcast threshold.### Why are the changes needed?Fix the invalid tests for broadcast hint.### Does this PR introduce _any_ user-facing change?'No'.Just modify the test cases.### How was this patch tested?Correct test cases.
2	-2	### What changes were proposed in this pull request?This PR enhances `UnwrapCastInBinaryComparison` to support unwrapping date type to string type.### Why are the changes needed?Avoid always fetching all partitions because the partition filters cannot be pushed down to the Hive metastore. For example:```sqlCREATE TABLE t1(id int, dt string) using parquet PARTITIONED BY (dt);EXPLAIN SELECT * FROM t1 WHERE dt > date_add(current_date(), -7);```Before Spark 3.0. It pushes partition filters to Hive metastore:```== Physical Plan ==*(1) FileScan parquet default.t1[id#2,dt#3] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(dt#3), (dt#3 > 2023-02-27)], PushedFilters: [], ReadSchema: struct<id:int>```After SPARK-27638. Because it can not [convert partition filters](https://github.com/apache/spark/blob/v3.0.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveShim.scala#L794-L798) to hive metastore filters, it will not push partition filters to Hive metastore. As a result, it always takes all the parititons:```== Physical Plan ==*(1) ColumnarToRow+- FileScan parquet default.t1[id#5,dt#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(0 paths)[], PartitionFilters: [isnotnull(dt#6), (cast(dt#6 as date) > 2023-02-27)], PushedFilters: [], ReadSchema: struct<id:int>```After this PR. It unwraps date type to string type and then pushes partition filters to Hive metastore:```== Physical Plan ==*(1) ColumnarToRow+- FileScan parquet spark_catalog.default.t1[id#0,dt#1] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(0 paths)[], PartitionFilters: [isnotnull(dt#1), (dt#1 > 2023-02-27)], PushedFilters: [], ReadSchema: struct<id:int>```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Currently the grammar for ALTER TABLE ADD|REPLACE column is:```qualifiedColTypeWithPosition: name=multipartIdentifier dataType (NOT NULL)? defaultExpression? commentSpec? colPosition?;```This enforces a constraint on the order of: (NOT NULL, DEFAULT value, COMMENT value FIRST|AFTER value). We can update the grammar to allow these options in any order instead, to improve usability.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This helps make the SQL syntax more usable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the SQL syntax updates slightly.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing and new unit tests
1	-1	### What changes were proposed in this pull request?Spark SQL have the helper function `withSQLConf` that is easy to change SQL config and make test easy.### Why are the changes needed?Make the connect test cases easy to implement.### Does this PR introduce _any_ user-facing change?No, it is a test only change.### How was this patch tested?Test case updated.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Design doc:https://docs.google.com/document/d/1V5rOgksmOnA8AsJFZ_rasSYDQuP06_vrcfp3RY_22o8/edit#### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Testing code:run command `bin/pyspark --remote local `, in python REPL, run following code:```from pyspark.ml.classification import LogisticRegression, LogisticRegressionModellor = LogisticRegression()# Set paramslor.setMaxIter(2)df0 = spark.read.format(\"libsvm\").load(\"data/mllib/sample_binary_classification_data.txt\")# Train modellor_model = lor.fit(df0)infer_df = df0.sample(0.5)# Predictionprediction_df = lor_model.transform(infer_df)prediction_df.show()# Test model attributesprint(lor_model.coefficients)print(lor_model.intercept)print(lor_model.coefficientMatrix)print(lor_model.interceptVector)# Test model summary methodsprint(lor_model.summary.featuresCol)lor_model.summary.roc.show()print(lor_model.summary.areaUnderROC)lor_model.summary.pr.show()lor_model.summary.fMeasureByThreshold.show()lor_model.summary.precisionByThreshold.show()print(lor_model.summary.weightedFalsePositiveRate)print(lor_model.summary.precisionByLabel)print(lor_model.summary.objectiveHistory)summary2 = lor_model.evaluate(infer_df)summary2.roc.show()print(summary2.precisionByLabel)# save estimatorlor.write().overwrite().save(\"/tmp/lore_001\")loaded_lor = LogisticRegression.load(\"/tmp/lore_001\")# save modellor_model.write().overwrite().save(\"/tmp/lor_001\")# load modelloaded_model = LogisticRegressionModel.read().load(\"/tmp/lor_001\")# Test loaded model transformationloaded_model.transform(infer_df).show()```
2	-2	### What changes were proposed in this pull request?Introduce spark.hive.exec.dynamic.partition.savePartitions=true (default false) spark.hive.exec.dynamic.partition.savePartitions.tableNamePrefix=hive_dynamic_inserted_partitionswhen spark.hive.exec.dynamic.partition.savePartitions=true we save the partitions to the temporary view $spark.hive.exec.dynamic.partition.savePartitions.tableNamePrefix_$dbName_$tableName### Why are the changes needed?When hive.exec.dynamic.partition=true and hive.exec.dynamic.partition.mode=nonstrict, we can insert table by sql like 'insert overwrite table aaa partition(dt) select xxxx',  of course we can know the partitions inserted into the table by the sql itself,  but if we want do something for common use, we need some common way to get the inserted partitions,  for example:    spark.sql(\"insert overwrite table aaa partition(dt) select xxxx\")  //insert table    val partitions = getInsertedPartitions()   //need some way to get inserted partitions    monitorInsertedPartitions(partitions)    //do something for common useThis pull request will allow user to get inserted partitions in a common way### Does this PR introduce _any_ user-facing change?no### How was this patch tested?new unit test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Following generated columns, column default value should also have a catalog capability and v2 catalogs must explicitly declare SUPPORT_COLUMN_DEFAULT_VALUE to support it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->column default value needs dedicated handling and if a catalog simply ignores it, then query result can be wrong.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
1	-2	### What changes were proposed in this pull request?Today, if a data source already has an output column called `_metadata`, queries cannot access the file-source metadata column that normally carries that name. This PR addresses the issue by automatically renaming any metadata column whose name conflicts with an output column. We also introduce a new dataframe method, `metadataColumn`, which mirrors the existing `col` method but (a) only works for metadata columns; and (b) will reliably find metadata columns even if they were renamed due to a name conflict.### Why are the changes needed?Today, it's too easy to lose access to metadata columns if the user's table happened to have the wrong column name. This sharp edge limits the utility of metadata columns in general, because the feature doesn't work reliably for all table schemas.### Does this PR introduce _any_ user-facing change?Suppose a table defines a `StringType` column called `_metadata`. Today, the metadata column is not accessible, and the example below would return the table's string column:```scaladf.select(\"_metadata\")```With the change, the original query still returns a string, but the file-source metadata column can be found and accessed by invoking `DataSet.metadataColumn`:```scaladf.withColumn(\"m\ df.metadataColumn(\"_metadata\"))```NOTE: Adding a SQL user surface for accessing metadata columns with conflicting names is out of scope for this pull request and can be addressed as future work.### How was this patch tested?New unit tests added.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Optimize `Utils.bytesToString`. Arithmetic ops on `BigInt` and `BigDecimal` are order(s) of magnitude slower than the ops on primitive types. Division is an especially slow operation and it is used en masse here.To avoid heating up the Earth while formatting byte counts for human consumption we observe that most formatting operations are not in the 10s of EiBs but on counts that fit in 64-bits and use (fastpath) 64-bit operations to format them.### Why are the changes needed?Use of `Utils.bytesToString` is prevalent through the codebase and they are mainly used in logs. If the logs are emitted then this becomes a heavyweight operation.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Defer formatting of bytes until debug logging is required. Otherwise we are always spending cycles doing formatting irrespective of if debug logging is enabled.### Why are the changes needed?Formatting shows up in profiling scan benchmarks.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually.
1	-1	### What changes were proposed in this pull request?Make the server to start with Hive dependency and make the script to build with the correct scala version set in pom files.Also allow the server to take any extra spark configurations.### Why are the changes needed?Adding hive dependency to allow some tests of write ops easier.Allow the server to start with the same Scala version set in the project.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually tested.
1	-3	### What changes were proposed in this pull request?Mute the UDF test.### Why are the changes needed?The test fails during maven test runs because the server cannot find the udf in the classpath. The test will be fixed once the udf artifact sync is finished.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?N/A
1	-1	### What changes were proposed in this pull request?Add spark connect shell to start the spark shell with spark connect enabled.Added \"-Pconnect\" to build the spark connect in the distributions.Simplified the dev shell scripts with \"-Pconnect\" command.### Why are the changes needed?Allow users to play with spark connect easily.### Does this PR introduce _any_ user-facing change?Yes. Added a new shell script and \"-Pconnect\" build option.### How was this patch tested?Manually tested.
3	-3	`pivot` is an unsupported operation in structured streaming but produces a bad error message that is quite misleading.The following is the current error message for the pivot in SS:```AnalysisException: Queries with streaming sources must be executed with writeStream.start();```We should report that an unsupported operation is happening instead of stating another operation should start the stream.### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Throw AnalysisExeception when `pivot` is called in Structure Streaming### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better error message for a better user journey### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
1	-3	### What changes were proposed in this pull request?Currently, if there is an executor node loss, we assume the shuffle data on that node is also lost. This is not necessarily the case if there is a shuffle component managing the shuffle data and reliably maintaining it (for example, in distributed filesystem or in a disaggregated shuffle cluster).### Why are the changes needed?Downstream projects have patches to Apache Spark in order to workaround this issue, for example Apache Celeborn has [this](https://github.com/apache/incubator-celeborn/blob/main/assets/spark-patch/RSS_RDA_spark3.patch).### Does this PR introduce _any_ user-facing change?Enhances the `ShuffleDriverComponents` API, but defaults to current behavior.### How was this patch tested?Existing unit tests, and added more tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a rule to align UPDATE assignments with table attributes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed so that we can rewrite UPDATE statements into executable plans for tables that support row-level operations. In particular, our row-level mutation framework assumes Spark is responsible for building an updated version of each affected row and that row is passed back to the data source.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Rename Connect proto requests `client_id` to `session_id`.On the one hand when I read `client_id` I was confused on what it is used to, even after reading the proto documentation.On the other hand,  client sides already use session_id:https://github.com/apache/spark/blob/9bf174f9722e34f13bfaede5e59f989bf2a511e9/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala#L51https://github.com/apache/spark/blob/9bf174f9722e34f13bfaede5e59f989bf2a511e9/python/pyspark/sql/connect/client.py#L522### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code readability### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
2	-2	### What changes were proposed in this pull request?Fixes `createDataFrame` to autogenerate missing column names.### Why are the changes needed?Currently the number of the column names specified to `createDataFrame` does not match the actual number of columns, it raises an error:```py>>> spark.createDataFrame([[\"a\ \"b\"]], [\"col1\"])Traceback (most recent call last):...ValueError: Length mismatch: Expected axis has 1 elements, new values have 2 elements```but it should auto-generate the missing column names.### Does this PR introduce _any_ user-facing change?It will auto-generate the missing columns:```py>>> spark.createDataFrame([[\"a\ \"b\"]], [\"col1\"])DataFrame[col1: string, _2: string]```### How was this patch tested?Enabled the related test.
1	-2	### What changes were proposed in this pull request?This PR proposes to disable ANSI mode in both `replace float with nan` and `replace double with nan` tests.### Why are the changes needed?To recover the build https://github.com/apache/spark/actions/runs/4349682658 with ANSI mode on.Spark Connect side does not fully leverage the error framework yet .. so simply disabling it for now.### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?Manually ran them in IDE with ANSI mode on.
2	-1	### What changes were proposed in this pull request?We could handle the steam side skew of BroadcastHashJoin to improve the join performanceBefore | After-- | --<img src=\"https://user-images.githubusercontent.com/7522130/223371603-c83da69a-cf2c-445f-b937-2af92c5f0953.png\" width=\"400\" height=\"450\"> | <img src=\"https://user-images.githubusercontent.com/7522130/223371590-56d4fd20-0c32-4806-967e-bb6e47851f29.png\" width=\"400\" height=\"450\">### Why are the changes needed?We can extend the OptimizeSkewedJoin to handle this case### Does this PR introduce _any_ user-facing change?NO### How was this patch tested?UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix /api/v1/applications to return total uptime instead of 0 for duration### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix REST API OneApplicationResource### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, /api/v1/applications will return the total uptime instead of 0 for the duration### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->locally build and run```json[ {  \"id\" : \"local-1678183638394\  \"name\" : \"SparkSQL::10.221.102.180\  \"attempts\" : [ {    \"startTime\" : \"2023-03-07T10:07:17.754GMT\    \"endTime\" : \"1969-12-31T23:59:59.999GMT\    \"lastUpdated\" : \"2023-03-07T10:07:17.754GMT\    \"duration\" : 20317,    \"sparkUser\" : \"kentyao\    \"completed\" : false,    \"appSparkVersion\" : \"3.5.0-SNAPSHOT\    \"startTimeEpoch\" : 1678183637754,    \"endTimeEpoch\" : -1,    \"lastUpdatedEpoch\" : 1678183637754  } ]} ]```
3	-2	### What changes were proposed in this pull request?Currently when we run yarn-client mode in SparkSubmit, when we catch exceptions during `runMain()`Spark won't call `SparkContext.stop()` since pr https://github.com/apache/spark/pull/33403 remove the behavior.Then AM side will mark the application as SUCCESS.In this pr, we will revert the behavior of https://github.com/apache/spark/pull/33403 then YARN mode, it will call `sc.stop()` in YARN env, and also, the client side will pass the correct exit code to the YARN AM side. This pr fixes this issue.### Why are the changes needed?Keep the same exit code between the client and AM### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Mannul testedThe screenshot is our internal platform to show each app's status, the application status information is from YARN rm and timeline service.Before change <img width=\"1140\" alt=\"截屏2023-03-08 上午11 45 31\" src=\"https://user-images.githubusercontent.com/46485123/223614465-0496e9bb-3cd5-41d8-b2f6-05d960db6963.png\">After change <img width=\"1255\" alt=\"截屏2023-03-08 上午11 45 13\" src=\"https://user-images.githubusercontent.com/46485123/223614448-51f3706b-ba69-4821-a074-853ed10ea5a2.png\">
2	-2	### What changes were proposed in this pull request?Since in https://github.com/apache/spark/pull/35594 we support pass a exit code to AM, when SparkConnectServer exit with -1, need pass this to AM too.### Why are the changes needed?Keep same exit code### Does this PR introduce _any_ user-facing change?No### How was this patch tested?
1	-1	### What changes were proposed in this pull request?The pr aims to fix ”createDataFrame doesn't work with non-nullable schema“### Why are the changes needed?Fix Bug### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?Run the following commands```build/mvn clean install -DskipTests -pl connector/connect/server -ambuild/mvn test -pl connector/connect/server```then we can see the following error message due to [SPARK-42555](https://issues.apache.org/jira/browse/SPARK-42555) A adds the loading `org.h2.Driver` in `beforeAll` of `ProtoToParsedPlanTestSuite`, but ```ProtoToParsedPlanTestSuite:*** RUN ABORTED ***  java.lang.ClassNotFoundException: org.h2.Driver  at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)  at java.base/java.lang.Class.forName0(Native Method)  at java.base/java.lang.Class.forName(Class.java:398)  at org.apache.spark.util.Utils$.classForName(Utils.scala:225)  at org.apache.spark.sql.connect.ProtoToParsedPlanTestSuite.beforeAll(ProtoToParsedPlanTestSuite.scala:68)  at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)  at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)  at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)  ...```So this pr add `h2` as test dependency of connect-server module to make maven test pass. ### Why are the changes needed?Add `h2` as test dependency of connect-server module to make maven test pass.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?manual check：```build/mvn clean install -DskipTests -pl connector/connect/server -ambuild/mvn test -pl connector/connect/server```with this pr, all test passed
1	-1	### What changes were proposed in this pull request?This pr add the same `ClassNotFoundException` catch  to `repl.Main` for Scala 2.13 as https://github.com/apache/spark/pull/40305 due `org/apache/spark/repl/Main.scala` is Scala version sensitive。### Why are the changes needed?Make sure Scala 2.12 and 2.13 have the same logic ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual check
1	-1	### What changes were proposed in this pull request?This pr aims to implement Dataset.toJSON.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test- Manually checked Scala 2.13
2	-2	### What changes were proposed in this pull request?I was reviewing this markdown document about proto parsing, and found that the formatting of code blocks looked incorrect:some examples:<img width=\"1238\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1002986/223468227-8fdd1db0-51ef-4c69-adaf-bca5f8ba92ce.png\"><img width=\"1124\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1002986/223468420-a0813fe8-4546-48ce-b847-2ddac46bee07.png\">In this commit I changed the formatting to use github's markdown syntax for code blocks, \" ``` \ and added syntax highlighting for each of them based on the language that it looked like.They now look a bit more like this:<img width=\"855\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1002986/223469481-4b1e2cfc-1591-40f6-aec4-05eb0d5a463b.png\">### Why are the changes needed?To help improve the readability of the documentation.### Does this PR introduce _any_ user-facing change?Yes, it updates user-facing documentation.### How was this patch tested?I inspected the github markdown preview to test that the changes looked correct.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The `AddMetadataColumns` analyzer rule is designed to resolve metadata columns using `LogicalPlan.metadataOutput` -- even if the plan already contains projections whose output does not specifically include a requested metadata column.Meanwhile, the `SubqueryAlias` plan node intentionally does _NOT_ propagate metadata columns automatically from a non-leaf/non-subquery child node, because the following should _NOT_ work:```scalaspark.read.table(\"t\").select(\"a\ \"b\").as(\"s\").select(\"_metadata\")```However, the current implementation is too strict in breaking the metadata chain, in case the child node's output already includes the metadata column:```scala// expected to work (and does)spark.read.table(\"t\")  .select(\"a\ \"b\").select(\"_metadata\")// by extension, this should also work (but does not)spark.read.table(\"t\").select(\"a\ \"b\ \"_metadata\").as(\"s\")  .select(\"a\ \"b\").select(\"_metadata\")```The solution is for `SubqueryAlias` to propagate metadata columns that are already in the child's output, thus preserving the `metadataOutput` chain for such columns.### Why are the changes needed?The current implementation of `SubqueryAlias` breaks the intended behavior of metadata column propagation. ### Does this PR introduce _any_ user-facing change?Yes. The following now works, where previously it did not:```scalaspark.read.table(\"t\").select(\"a\ \"b\ \"_metadata\").as(\"s\")  .select(\"a\ \"b\").select(\"_metadata\")```### How was this patch tested?New unit tests verify the expected behavior holds, with and without subqueries in the plan.
2	-2	### What changes were proposed in this pull request?I added a better way to show the error instead of having it be confusing for the reader.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->User experience.### Does this PR introduce _any_ user-facing change?Just the error that will be shown to the user.### How was this patch tested?Tested it out locally.
2	-1	### What changes were proposed in this pull request?Fixes `spark.sql` to return values from the command.### Why are the changes needed?Currently `spark.sql` doesn't return the result from the commands.```py>>> spark.sql(\"show functions\").show()+--------+|function|+--------++--------+```### Does this PR introduce _any_ user-facing change?`spark.sql` with commands will return the values.### How was this patch tested?Added a test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Adding a Spark Connect overview page to the Spark 3.4 documentation and a short section on the Spark overview page with a link to it.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?The first version of Spark Connect is released as part of Spark 3.4.0 and this adds an overview for it to the documentation.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the user facing documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built the doc website locally and tested the pages.SKIP_SCALADOC=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->index.html![index html](https://user-images.githubusercontent.com/112507318/224800134-7d0093bb-261a-4abe-a3da-b1e8784854c1.png)spark-connect-overview.html![spark-connect-overview html](https://user-images.githubusercontent.com/112507318/224800167-8f6308e0-2e0e-4b15-81a8-5bd9da12d0ce.png)
1	-1	### What changes were proposed in this pull request?This PR updates the developer documentation by removing the warnings about API compatibility.### Why are the changes needed?We actually are going to keep the compatibility there. Such warnings cause a misconception that we can just break the compatibility there.### Does this PR introduce _any_ user-facing change?No, it's developer-only docs.### How was this patch tested?Linters in CI should test it out.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Improve README doc for developers about protobuf java file can't be indexed.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To make sure developers to start spark easier.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?NO<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?No<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-3	### What changes were proposed in this pull request?This PR proposes to remove the parent directory in `shell.py` execution when IPython is used. This is a general issue for PySpark shell specifically with IPython - IPython temporarily adds the parent directory of the script into the Python path (`sys.path`), which results in searching packages under `pyspark` directory. For example, `import pandas` attempts to import `pyspark.pandas`.So far, we haven't had such cases within PySpark itself importing code path, but Spark Connect now has the case via checking dependency checking (which attempts to import pandas) which exposes the actual problem.Running it with IPython can easily reproduce the error:```bashPYSPARK_PYTHON=ipython bin/pyspark --remote \"local[*]\"```### Why are the changes needed?To make PySpark shell properly import other packages even when the names conflict with subpackages (e.g., `pyspark.pandas` vs `pandas`)### Does this PR introduce _any_ user-facing change?No to the end users:- Because this path is only inserted for `shell.py` execution, and thankfully we didn't have such relative import case so far.- It fixes the issue in the unreleased, Spark Connect.### How was this patch tested?Manually tested.```bashPYSPARK_PYTHON=ipython bin/pyspark --remote \"local[*]\"```**Before:**```Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:42:20)Type 'copyright', 'credits' or 'license' for more informationIPython 8.10.0 -- An enhanced Interactive Python. Type '?' for help./.../spark/python/pyspark/shell.py:45: UserWarning: Failed to initialize Spark session.  warnings.warn(\"Failed to initialize Spark session.\")Traceback (most recent call last):  File \"/.../spark/python/pyspark/shell.py\ line 40, in <module>    spark = SparkSession.builder.getOrCreate()  File \"/.../spark/python/pyspark/sql/session.py\ line 437, in getOrCreate    from pyspark.sql.connect.session import SparkSession as RemoteSparkSession  File \"/.../spark/python/pyspark/sql/connect/session.py\ line 19, in <module>    check_dependencies(__name__, __file__)  File \"/.../spark/python/pyspark/sql/connect/utils.py\ line 33, in check_dependencies    require_minimum_pandas_version()  File \"/.../spark/python/pyspark/sql/pandas/utils.py\ line 27, in require_minimum_pandas_version    import pandas  File \"/.../spark/python/pyspark/pandas/__init__.py\ line 29, in <module>    from pyspark.pandas.missing.general_functions import MissingPandasLikeGeneralFunctions  File \"/.../spark/python/pyspark/pandas/__init__.py\ line 34, in <module>    require_minimum_pandas_version()  File \"/.../spark/python/pyspark/sql/pandas/utils.py\ line 37, in require_minimum_pandas_version    if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):AttributeError: partially initialized module 'pandas' has no attribute '__version__' (most likely due to a circular import)...```**After:**```Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:42:20)Type 'copyright', 'credits' or 'license' for more informationIPython 8.10.0 -- An enhanced Interactive Python. Type '?' for help.Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/03/08 13:30:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Feb  1 2023 21:42:20)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.In [1]:```
1	-1	### What changes were proposed in this pull request?This PR proposes to add a check for `__file__` attributes.### Why are the changes needed?`__file__` might not be available everywhere. See also https://github.com/scikit-learn/scikit-learn/issues/20081### Does this PR introduce _any_ user-facing change?If users' Python environment does not have `__file__`, now users can use PySpark in their environment too.### How was this patch tested?Manually tested.
1	-1	### What changes were proposed in this pull request?Rename FrameMap proto to MapPartitions.### Why are the changes needed?For readability.Frame Map API refers to mapInPandas and mapInArrow, which are equivalent to MapPartitions.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
2	-1	### What changes were proposed in this pull request?Improve docstring of mapInPandas and mapInArrow ### Why are the changes needed?For readability. We call out they are not scalar - the input and output of the function might be of different sizes.### Does this PR introduce _any_ user-facing change?No. Doc change only.### How was this patch tested?Existing tests.
1	-1	### What changes were proposed in this pull request?Add '__getattr__' and '__getitem__' of DataFrame and Column to API reference### Why are the changes needed? '__getattr__' and '__getitem__' are widely used, but we did not document them.### Does this PR introduce _any_ user-facing change?yes, new doc### How was this patch tested?added doctests
1	-1	### What changes were proposed in this pull request?This pr add a new proto message ```message Parse {  // (Required) Input relation to Parse. The input is expected to have single text column.  Relation input = 1;  // (Required) The expected format of the text.  ParseFormat format = 2;  // (Optional) DataType representing the schema. If not set, Spark will infer the schema.  optional DataType schema = 3;  // Options for the csv/json parser. The map key is case insensitive.  map<string, string> options = 4;  enum ParseFormat {    PARSE_FORMAT_UNSPECIFIED = 0;    PARSE_FORMAT_CSV = 1;    PARSE_FORMAT_JSON = 2;  }}```and implement CSV/JSON parsing functions for Scala client.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass Github Actions- Manual checked Scala 2.13
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR fixes a few issues of parameterized query:1. replace placeholders in CTE/subqueries2. don't replace placeholders in non-DML commands as it may store the original SQL text with placeholders and we can't resolve it later (e.g. CREATE VIEW).### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->make the parameterized query feature complete### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, bug fix### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
2	-2	### What changes were proposed in this pull request?A `DataSourceV2` reporting a `KeyGroupedPartitioning` through `SupportsReportPartitioning` does not have to implement `HasPartitionKey`, and is thus not limited to a single key per partition.### Why are the changes needed?Before Spark 3.3, `DataSourceV2` implementations could report a `ClusteredDistribution` with more than just a single cluster value per partition. This allowed Spark to exploit the existing partitioning and avoid an extra hash partitioning shuffle step. Transformations like `groupBy` or window functions (with the right group / partitioning keys) would then be executed on the partitioning provided by the data source.### Does this PR introduce _any_ user-facing change?This improves performance as existing partitioning is reused.### How was this patch tested?Existing tests have been fixed. They falsely reported partitions with multiple keys via `HasPartitionKey`.
1	-2	### What changes were proposed in this pull request?The pr aims to upgrade mysql-connector-java from 8.0.31 to 8.0.32### Why are the changes needed?1.This version brings some bugs fixes, the release note as follows:https://dev.mysql.com/doc/relnotes/connector-j/8.0/en/news-8-0-32.html    2.Bugs Fixed(https://dev.mysql.com/doc/relnotes/connector-j/8.0/en/news-8-0-32.html#connector-j-8-0-32-bug), eg:<img width=\"955\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/223701685-b73c140f-1485-4c1b-83ef-a0df02866176.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-2	### What changes were proposed in this pull request?This PR proposes to document Spark SQL error classes to [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html).- Error Conditions<img width=\"1077\" alt=\"Screen Shot 2023-03-08 at 8 54 43 PM\" src=\"https://user-images.githubusercontent.com/44108233/223706823-7817b57d-c032-4817-a440-7f79119fa0b4.png\">- SQLSTATE Codes<img width=\"1139\" alt=\"Screen Shot 2023-03-08 at 8 54 54 PM\" src=\"https://user-images.githubusercontent.com/44108233/223706860-3f64b00b-fa0d-47e0-b154-0d7be92b8637.png\">- Error Classes that includes sub-error classes (`INVALID_FORMAT` as an example)<img width=\"1045\" alt=\"Screen Shot 2023-03-08 at 9 10 22 PM\" src=\"https://user-images.githubusercontent.com/44108233/223709925-74144f41-8836-45dc-b851-5d96ac8aa38c.png\">### Why are the changes needed?To improve the usability for error messages for Spark SQL.### Does this PR introduce _any_ user-facing change?No API change, but yes, it's user-facing documentation.### How was this patch tested?Manually built docs and check the contents one-by-one compare to [error-classes.json](https://github.com/apache/spark/blob/master/core/src/main/resources/error/error-classes.json).
1	-1	### What changes were proposed in this pull request?This pr aims to upgrade rocksdbjni from 7.9.2 to 7.10.2.### Why are the changes needed?This version bring the performance related DB reads:- https://github.com/facebook/rocksdb/pull/10975The full release notes as follows:- https://github.com/facebook/rocksdb/releases/tag/v7.10.2 ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual test `RocksDBBenchmark`:**7.9.2**```[INFO] Running org.apache.spark.util.kvstore.RocksDBBenchmark                                                count   mean    min     max     95thdbClose                                         4       0.406   0.288   0.706   0.706dbCreation                                      4       68.701  4.019   262.992 262.992naturalIndexCreateIterator                      1024    0.005   0.002   1.473   0.007naturalIndexDescendingCreateIterator            1024    0.006   0.005   0.061   0.007naturalIndexDescendingIteration                 1024    0.007   0.004   0.278   0.011naturalIndexIteration                           1024    0.007   0.004   0.071   0.012randomDeleteIndexed                             1024    0.042   0.031   0.415   0.060randomDeletesNoIndex                            1024    0.015   0.013   0.044   0.018randomUpdatesIndexed                            1024    0.085   0.032   30.240  0.094randomUpdatesNoIndex                            1024    0.054   0.050   0.740   0.059randomWritesIndexed                             1024    0.127   0.034   54.828  0.125randomWritesNoIndex                             1024    0.063   0.053   1.895   0.087refIndexCreateIterator                          1024    0.004   0.003   0.022   0.005refIndexDescendingCreateIterator                1024    0.003   0.003   0.026   0.004refIndexDescendingIteration                     1024    0.006   0.005   0.044   0.008refIndexIteration                               1024    0.010   0.005   0.051   0.015sequentialDeleteIndexed                         1024    0.022   0.017   0.119   0.032sequentialDeleteNoIndex                         1024    0.015   0.012   0.046   0.018sequentialUpdatesIndexed                        1024    0.069   0.055   2.321   0.098sequentialUpdatesNoIndex                        1024    0.067   0.046   0.927   0.088sequentialWritesIndexed                         1024    0.085   0.055   2.308   0.113sequentialWritesNoIndex                         1024    0.055   0.046   2.872   0.060```**7.10.2**```[INFO] Running org.apache.spark.util.kvstore.RocksDBBenchmark                                                count   mean    min     max     95thdbClose                                         4       0.348   0.287   0.521   0.521dbCreation                                      4       69.416  3.810   267.620 267.620naturalIndexCreateIterator                      1024    0.005   0.002   1.389   0.007naturalIndexDescendingCreateIterator            1024    0.005   0.005   0.057   0.007naturalIndexDescendingIteration                 1024    0.006   0.004   0.260   0.009naturalIndexIteration                           1024    0.006   0.004   0.053   0.011randomDeleteIndexed                             1024    0.028   0.019   0.306   0.046randomDeletesNoIndex                            1024    0.015   0.012   0.045   0.018randomUpdatesIndexed                            1024    0.086   0.032   29.664  0.109randomUpdatesNoIndex                            1024    0.033   0.029   0.817   0.036randomWritesIndexed                             1024    0.120   0.033   52.703  0.137randomWritesNoIndex                             1024    0.039   0.033   1.734   0.044refIndexCreateIterator                          1024    0.004   0.004   0.017   0.006refIndexDescendingCreateIterator                1024    0.003   0.002   0.027   0.004refIndexDescendingIteration                     1024    0.006   0.005   0.043   0.008refIndexIteration                               1024    0.007   0.005   0.056   0.010sequentialDeleteIndexed                         1024    0.022   0.017   1.388   0.024sequentialDeleteNoIndex                         1024    0.015   0.012   0.037   0.018sequentialUpdatesIndexed                        1024    0.041   0.035   0.852   0.050sequentialUpdatesNoIndex                        1024    0.038   0.030   0.682   0.050sequentialWritesIndexed                         1024    0.048   0.038   2.031   0.062sequentialWritesNoIndex                         1024    0.035   0.030   2.617   0.039```
3	-2	### What changes were proposed in this pull request?I've convert internal typing symbols in `__init__.py` to private. When these changes are agreed upon, I can expand this MR with more `TypeVar` this applies to.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Editors consider all public symbols in a library importable. A common pattern is to use a shorthand for pyspark functions:```pythonimport pyspark.sql.functions as FF.col(...)```Since `pyspark.F` is a valid symbol according to `__init__.py`, editors will suggest this to users, while it is not a valid use-case of pyspark.This change is in line with Pyright's [Typing Guidance for Python Libraries](https://github.com/microsoft/pyright/blob/main/docs/typed-libraries.md#generic-classes-and-functions)<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Verified with PyCharm auto-importing.
2	-1	### What changes were proposed in this pull request?`MapOutputTracker#getMapLocation` should respect `spark.shuffle.reduceLocality.enabled`### Why are the changes needed?Discuss as https://github.com/apache/spark/pull/40307getPreferredLocations in ShuffledRowRDD should return Nil at the very beginning in case spark.shuffle.reduceLocality.enabled = false (conceptually).This logic is pushed into MapOutputTracker though - and getPreferredLocationsForShuffle honors spark.shuffle.reduceLocality.enabled - but getMapLocation does not.So the fix would be to fix getMapLocation to honor the parameter.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?New ut
2	-3	### What changes were proposed in this pull request?In the PR, I propose to add new function `try_aes_decrypt()` which binds to new expression `TryAesDecrypt` that is a runtime replaceable expression of the combination of `TryEval` and `AesDecrypt`.### Why are the changes needed?The changes improve user experience with Spark SQL. The existing function `aes_decrypt()` fails w/ an exception as soon as it faces to some invalid input that cannot be decrypted, and the rest (even if the values can be decrypted) is ignored. New function returns `NULL` on bad inputs and decrypts other values.### Does this PR introduce _any_ user-facing change?No. This PR just extends existing API.### How was this patch tested?By running the affected test suites:```$ build/sbt \"sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite\"$ build/sbt \"sql/testOnly *ExpressionsSchemaSuite\"```
2	-2	### What changes were proposed in this pull request?In orc batch read, the byte arrays is used to store the data of the read columns. When the total data of this batch exceeds Int.MaxValue can be caused NegativeArraySizeException, catch and throw the same exeception with a friendly msg.### Why are the changes needed?Friendly msg where read orc file get exception about java.lang.NegativeArraySizeException.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
2	-2	### What changes were proposed in this pull request?This adds an gRPC interceptor in spark-connect server. It logs all the incoming RPC requests and responses.  - How to enable: Set interceptor config. e.g.        ./sbin/start-connect-server.sh --conf spark.connect.grpc.interceptor.classes=org.apache.spark.sql.connect.service.LoggingInterceptor  --jars connector/connect/server/target/spark-connect_*-SNAPSHOT.jar  - Sample output:        23/03/08 10:54:37 INFO LoggingInterceptor: Received RPC Request spark.connect.SparkConnectService/ExecutePlan (id 1868663481):           {          \"client_id\": \"6844bc44-4411-4481-8109-a10e3a836f97\          \"user_context\": {            \"user_id\": \"raghu\"          },          \"plan\": {            \"root\": {              \"common\": {                \"plan_id\": \"37\"              },              \"show_string\": {                \"input\": {                  \"common\": {                    \"plan_id\": \"36\"                  },                  \"read\": {                    \"data_source\": {                      \"format\": \"csv\                      \"schema\": \"\                      \"paths\": [\"file:///tmp/x-in\"]                    }                  }                },                \"num_rows\": 20,                \"truncate\": 20              }            }          },          \"client_type\": \"_SPARK_CONNECT_PYTHON\"        }      ### Why are the changes needed?This is useful in  development. It might be useful to debug some problems in production as well.### Does this PR introduce _any_ user-facing change?no### How was this patch tested? - Manually in development - Unit test
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As of now that Connect Python client cache the schema when calling `def schema()`. However this might cause stale data issue. For example:```1. Create table2. table.schema3. drop table and recreate the table with different schema4. table.schema // now this is incorrect```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->Fix the behavior when the cached schema could be stale.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This is actually a fix that users now can always see the most up-to-dated schema.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
3	-2	### What changes were proposed in this pull request?The spark-connect script is broken as it need a jar at the end.Also ensured when scala 2.13 is set, all commands in the scripts runs with `-PScala-2.13`Example usage:Start spark connect with default settings: * `./connector/connect/bin/spark-connect-shell` * or `./connector/connect/bin/spark-connect` (Enter \"q\" <new line> to exit the program)Start Scala client with default settings: `./connector/connect/bin/spark-connect-scala-client`Start spark connect with extra configs:* `./connector/connect/bin/spark-connect-shell --conf spark.connect.grpc.binding.port=8888`* or `./connector/connect/bin/spark-connect --conf spark.connect.grpc.binding.port=8888`Start Scala client with a connection string:```export SPARK_REMOTE=\"sc://localhost:8888/\"./connector/connect/bin/spark-connect-scala-client```### Why are the changes needed?Bug fix### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually tested on 2.12 and 2.13 for all the scripts changed.Test example with expected results:`./connector/connect/bin/spark-connect-shell` :<img width=\"1050\" alt=\"Screen Shot 2023-03-08 at 2 14 31 PM\" src=\"https://user-images.githubusercontent.com/4190164/223863343-d5d159d9-da7c-47c7-b55a-a2854c5f5d76.png\">Verify the spark connect server is started at the correct port, e.g. ```>Telnet localhost 15002 Trying ::1...Connected to localhost.Escape character is '^]'.````./connector/connect/bin/spark-connect`:<img width=\"1680\" alt=\"Screen Shot 2023-03-08 at 2 13 09 PM\" src=\"https://user-images.githubusercontent.com/4190164/223863099-41195599-c49d-4db4-a1e2-e129a649cd81.png\">Server started successfully when seeing the last line output.`./connector/connect/bin/spark-connect-scala-client`:<img width=\"1658\" alt=\"Screen Shot 2023-03-08 at 2 11 58 PM\" src=\"https://user-images.githubusercontent.com/4190164/223862992-c8a3a36a-9f69-40b8-b82e-5dab85ed14ce.png\">Verify the client can run some simple quries.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support parsing `timestamp_ltz` as `TimestampType` in schema JSON string.It also add tests for both parsing JSON/DDL for \"timestamp_ltz\" and \"timestamp_ntz\"### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`timestamp_ltz` becomes an alias for TimestampType since Spark 3.4### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the new keyword is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SparkSession created by newSession should not share the channel. This is because that a SparkSession might be called `stop` in which the channel it uses will be shutdown. If the channel is shared, other non-stop SparkSession that is sharing this channel will get into trouble.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This fixes the issue when one SparkSession is stopped to cause other active SparkSession not working in Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The build/sbt script's usage information have several error. See the info below:```(base) spark% ./build/sbt -helpUsage:  [options]  -h | -help         print this message  -v | -verbose      this runner is chattier```There is no script name after the usage. With this change, the info become:```(base) spark % ./build/sbt -helpUsage: sbt [options]  -h | -help         print this message  -v | -verbose      this runner is chattier```This changes also fix several shellcheck error. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug fix.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the user of build/sbt tool will see usage information updated.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual check the script codes.
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade buf from 1.15.0 to 1.15.1### Why are the changes needed?Release Notes: https://github.com/bufbuild/buf/releaseshttps://github.com/bufbuild/buf/compare/v1.15.0...v1.15.1Manually test: dev/connect-gen-protos.shThis upgrade will not change the generated files.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually test and Pass GA.
2	-1	### What changes were proposed in this pull request?Make LiteralExpression support array### Why are the changes needed?MLIib requires literal to carry the array params, like  `IntArrayParam`, `DoubleArrayArrayParam`.Note that this PR doesn't affect existing `functions.lit` method which apply unresolved `CreateArray` expression to support array input.### Does this PR introduce _any_ user-facing change?No, dev-only### How was this patch tested?added UT
2	-1	### What changes were proposed in this pull request?Implement `DataFrame.mapInArrow`.### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. `DataFrame.mapInArrow` is supported as shown below.```>>> import pyarrow>>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\ \"age\"))>>> def filter_func(iterator):...   for batch in iterator:...     pdf = batch.to_pandas()...     yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])... >>> df.mapInArrow(filter_func, df.schema).show()+---+---+                                                                       | id|age|+---+---+|  1| 21|+---+---+```### How was this patch tested?Unit tests.
2	-2	### What changes were proposed in this pull request?Special treatment for the root directory when split `userClassPath`### Why are the changes needed?I found that executing the spark command in the \"/\" directory will report an error. The reason is that `userClassPath` is split according to \"/\"**Method to reproduce the issue:**<img width=\"631\" alt=\"image\" src=\"https://user-images.githubusercontent.com/35296098/223975469-18a3dd6a-7fc4-40c4-b6c1-7c9e62e8f48d.png\">**Exception information：**```23/03/09 17:10:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.23/03/09 17:10:53 ERROR SparkContext: Error initializing SparkContext.java.util.NoSuchElementException: next on empty iterator\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:41)\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\tat scala.collection.IterableLike.head(IterableLike.scala:109)\tat scala.collection.IterableLike.head$(IterableLike.scala:108)\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:198)\tat scala.collection.IndexedSeqOptimized.head(IndexedSeqOptimized.scala:129)\tat scala.collection.IndexedSeqOptimized.head$(IndexedSeqOptimized.scala:129)\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:198)\tat scala.collection.TraversableLike.last(TraversableLike.scala:519)\tat scala.collection.TraversableLike.last$(TraversableLike.scala:518)\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$last(ArrayOps.scala:198)\tat scala.collection.IndexedSeqOptimized.last(IndexedSeqOptimized.scala:135)\tat scala.collection.IndexedSeqOptimized.last$(IndexedSeqOptimized.scala:135)\tat scala.collection.mutable.ArrayOps$ofRef.last(ArrayOps.scala:198)\tat org.apache.spark.executor.Executor.$anonfun$createClassLoader$1(Executor.scala:869)\tat org.apache.spark.executor.Executor.$anonfun$createClassLoader$1$adapted(Executor.scala:868)\tat scala.collection.immutable.List.foreach(List.scala:392)\tat org.apache.spark.executor.Executor.createClassLoader(Executor.scala:868)\tat org.apache.spark.executor.Executor.<init>(Executor.scala:159)\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:595)\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2681)\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)\tat scala.Option.getOrElse(Option.scala:189)\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:52)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:334)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:166)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)23/03/09 17:10:53 ERROR Utils: Uncaught exception in thread mainjava.lang.NullPointerException\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.org$apache$spark$scheduler$local$LocalSchedulerBackend$$stop(LocalSchedulerBackend.scala:173)\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:144)\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:881)\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2371)\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2078)\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1489)\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2078)\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:674)\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2681)\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)\tat scala.Option.getOrElse(Option.scala:189)\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:52)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:334)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:166)\tat org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\tat java.lang.reflect.Method.invoke(Method.java:498)\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?
1	-1	### What changes were proposed in this pull request?This is pr using `BloomFilterAggregate` to implement `bloomFilter` function for `DataFrameStatFunctions`. ### Why are the changes needed?Add Spark connect jvm client api coverage. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test - Manually check Scala 2.13
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update spark connect session `getOrCreate` behavior to check existing global `_active_spark_session` first, if existing, return it.Spark connect ML needs this API to get active session in some cases (e.g. fetching model attributes from server side).### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Manually.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. Implemented `pyspark.sql.connect.session.SparkSession.getActiveSession` API. ### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
2	-1	### What changes were proposed in this pull request?This PR changes `RemoteSparkSession` to add an overrideable `sparkConf` field that can be used by tests to pass additional configurations ### Why are the changes needed?Makes `RemoteSparkSession` more like `SharedSparkSession`. This allows plugins for Spark to reuse `RemoteSparkSession` while also being able to enable their extensions to the Spark Connect planner.### Does this PR introduce _any_ user-facing change?No, this is a test-only change.### How was this patch tested?Using existing tests
2	-1	### What changes were proposed in this pull request?Spark connect already supported `functions.lit`, but `functions.typedlit`.This PR add some new msg to the connect protocol and support `functions.typedlit`.### Why are the changes needed?Spark connect need to add `functions.typedlit`.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
2	-2	### What changes were proposed in this pull request?Fixes `DataFrameWriter.save` to work without path parameter.### Why are the changes needed?`DataFrameWriter.save` should work without path parameter because some data sources, such as jdbc, noop, works without those parameters.```py>>> print(spark.range(10).write.format(\"noop\").mode(\"append\").save())Traceback (most recent call last):...AssertionError: Invalid configuration of WriteCommand, neither path or table present.```### Does this PR introduce _any_ user-facing change?The data sources that don't need path parameter will work.```py>>> print(spark.range(10).write.format(\"noop\").mode(\"append\").save())None```### How was this patch tested?Added a test.
3	-1	### What changes were proposed in this pull request?In the release script, add a check to ensure release tag to be pushed to release branch.### Why are the changes needed?To ensure the success of a RC cut. Otherwise, release conductors have to manually check that.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.```~/spark [_d_branch] $ git commit -am '_d_commmit'...~/spark [_d_branch] $ git tag '_d_tag'~/spark [_d_branch] $ git push origin _d_tag~/spark [_d_branch] $ git branch -r --contains tags/_d_tag | grep origin~/spark [_d_branch] $ echo $?1~/spark [_d_branch] $ git push origin HEAD:_d_branch...~/spark [_d_branch] $ git branch -r --contains tags/_d_tag | grep origin  origin/_d_branch~/spark [_d_branch] $ echo $?                                           0```In tags.log, there will be```To https://gitbox.apache.org/repos/asf/spark.git   49cf58e30c..bc1671023c  HEAD -> branch-3.4  origin/branch-3.4Pushed v3.4.0-rc4 to branch-3.4.```
2	-2	### What changes were proposed in this pull request?Fixes `DataFrameWriter.save` to work without path or table parameter.Added support of jdbc method in the writer as it is one of the impl that does not contains a path or table.### Why are the changes needed?DataFrameWriter.save should work without path parameter because some data sources, such as jdbc, noop, works without those parameters.The follow up fix for scala client of https://github.com/apache/spark/pull/40356### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit and E2E test
2	-2	### What changes were proposed in this pull request?Currently, the DS V2 pushdown framework pushed offset as `OFFSET n` in default and pushed it with limit as `LIMIT m OFFSET n`. But some built-in dialect doesn't support these syntax. So, when Spark pushdown offset into these databases, them throwing errors.### Why are the changes needed?Fix the bug that pushdown offset or paging is invalid for some built-in dialect.### Does this PR introduce _any_ user-facing change?'Yes'.The bug will be fixed.### How was this patch tested?New test cases.
1	-1	### What changes were proposed in this pull request?This PR makes `UnwrapCastInBinaryComparison` not to unwrap casts in binary comparison when literal is null.### Why are the changes needed?In order to make the logic of `UnwrapCastInBinaryComparison` more clear. Null literals are already handled by `NullPropagation`:https://github.com/apache/spark/blob/2de0d45887509fac8d5fc9448764a0e71f618797/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L823-L824https://github.com/apache/spark/blob/2de0d45887509fac8d5fc9448764a0e71f618797/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L850-L851### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When start spark on k8s，driver pod  use spark.kubernetes.driver.master to get apiserver address. This config  us  https://kubernetes.default.svc/ as default and do not care about the apiserver port.In our case, apiserver port is not 443 will driver will throw connectException. As k8s doc mentioned （https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/#directly-accessing-the-rest-api）, we can get master url by getting KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT_HTTPS environment variables from pod. So we add a new conf spark.kubernetes.driver.master.from.pod.env to allow driver get master url from env in cluster mode on k8s### Why are the changes needed?Add a new conf spark.kubernetes.driver.master.from.pod.env  to let the driver pod get apiserver automatically from pod env instead of by  spark.kubernetes.driver.master.### Does this PR introduce _any_ user-facing change?Yes. When user set new conf spark.kubernetes.driver.master.from.pod.env as true, the logic of driver get apiserver url will changed. In some case it will help user to get right apiserver url.By default, the conf spark.kubernetes.driver.master.from.pod.env  is false, and the driver logic changes nothing.### How was this patch tested?No. the apiserver is mocked in unit test. we tested this feature in our k8s cluster
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support analyze TimestampNTZ columns ```ANALYZE TABLE table_name [ PARTITION clause ]    COMPUTE STATISTICS [ NOSCAN | FOR COLUMNS col1 [, ...] | FOR ALL COLUMNS ]```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Support computing statistics of TimestmapNTZ columns, which can be used for optimizations.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, the TimestampNTZ type is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Update existing UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Let driver delete uploaded file when job finish.### Why are the changes needed?Now there is no deletion for files uploaded by client, which causes file leaks on remote file system.### Does this PR introduce _any_ user-facing change?Yes. This PR add a new configuration spark.kubernetes.uploaded.file.delete.on.termination. By default, this configuration is false and the behavior is the same with current version. When the configuration is set to true, driver will try to delete uploaded files when job finish.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-3	### What changes were proposed in this pull request?After https://github.com/apache/spark/pull/37525 (SPARK-40086 / SPARK-42049) the following, simple subselect expression containing query:```select (select sum(id) from t1)```fails with:```09:48:57.645 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 3.0 (TID 3)java.lang.NullPointerException\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch$lzycompute(BatchScanExec.scala:47)\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch(BatchScanExec.scala:47)\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.hashCode(BatchScanExec.scala:60)\tat scala.runtime.Statics.anyHash(Statics.java:122)        ...\tat org.apache.spark.sql.catalyst.trees.TreeNode.hashCode(TreeNode.scala:249)\tat scala.runtime.Statics.anyHash(Statics.java:122)\tat scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)\tat scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)\tat scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)\tat scala.collection.mutable.HashTable.addEntry(HashTable.scala:149)\tat scala.collection.mutable.HashTable.addEntry$(HashTable.scala:148)\tat scala.collection.mutable.HashMap.addEntry(HashMap.scala:44)\tat scala.collection.mutable.HashTable.init(HashTable.scala:110)\tat scala.collection.mutable.HashTable.init$(HashTable.scala:89)\tat scala.collection.mutable.HashMap.init(HashMap.scala:44)\tat scala.collection.mutable.HashMap.readObject(HashMap.scala:195)        ...\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:85)\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\tat java.lang.Thread.run(Thread.java:750)```when DSv2 is enabled.This PR proposes to fix `BatchScanExec` as its `equals()` and `hashCode()` as those shouldn't throw NPE in any circumstances.But if we dig deeper we realize that the NPE orrurs since https://github.com/apache/spark/pull/37525 and the root cause of the problem is changing `AliasAwareOutputExpression.aliasMap` from immutable to mutable. The mutable map deserialization invokes the `hashCode()` of the keys while that is not the case with immutable maps. In this case the key is a subquery expression whose plan contains the `BatchScanExec`.Please note that the mutability of `aliasMap` shouldn't be an issue as it is a `private` field of `AliasAwareOutputExpression` (though adding a simple `.toMap` would also help to avoid the NPE).Based on the above findings this PR also proposes making `aliasMap` to transient as it isn't needed on executors.A side quiestion is if adding any subqery expressions to `AliasAwareOutputExpression.aliasMap` makes any sense because `AliasAwareOutputExpression.projectExpression()` mainly projects `child.outputPartitioning` and `child.outputOrdering` that can't contain subquery expressions. But there are a few exceptions (`SortAggregateExec`, `TakeOrderedAndProjectExec`) where `AliasAwareQueryOutputOrdering.orderingExpressions` doesn't come from the `child` and actually leaving those expressions in the map doesn't do any harm.### Why are the changes needed?To fix regression introduced with https://github.com/apache/spark/pull/37525.### Does this PR introduce _any_ user-facing change?Yes, the query works again.### How was this patch tested?Added new UT.
1	-2	### What changes were proposed in this pull request?Fix incorrect comment in `LimitPushDownThroughWindow`.### Why are the changes needed?Fix incorrect comment.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?No need.
1	-1	### What changes were proposed in this pull request?Implement `Dataset.semanticHash` for scala and python API of Spark connect.### Why are the changes needed?Implement `Dataset.semanticHash` for scala and python API of Spark connect.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
2	-3	### What changes were proposed in this pull request?Add a hook `onParamChange` in `Params.{set, setDefault, clear}`, so that subclass can update the internal status within it.### Why are the changes needed?In 3.1, we added internal auxiliary variables in LoR and AFT to optimize prediction/transformation.In LoR, when users call `model.{setThreshold, setThresholds}`, the internal status will be correctly updated.But users still can call `model.set(model.threshold, value)`, then the status will not be updated.And when users call `model.clear(model.threshold)`, the status should be updated with default threshold value 0.5.for example:```import org.apache.spark.ml.linalg._import org.apache.spark.ml.classification._val df = Seq((1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0, 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0))).toDF(\"label\ \"weight\ \"features\")val lor = new LogisticRegression().setWeightCol(\"weight\")val model = lor.fit(df)val vec = Vectors.dense(0.0, 5.0)val p0 = model.predict(vec)                                               //  return 0.0model.setThreshold(0.05)                                                 //  change statusval p1 = model.set(model.threshold, 0.5).predict(vec)   //  return 1.0; but should be 0.0val p2 = model.clear(model.threshold).predict(vec)      //  return 1.0; but should be 0.0```what makes it even worse it that `pyspark.ml` always set params via `model.set(model.threshold, value)`, so the internal status is easily out of sync, see the example in [SPARK-42747](https://issues.apache.org/jira/browse/SPARK-42747)### Does this PR introduce _any_ user-facing change?no### How was this patch tested?added ut
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds server-side artifact management as a follow up to the client-side artifact transfer introduced in https://github.com/apache/spark/pull/40256.Note: The artifacts added on the server are visible to **all users** of the cluster. This is a limitation of the current spark architecture (unisolated classloaders).Apart from storing generic artifacts, we handle jars and classfiles in specific ways:- Jars:   - Jars may be added but not removed or overwritten.  - Added jars would be visible to **all** users/tasks/queries.- Classfiles:  - Classfiles may not be explicitly removed but are allowed to be overwritten.  - We piggyback on top of the REPL architecture to serve classfiles to the executors    -  If a REPL is initialized, classfiles are stored in the existing `spark.repl.class.outputDir` and share the URI with `spark.repl.class.uri`.    - If a REPL is not being used, we use a custom directory (root: `sparkContext. sparkConnectArtifactDirectory`) to store classfiles and point the `spark.repl.class.uri` towards it.  - Class files are visible to **all** users/tasks/queries.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->https://github.com/apache/spark/pull/40256 implements the client-side transfer of artifacts to the server but currently, the server does not process these requests.We need to implement a server-side management mechanism to handle the storage of these artifacts on the driver as well as perform further processing (such as adding jars and moving class files to the right directories).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, a new experimental API but no behavioural changes.A new method called `sparkConnectArtifactDirectory` is accessible through SparkContext (the directory storing all artifacts from SparkConnect)### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/40049 to fix a small issue: `DelegatingCatalogExtension` should also override the new `createTable` function and call the session catalog, instead of using the default implementation.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A, too trivial.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `inclusive` parameter for (DataFrame|Series).between_time to support the pandas 2.0.0### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, the API changesBefore:` (DataFrame|Series).between_time(start_time, end_time, include_start, include_end, axis)`After:` (DataFrame|Series).between_time(start_time, end_time, inclusive, axis)`### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests were updated
3	-3	This reverts commit 827ca9b82476552458e8ba7b01b90001895e8384.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->After more thinking, it's a bit fragile to propagate metadata columns through Union. We have added quite some new fields in the file source `_metadata` metadata column such as `row_index`, `block_start`, etc. Some are parquet only. The same thing may happen in other data sources as well. If one day one table under Union adds a new metadata column (or add a new field if the metadata column is a struct type), but other tables under Union do not have this new column, then Union can't propagate metadata columns and the query will suddenly fail to analyze.To be future-proof, let's revert this support.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to make the analysis behavior more robust.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, but propagating metadata columns through Union is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
3	-3	Ignore SQLConf initialization exceptions during Python exception creation.Otherwise there is no diagnostics for the issue in the following scenario:1. download a standard \"Hadoop Free\" build2. Start PySpark REPL with Hive support```bashSPARK_DIST_CLASSPATH=$(~/dist/hadoop-3.4.0-SNAPSHOT/bin/hadoop classpath) \\  ~/dist/spark-3.2.3-bin-without-hadoop/bin/pyspark --conf spark.sql.catalogImplementation=hive```3. Execute any simple dataframe operation```Python>>> spark.range(100).show()Traceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/session.py\ line 416, in range    jdf = self._jsparkSession.range(0, int(start), int(step), int(numPartitions))  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\ line 1321, in __call__  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/utils.py\ line 117, in deco    raise converted from Nonepyspark.sql.utils.IllegalArgumentException: <exception str() failed>```4. In fact just spark.conf already exhibits the issue```Python>>> spark.confTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/session.py\ line 347, in conf    self._conf = RuntimeConfig(self._jsparkSession.conf())  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\ line 1321, in __call__  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/utils.py\ line 117, in deco    raise converted from Nonepyspark.sql.utils.IllegalArgumentException: <exception str() failed>```There are probably two issues here:1) that Hive support should be gracefully disabled if it the dependency not on the classpath as claimed by https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html2) but at the very least the user should be able to see the exception to understand the issue, and take an action### What changes were proposed in this pull request?Ignore exceptions during `CapturedException` creation### Why are the changes needed?To make the cause visible to the user```PythonTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/home/user/gits/apache/spark/python/pyspark/sql/session.py\ line 679, in conf    self._conf = RuntimeConfig(self._jsparkSession.conf())  File \"/home/user/gits/apache/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\ line 1322, in __call__  File \"/home/user/gits/apache/spark/python/pyspark/errors/exceptions/captured.py\ line 166, in deco    raise converted from Nonepyspark.errors.exceptions.captured.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':JVM stacktrace:java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':        at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1237)        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)        at scala.Option.getOrElse(Option.scala:189)        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)        at org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)        at org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.lang.reflect.Method.invoke(Method.java:498)        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)        at py4j.Gateway.invoke(Gateway.java:282)        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)        at py4j.commands.CallCommand.execute(CallCommand.java:79)        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)        at java.lang.Thread.run(Thread.java:750)Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.hive.HiveSessionStateBuilder        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)        at java.lang.Class.forName0(Native Method)        at java.lang.Class.forName(Class.java:348)        at org.apache.spark.util.Utils$.classForName(Utils.scala:225)        at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1232)        ... 18 more```### Does this PR introduce _any_ user-facing change?The only semantic change is that the conf `spark.sql.pyspark.jvmStacktrace.enabled` is ignored if the SQLConf is broken. ### How was this patch tested?Manual testing using the repro steps above
2	-2	[This is not meant for merge, but a preliminary POC for streaming support in Spark Connect].This includes basic functionality to run streaming queries over spark connect. Expectation is 1:1 parity with standard streaming API.How to try it in local mode ( `./bin/pyspark --remote \"local[*]\"`)```>>> >>> query = ( ...   spark...     .readStream...     .format(\"rate\")...     .option(\"numPartitions\ \"1\")...     .load()...     .withWatermark(\"timestamp\ \"1 minute\")...     .groupBy(window(\"timestamp\ \"10 seconds\"))...     .count() # count for each 10 sedonds....     .writeStream...     .format(\"memory\")...     .queryName(\"rate_table\")...     .trigger(processingTime=\"10 seconds\")...     .start()... )>>>>>> query.isActiveTrue>>> >>> >>> spark.sql(\"select window.start, count from rate_table\").show()+-------------------+-----+|              start|count|+-------------------+-----+|2023-03-11 22:45:40|    6||2023-03-11 22:45:50|   10|+-------------------+-----+>>> >>> # Query Status>>> print(json.dumps(query.status, indent=4)){    \"message\": \"Waiting for next trigger\    \"isDataAvailable\": true,    \"isTriggerActive\": false}>>> # Streaming Progress (lastProgress)>>> print(json.dumps(query.lastProgress, indent=4)){    \"id\": \"88064cec-3418-4ad3-90aa-17043458f540\    \"runId\": \"24ff7f44-7f4b-424a-b632-dad62c6c92dd\    \"name\": \"rate_table\    \"timestamp\": \"2023-03-12T06:48:40.006Z\    \"batchId\": 18,    \"numInputRows\": 10,    \"inputRowsPerSecond\": 0.9997000899730081,    \"processedRowsPerSecond\": 33.670033670033675,    \"durationMs\": {        \"addBatch\": 109,        \"commitOffsets\": 85,        \"getBatch\": 0,        \"latestOffset\": 0,        \"queryPlanning\": 5,        \"triggerExecution\": 297,        \"walCommit\": 98    },    \"eventTime\": {        \"avg\": \"2023-03-12T06:48:34.306Z\        \"max\": \"2023-03-12T06:48:38.806Z\        \"min\": \"2023-03-12T06:48:29.806Z\        \"watermark\": \"2023-03-12T06:47:28.806Z\"    },    \"stateOperators\": [        {            \"operatorName\": \"stateStoreSave\            \"numRowsTotal\": 8,            \"numRowsUpdated\": 2,     [...]>>> query.stop()```### Next steps:We need add remaining features :  * Scala API * Streaming listener * Unimplemented / missing APIs. e.g.   * All the triggers   * to_table()    * text() etc. * StreamingQueryManager API * tests, including running existing tests with spark-connect * and some more.
2	-3	### What changes were proposed in this pull request?This is a follow-up to fix Scala linter failure at `LoggingInterceptor.scala`.### Why are the changes needed?To recover CI.- **master**: https://github.com/apache/spark/actions/runs/4389407261/jobs/7686936044- **branch-3.4**: https://github.com/apache/spark/actions/runs/4389407870/jobs/7686935027```The scalafmt check failed on connector/connect at following occurrences:Requires formatting: LoggingInterceptor.scalaBefore submitting your change, please make sure to format your code using the following command:./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvmError: Process completed with exit code 1.```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass the CI.
2	-1	### What changes were proposed in this pull request?Factor literal value conversion out to `connect-common`.### Why are the changes needed?when trying to build protos of literal array in the server side for ml, I found we have two implementations:`LiteralExpressionProtoConverter. toConnectProtoValue`  in server module, but it doesn't support array;`LiteralProtoConverter. toLiteralProto` in client module, it support more types;We'd better factor it out to common module, so that both client and server can leverage it.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing UT
2	-1	### What changes were proposed in this pull request?Helper function to convert proto literal to value in Python Client### Why are the changes needed?needed in .ml### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?added ut
1	-1	### What changes were proposed in this pull request?The pr aims to implement textFile for DataFrameReader.### Why are the changes needed?API coverage.### Does this PR introduce _any_ user-facing change?New method.### How was this patch tested?Add new UT.
1	-1	### What changes were proposed in this pull request?The pr aims to remove dependency `shapeless` on breeze.### Why are the changes needed?After pr https://github.com/apache/spark/pull/37002, `shapeless` has been deleted in [spark-deps-hadoop-2-hive-2.3](https://github.com/apache/spark/pull/37002/files#diff-670b971a2758f55d602f0d1ef63f7af5f8d9ca095b5a55664bc3275e274ca395).### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-1	### What changes were proposed in this pull request?### Why are the changes needed?### Does this PR introduce _any_ user-facing change?### How was this patch tested?
3	-2	### What changes were proposed in this pull request?Point out  a misleading example in doc and give an explanation ### Why are the changes needed?The original statement is misleading, and if the code is copied, it will give an error.So I post the reason for the error, and a feasible solution to write multiple columns.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Only parts of the documentation changed, and I  successfully run the illustrative example code
1	-1	### What changes were proposed in this pull request?Upgrade fabric8:kubernetes-client from 6.4.1 to 6.5.0[Release notes](https://github.com/fabric8io/kubernetes-client/releases/tag/v6.5.0)### Why are the changes needed?### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA
3	-2	### What changes were proposed in this pull request?Fixes `spark.createDataFrame` to apply the given schema to work with non-nullable data types.### Why are the changes needed?Currently `spark.createDataFrame` won't work with non-nullable schema as below:```py>>> from pyspark.sql.types import *>>> schema_false = StructType([StructField(\"id\ IntegerType(), False)])>>> spark.createDataFrame([[1]], schema=schema_false)Traceback (most recent call last):...pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.```whereas it works fine with nullable schema:```py>>> from pyspark.sql.types import *>>> schema_false = StructType([StructField(\"id\ IntegerType(), False)])>>> spark.createDataFrame([[1]], schema=schema_false)DataFrame[id: int]```### Does this PR introduce _any_ user-facing change?`spark.createDataFrame` with non-nullable schema will work.### How was this patch tested?Added related tests.
1	-3	### What changes were proposed in this pull request?Track network connections during exec ID allocation.### Why are the changes needed?Disconnect log messages are overwhelmed making it difficult to isolate networking issues.### Does this PR introduce _any_ user-facing change?Log will become easier to read.### How was this patch tested?Existing tests indicate no regression, log only change.
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade `ZooKeeper` from 3.6.3 to 3.6.4.### Why are the changes needed?Release Notes: https://zookeeper.apache.org/doc/r3.6.4/releasenotes.htmlRoutine upgrade, ZooKeeper 3.6.4 contains some bugfixes:<img width=\"1184\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/224519753-071d7e03-bac0-48bd-bc3d-3dd04c2f8ae2.png\">Why is 3.6.4 but not higher?https://github.com/apache/spark/pull/37507https://github.com/apache/spark/pull/32572### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR addresses a rare bug with the EXPLAIN function and Spark UI that can happen when AQE takes effect with multiple ReusedExchange nodes. The bug causes the ReusedExchange to point to an unknown child since that child subtree was \"pruned\" in a previous AQE iteration. This PR fixes the issue by finding all the ReusedExchange nodes in the tree that have a `child` node that has NOT been processed in the final plan (meaning it has no ID or it has an incorrect ID generated from the previous AQE iteration). It then traverses the child subtree and generates correct IDs for them. We print this missing subtree in a new section called `Adaptively Optimized Out Exchanges`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Below is an example to demonstrate the root cause:> AdaptiveSparkPlan>   |-- SomeNode X (subquery xxx)>       |-- Exchange A>           |-- SomeNode Y>               |-- Exchange B> > Subquery:Hosting operator = SomeNode Hosting Expression = xxx dynamicpruning#388> AdaptiveSparkPlan>   |-- SomeNode M>       |-- Exchange C>           |-- SomeNode N>               |-- Exchange D> Step 1: Exchange B is materialized and the QueryStage is added to stage cacheStep 2: Exchange D reuses Exchange BStep 3: Exchange C is materialized and the QueryStage is added to stage cacheStep 4: Exchange A reuses Exchange CThen the final plan looks like:> AdaptiveSparkPlan>   |-- SomeNode X (subquery xxx)>       |-- Exchange A -> ReusedExchange (reuses Exchange C)> > > Subquery:Hosting operator = SomeNode Hosting Expression = xxx dynamicpruning#388> AdaptiveSparkPlan>   |-- SomeNode M>       |-- Exchange C -> PhotonShuffleMapStage ....>           |-- SomeNode N>               |-- Exchange D -> ReusedExchange (reuses Exchange B)> As a result, the ReusedExchange (reuses Exchange B) will refer to a non-exist node.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->**Explain Text Before and After****Before:**```+- ReusedExchange (105)(105) ReusedExchange [Reuses operator id: unknown]Output [3]: [sr_customer_sk#303, sr_store_sk#307, sum#413L]```**After:**```+- ReusedExchange (105)   +- Exchange (132)      +- * HashAggregate (131)         +- * Project (130)            +- * BroadcastHashJoin Inner BuildRight (129)               :- * Filter (128)               :  +- * ColumnarToRow (127)               :     +- Scan parquet hive_metastore.tpcds_sf1000_delta.store_returns (126)               +- ShuffleQueryStage (115), Statistics(sizeInBytes=5.7 KiB, rowCount=366, [d_date_sk#234 -> ColumnStat(Some(362),Some(2415022),Some(2488070),Some(0),Some(4),Some(4),None,2)], isRuntime=true)                  +- ReusedExchange (114)(105) ReusedExchange [Reuses operator id: 132]Output [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L](126) Scan parquet hive_metastore.tpcds_sf1000_delta.store_returnsOutput [4]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214]Batched: trueLocation: PreparedDeltaFileIndex [dbfs:/mnt/performance-datasets/2018TPC/tpcds-2.4/sf1000_delta/store_returns]PartitionFilters: [isnotnull(sr_returned_date_sk#214), dynamicpruningexpression(sr_returned_date_sk#214 IN dynamicpruning#329)]PushedFilters: [IsNotNull(sr_store_sk)]ReadSchema: struct<sr_customer_sk:int,sr_store_sk:int,sr_return_amt:decimal(7,2)>(127) ColumnarToRowInput [4]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214](128) FilterInput [4]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214]Condition : isnotnull(sr_store_sk#221)(114) ReusedExchange [Reuses operator id: 8]Output [1]: [d_date_sk#234](115) ShuffleQueryStageOutput [1]: [d_date_sk#234]Arguments: 2, Statistics(sizeInBytes=5.7 KiB, rowCount=366, [d_date_sk#234 -> ColumnStat(Some(362),Some(2415022),Some(2488070),Some(0),Some(4),Some(4),None,2)], isRuntime=true)(129) BroadcastHashJoinLeft keys [1]: [sr_returned_date_sk#214]Right keys [1]: [d_date_sk#234]Join type: InnerJoin condition: None(130) ProjectOutput [3]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225]Input [5]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225, sr_returned_date_sk#214, d_date_sk#234](131) HashAggregateInput [3]: [sr_customer_sk#217, sr_store_sk#221, sr_return_amt#225]Keys [2]: [sr_customer_sk#217, sr_store_sk#221]Functions [1]: [partial_sum(UnscaledValue(sr_return_amt#225)) AS sum#327L]Aggregate Attributes [1]: [sum#326L]Results [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L](132) ExchangeInput [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L]Arguments: hashpartitioning(sr_store_sk#221, 200), ENSURE_REQUIREMENTS, [plan_id=1791]```**Spark UI Before and After****Before:**<img width=\"339\" alt=\"Screenshot 2023-03-10 at 10 52 46 AM\" src=\"https://user-images.githubusercontent.com/83618776/224406011-e622ad11-37e6-48c6-b556-cd5c7708e237.png\">**After:**![image](https://user-images.githubusercontent.com/83618776/224406076-4fcbf918-2a8d-4776-b91a-36815752cf2a.png)### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests were added to `ExplainSuite`. And manually tested with ExplainSuite.
2	-3	### What changes were proposed in this pull request?Although this is weird because of `[MINOR][SQL][FOLLOWUP]'. This PR aims to recover the CI failure on the master branch from the following commit.https://github.com/apache/spark/commit/c75b3ab6f608037b6e210d60e29392f59d113a8a### Why are the changes needed?- https://github.com/apache/spark/actions/runs/4397853036![Screenshot 2023-03-12 at 1 23 49 PM](https://user-images.githubusercontent.com/9700541/224571434-55a3b27f-619a-4c28-83b8-25c5657c7ee7.png)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
2	-3	### What changes were proposed in this pull request?This PR aims to parameterize the max number of attempts for driver props fetcher in `KubernetesExecutorBackend` by introducing a new environment variable, `EXECUTOR_DRIVER_PROPS_FETCHER_MAX_ATTEMPTS`, for testing purpose.Note that this feature is proposed as a new environment variable because this happens before getting `SparkConf` from the driver.### Why are the changes needed?In case of K8s network issues, the executor pods could fail with `UnknownHostException`. `EXECUTOR_DRIVER_PROPS_FETCHER_MAX_ATTEMPTS` could be helpful in that case.```Caused by: java.io.IOException: Failed to connect to pi-....svc/<unresolved>:7078Caused by: java.net.UnknownHostException: pi-....svc```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
2	-2	### What changes were proposed in this pull request?Enable users to import pandas_udf via `pyspark.sql.connect.functions.pandas_udf`.Previously, only `pyspark.sql.functions.pandas_udf` is supported.### Why are the changes needed?Usability.### Does this PR introduce _any_ user-facing change?Yes. Now users can import pandas_udf via `pyspark.sql.connect.functions.pandas_udf`.Previously only `pyspark.sql.functions.pandas_udf` is supported in Connect; importing `pyspark.sql.connect.functions.pandas_udf` raises an error instead, as shown below```sh>>> pyspark.sql.connect.functions.pandas_udf()Traceback (most recent call last):...NotImplementedError: pandas_udf() is not implemented.```Now, `pyspark.sql.connect.functions.pandas_udf` point to `pyspark.sql.functions.pandas_udf`, as shown below,```sh>>> from pyspark.sql.connect import functions as CF>>> from pyspark.sql import functions as SF>>> getattr(CF, \"pandas_udf\")<function pandas_udf at 0x7f9c88812700>>>> getattr(SF, \"pandas_udf\")<function pandas_udf at 0x7f9c88812700>```### How was this patch tested?Unit test.
3	-4	### What changes were proposed in this pull request?This pr adds a precondition before `RemoteSparkSession` starts connect server to check whether `spark-hive-**.jar` exists in the `assembly/target/scala-*/jars` directory, and will fallback to using `spark.sql.catalogImplementation=in-memory` to start the connect server if `spark-hive-**.jar` doesn't exist. When using `spark.sql.catalogImplementation=in-memory` to start connect server, some test cases that strongly rely on the hive module will be ignored rather than fail rudely.  At the same time, developers can see the following message on the terminal:```[info] ClientE2ETestSuite:Will start Spark Connect server with `spark.sql.catalogImplementation=in-memory`, some tests that rely on Hive will be ignored. If you don't want to skip them:1. Test with maven: run `build/mvn install -DskipTests -Phive` before testing2. Test with sbt: run test with `-Phive` profile```### Why are the changes needed?Avoid rough failure of connect client module UTs due to lack of hive-related dependency.### Does this PR introduce _any_ user-facing change?No, just for test### How was this patch tested?- Manual  checked test with `-Phive` is same as before- Manual test:  - Mavenrun ```build/mvn clean install -DskipTestsbuild/mvn test -pl connector/connect/client/jvm ```**Before**```Run completed in 14 seconds, 999 milliseconds.Total number of tests run: 684Suites: completed 12, aborted 0Tests: succeeded 678, failed 6, canceled 0, ignored 1, pending 0*** 6 TESTS FAILED ***```**After**```Discovery starting.Discovery completed in 761 milliseconds.Run starting. Expected test count is: 684ClientE2ETestSuite:Will start Spark Connect server with `spark.sql.catalogImplementation=in-memory`, some tests that rely on Hive will be ignored. If you don't want to skip them:1. Test with maven: run `build/mvn install -DskipTests -Phive` before testing2. Test with sbt: run test with `-Phive` profile...Run completed in 15 seconds, 994 milliseconds.Total number of tests run: 682Suites: completed 12, aborted 0Tests: succeeded 682, failed 0, canceled 2, ignored 1, pending 0All tests passed.```  - SBT   run `build/sbt clean \"connect-client-jvm/test\"`**Before**```[info] ClientE2ETestSuite:[info] org.apache.spark.sql.ClientE2ETestSuite *** ABORTED *** (1 minute, 3 seconds)[info]   java.lang.RuntimeException: Failed to start the test server on port 15960.[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll(RemoteSparkSession.scala:129)[info]   at org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll$(RemoteSparkSession.scala:120)[info]   at org.apache.spark.sql.ClientE2ETestSuite.beforeAll(ClientE2ETestSuite.scala:37)[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[info]   at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:37)[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[info]   at java.lang.Thread.run(Thread.java:750)```**After**```[info] ClientE2ETestSuite:Will start Spark Connect server with `spark.sql.catalogImplementation=in-memory`, some tests that rely on Hive will be ignored. If you don't want to skip them:1. Test with maven: run `build/mvn install -DskipTests -Phive` before testing2. Test with sbt: run test with `-Phive` profile....[info] Run completed in 22 seconds, 44 milliseconds.[info] Total number of tests run: 682[info] Suites: completed 11, aborted 0[info] Tests: succeeded 682, failed 0, canceled 2, ignored 1, pending 0[info] All tests passed.```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr enables the `spark.sql.optimizer.canChangeCachedPlanOutputPartitioning` by default.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We have fixed all known issues when enable cache + AQE since SPARK-42101. There is no reason to skip AQE optimizing cached plan.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, the default config changed### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In production environment, we hit an issue like this:If we request 10 containers form nodeA and nodeB, first response from Yarn return 5 contianers from nodeA and nodeB, then nodeA blacklisted, and second response from Yarn maybe return some containers from nodeA and launching containers, but when containers(Executor) setup and send register request to Driver, it will be rejected and this failure will be counted to `spark.yarn.max.executor.failures` and will casue app failed: `Max number of executor failures ($maxNumExecutorFailures) reached`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Filtering excluded nodes when launching containers to avoid failing the app.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added UT
2	-1	### What changes were proposed in this pull request?Like `SPARK_EXECUTOR_POD_IP`, this PR aims to add a new environment variable `ENV_DRIVER_POD_IP` to all executor pods.```bash$ kubectl get pod pi-exec-1 -oyaml | grep -C1 SPARK_DRIVER_POD_IP      value: \"0\"    - name: SPARK_DRIVER_POD_IP      value: 10.1.0.99```### Why are the changes needed?This is helpful for some executor pods to connect driver pods via IP.### Does this PR introduce _any_ user-facing change?No, this is a new environment variable.### How was this patch tested?Pass the CIs with the newly added test case.
3	-3	### What changes were proposed in this pull request?Copy the logic of handleTaskCompletion in DAGScheduler for processing the last shuffleMapTask into submitMissingTasks.### Why are the changes needed?In condition of push-based shuffle being enabled and speculative tasks existing, a shuffleMapStage will be resubmitting once fetchFailed occurring, then its parent stages will be resubmitting firstly and it will cost some time to compute. Before the shuffleMapStage being resubmitted, its all speculative tasks success and register map output, but speculative task successful events can not trigger shuffleMergeFinalized( shuffleBlockPusher.notifyDriverAboutPushCompletion ) because this stage has been removed from runningStages.Then this stage is resubmitted, but speculative tasks have registered map output and there are no missing tasks to compute, resubmitting stages will also not trigger shuffleMergeFinalized. Eventually this stage‘s _shuffleMergedFinalized keeps false.Then AQE will submit next stages which are dependent on  this shuffleMapStage occurring fetchFailed. And in getMissingParentStages, this stage will be marked as missing and will be resubmitted, but next stages are added to waitingStages after this stage being finished, so next stages will not be submitted even though this stage's resubmitting has been finished.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?This extreme case is very difficult to construct, and we added logs to our production environment to capture the number of problems and verify the stability of the job. I am happy to provide a timeline of the various events in which the problem arose。
1	-1	### What changes were proposed in this pull request?The pr aims to refactor HiveGenericUDF.### Why are the changes needed?Following https://github.com/apache/spark/pull/39949.Make the code more concise.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-3	### What changes were proposed in this pull request?Run `LocalDateTime.now()` and `Instant.now()` with Java 8 & 11 always get microseconds on both Linux and MacOS, but there are some differences when using Java 17, it will get accurate nanoseconds on Linux, but still get the microseconds on MacOS. On Linux(CentOs)```jshell> java.time.LocalDateTime.now()$1 ==> 2023-03-13T18:09:12.498162194jshell> java.time.Instant.now()$2 ==> 2023-03-13T10:09:16.013993186Z```On MacOS```jshell> java.time.LocalDateTime.now()$1 ==> 2023-03-13T17:13:47.485897jshell> java.time.Instant.now()$2 ==> 2023-03-13T09:15:12.031850Z``` At present, Spark always converts them to microseconds, this will cause `test implicit encoder resolution` in SQLImplicitsTestSuite test fail when using Java 17 on Linux, so this pr add `truncatedTo(ChronoUnit.MICROS)` whentesting on Linux using Java 17 to ensure the accuracy of test input data is also microseconds.### Why are the changes needed?Make Java 17 daily test GA task run successfully. The Java 17 daily test GA task failed as follows:```[info] - test implicit encoder resolution *** FAILED *** (1 second, 329 milliseconds)4429[info]   2023-03-02T23:00:20.404434 did not equal 2023-03-02T23:00:20.404434875 (SQLImplicitsTestSuite.scala:63)4430[info]   org.scalatest.exceptions.TestFailedException:4431[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)4432[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)4433[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)4434[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)4435[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.testImplicit$1(SQLImplicitsTestSuite.scala:63)4436[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.$anonfun$new$2(SQLImplicitsTestSuite.scala:133)```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual checked with Java 17
2	-2	### What changes were proposed in this pull request?Currently, DS V2 pushdown could let JDBC dialect decide to push down `OFFSET`, `LIMIT` and table sample. Because some databases doesn't support one of them, so we should change the default value of these pushdown API false. If one database support the syntax, the JDBC dialect should overwrite the value.We also have a lot of JDBC options about push down, such as `pushDownOffset`. Users could change the option value to allow or disallow push down.### Why are the changes needed?This PR change all JDBC v2 pushdown options to true and change all the dialect's pushdown API to false.### Does this PR introduce _any_ user-facing change?'Yes'.The default behavior of pushdown framework is not push down SQL syntax to JDBC data source.Users could control the pushdown enable or disable with JDBC options about push down, such as `pushDownOffset`. ### How was this patch tested?Test cases updated.
2	-1	### What changes were proposed in this pull request?- As a subtask of [SPARK-42050](https://issues.apache.org/jira/browse/SPARK-42050), this PR adds Codegen Support for HiveSimpleUDF- Extract a`HiveUDFEvaluatorBase` class for the common behaviors of HiveSimpleUDFEvaluator & HiveGenericUDFEvaluator.### Why are the changes needed?- Improve codegen coverage and performance.- Following https://github.com/apache/spark/pull/39949. Make the code more concise.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add new UT.Pass GA.
2	-2	### What changes were proposed in this pull request?The documentation for the `translate` SQL function is a bit difficult to parse and understand. I propose the new texting.### Why are the changes needed?To improve documentation### Does this PR introduce _any_ user-facing change?I'm not sure, and I don't quite understand if I need to do something to make this documentation change become visible in the online documentation. I'd appreciate help here to improve the PR, if needed.### How was this patch tested?No tests added or executed.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/39624 .`TableCacheQueryStageExec.cancel` is a noop and we can move `def cancel` out from `QueryStageExec`. Due to this movement, I renamed `ReusableQueryStageExec` to `ExchangeQueryStageExec`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->type safe### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
1	-1	What changes were proposed in this pull request?The main change of this pr is refactor UnsafeRow#isMutable  and UnsafeRow#isFixedLength  method to use PhysicalDataType instead of DataType.Why are the changes needed?Simplify type match.Does this PR introduce any user-facing change?NoHow was this patch tested?Existing UnsafeRowSuite
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Changing the 3.4.0 version change message for PySpark functionality from \"Support Spark Connect\" to \"Supports Spark Connect\".<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Grammatical improvement.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, these messages are shown in the user-facing PySpark documentation.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built Spark and the documentation successfully on my computer and checked the PySpark documentation../build/sbt -Phive clean packagePRODUCTION=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->![supports_spark_connect_version_change_message](https://user-images.githubusercontent.com/112507318/224786569-d600ba83-483e-4a41-83a2-b3bf99d38af1.png)
2	-1	### What changes were proposed in this pull request?Supports `UserDefinedType` in Spark Connect.### Why are the changes needed?Currently Spark Connect doesn't support UDTs.### Does this PR introduce _any_ user-facing change?Yes, UDTs will be available in Spark Connect.### How was this patch tested?Enabled the related tests.
2	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/39268 / [SPARK-41752](https://issues.apache.org/jira/browse/SPARK-41752) added a new non-optional `rootExecutionId: Long` field to the SparkListenerSQLExecutionStart case class.When JsonProtocol deserializes this event it uses the \"ignore missing properties\" Jackson deserialization option, causing the rootExecutionField to be initialized with a default value of 0.The value 0 is a legitimate execution ID, so in the deserialized event we have no ability to distinguish between the absence of a value and a case where all queries have the first query as the root.Thanks @JoshRosen for reporting and investigating this issue.### Why are the changes needed?Bug fix### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->When `spark.sql.cbo.planStats.enabled` or `spark.sql.cbo.enabled` is enabled, the logical plan will fetch row counts and column statistics from catalog.This PR is to support converting TimestampNTZ catalog stats to plan stats.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Implement a missing piece of the TimestampNTZ type.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, TimestampNTZ is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
2	-1	### What changes were proposed in this pull request?Implement Grouped Map API:`GroupedData.applyInPandas` and `GroupedData.apply`.### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. `GroupedData.applyInPandas` and `GroupedData.apply` are supported now, as shown below.```sh>>> df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],(\"id\ \"v\"))>>> def normalize(pdf):...     v = pdf.v...     return pdf.assign(v=(v - v.mean()) / v.std())... >>> df.groupby(\"id\").applyInPandas(normalize, schema=\"id long, v double\").show()+---+-------------------+                                                       | id|                  v|+---+-------------------+|  1|-0.7071067811865475||  1| 0.7071067811865475||  2|-0.8320502943378437||  2|-0.2773500981126146||  2| 1.1094003924504583|+---+-------------------+``````sh>>> @pandas_udf(\"id long, v double\ PandasUDFType.GROUPED_MAP)... def normalize(pdf):...     v = pdf.v...     return pdf.assign(v=(v - v.mean()) / v.std())... >>> df.groupby(\"id\").apply(normalize).show()/Users/xinrong.meng/spark/python/pyspark/sql/connect/group.py:228: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.  warnings.warn(+---+-------------------+                                                       | id|                  v|+---+-------------------+|  1|-0.7071067811865475||  1| 0.7071067811865475||  2|-0.8320502943378437||  2|-0.2773500981126146||  2| 1.1094003924504583|+---+-------------------+```### How was this patch tested?(Parity) Unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`CoalesceShufflePartitions` should make sure all leaves are `ExchangeQueryStageExec` to avoid collect `TableCacheQueryStage`. As we can not change the partition number of IMR.Add two tests to make sure `CoalesceShufflePartitions` works well with `TableCacheQueryStage`. Note that, these two tests work without this pr, thanks to `ValidateRequirements` the wrong plan has been reverted.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Avoid potential issue.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make `QueryStageExec` respect plan.supportsRowBased### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It is a long time issue that if the plan support both columnar and row, then it would add a unnecessary `ColumnarToRow`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
2	-1	### What changes were proposed in this pull request?Upgrade google Tink from 1.7.0 to 1.8.0[Release note](https://github.com/tink-crypto/tink-java/releases/tag/v1.8.0)NOTE; Google Tink have moved from maven to there own repo. https://github.com/tink-crypto/tink-java/issues/3### Why are the changes needed?[SNYK-JAVA-COMGOOGLEPROTOBUF-3040284](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3040284)[SNYK-JAVA-COMGOOGLEPROTOBUF-3167772](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3167772)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA
1	-1	### What changes were proposed in this pull request?This PR ports the [tests](https://github.com/apache/hive/blob/rel/release-3.1.3/ql/src/test/org/apache/hadoop/hive/ql/udf/TestUDFJson.java) for `get_json_object` from the Apache Hive project.### Why are the changes needed?Increase test coverage.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?N/A.
2	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40142 have an unrelated change and is actually a regression. The change let infer window group limit runs early.Infer window group limit should run as late as possible, it is more safe.### Why are the changes needed?Infer window group limit should run as late as possible.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?Exists test cases.Manually generate the micro benchmark.```Benchmark Top-K:                                                 Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative-----------------------------------------------------------------------------------------------------------------------------------------------ROW_NUMBER (PARTITION: , WindowGroupLimit: false)                        10972          11739         765          1.9         523.2       1.0XROW_NUMBER (PARTITION: , WindowGroupLimit: true)                          1700           1738          29         12.3          81.0       6.5XROW_NUMBER (PARTITION: PARTITION BY b, WindowGroupLimit: false)          24317          24452         113          0.9        1159.5       0.5XROW_NUMBER (PARTITION: PARTITION BY b, WindowGroupLimit: true)            6608           6965         348          3.2         315.1       1.7XRANK (PARTITION: , WindowGroupLimit: false)                              11549          11850         160          1.8         550.7       1.0XRANK (PARTITION: , WindowGroupLimit: true)                                2916           3211         267          7.2         139.1       3.8XRANK (PARTITION: PARTITION BY b, WindowGroupLimit: false)                24736          25951         565          0.8        1179.5       0.4XRANK (PARTITION: PARTITION BY b, WindowGroupLimit: true)                  6825           7256         497          3.1         325.5       1.6XDENSE_RANK (PARTITION: , WindowGroupLimit: false)                        11857          12513         652          1.8         565.4       0.9XDENSE_RANK (PARTITION: , WindowGroupLimit: true)                          2721           2937         113          7.7         129.8       4.0XDENSE_RANK (PARTITION: PARTITION BY b, WindowGroupLimit: false)          24976          25686         760          0.8        1191.0       0.4XDENSE_RANK (PARTITION: PARTITION BY b, WindowGroupLimit: true)            6568           6884         364          3.2         313.2       1.7X```
3	-2	What changes were proposed in this pull request?Point out a misleading example in doc and give an explanationWhy are the changes needed?The original statement is misleading, and if the code is copied, it will give an error.So I post the reason for the error, and a feasible solution to write multiple columns.Does this PR introduce any user-facing change?NoHow was this patch tested?Only parts of the documentation changed, and I successfully run the illustrative example code
1	-2	### What changes were proposed in this pull request?Fixed a minor issue with diskBlockManager after push-based shuffle is enabled### Why are the changes needed?this bug will affect the efficiency of push based shuffle### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit test
1	-1	### What changes were proposed in this pull request?Implement typed select methods in the Dataset.### Why are the changes needed?More APIs### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit and E2E tests
3	-2	### What changes were proposed in this pull request?After https://github.com/apache/spark/pull/37880 when user spark submit without `--deploy-mode XXX` or `–conf spark.submit.deployMode=XXXX`, may face NPE with this code.### Why are the changes needed?https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#164```scalaargs.deployMode.equals(\"client\") &&```Of course, submit without `deployMode` is not allowed and will throw an exception and terminate the application, but we should leave it to the later logic to give the appropriate hint instead of giving a NPE.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->![popo_2023-03-14  17-50-46](https://user-images.githubusercontent.com/52876270/224965310-ba9ec82f-e668-4a06-b6ff-34c3e80ca0b4.jpg)
1	-1	### What changes were proposed in this pull request?### Why are the changes needed?### Does this PR introduce _any_ user-facing change?### How was this patch tested?
2	-1	### What changes were proposed in this pull request?This PR proposes to document the configuration of Spark Connect defined in https://github.com/apache/spark/blob/master/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/config/Connect.scala### Why are the changes needed?To let users know which configuration are supported for Spark Connect.### Does this PR introduce _any_ user-facing change?Yes, it documents the configurations for Spark Connect.### How was this patch tested?Linters in CI should verify this change.Also manually built the docs as below:![Screen Shot 2023-03-14 at 8 24 51 PM](https://user-images.githubusercontent.com/6477701/224986645-3e3abfe3-4f6b-4810-8887-24cf24532f5e.png)
3	-2	this pr is for branch-3.4 https://github.com/apache/spark/pull/40407<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make `QueryStageExec` respect plan.supportsRowBased### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It is a long time issue that if the plan support both columnar and row, then it would add a unnecessary `ColumnarToRow`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
2	-2	### What changes were proposed in this pull request?Currently, The JDBC V2 test cases create a lot of method that tests some syntax or function. It is very easy missing the test cases if developer forget to call it.This PR let all the JDBC V2 could execute these tests by default. If one database doesn't support the related syntax of function, developer have to add the name of this test case to excluded list. We can ensure that use cases are tested.### Why are the changes needed?This PR abstract the excluded method for better test for JDBC docker tests.### Does this PR introduce _any_ user-facing change?'No'.Just related to tests.### How was this patch tested?Just related to tests.
2	-2	### What changes were proposed in this pull request?This PR adds a new rule to rewrite multiple `GetJsonObjects` to one `JsonTuple` if their json expressions are the same and the path is `.name`. For example:```=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RewriteGetJsonObject ===!Project [get_json_object(a#0, $.c1) AS c1#3, get_json_object(a#0, $.c2) AS c2#4, get_json_object(b#1, $.c1) AS c3#5, get_json_object(b#1, $.c2) AS c4#6, get_json_object(c#2, $.c1) AS c5#7]   Project [c1#13 AS c1#3, c2#14 AS c2#4, c1#15 AS c3#5, c2#16 AS c4#6, get_json_object(c#2, $.c1) AS c5#7]!+- LocalRelation <empty>, [a#0, b#1, c#2]                                                                                                                                                      +- Generate json_tuple(b#1, c1, c2), false, b, [c1#15, c2#16]!                                                                                                                                                                                                  +- Generate json_tuple(a#0, c1, c2), false, a, [c1#13, c2#14]!                                                                                                                                                                                                     +- LocalRelation <empty>, [a#0, b#1, c#2]```Please note that it needs to rewrite the path, otherwise it will not get the correct value. For example:```sqlspark-sql (default)> SELECT get_json_object('{\"name\": \"alice\ \"age\": 5}', '$.name') AS name;alicespark-sql (default)> SELECT name lateral view json_tuple('{\"name\": \"alice\ \"age\": 5}', '$.name') a AS name;NULLspark-sql (default)> select name lateral view json_tuple('{\"name\": \"alice\ \"age\": 5}', 'name') a AS name;alice```### Why are the changes needed?Improve query performance.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test and benchmark test.Benchmark result:No. of   GetJsonObjects | Default(ms) | Rewrite to JsonTuple(ms) | Relative-- | -- | -- | --2 | 37914 | 24887 | 1.5X3 | 52862 | 26752 | 2.0X4 | 71680 | 28452 | 2.5X5 | 108479 | 36830 | 2.9X10 | 153952 | 32436 | 4.7X15 | 224950 | 34806 | 6.5X20 | 293155 | 38148 | 7.7X25 | 366037 | 45725 | 8.0X30 | 439567 | 52949 | 8.3X35 | 526481 | 60586 | 8.7X
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support `isocalendar` from the pandas 2.0.0### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Added new method `DatetimeIndex.isocalendar` and removed two depreceted `DatetimeIndex.week` and `DatetimeIndex.weekofyear````dfs = ps.from_pandas(pd.date_range(start='2019-12-29', freq='D', periods=4).to_series())dfs.dt.isocalendar()                    year  week  day        2019-12-29  2019    52    7        2019-12-30  2020     1    1        2019-12-31  2020     1    2        2020-01-01  2020     1    3dfs.dt.isocalendar().week        2019-12-29    52        2019-12-30     1        2019-12-31     1        2020-01-01     1```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT was updated
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds an API for data sources to indicate the advisory partition size for V2 writes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Data sources have an API to request a particular distribution and ordering of data for V2 writes. If AQE is enabled, the default session advisory partition size (64MB) will be used as target. Unfortunately, this default value is still suboptimal and can lead to small files because the written data can be compressed nicely using columnar file formats. Spark should allow data sources to indicate the advisory shuffle partition size, just like it lets data sources request a particular number of partitions. This feature would allow data sources to estimate the compression ratio and incorporate that in the requested advisory partition size.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. However, the changes are backward compatible.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR extends the existing tests for V2 write distribution and ordering.
1	-2	### What changes were proposed in this pull request?Since jdk1.8 there is an additional function in reflection API `getParameterCount`, it is better to use that function instead of `getParameterTypes.length` because `getParameterTypes` function makes a copy of the parameter types array every invocation:```java    public Class<?>[] getParameterTypes() {        return parameterTypes.clone();    }````getParameterCount` returns amount of parameters directly:```java    public int getParameterCount() { return parameterTypes.length; }``` ### Why are the changes needed?To avoid redundant arrays creation.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?By existing unit tests
2	-1	### What changes were proposed in this pull request?Added support for when `spark.task.resource.gpus.amount > 1`.### Why are the changes needed?This was a configuration change that needed to be addressed.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Tested locally and usinge existing tests.
1	-1	### What changes were proposed in this pull request?This PR aims to add `build_profile_flags` to `connect` module.### Why are the changes needed?SPARK-42656 added `connect` profile.https://github.com/apache/spark/blob/4db8e7b7944302a3929dd6a1197ea1385eecc46a/assembly/pom.xml#L155-L164### Does this PR introduce _any_ user-facing change?No. This is a dev-only change.### How was this patch tested?Pass the CIs.
3	-3	We are seeing query failure which is caused by RocksDB acquisition failure for the retry tasks.* at t1, we shrink the cluster to only have one executor```23/03/05 22:47:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230305224215-0000/2 is now DECOMMISSIONED (worker decommissioned because of kill request from HTTP endpoint (data migration disabled))23/03/05 22:47:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20230305224215-0000/3 is now DECOMMISSIONED (worker decommissioned because of kill request from HTTP endpoint (data migration disabled)) ```* at t1+2min, task 7 at its first attempt (i.e. task 7.0) is scheduled to the alive executor```23/03/05 22:49:58 INFO TaskSetManager: Starting task 7.0 in stage 133.0 (TID 685) (10.166.225.249, executor 0, partition 7, ANY, ```It seems that task 7.0 is able to pass dataRDD.iterator(partition, ctxt) and acquires the rocksdb lock as we are seeing```23/03/05 22:51:59 WARN TaskSetManager: Lost task 4.1 in stage 133.1 (TID 700) (10.166.225.249 executor 0): java.lang.IllegalStateException: StateStoreId(opId=0,partId=7,name=default): RocksDB instance could not be acquired by [ThreadId: Some(50), task: partition 7.1 in stage 133.1, TID 700] as it was not released by [ThreadId: Some(449), task: partition 7.0 in stage 133.0, TID 685] after 60003 ms.23/03/05 22:52:59 WARN TaskSetManager: Lost task 4.2 in stage 133.1 (TID 702) (10.166.225.249 executor 0): java.lang.IllegalStateException: StateStoreId(opId=0,partId=7,name=default): RocksDB instance could not be acquired by [ThreadId: Some(1495), task: partition 7.2 in stage 133.1, TID 702] as it was not released by [ThreadId: Some(449), task: partition 7.0 in stage 133.0, TID 685] after 60006 ms.23/03/05 22:53:59 WARN TaskSetManager: Lost task 4.3 in stage 133.1 (TID 704) (10.166.225.249 executor 0): java.lang.IllegalStateException: StateStoreId(opId=0,partId=7,name=default): RocksDB instance could not be acquired by [ThreadId: Some(46), task: partition 7.3 in stage 133.1, TID 704] as it was not released by [ThreadId: Some(449), task: partition 7.0 in stage 133.0, TID 685] after 60003 ms.```Increasing the lockAcquireTimeoutMs to 2 minutes such that 4 task retries will give us 8 minutes to acquire the lock and it is larger than connectionTimeout with retries (3 * 120s).### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Increase the lockAcquireTimeoutMs to 2 minutes for acquiring the RocksDB state store in Structure Streaming### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->hanging the thread for lock acquisition rather than giving up easily### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Trivial change
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support accessing TimestampNTZ columns in CachedBatch### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Implement a missing feature for TimestampNTZ type### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, TimestampNTZ type is not released yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
3	-3	### What changes were proposed in this pull request?Add support for WRITE_FLUSH_BYTES for RocksDB used in streaming stateful operators### Why are the changes needed?Its useful to get this metric for bytes written during flush from RocksDB as part of the DB custom metrics. We propose to add this to the existing metrics that are collected. There is no additional overhead since we are just querying the internal ticker guage, similar to other metrics.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added unit test```[info] Run completed in 45 seconds, 260 milliseconds.[info] Total number of tests run: 18[info] Suites: completed 1, aborted 0[info] Tests: succeeded 18, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.[success] Total time: 152 s (02:32), completed Mar 14, 2023, 3:43:41 PM```Info log on executor:```StateStoreId(opId=0,partId=3,name=default): Committed 2, stats = {\"numCommittedKeys\":4,\"numUncommittedKeys\":4,\"totalMemUsageBytes\":7818,\"writeBatchMemUsageBytes\":272,\"totalSSTFilesBytes\":2614,\"nativeOpsHistograms\":{\"get\":{\"sum\":14,\"avg\":7.0,\"stddev\":1.0,\"median\":6.0,\"p95\":8.0,\"p99\":8.0,\"count\":2},\"put\":{\"sum\":37966,\"avg\":37966.0,\"stddev\":0.0,\"median\":37966.0,\"p95\":37966.0,\"p99\":37966.0,\"count\":1},\"compaction\":{\"sum\":0,\"avg\":0.0,\"stddev\":0.0,\"median\":0.0,\"p95\":0.0,\"p99\":0.0,\"count\":0}},\"lastCommitLatencyMs\":{\"fileSync\":188,\"writeBatch\":37,\"flush\":61,\"pause\":0,\"checkpoint\":61,\"compact\":0},\"filesCopied\":1,\"bytesCopied\":1280,\"filesReused\":1,\"zipFileBytesUncompressed\":7675,\"nativeOpsMetrics\":{\"writerStallDuration\":0,\"totalBytesReadThroughIterator\":254,\"totalBytesWrittenByFlush\":1490,\"readBlockCacheHitCount\":2,\"totalBytesWrittenByCompaction\":0,\"readBlockCacheMissCount\":0,\"totalBytesReadByCompaction\":0,\"totalBytesWritten\":272,\"totalBytesRead\":73}}```Info log on driver:```    \"customMetrics\" : {      \"rocksdbBytesCopied\" : 2544,      \"rocksdbCommitCheckpointLatency\" : 416,      \"rocksdbCommitCompactLatency\" : 0,      \"rocksdbCommitFileSyncLatencyMs\" : 742,      \"rocksdbCommitFlushLatency\" : 194,      \"rocksdbCommitPauseLatency\" : 0,      \"rocksdbCommitWriteBatchLatency\" : 132,      \"rocksdbFilesCopied\" : 2,      \"rocksdbFilesReused\" : 2,      \"rocksdbGetCount\" : 4,      \"rocksdbGetLatency\" : 0,      \"rocksdbPutCount\" : 5,      \"rocksdbPutLatency\" : 132,      \"rocksdbReadBlockCacheHitCount\" : 4,      \"rocksdbReadBlockCacheMissCount\" : 0,      \"rocksdbSstFileSize\" : 5143,      \"rocksdbTotalBytesRead\" : 138,      \"rocksdbTotalBytesReadByCompaction\" : 0,      \"rocksdbTotalBytesReadThroughIterator\" : 714,      \"rocksdbTotalBytesWritten\" : 548,      \"rocksdbTotalBytesWrittenByCompaction\" : 0,      \"rocksdbTotalBytesWrittenByFlush\" : 2948,      \"rocksdbTotalCompactionLatencyMs\" : 0,      \"rocksdbWriterStallLatencyMs\" : 0,      \"rocksdbZipFileBytesUncompressed\" : 36542    }  } ],```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Grammatical improvements to the Spark Connect content as a follow-up on https://github.com/apache/spark/pull/40324/<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To improve readability of the pages.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, user-facing documentation is updated.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built the doc website locally and checked the updates.PRODUCTION=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?This PR fixed the counter-intuitive behaviors of the `ApproximatePercentile` expression mentioned in https://issues.apache.org/jira/browse/SPARK-42775. See the following *user-facing* changes for details.### Does this PR introduce _any_ user-facing change?Yes. When working on decimals, this expression has the same output type as the input type. When the result that doesn't fit into output decimal type, it silently produces the result since the `Decimal` object is capable of representing decimals of arbitrary precision and scale. However, this can lead to weird behaviors because the value doesn't fit into its type. We should throw an exception immediately.Old results:```sql-- ApproximatePercentile will first cast decimal value 9999999999999999999 into double, which results in 1E20, and cast 1E20 back into decimal, which doesn't fit the type decimal(19, 0).-- Here it is producing \"NULL\" because the value doesn't fit into its type, but it is actually not NULL.spark-sql> select approx_percentile(col, 0.5) from values (9999999999999999999) as tab(col);NULLspark-sql> select approx_percentile(col, 0.5) is null from values (9999999999999999999) as tab(col);falsespark-sql> select cast(approx_percentile(col, 0.5) as string) from values (9999999999999999999) as tab(col);10000000000000000000```New results:```sqlspark-sql> select approx_percentile(col, 0.5) from values (9999999999999999999) as tab(col);throws SparkArithmeticException```### How was this patch tested?Pass existing tests and some new tests.
1	-2	### What changes were proposed in this pull request?This pr aims upgrade protobuf-java from 3.22.0 to 3.22.3.### Why are the changes needed?The new version fixed the issue of `NoSuchMethodError` thrown when using Java 8 to run proto compiled with Java 9+ (even if --target 1.8):  - https://github.com/protocolbuffers/protobuf/issues/11393 / https://github.com/protocolbuffers/protobuf/pull/12035The full release notes as follows:- https://github.com/protocolbuffers/protobuf/releases/tag/v22.1- https://github.com/protocolbuffers/protobuf/releases/tag/v22.2- https://github.com/protocolbuffers/protobuf/releases/tag/v22.3- https://github.com/protocolbuffers/protobuf/compare/v3.22.0...v3.22.3### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This PR aims to update `XercesImpl` version to `2.12.2` from `2.12.0` in order to match with the version of `pom.xml`.https://github.com/apache/spark/blob/149e020a5ca88b2db9c56a9d48e0c1c896b57069/pom.xml#L1429-L1433### Why are the changes needed?When we updated this version via SPARK-39183, we missed to update `SparkBuild.scala`.- https://github.com/apache/spark/pull/36544### Does this PR introduce _any_ user-facing change?No, this is a dev-only change because the release artifact' dependency is managed by Maven.### How was this patch tested?Pass the CIs.
2	-2	### What changes were proposed in this pull request?Implement ml function `{array_to_vector, vector_to_array}`### Why are the changes needed?function parity### Does this PR introduce _any_ user-facing change?yes, new functions### How was this patch tested?added ut and manually check```(spark_dev) ➜  spark git:(connect_ml_functions) ✗ bin/pyspark --remote \"local[*]\"    Python 3.9.16 (main, Mar  8 2023, 04:29:24) Type 'copyright', 'credits' or 'license' for more informationIPython 8.11.0 -- An enhanced Interactive Python. Type '?' for help.Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/03/15 11:56:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Mar  8 2023 04:29:24)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.In [1]: In [1]:         query = \"\"\"   ...:             SELECT * FROM VALUES   ...:             (1, 4, ARRAY(1.0, 2.0, 3.0)),   ...:             (1, 2, ARRAY(-1.0, -2.0, -3.0))   ...:             AS tab(a, b, c)   ...:             \"\"\"In [2]: cdf = spark.sql(query)In [3]:     from pyspark.sql.connect.ml import functions as CFIn [4]: cdf1 = cdf.select(\"a\ CF.array_to_vector(cdf.c).alias(\"d\"))In [5]: cdf1.show()+---+----------------+                                              (0 + 1) / 1]|  a|               d|+---+----------------+|  1|   [1.0,2.0,3.0]||  1|[-1.0,-2.0,-3.0]|+---+----------------+In [6]: cdf1.schemaOut[6]: StructType([StructField('a', IntegerType(), False), StructField('d', VectorUDT(), True)])In [7]: cdf1.select(CF.vector_to_array(cdf1.d))Out[7]: DataFrame[UDF(d): array<double>]In [8]: cdf1.select(CF.vector_to_array(cdf1.d)).show()+------------------+|            UDF(d)|+------------------+|   [1.0, 2.0, 3.0]||[-1.0, -2.0, -3.0]|+------------------+In [9]: cdf1.select(CF.vector_to_array(cdf1.d)).schemaOut[9]: StructType([StructField('UDF(d)', ArrayType(DoubleType(), False), False)])```
3	-2	### What changes were proposed in this pull request?Cherry-pick for https://github.com/apache/spark/pull/40336.This PR proposes to document Spark SQL error classes to [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html).- Error Conditions <img width=\"1077\" alt=\"Screen Shot 2023-03-08 at 8 54 43 PM\" src=\"https://user-images.githubusercontent.com/44108233/223706823-7817b57d-c032-4817-a440-7f79119fa0b4.png\">- SQLSTATE Codes <img width=\"1139\" alt=\"Screen Shot 2023-03-08 at 8 54 54 PM\" src=\"https://user-images.githubusercontent.com/44108233/223706860-3f64b00b-fa0d-47e0-b154-0d7be92b8637.png\">- Error Classes that includes sub-error classes (`INVALID_FORMAT` as an example) <img width=\"1045\" alt=\"Screen Shot 2023-03-08 at 9 10 22 PM\" src=\"https://user-images.githubusercontent.com/44108233/223709925-74144f41-8836-45dc-b851-5d96ac8aa38c.png\">### Why are the changes needed?To improve the usability for error messages for Spark SQL.### Does this PR introduce _any_ user-facing change?No API change, but yes, it's user-facing documentation.### How was this patch tested?Manually built docs and check the contents one-by-one compare to [error-classes.json](https://github.com/apache/spark/blob/master/core/src/main/resources/error/error-classes.json).Closes #40336 from itholic/SPARK-42706.Authored-by: itholic <haejoon.lee@databricks.com><!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-3	### What changes were proposed in this pull request?This PR aims to ignore the flaky `write jdbc` test of `ClientE2ETestSuite` on Java 8 ![Screenshot 2023-03-14 at 10 56 34 PM](https://user-images.githubusercontent.com/9700541/225219845-94eaea79-ade6-435d-9d03-19fc73cb8617.png)### Why are the changes needed?Currently, this happens on `branch-3.4` with Java 8 only.**BRANCH-3.4**https://github.com/apache/spark/commits/branch-3.4![Screenshot 2023-03-14 at 10 55 29 PM](https://user-images.githubusercontent.com/9700541/225219670-f8a68dc0-5aa6-428f-9c02-ae41580a38bc.png)**JAVA 8**1. Currently, `Connect` server is using `Hive` catalog during testing and uses `Derby` with disk store when it creates a table2. `Connect Client` is trying to use `Derby` with `mem` store and it fails with `No suitable driver` at the first attempt.```$ bin/spark-shell -c spark.sql.catalogImplementation=hiveSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/03/14 21:50:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context available as 'sc' (master = local[64], app id = local-1678855843831).Spark session available as 'spark'.Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1-SNAPSHOT      /_/Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)Type in expressions to have them evaluated.Type :help for more information.scala> sc.setLogLevel(\"INFO\")scala> sql(\"CREATE TABLE t(a int)\")23/03/14 21:51:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.23/03/14 21:51:08 INFO SharedState: Warehouse path is 'file:/Users/dongjoon/APACHE/spark-merge/spark-warehouse'.23/03/14 21:51:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.23/03/14 21:51:10 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.23/03/14 21:51:11 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/Users/dongjoon/APACHE/spark-merge/spark-warehouseres1: org.apache.spark.sql.DataFrame = []scala> java.sql.DriverManager.getConnection(\"jdbc:derby:memory:1234;create=true\").createStatement().execute(\"CREATE TABLE s(a int)\");java.sql.SQLException: No suitable driver found for jdbc:derby:memory:1234;create=true  at java.sql.DriverManager.getConnection(DriverManager.java:689)  at java.sql.DriverManager.getConnection(DriverManager.java:270)  ... 47 elided```**JAVA 11**```$ bin/spark-shell -c spark.sql.catalogImplementation=hiveSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/03/14 21:57:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://localhost:4040Spark context available as 'sc' (master = local[*], app id = local-1678856279685).Spark session available as 'spark'.Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1-SNAPSHOT      /_/Using Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 11.0.18)Type in expressions to have them evaluated.Type :help for more information.scala> sql(\"CREATE TABLE hive_t2(a int)\")23/03/14 21:58:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.23/03/14 21:58:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist23/03/14 21:58:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist23/03/14 21:58:07 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.023/03/14 21:58:07 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore dongjoon@127.0.0.123/03/14 21:58:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.23/03/14 21:58:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist23/03/14 21:58:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist23/03/14 21:58:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist23/03/14 21:58:07 WARN HiveMetaStore: Location: file:/Users/dongjoon/APACHE/spark-merge/spark-warehouse/hive_t2 specified for non-external table:hive_t2res0: org.apache.spark.sql.DataFrame = []scala> java.sql.DriverManager.getConnection(\"jdbc:derby:memory:1234;create=true\").createStatement().execute(\"CREATE TABLE derby_t2(a int)\");res1: Boolean = falsescala> :quit```### Does this PR introduce _any_ user-facing change?No. This is a test only PR.### How was this patch tested?Pass the CIs.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Removing the last \">>>\" in a Python code example based on feedback and adding type(spark) as an example of checking whether a session is Spark Connect.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To help readers determine whether a session is Spark Connect + removing unnecessary extra line for cleaner reading.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, updating user-facing documentation<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Built the doc website locally and checked the pages.PRODUCTION=1 SKIP_RDOC=1 bundle exec jekyll build<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Added `show_counts` parameter for DataFrame.info ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Changed the name of the parameter `null_counts` to `show_counts` of the method DataFrame.info### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to 3 points:1) Beautify the output of cast `StructType` to `StringType`, Such as {1, 2} -> {a:1, b:2};2) `SparkSQLDriver` use the spark result string that is consistent with that of `df.show`;3) The spark-sql shell result output of `SHOW TABLES/VIEWS` is not consistent with their schema, Let's say have a case like the following with `spark.sql.cli.print.header true`:    ```shell    spark-sql> create table tbl1 (id string) using parquet;    Response code    Time taken: 1.076 seconds    ```        Before this PR:    ```shell    spark-sql> show tables;    namespace\ttableName\tisTemporary    tbl1    ```    After this PR:    ```shell    spark-sql> show tables;    tableName    tbl1    ```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve and Bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Exist tests and new test.
2	-1	### What changes were proposed in this pull request?This pr aims add `Catalog`  Api support for Spark connect jvm client.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new tests- Manually checked Scala 2.13- Manually checked  with or without `-Phive`- Manually checked  maven test
1	-1	### What changes were proposed in this pull request?Add attributes to MiscellaneousProcessDetails event so that SHS can customize the log url according to attributes.### Why are the changes needed?Now the am log in yarn client mode does not support custom url (`spark.history.custom.executor.log.url`)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
2	-2	### What changes were proposed in this pull request?The return value of `Runtime.getRuntime.availableProcessors` is generally a fixed value. It is not necessary to obtain it every time `getStatistics` is called to avoid a native method call.### Why are the changes needed?### Does this PR introduce _any_ user-facing change?No### How was this patch tested?exist UT
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade scala-maven-plugin from 4.8.0 to 4.8.1.### Why are the changes needed?Routine upgrade.https://github.com/davidB/scala-maven-plugin/compare/4.8.0...4.8.1### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The missing `client_type` is added to the `AddArtifactsRequest` protobuf message.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Consistency with the other RPCs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, new field in proto message.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->On K8s cluster mode, 1. when `spark.kubernetes.submission.waitAppCompletion=false`, print the application information on `spark-submit` exit, as it did before [SPARK-35174](https://issues.apache.org/jira/browse/SPARK-35174)2. add `appId` in the output message### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->On K8s cluster mode, when `spark.kubernetes.submission.waitAppCompletion=false`,before [SPARK-35174](https://issues.apache.org/jira/browse/SPARK-35174), the `spark-submit` will exit quickly w/ the basic application information.```logInfo(s\"Deployed Spark application ${conf.appName} with submission ID $sId into Kubernetes\")```After [SPARK-35174](https://issues.apache.org/jira/browse/SPARK-35174), those part of code is unreachable, so nothing is output.This PR also proposes to add `appId` in the output message, to make it consistent w/ the context (if you look at the `LoggingPodStatusWatcherImpl`, this is kind of an exception, `... application $appId ...` is used in other places), and YARN.https://github.com/apache/spark/blob/8860f69455e5a722626194c4797b4b42cccd4510/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1311-L1318### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, changes contain1) when `spark.kubernetes.submission.waitAppCompletion=false`, the user can see the app information when `spark-submit` exit.2) the end of `spark-submit` information contains app id now, which is consistent w/ the context and other resource managers like YARN.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI.
1	-2	### What changes were proposed in this pull request?This pr aims to upgrade the following maven plugins- maven-enforcer-plugin 3.0.0-M2 -> 3.2.1- build-helper-maven-plugin 3.2.0 -> 3.3.0- maven-compiler-plugin 3.10.1 -> 3.11.0- maven-surefire-plugin 3.0.0-M9 -> 3.0.0- maven-javadoc-plugin 3.4.1 -> 3.5.0- maven-deploy-plugin 3.0.0 -> 3.1.0### Why are the changes needed?The release notes as follows:- maven-enforcer-plugin  - https://github.com/apache/maven-enforcer/releases/tag/enforcer-3.2.1  - https://github.com/apache/maven-enforcer/releases/tag/enforcer-3.1.0- build-helper-maven-plugin  - https://github.com/mojohaus/build-helper-maven-plugin/releases/tag/build-helper-maven-plugin-3.3.0- maven-compiler-plugin  - https://github.com/apache/maven-compiler-plugin/releases/tag/maven-compiler-plugin-3.11.0- maven-surefire-plugin  - https://github.com/apache/maven-surefire/releases/tag/surefire-3.0.0- maven-javadoc-plugin  - https://github.com/apache/maven-javadoc-plugin/releases/tag/maven-javadoc-plugin-3.5.0- maven-deploy-plugin  - https://github.com/apache/maven-deploy-plugin/releases/tag/maven-deploy-plugin-3.1.0### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual: maven build&test, enforcer rejects lower versions of java and maven
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new config to shortcut subexpression elimination for expression `and`, `or`.The subexpression may not need to eval even if it appears more than once.e.g., `if(or(a, and(b, b)))`, the expression `b` would be skipped if `a` is true.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->avoid eval unnecessary expression.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
2	-3	### What changes were proposed in this pull request?This change lifts the default message size of 4MB to 128MB and makes it configurable. While 128MB is a \"random number\" it supports creating DataFrames from reasonably sized local data without failing.### Why are the changes needed?Usability### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual
1	-2	### What changes were proposed in this pull request?Removed the logging of shuffle service name multiple times in the driver log. It gets logged everytime a new executor is allocated.### Why are the changes needed?This is needed because currently the driver logs gets polluted by these logs:```22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'22/08/03 20:42:07 INFO ExecutorRunnable: Initializing service data for shuffle service using name 'spark_shuffle_311'```### Does this PR introduce _any_ user-facing change?Yes, the shuffle service name will be just logged once in the driver.### How was this patch tested?Tested manually since it just changes the logging.With this see this logged in the driver logs:`23/03/15 16:50:54 INFO  ApplicationMaster: Initializing service data for shuffle service using name 'spark_shuffle_311'`
1	-2	### What changes were proposed in this pull request?This PR updates the `SQLQueryTestSuite` to also consume the same input SQL queries from the input files in second pass and then perform analysis and generate the string representation of the analyzed plans, in a separate new directory. It works much the same way as the previous `SQLQueryTestSuite` behavior except that it now also produces analyzed plan string instead of just query results in the output golden files.### Why are the changes needed?This framework will help us guard against bugs in future development by showing a clear signal when analyzer updates result in changes to the query plans for a body of test queries. PR authors and reviewers will be able to see the diffs for all changed plans during the review, and any unintentional plan changes will act as a signal for PR authors to adjust their code to prevent the changes from happening.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?This test adds a new test suite and initial golden file. The new test suite passes as of this PR.
1	-1	### What changes were proposed in this pull request?Implements `DataFrameReader/Writer.jdbc`.### Why are the changes needed?Missing API.### Does this PR introduce _any_ user-facing change?Yes, `DataFrameReader/Writer.jdbc` will be available.### How was this patch tested?Added related tests.
2	-1	### What changes were proposed in this pull request?Follow-up of #40450.Adds `versionchanged` to the docstring.### Why are the changes needed?The `versionchanged` is missing in the API newly supported in Spark Connect.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
1	-1	### What changes were proposed in this pull request?This pr just add comments of `xercesImpl` upgrade precautions in `pom.xml`.### Why are the changes needed?Add comments to remind developers that `xercesImpl` should update versions in both `pom.xml` and `SparkBuild.scala` at the same time.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This PR aims to update ORC to 1.8.3.### Why are the changes needed?This will bring the following bug fixes. - https://orc.apache.org/news/2023/03/15/ORC-1.8.3/### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
1	-2	### What changes were proposed in this pull request?The pr aims to remove unused parameters in PartitionedFileUtil.splitFiles methods### Why are the changes needed?Make the code more concise.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-3	### What changes were proposed in this pull request?Add support for setting max_write_buffer_number and write_buffer_size for RocksDB used in streaming### Why are the changes needed?We need these settings in order to control memory tuning for RocksDB. We already expose settings for blockCache size. However, these 2 settings are missing. This change proposes to add them.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added unit tests and docs in the guide docRocksDBSuite```[info] Run completed in 59 seconds, 336 milliseconds.[info] Total number of tests run: 27[info] Suites: completed 1, aborted 0[info] Tests: succeeded 27, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.[success] Total time: 165 s (02:45), completed Mar 15, 2023, 11:24:17 PM```RocksDBStateStoreSuite```[info] Run completed in 1 minute, 16 seconds.[info] Total number of tests run: 73[info] Suites: completed 1, aborted 0[info] Tests: succeeded 73, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```
1	-1	### What changes were proposed in this pull request?This PR replaces `DataFrame.withSequenceColumn` to `DataFrame.select(distributed_sequence_column, col(\"*\")` internally because this essentially attaches a column and it should be treated as a scalar expression at the logical level.This is used to generate the unique index only for pandas API on Spark.### Why are the changes needed?For better readability of codes, and for cleaner definition of Spark Connect protobuf message, see also https://github.com/apache/spark/pull/40270.### Does this PR introduce _any_ user-facing change?No, it's internal change only.### How was this patch tested?Existing test cases in pandas API on Spark verify this change.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Currently, we only support initializing spark-sql shell with a single-part schema, which also must be forced to the session catalog.#### case 1, specifying catalog field for v1sessioncatalog```sqlbin/spark-sql --database spark_catalog.defaultException in thread \"main\" org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'spark_catalog.default' not found```#### case 2, setting the default catalog to another one```sqlbin/spark-sql -c spark.sql.defaultCatalog=testcat -c spark.sql.catalog.testcat=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog -c spark.sql.catalog.testcat.url='jdbc:derby:memory:testcat;create=true' -c spark.sql.catalog.testcat.driver=org.apache.derby.jdbc.AutoloadedDriver -c spark.sql.catalogImplementation=in-memory  --database SYS23/03/16 18:40:49 WARN ObjectStore: Failed to get database sys, returning NoSuchObjectExceptionException in thread \"main\" org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'sys' not found```In this PR, we switch to use-statement to support multipart namespaces, which helps us resovleto catalog correctly.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make spark-sql shell better support the v2 catalog framework.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, `--database` option supports multipart namespaces and works for v2 catalogs now. And you will see this behavior on spark web ui.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new ut
3	-2	### What changes were proposed in this pull request?This pull request proposes an improvement to the error message when trying to access a JVM attribute that is not supported in Spark Connect. Specifically, it adds a more informative error message that clearly indicates which attribute is not supported due to Spark Connect's lack of dependency on the JVM.### Why are the changes needed?Currently, when attempting to access an unsupported JVM attribute in Spark Connect, the error message is not very clear, making it difficult for users to understand the root cause of the issue. This improvement aims to provide more helpful information to users to address this problem as below:**Before**```python>>> spark._jscTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>AttributeError: 'SparkSession' object has no attribute '_jsc'```**After**```python>>> spark._jscTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/Users/haejoon.lee/Desktop/git_store/spark/python/pyspark/sql/connect/session.py\ line 490, in _jsc    raise PySparkAttributeError(pyspark.errors.exceptions.base.PySparkAttributeError: [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `_jsc` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, use the original PySpark instead of Spark Connect.```### Does this PR introduce _any_ user-facing change?This PR does not introduce any user-facing change in terms of functionality. However, it improves the error message, which could potentially affect the user experience in a positive way.### How was this patch tested?This patch was tested by adding new unit tests that specifically target the error message related to unsupported JVM attributes. The tests were run locally on a development environment.
2	-1	### What changes were proposed in this pull request?This PR proposes to add a migration note for update to supported pandas version.### Why are the changes needed?Some APIs have been deprecated or removed from SPARK-42593 to follow pandas 2.0.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual review is required.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Be more explicit in the `Callable` type annotation for `dfapi` and `df_varargs_api` to explicitly return a `DataFrame`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In PySpark 3.3.x, type hints now infer the return value of something like `df.groupBy(...).count()` to be `Any`, whereas it should be `DataFrame`. This breaks type checking.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No runtime changes introduced, so just relied on CI tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?If the result expressions in AggregateExec are not empty, we should display them. Or we will get confused because some important expressions do not show up in the DAG.### Why are the changes needed?For example, the plan for query `SELECT sum(p) from values(cast(23.4 as decimal(7,2))) t(p)` was incorrect because the result expression `MakeDecimal(sum(UnscaledValue(p#0))#1L,17,2) AS sum(p)#2` is not displayedBefore```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- HashAggregate(keys=[], functions=[sum(UnscaledValue(p#0))], output=[sum(p)#2])   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=11]      +- HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(p#0))], output=[sum#5L])         +- LocalTableScan [p#0]```After```== Physical Plan ==     AdaptiveSparkPlan isFinalPlan=false+- HashAggregate(keys=[], functions=[sum(UnscaledValue(p#0))], results=[MakeDecimal(sum(UnscaledValue(p#0))#1L,17,2) AS sum(p)#2], output=[sum(p)#2])   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=38]      +- HashAggregate(keys=[], functions=[partial_sum(UnscaledValue(p#0))], results=[sum#13L], output=[sum#13L])         +- LocalTableScan [p#0]```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Local test
2	-2	### What changes were proposed in this pull request?This PR enhances `CollapseRepartition` to remove repartition if it is the child of `LocalLimit`. Because its output is determined by the number of partitions and the expressions of the Repartition. Therefore, it is feasible to remove Repartition except for repartition by nondeterministic expressions, because users may expect to randomly take data.For example:```sqlSELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5;```Before this PR:```== Optimized Logical Plan ==GlobalLimit 5+- LocalLimit 5   +- RebalancePartitions      +- Filter (isnotnull(id#0L) AND (id#0L > 1))         +- Relation spark_catalog.default.t[id#0L] parquet```After this PR:```== Optimized Logical Plan ==GlobalLimit 5+- LocalLimit 5   +- Filter (isnotnull(id#0L) AND (id#0L > 1))      +- Relation spark_catalog.default.t[id#0L] parquet```Note that we don't remove repartition if it looks like the user might want to take data randomly. For example:```sqlSELECT /*+ REPARTITION(3) */ * FROM t WHERE id > 1 LIMIT 5;SELECT * FROM t WHERE id > 1 DISTRIBUTE BY random() LIMIT 5;```### Why are the changes needed?Reduce shuffle to improve query performance. The use case is that we add a repartition to improve the parallelism on a JDBC table:<img src=\"https://user-images.githubusercontent.com/5399861/225855582-c3c81c7d-4617-4104-b669-76749a7468a0.png\" width=\"400\" height=\"700\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
1	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40275 has implemented the `functions#broadcast`, so this pr remove the corresponding `ProblemFilters.exclude` rule from `CheckConnectJvmClientCompatibility`### Why are the changes needed?Remove `unnecessary` `ProblemFilters.exclude` rule.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual check `dev/connect-jvm-client-mima-check` passed
1	-2	### What changes were proposed in this pull request?Scheduler micro optimizations to speed up the scheduling loop.### Why are the changes needed?The scheduler is single threaded and the faster we can schedule the faster a query executes. Changes to the scheduler usually adversely affect scheduling throughput which makes it hard to prototype novel changes. Getting some headroom makes experimentation faster while not hurting production.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
1	-1	### What changes were proposed in this pull request?This PR proposes to do a minor refactoring in `SparkSession`, particularly the private method `applyExtensions`### Why are the changes needed?This is a pure refactoring to reduce some code duplications such as `getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty)`### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing tests
1	-1	### What changes were proposed in this pull request?Recently, I found Column.explain missing test cases.This PR want add these test cases for easy to find the change if the `def toString` or `def sql` changed.### Why are the changes needed?Add test cases for Column.explain### Does this PR introduce _any_ user-facing change?'No'.Just add test cases for Column.explain.### How was this patch tested?New test cases.
2	-1	### What changes were proposed in this pull request?Currently, connect display the structure of the proto in both the regular and extended version of explain. We should display a more compact sql-a-like string for the regular version.### Why are the changes needed?Improve output of Column.explain so as display the same as the explain API of sql expression.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
1	-1	### What changes were proposed in this pull request?Implements `DataFrame.registerTempTable`.### Why are the changes needed?Missing API.### Does this PR introduce _any_ user-facing change?`DataFrame.registerTempTable` will be available.### How was this patch tested?Enabled a related test.
1	-1	### What changes were proposed in this pull request?Enables more parity tests.### Why are the changes needed?We can enable more parity tests.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled related parity tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The optimizer rule `CombineFilters` is included in `PushDownPredicates`. However, both `PushDownPredicates` and `CombineFilters` shows up in the `defaultBatches` of Optimizer.This PR is to remove the duplicated rule CombineFilters in the Optimizer.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Remove a duplicated rule in the Optimizer.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
1	-1	### What changes were proposed in this pull request?Fix `UserDefinedFunction` to have `returnType`.### Why are the changes needed?Currently `UserDefinedFunction` doesn't have `returnType` attribute.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled/modified the related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In `EquivalentExpressions.addExpr()`, add a guard `supportedExpression()` to make it consistent with `addExprTree()` and `getExprState()`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This fixes a regression caused by https://github.com/apache/spark/pull/39010 which added the `supportedExpression()` to `addExprTree()` and `getExprState()` but not `addExpr()`.One example of a use case affected by the inconsistency is the `PhysicalAggregation` pattern in physical planning. There, it calls `addExpr()` to deduplicate the aggregate expressions, and then calls `getExprState()` to deduplicate the result expressions. Guarding inconsistently will cause the aggregate and result expressions go out of sync, eventually resulting in query execution error (or whole-stage codegen error).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This fixes a regression affecting Spark 3.3.2+, where it may manifest as an error running aggregate operators with higher-order functions.Example running the SQL command:```sqlselect max(transform(array(id), x -> x)), max(transform(array(id), x -> x)) from range(2)```example error message before the fix:```java.lang.IllegalStateException: Couldn't find max(transform(array(id#0L), lambdafunction(lambda x#2L, lambda x#2L, false)))#4 in [max(transform(array(id#0L), lambdafunction(lambda x#1L, lambda x#1L, false)))#3]```after the fix this error is gone.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added new test cases to `SubexpressionEliminationSuite` for the immediate issue, and to `DataFrameAggregateSuite` for an example of user-visible symptom.
2	-2	### What changes were proposed in this pull request?This PR reverts the follow-up PR of SPARK-41468: https://github.com/apache/spark/pull/39046### Why are the changes needed?These changes are not needed and actually might cause performance regression due to preventing higher order function subexpression elimination in `EquivalentExpressions`. Please find related conversation here: https://github.com/apache/spark/pull/40473#issuecomment-1474848224### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing UTs.
1	-1	### What changes were proposed in this pull request?The pr aims to remove unused properties in pom file.### Why are the changes needed?Make the code more concise.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`DeduplicateRelations` rule show process `LOGICAL_RDD`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Spark throw `AnalysisException` when checkout dataset join with origin dataset. Beacause checkout dataset will be skip in `DeduplicateRelations` rule.```    val df = spark.range(10).toDF(\"id\")    val cdf = df.checkpoint()``````Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Failure when resolving conflicting references in Join:'Join Inner:- LogicalRDD [id#5L], false+- Project [id#3L AS id#5L]   +- Range (0, 10, step=1, splits=Some(8))Conflicting attributes: id#5L;'Join Inner:- LogicalRDD [id#5L], false+- Project [id#3L AS id#5L]   +- Range (0, 10, step=1, splits=Some(8))    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:56)    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:55)```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add unit test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR addresses non-blocking comments for PR #40421.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to make sure the new logic only applies in expected cases.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
2	-3	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/40097 that proposes to exclude org.apache.spark.ml.param.FloatParam$ for Scala 2.13:```[error] spark-mllib: Failed binary compatibility check against org.apache.spark:spark-mllib_2.13:3.3.0! Found 1 potential problems (filtered 646)[error]  * object org.apache.spark.ml.param.FloatParam does not have a correspondent in current version[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.ml.param.FloatParam$\")[error] java.lang.RuntimeException: Failed binary compatibility check against org.apache.spark:spark-mllib_2.13:3.3.0! Found 1 potential problems (filtered 646)[error] \tat scala.sys.package$.error(package.scala:30)[error] \tat com.typesafe.tools.mima.plugin.SbtMima$.reportModuleErrors(SbtMima.scala:89)[error] \tat com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2(MimaPlugin.scala:36)[error] \tat com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2$adapted(MimaPlugin.scala:26)[error] \tat scala.collection.Iterator.foreach(Iterator.scala:943)```See https://github.com/apache/spark/actions/runs/4430000174/jobs/7771194350.### Why are the changes needed?To make Scala 2.13 build pass### Does this PR introduce _any_ user-facing change?No, dev-only.### How was this patch tested?Will monitor the build after this gets merged at https://github.com/apache/spark/actions.
2	-1	### What changes were proposed in this pull request?This pr aims support  `functions#array_prepend` for Scala connect client.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Add new test
3	-3	### What changes were proposed in this pull request?This pr aims to  Revert \"[SPARK-42809][BUILD] Upgrade scala-maven-plugin from 4.8.0 to 4.8.1\".### Why are the changes needed?As mentioned in https://github.com/apache/spark/pull/40442,  there are some regression with the 4.8.1：- Run `./build/mvn -DskipTests clean package` with Java 17 will failed```[INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ spark-core_2.12 ---[INFO] Not compiling main sources[INFO][INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)[INFO] compiling 597 Scala sources and 103 Java sources to /home/bjorn/github/spark/core/target/scala-2.12/classes ...[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: value sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: value sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: value Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: value Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: value Signal[ERROR] 19 errors found[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:[INFO][INFO] Spark Project Parent POM ........................... SUCCESS [ 3.848 s][INFO] Spark Project Tags ................................. SUCCESS [ 12.106 s][INFO] Spark Project Sketch ............................... SUCCESS [ 10.685 s][INFO] Spark Project Local DB ............................. SUCCESS [ 8.743 s][INFO] Spark Project Networking ........................... SUCCESS [ 9.362 s][INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 7.828 s][INFO] Spark Project Unsafe ............................... SUCCESS [ 9.071 s][INFO] Spark Project Launcher ............................. SUCCESS [ 4.776 s][INFO] Spark Project Core ................................. FAILURE [ 17.228 s]```- Run `build/mvn clean install  -DskipTests -Pscala-2.13` with Java8 + Scala 2.13There are compilation errors as `ERROR] -release is only supported on Java 9 and higher`  although it does not cause compilation failures. More, I saw https://github.com/davidB/scala-maven-plugin/issues/686, So  it seems that 4.8.1 and Java 8 are not compatible well.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GA
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix RegexFilter's attribute 'regex' spelling issue### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->test-only### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
2	-2	### What changes were proposed in this pull request?This PR supports eliminate sorts in AQE Optimizer.### Why are the changes needed?Reduce sort to improve query performance. Use case is we push down topK via left outer join to reduce join data. But the number of rows on the left side may be less than the limit.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
2	-1	### What changes were proposed in this pull request?Move `toCatalystValue` to connect-common### Why are the changes needed?ml support for SCSC also needs this, see https://github.com/apache/spark/pull/40479/files#r1141524125, so we'd better move it to the `connect-common`### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?existing UT
2	-1	### What changes were proposed in this pull request?Implement CoGrouped Map API: `applyInPandas`.### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. CoGrouped Map API is supported as shown below.```sh>>> import pandas as pd>>> df1 = spark.createDataFrame(...   [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)], (\"time\ \"id\ \"v1\"))>>> >>> df2 = spark.createDataFrame(...   [(20000101, 1, \"x\"), (20000101, 2, \"y\")], (\"time\ \"id\ \"v2\"))>>> >>> def asof_join(l, r):...   return pd.merge_asof(l, r, on=\"time\ by=\"id\")... >>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(...   asof_join, schema=\"time int, id int, v1 double, v2 string\"... ).show()+--------+---+---+---+|    time| id| v1| v2|+--------+---+---+---+|20000101|  1|1.0|  x||20000102|  1|3.0|  x||20000101|  2|2.0|  y||20000102|  2|4.0|  y|+--------+---+---+---+```### How was this patch tested?Parity unit tests.
2	-2	### What changes were proposed in this pull request?This PR proposes to replace `EquivalentExpressions` to a simple mutable map in `PhysicalAggregation`, the only place where `EquivalentExpressions.addExpr()` is used. `EquivalentExpressions` is useful for common subexpression elimination but in `PhysicalAggregation` it is used only to deduplicate whole expressions which can be easily done with a simple map.### Why are the changes needed?`EquivalentExpressions.addExpr()` is not guarded by `supportedExpression()` and so it can cause inconsistent results when used together with `EquivalentExpressions.getExprState()`. This PR proposes replacing `.addExpr()` with other alternatives as its boolean result is a bit counter-intuitive to other collections' `.add()` methods. It returns `false` if the expression was missing and either adds the expression or not depending on if the expression is deterministic.After this PR we no longer use `EquivalentExpressions.addExpr()` so it can be deprecated or even removed...### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added new UTs from @rednaxelafx's PR: https://github.com/apache/spark/pull/40473. Please note that those UTs actually pass after https://github.com/apache/spark/pull/40475, but they are added here to make sure there will be no regression in the future.
1	-1	### What changes were proposed in this pull request?This pr aims upgrade slf4j from 2.0.6 to 2.0.7### Why are the changes needed?This version fixed several OSGi MANIFEST.MF related issues, the release notes as follows:- https://www.slf4j.org/news.html#2.0.7### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-1	### What changes were proposed in this pull request?This version aims upgrade log4j2 from 2.19.0 to 2.20.0### Why are the changes needed?This version brings some bug fix like [Fix java.sql.Time object formatting in MapMessage ](https://issues.apache.org/jira/browse/LOG4J2-2297) and [Fix level propagation in Log4jBridgeHandler](https://issues.apache.org/jira/browse/LOG4J2-3634), and some new support like [Add support for timezones in RollingFileAppender](https://issues.apache.org/jira/browse/LOG4J2-1631), the release notes as follows:- https://logging.apache.org/log4j/2.x/release-notes/2.20.0.html### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-2	### What changes were proposed in this pull request?When using the Spark Kubernetes library to launch multiple jobs, the name for the config maps is only generated once, meaning all jobs will try to use the same one. This results in an error.Relates issue: https://issues.apache.org/jira/browse/SPARK-41006This PR makes the name of the config-maps dynamic so that each run gets its own. Relate Unit Tests have been updated to take the change into account.### Why are the changes needed?The change is needed to allow launching multiple jobs using the Spark Kubernetes library.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?The fixed library has been tested on a private K8S cluster to ensure we can launch multiple jobs with the config maps getting different names.
1	-2	### What changes were proposed in this pull request?This pr re-generates golden files for `array_prepend` functions. It seems that the newly added case in https://github.com/apache/spark/pull/38947 is missing from the golden files due to lack of rebase when merging https://github.com/apache/spark/pull/40449.### Why are the changes needed? Re-generates golden files for `array_prepend` functions to Pass GitHub Actions### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manually checked with Scala 2.13
3	-2	…ROR_TEMP_2003https://issues.apache.org/jira/browse/SPARK-42839modified:1、error-classes.json: \"_LEGACY_ERROR_TEMP_2003\" --> \"CANNOT_ZIP_MAPS\"2、create a test case：named ”LegacyErrorTempSuit.scala“ under package /spark/sql/core/src/test/scala/org/apache/spark/sql/3、use sbt> sql/testOnly LegacyErrorTempSuit, All tests passed<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix typos in the repo.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve readability.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No tests are needed.
2	-1	only for test, please ignore it
2	-2	### What changes were proposed in this pull request?This PR enables the new golden file test framework for analysis for all input files.Background:* In https://github.com/apache/spark/pull/40449 we added the ability to exercise the analyzer on the SQL queries in existing golden files in the `sql/core/src/test/resources/sql-tests/inputs` directory, writing separate output test files in the new `sql/core/src/test/resources/sql-tests/analyzer-results` directory in additional to the original output directory for full end-to-end query execution results.* That PR also added an allowlist of input files to include in this new dual-run mode.* In this PR, we remove that allowlist exercise the new dual-run mode for all the input files. We also extend the analyzer testing to support separate test cases in ANSI-mode, TimestampNTZ, and UDFs.### Why are the changes needed?This improves test coverage and helps prevent against accidental regressions in the future as we edit the code.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?This PR adds testing only.
1	-1	### What changes were proposed in this pull request?Fix `DataFrame.toPandas()` to handle timezone and map types properly.### Why are the changes needed?Currently `DataFrame.toPandas()` doesn't handle timezone for timestamp type, and map types properly.For example:```py>>> schema = StructType().add(\"ts\ TimestampType())>>> spark.createDataFrame([(datetime(1969, 1, 1, 1, 1, 1),), (datetime(2012, 3, 3, 3, 3, 3),), (datetime(2100, 4, 4, 4, 4, 4),)], schema).toPandas()                         ts0 1969-01-01 01:01:01-08:001 2012-03-03 03:03:03-08:002 2100-04-04 03:04:04-08:00```which should be:```py                   ts0 1969-01-01 01:01:011 2012-03-03 03:03:032 2100-04-04 04:04:04```### Does this PR introduce _any_ user-facing change?The result of `DataFrame.toPandas()` with timestamp type and map type will be the same as PySpark.### How was this patch tested?Enabled the related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->It turns out that `spark.read.option.table` is a valid call chain and the `table` API does accept options when open a table.Existing Spark Connect implementation does not consider it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Feature parity.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`physicalDataType` should not be a public API but be private[sql].### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is to limit API scope to not expose unnecessary API to be public.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No since we have not released Spark 3.4.0 yet.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
1	-2	### What changes were proposed in this pull request?Make `IsotonicRegression.PointsAccumulator` private, which was introduced in https://github.com/apache/spark/commit/3d05c7e037eff79de8ef9f6231aca8340bcc65ef### Why are the changes needed?`PointsAccumulator` is implementation details, should not be exposed### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing UT
1	-2	### What changes were proposed in this pull request?Make `IsotonicRegression.PointsAccumulator` private, which was introduced in https://github.com/apache/spark/commit/3d05c7e037eff79de8ef9f6231aca8340bcc65ef### Why are the changes needed?`PointsAccumulator` is implementation details, should not be exposed### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Adds Repeat Identifier: to the cached RDD node on the Stages page. Made the Repeat Identifier: have bolded text so that it's easier to distinguish from the rest of the text.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Currently there is no way to distinguish which cached RDD is being executed in a particular stage. This aims to fix that.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, on the Stages page in the UI when there is a cached RDD. One example is <img width=\"860\" alt=\"Screen Shot 2023-03-20 at 3 55 40 PM\" src=\"https://user-images.githubusercontent.com/16739760/226527044-4c0719f4-aaf8-4e99-8bce-fd033106379c.png\"><!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Manual test locally in SQL UI.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Adds text on the UI that shows which executed stage a skipped stage on the UI refers to.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Helps find the execution details, in terms of  figuring out which stages from earlier jobs feed into a later stage in a specific job.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the jobs page can look like the following, where the skipped stage is now clickable and redirects to the stage that was actually executed <img width=\"357\" alt=\"Screen Shot 2023-03-20 at 5 36 14 PM\" src=\"https://user-images.githubusercontent.com/16739760/226529417-a2384904-7e46-48f6-bcd5-c708416e6353.png\"><!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Manual test locally on UI.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request? Update log4j1 syntax to log4j2, and use ${sys:spark.yarn.app.container.log.dir} to relocate log path.see https://issues.apache.org/jira/browse/SPARK-42880### Why are the changes needed?Since Spark3.3 has changed log4j1 to log4j2, some documents should also be updated. ### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Just doc.
1	-2	### What changes were proposed in this pull request?This PR proposes to Remove SparkSession constructor invocation in the example.While I am here, I piggyback and add an example of Spark Connect.### Why are the changes needed?SparkSession's constructor is not meant to be exposed to the end users. This is also hidden in Scala side.### Does this PR introduce _any_ user-facing change?Yes, it removes the usage of SparkSession constructor in the user facing docs.### How was this patch tested?Linters should verify the changes in the CI.
2	-1	### What changes were proposed in this pull request?The PR adds Codegen Support for get_json_object.### Why are the changes needed?Improve codegen coverage and performance.Github benchmark data(https://github.com/panbingkun/spark/actions/runs/4497396473/jobs/7912952710):<img width=\"879\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/227117793-bab38c42-dcc1-46de-a689-25a87b8f3561.png\">Local benchmark data:<img width=\"895\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/227098745-9b360e60-fe84-4419-8b7d-073a0530816a.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add new UT.Pass GA.
2	-2	### What changes were proposed in this pull request?This PR proposes adding the `DistributedSequenceID` message to support pandas API on Spark in Spark Connect. `DistributedSequenceID` create the distributed-sequence column which is used to generate [default index type](https://spark.apache.org/docs/3.2.0/api/python/user_guide/pandas_on_spark/options.html#default-index-type) for pandas API on Spark.```python>>> from pyspark.sql.connect.expressions import DistributedSequenceID>>> from pyspark.sql.connect.column import Column>>> data = [(\"Alice\ 1), (\"Bob\ 2), (\"Charlie\ 3)]>>> sdf = spark.createDataFrame(data, [\"name\ \"age\"])>>> sdf.show()+-------+---+|   name|age|+-------+---+|  Alice|  1||    Bob|  2||Charlie|  3|+-------+---+>>> sdf.select(Column(DistributedSequenceID()).alias(\"sequence-index\"), \"*\").show()+--------------+-------+---+|sequence-index|   name|age|+--------------+-------+---+|             0|  Alice|  1||             1|    Bob|  2||             2|Charlie|  3|+--------------+-------+---+```### Why are the changes needed?Spark Connect cannot reuse the existing logic for pandas API on Spark, because the existing logic uses Py4J to utilize functions in the JVM.### Does this PR introduce _any_ user-facing change?No, this is an internal function.### How was this patch tested?The patch was tested by adding unit tests and manually verifying the results.
2	-1	### What changes were proposed in this pull request?In the PR, I propose to clarify the comment of `args` in parameterized `sql()`.### Why are the changes needed?To make the comment more clear and highlight that input strings are parsed (not evaluated), and considered as SQL literal expressions. Also while parsing the fragments w/ SQL comments in the string values are skipped.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?By checking coding style:```$ ./dev/lint-python$ ./dev/scalastyle```
1	-2	### What changes were proposed in this pull request?This PR aims to upgrade `kubernetes-client` to 6.5.1.### Why are the changes needed?To bring the latest bug fixes.- https://github.com/fabric8io/kubernetes-client/releases/tag/v6.5.1### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs
1	-1	### What changes were proposed in this pull request?Implements `DataFrame.cache`, `persist`, `unpersist`, and `storageLevel`.### Why are the changes needed?Missing APIs.### Does this PR introduce _any_ user-facing change?`DataFrame.cache`, `persist`, `unpersist`, and `storageLevel` will be available.### How was this patch tested?Added/enabled the related tests.
3	-2	### What changes were proposed in this pull request?Upgrade the [GCS Connector](https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/v2.2.11/gcs) bundled in the Spark distro from version 2.2.7 to 2.2.11.### Why are the changes needed?The new release contains multiple bug fixes and enhancements discussed in the [Release Notes](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v2.2.11/gcs/CHANGES.md). Notable changes include:* Improved socket timeout handling.* Trace logging capabilities.* Fix bug that prevented usage of GCS as a [Hadoop Credential Provider](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html).* Dependency upgrades.* Support OAuth2 based client authentication.### Does this PR introduce _any_ user-facing change?Distributions built with `-Phadoop-cloud` now include GCS connector 2.2.11 instead of 2.2.7.```cnauroth@cnauroth-2-1-m:~/spark-3.5.0-SNAPSHOT-bin-custom-spark$ ls -lrt jars/gcs*-rw-r--r-- 1 cnauroth cnauroth 36497606 Mar 21 00:42 jars/gcs-connector-hadoop3-2.2.11-shaded.jar```### How was this patch tested?**Build**I built a custom distro with `-Phadoop-cloud`:```./dev/make-distribution.sh --name custom-spark --pip --tgz -Phadoop-3 -Phadoop-cloud -Pscala-2.12```**Run**I ran a PySpark job that successfully reads and writes using GCS:```from pyspark.sql import SparkSessiondef main() -> None:  # Create SparkSession.  spark = (SparkSession.builder           .appName('copy-shakespeare')           .getOrCreate())  # Read.  df = spark.read.text('gs://dataproc-datasets-us-central1/shakespeare')  # Write.  df.write.text('gs://cnauroth-hive-metastore-proxy-dist/output/copy-shakespeare')  spark.stop()if __name__ == '__main__':  main()```Authored-by: Chris Nauroth <cnauroth@apache.org>
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR moves the following methods from `DataType`:1. equalsIgnoreNullability2. sameType3. equalsIgnoreCaseAndNullabilityThe moved methods are put together into a Util class.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->To make `DataType` become a simpler interface, non-public methods can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No as the moved methods are private within Spark.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT.
2	-2	### What changes were proposed in this pull request?Block the usage of Arrow-optimized Python UDFs in Apache Spark 3.4.0. ### Why are the changes needed?Considering the upcoming improvements on the result inconsistencies between traditional Pickled Python UDFs and Arrow-optimized Python UDFs, we'd better block the feature, otherwise, users who try out the feature will expect behavior changes in the next release.In addition, since Spark Connect Python Client(SCPC) has been introduced in Spark 3.4, we'd better ensure the feature is ready in both vanilla PySpark and SCPC at the same time for compatibility.### Does this PR introduce _any_ user-facing change?Yes. Arrow-optimized Python UDFs are blocked.### How was this patch tested?Existing tests.
1	-1	### What changes were proposed in this pull request?This is a follow-up of #38947.Add `array_prepend` function to Spark Connect Python client.### Why are the changes needed?`array_prepend` was added at #38947 without Spark Connect Python client.### Does this PR introduce _any_ user-facing change?`array_prepend`  will be available in Spark Connect Python client.### How was this patch tested?Enabled the related test.
1	-2	### What changes were proposed in this pull request?This PR adds Ammonite REPL integration for Spark Connect. This has a couple of benefits:- It makes it a lot less cumbersome for users to start a spark connect REPL. You don't have to add custom scripts, and you can use `coursier` to launch a fully function REPL for you.- It adds REPL integration for to the actual build. This makes it easier to validate the code we add is actually working.### Why are the changes needed?A REPL is arguably the first entry point for a lot of users.### Does this PR introduce _any_ user-facing change?Yes it adds REPL integration.### How was this patch tested?Added tests for the command line parsing. Manually tested the REPL.
2	-1	### What changes were proposed in this pull request?This pr follow SPARK-42889 to support `cache`/`persist`/`unpersist`/`storageLevel` for Spark connect jvm client### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test
1	-2	### What changes were proposed in this pull request?Revert \"[SPARK-42508][CONNECT][ML] Extract the common .ml classes to `mllib-common`\"but still keep Connect Server depends on `mllib` for [SPARK-42800](https://issues.apache.org/jira/browse/SPARK-42800)### Why are the changes needed?the [previous change](https://github.com/apache/spark/commit/5a5f04c488074ec159be77079647c971fc32eb68) was too invasive, I want to be more careful and rethink before we go ahead for Spark Connect.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing UT
2	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40510 introduce `message StorageLevel` to `base.proto`, but if we try to import `base.proto` in `catalog.proto` to reuse `StorageLevel` in `message CacheTable` and run `build/sbt \"connect-common/compile\" to compile, there will be following message in compile log:```spark/connect/base.proto:23:1: File recursively imports itself: spark/connect/base.proto -> spark/connect/commands.proto -> spark/connect/relations.proto -> spark/connect/catalog.proto -> spark/connect/base.protospark/connect/catalog.proto:22:1: Import \"spark/connect/base.proto\" was not found or had errors.spark/connect/catalog.proto:144:12: \"spark.connect.DataType\" seems to be defined in \"spark/connect/types.proto\ which is not imported by \"spark/connect/catalog.proto\".  To use it here, please add the necessary import.spark/connect/catalog.proto:161:12: \"spark.connect.DataType\" seems to be defined in \"spark/connect/types.proto\ which is not imported by \"spark/connect/catalog.proto\".  To use it here, please add the necessary import.spark/connect/relations.proto:25:1: Import \"spark/connect/catalog.proto\" was not found or had errors.spark/connect/relations.proto:84:5: \"Catalog\" is not defined.spark/connect/commands.proto:22:1: Import \"spark/connect/relations.proto\" was not found or had errors.spark/connect/commands.proto:63:3: \"Relation\" is not defined.spark/connect/commands.proto:81:3: \"Relation\" is not defined.spark/connect/commands.proto:142:3: \"Relation\" is not defined.spark/connect/base.proto:23:1: Import \"spark/connect/commands.proto\" was not found or had errors.spark/connect/base.proto:25:1: Import \"spark/connect/relations.proto\" was not found or had errors.....```So this pr move `message StorageLevel` to a separate file to avoid this potential file recursively imports.### Why are the changes needed?To avoid potential file recursively imports.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual check:    - Add `import \"spark/connect/common.proto\";` to `catalog.proto`    - run `build/sbt \"connect-common/compile\"`No compilation logs related to `File recursively imports itself` .
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Make mapInPandas / mapInArrow support barrier mode execution### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is the preparation PR for supporting mapInPandas / mapInArrow barrier execution in spark connect mode. The feature is required by machine learning use cases.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?Fix some deprecated urls ### Why are the changes needed?Repository about https://github.com/pyspark.pandas was deprecated, using https://github.com/databricks/koalas replace.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Matched urls and existing tests
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/39624 . `QueryStageExec.isMeterialized` should only return true if `resultOption` is assigned. It can be a potential bug to have this inconsistency.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix potential bug### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?For example:```  val df1 = spark.range(5).select($\"id\".as(\"k1\"))  val df2 = spark.range(10).select($\"id\".as(\"k2\"))  df1.join(df2.hint(\"SHUFFLE_MERGE\"),      $\"k1\" === $\"k2\" % 3 && $\"k1\" + 3 =!= $\"k2\" && $\"k1\" + 5 =!= $\"k2\ \"full_outer\")```the join condition `$\"k1\" + 3 =!= $\"k2\"` and `$\"k1\" + 5 =!= $\"k2\"` will evaluate the variable **k1** twice and caused the codegen failed.```09:43:30.155 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 257, Column 9: Redefinition of local variable \"smj_isNull_9\" org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 257, Column 9: Redefinition of local variable \"smj_isNull_9\" ```Before this PR, we will evaluate multiple times for the variables in the join condition, and throw `Redefinition of local variable`  exception.### Why are the changes needed?Bug fix for codegen issue in FullOuter SMJ.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added UT
1	-1	### What changes were proposed in this pull request?This removes the need for a time zone id when casting from StringType -> DateType and DateType -> StringType.### Why are the changes needed?It is mostly for consistency with what the code is actually doing.  Marking it as needing A time zone id has no real impact to the actual execution.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?I just compiled it for now. I assume that the existing tests will cover it.
2	-2	### What changes were proposed in this pull request?This PR proposes to support pandas API on Spark for Spark Connect. This PR includes minimal changes to support basic functionality of the pandas API in Spark Connect, and sets up a testing environment into `pyspark/pandas/tests/connect` using all existing pandas API on Spark test bases to test the functionality of the pandas API on Spark in a remote Spark session.Here is a summary of the key tasks:1. All pandas-on-Spark tests under the `python/pyspark/pandas/tests/` directory can now be performed in Spark Connect by adding corresponding tests to the `python/pyspark/pandas/tests/connect/` directory.2. Unlike with Spark SQL, we did not create a separate package directory such as `python/pyspark/sql/connect` for Spark Connect, so I modified the existing files of `pyspark.pandas`. This allows users to use the existing pandas-on-Spark code as it is on Spark Connect.3. Because of 2, I added two typing rules into `python/pyspark/pandas/_typing.py` for addressing both PySpark Column and Spark Connect Column in the single path.   - Added `GenericColumn` for typing both PySpark Column and Spark Connect Column.   - Added `GenericDataFrame` for typing both PySpark DataFrame and Spark Connect DataFrame.### Why are the changes needed?By supporting the pandas API in Spark Connect, it can significantly improve the usability for existing PySpark and pandas users.### Does this PR introduce _any_ user-facing change?No, because it is designed to allow existing code for regular Spark sessions to be used without any user-facing changes other than switching the regular Spark session to remote Spark session. However, since some features of the existing pandas API on Spark are not fully supported yet, some features may be limited.### How was this patch tested?A testing bed has been set up to reproduce all existing pandas-on-Spark tests for Spark Connect, ensuring that the existing tests can be replicated in Spark Connect. The current result for all tests as below:| Test file                                           | Test total | Test passed | Coverage || --------------------------------------------------- | ---------- | ----------- | -------- || test_parity_dataframe.py                            | 105        | 85          | 80.95%   || test_parity_dataframe_slow.py                       | 66         | 48          | 72.73%   || test_parity_dataframe_conversion.py                 | 11         | 11          | 100.00%  || test_parity_dataframe_spark_io.py                   | 8          | 7           | 87.50%   || test_parity_ops_on_diff_frames.py                   | 75         | 75          | 100.00%  || test_parity_series.py                               | 131        | 104         | 79.39%   || test_parity_series_datetime.py                      | 41         | 34          | 82.93%   || test_parity_categorical.py                          | 29         | 22          | 75.86%   || test_parity_config.py                               | 7          | 7           | 100.00%  || test_parity_csv.py                                  | 18         | 18          | 100.00%  || test_parity_default_index.py                        | 4          | 1           | 25.00%   || test_parity_ewm.py                                  | 3          | 1           | 33.33%   || test_parity_expanding.py                            | 22         | 2           | 9.09%    || test_parity_extention.py                            | 7          | 7           | 100.00%  || test_parity_frame_spark.py                          | 6          | 2           | 33.33%   || test_parity_generic_functions.py                    | 4          | 1           | 25.00%   || test_parity_groupby.py                              | 49         | 36          | 73.47%   || test_parity_groupby_slow.py                         | 205        | 147         | 71.71%   || test_parity_indexing.py                             | 3          | 3           | 100.00%  || test_parity_indexops_spark.py                       | 3          | 3           | 100.00%  || test_parity_internal.py                             | 1          | 0           | 0.00%    || test_parity_namespace.py                            | 29         | 26          | 89.66%   || test_parity_numpy_compat.py                         | 6          | 4           | 66.67%   || test_parity_ops_on_diff_frames_groupby.py           | 22         | 13          | 59.09%   || test_parity_ops_on_diff_frames_groupby_expanding.py | 7          | 0           | 0.00%    || test_parity_ops_on_diff_frames_groupby_rolling.py   | 7          | 0           | 0.00%    || test_parity_ops_on_diff_frames_slow.py              | 22         | 15          | 68.18%   || test_parity_repr.py                                 | 5          | 5           | 100.00%  || test_parity_resample.py                             | 5          | 3           | 60.00%   || test_parity_reshape.py                              | 10         | 8           | 80.00%   || test_parity_rolling.py                              | 21         | 1           | 4.76%    || test_parity_scalars.py                              | 1          | 1           | 100.00%  || test_parity_series_conversion.py                    | 2          | 2           | 100.00%  || test_parity_series_string.py                        | 56         | 55          | 98.21%   || test_parity_spark_functions.py                      | 1          | 1           | 100.00%  || test_parity_sql.py                                  | 7          | 4           | 57.14%   || test_parity_stats.py                                | 15         | 7           | 46.67%   || test_parity_typedef.py                              | 10         | 10          | 100.00%  || test_parity_utils.py                                | 5          | 5           | 100.00%  || test_parity_window.py                               | 2          | 2           | 100.00%  || test_parity_frame_plot.py                           | 7          | 5           | 71.43%   || plot/test_parity_frame_plot_matplotlib.py           | 13         | 11          | 84.62%   || plot/test_parity_frame_plot_plotly.py               | 12         | 9           | 75.00%   || plot/test_parity_series_plot.py                     | 3          | 3           | 100.00%  || plot/test_parity_series_plot_matplotlib.py          | 14         | 8           | 57.14%   || plot/test_parity_series_plot_plotly.py              | 9          | 7           | 77.78%   || indexes/test_parity_base.py                         | 144        | 75          | 52.08%   || indexes/test_parity_category.py                     | 16         | 7           | 43.75%   || indexes/test_parity_datetime.py                     | 13         | 11          | 84.62%   || indexes/test_parity_timedelta.py                    | 2          | 1           | 50.00%   || data_type_ops/test_parity_base.py                   | 2          | 2           | 100.00%  || data_type_ops/test_parity_binary_ops.py             | 30         | 25          | 83.33%   || data_type_ops/test_parity_boolean_ops.py            | 31         | 26          | 83.87%   || data_type_ops/test_parity_categorical_ops.py        | 30         | 23          | 76.67%   || data_type_ops/test_parity_complex_ops.py            | 30         | 30          | 100.00%  || data_type_ops/test_parity_date_ops.py               | 30         | 25          | 83.33%   || data_type_ops/test_parity_datetime_ops.py           | 30         | 25          | 83.33%   || data_type_ops/test_parity_null_ops.py               | 26         | 19          | 73.08%   || data_type_ops/test_parity_num_ops.py                | 33         | 25          | 75.76%   || data_type_ops/test_parity_string_ops.py             | 30         | 23          | 76.67%   || data_type_ops/test_parity_timedelta_ops.py          | 26         | 19          | 73.08%   || data_type_ops/test_parity_udf_ops.py                | 26         | 18          | 69.23%   || Total                                               | 1588       | 1173        | 73.87%   |
2	-3	### What changes were proposed in this pull request?Fixes `DataFrame.to(schema)` to handle the case where there is a non-nullable nested field in a nullable field.### Why are the changes needed?`DataFrame.to(schema)` fails when it contains non-nullable nested field in nullable field:```scalascala> val df = spark.sql(\"VALUES (1, STRUCT(1 as i)), (NULL, NULL) as t(a, b)\")df: org.apache.spark.sql.DataFrame = [a: int, b: struct<i: int>]scala> df.printSchema()root |-- a: integer (nullable = true) |-- b: struct (nullable = true) |    |-- i: integer (nullable = false)scala> df.to(df.schema)org.apache.spark.sql.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `b`.`i` is nullable while it's required to be non-nullable.```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added the related tests.
3	-1	### What changes were proposed in this pull request?Fixes `createDataFrame` to respect inference and column names.### Why are the changes needed?Currently when a column name list is provided as a schema, the type inference result is not taken care of.As a result, `createDataFrame` from UDT objects with column name list doesn't take the UDT type.For example:```py>>> from pyspark.ml.linalg import Vectors>>> df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0))], [\"label\ \"weight\ \"features\"])>>> df.printSchema()root |-- label: double (nullable = true) |-- weight: double (nullable = true) |-- features: struct (nullable = true) |    |-- type: byte (nullable = false) |    |-- size: integer (nullable = true) |    |-- indices: array (nullable = true) |    |    |-- element: integer (containsNull = false) |    |-- values: array (nullable = true) |    |    |-- element: double (containsNull = false)```, which should be:```py>>> df.printSchema()root |-- label: double (nullable = true) |-- weight: double (nullable = true) |-- features: vector (nullable = true)```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added the related tests.
2	-1	### What changes were proposed in this pull request?Currently, connect display the structure of the proto in both the regular and extended version of explain. We should display a more compact sql-a-like string for the regular version.### Why are the changes needed?Improve output of `Column.explain` so as display the same as the explain API of sql expression.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?On the SQL page in the Web UI, this PR aims to add a repeat identifier to distinguish which InMemoryTableScan is being used at a certain location on the DAG. Ideally this would work in sync with changes from SPARK-42829.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Currently there is no distinction for which InMemoryTableScan is being used at a specific point in the DAG.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, on the SQL page in the UI when there is a InMemoryTableScan. One example is<img width=\"1012\" alt=\"Screen Shot 2023-03-22 at 2 06 14 PM\" src=\"https://user-images.githubusercontent.com/16739760/227093764-618cee51-bd0a-4c27-9945-2cc122819c74.png\"><!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Manual test locally in SQL UI along with a few Unit Tests<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-1	### What changes were proposed in this pull request?This PR proposes to remove None as as a return value in docstring.### Why are the changes needed?To be consistent with the current documentation. Also, it's idiomatic to don't specify the return for `return None`.### Does this PR introduce _any_ user-facing change?Yes, it changes the user-facing documentation.### How was this patch tested?Doc build in the CI should verify them.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add type mapping for spark char/varchar to jdbc types.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The STANDARD JDBC 1.0 and other modern databases define char/varchar normatively.This is currently a kind of bug for DDLs on JDBCCatalogs for encountering errors like```Cause: org.apache.spark.SparkIllegalArgumentException: Can't get JDBC type for varchar(10).[info]   at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGetJdbcTypeError(QueryExecutionErrors.scala:1005)```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, char/varchar are allow for jdbc catalogs### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new ut
2	-2	### What changes were proposed in this pull request?Avoid documenting None as as a return value in docstring.### Why are the changes needed?In Python, it's idiomatic to don't specify the return for return None.### Does this PR introduce _any_ user-facing change?No.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change the generated resource name prefix to meet K8s requirements> DNS-1035 label must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name',  or 'abc-123', regex used for validation is '[a-z]([-a-z0-9]*[a-z0-9])?')### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In current implementation, the following app name causes error```bin/spark-submit \\\t --master k8s://https://*.*.*.*:6443 \\\t --deploy-mode cluster \\\t --name 你好_187609 \\         ...``````Exception in thread \"main\" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing:   POST at: https://*.*.*.*:6443/api/v1/namespaces/spark/services. Message:  Service \"187609-f19020870d12c349-driver-svc\" is invalid: metadata.name: Invalid value: \"187609-f19020870d12c349-driver-svc\": a DNS-1035 label must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name',  or 'abc-123', regex used for validation is '[a-z]([-a-z0-9]*[a-z0-9])?'). ```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT.
2	-2	### What changes were proposed in this pull request?Raise RuntimeError when SparkContext is required but not initialized.### Why are the changes needed?Error improvement.### Does this PR introduce _any_ user-facing change?Error type and message change. Raise a RuntimeError with a clear message (rather than an AssertionError) when SparkContext is required but not initialized yet.### How was this patch tested?Unit test.
2	-2	### What changes were proposed in this pull request? Implement Avro functions### Why are the changes needed?For function parity### Does this PR introduce _any_ user-facing change?yes, new APIs### How was this patch tested?added doctest and manually check```(spark_dev) ➜  spark git:(connect_avro_functions) ✗ bin/pyspark --remote \"local[*]\" --jars connector/avro/target/scala-2.12/spark-avro_2.12-3.5.0-SNAPSHOT.jarPython 3.9.16 (main, Mar  8 2023, 04:29:24) Type 'copyright', 'credits' or 'license' for more informationIPython 8.11.0 -- An enhanced Interactive Python. Type '?' for help.23/03/23 16:28:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Mar  8 2023 04:29:24)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.In [1]:     >>> from pyspark.sql import Row   ...:     >>> from pyspark.sql.avro.functions import from_avro, to_avro   ...:     >>> data = [(1, Row(age=2, name='Alice'))]   ...:     >>> df = spark.createDataFrame(data, (\"key\ \"value\"))   ...:     >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))In [2]: avroDf.collect()Out[2]: [Row(avro=bytearray(b'\\x00\\x00\\x04\\x00\Alice'))]```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR improves error messages when users attempt to invoke session operations on a stopped Spark session.### Why are the changes needed?To make the error messages more user-friendly.For example:```pythonspark.stop()spark.sql(\"select 1\")```Before this PR, this code will throw two exceptions:```ValueError: Cannot invoke RPC: Channel closed!During handling of the above exception, another exception occurred:Traceback (most recent call last):  ...    return e.code() == grpc.StatusCode.UNAVAILABLEAttributeError: 'ValueError' object has no attribute 'code'```After this PR, it will show this exception:```[NO_ACTIVE_SESSION] No active Spark session found. Please create a new Spark session before running the code.```### Does this PR introduce _any_ user-facing change?Yes. This PR modifies the error messages.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit test.
1	-3	### What changes were proposed in this pull request?With the while loop around service startup, any ENTER hit in the SimpleSparkConnectService console made it loop around, try to start the service anew, and fail with address already in use.Change the loop to be around the `StdIn.readline()` entry.### Why are the changes needed?Better testing / development / debugging with SparkConnect### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual use of `connector/connect/bin/spark-connect`
2	-2	### What changes were proposed in this pull request?Introduces more basic exceptions.- ArithmeticException- ArrayIndexOutOfBoundsException- DateTimeException- NumberFormatException- SparkRuntimeException### Why are the changes needed?There are more exceptions that Spark throws but PySpark doesn't capture.We should introduce more basic exceptions; otherwise we still see `Py4JJavaError` or `SparkConnectGrpcException`.```py>>> spark.conf.set(\"spark.sql.ansi.enabled\ True)>>> spark.sql(\"select 1/0\")DataFrame[(1 / 0): double]>>> spark.sql(\"select 1/0\").show()Traceback (most recent call last):...py4j.protocol.Py4JJavaError: An error occurred while calling o44.showString.: org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:225)... JVM's stacktrace``````py>>> spark.sql(\"select 1/0\").show()Traceback (most recent call last):...pyspark.errors.exceptions.connect.SparkConnectGrpcException: (org.apache.spark.SparkArithmeticException) [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^```### Does this PR introduce _any_ user-facing change?The error message is more readable.```py>>> spark.sql(\"select 1/0\").show()Traceback (most recent call last):...pyspark.errors.exceptions.captured.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^```or```py>>> spark.sql(\"select 1/0\").show()Traceback (most recent call last):...pyspark.errors.exceptions.connect.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^```### How was this patch tested?Added the related tests.
2	-1	### What changes were proposed in this pull request?Implement CoGrouped Map API: `applyInPandas`.The PR is a cherry-pick of https://github.com/apache/spark/commit/1fbc7948e57cbf05a46cb0c7fb2fad4ec25540e6, with minor changes on test class names to adapt to branch-3.4.### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. CoGrouped Map API is supported as shown below.```sh>>> import pandas as pd>>> df1 = spark.createDataFrame(...   [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)], (\"time\ \"id\ \"v1\"))>>>>>> df2 = spark.createDataFrame(...   [(20000101, 1, \"x\"), (20000101, 2, \"y\")], (\"time\ \"id\ \"v2\"))>>>>>> def asof_join(l, r):...   return pd.merge_asof(l, r, on=\"time\ by=\"id\")...>>> df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(...   asof_join, schema=\"time int, id int, v1 double, v2 string\"... ).show()+--------+---+---+---+|    time| id| v1| v2|+--------+---+---+---+|20000101|  1|1.0|  x||20000102|  1|3.0|  x||20000101|  2|2.0|  y||20000102|  2|4.0|  y|+--------+---+---+---+```### How was this patch tested?Parity unit tests.
1	-1	### What changes were proposed in this pull request?This PR proposes refactoring `DistributedSequenceID` by leveraging `transformUnregisteredFunction` and remote it from proto message.### Why are the changes needed?To follow the existing structure and make proto messages simpler.### Does this PR introduce _any_ user-facing change?No, it's used for internal purpose.### How was this patch tested?Updated UTs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is the only issue I found during SQL module API auditing via https://github.com/apache/spark-website/pull/443/commits/615986022c573aedaff8d2b917a0d2d9dc2b67ef . Somehow `protected[sql]` also generates API doc which is unexpected. `private[sql]` solves the problem and I generated doc locally to verify it.Another API issue has been fixed by https://github.com/apache/spark/pull/40499### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix api doc### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
2	-1	### What changes were proposed in this pull request?The PR adds Codegen Support for sentences.### Why are the changes needed?Improve codegen coverage and performance.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add new UT.Pass GA.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this PR, we make the JDBCTableCatalog mapping the Char/Varchar to the raw implementation to avoid losing meta information.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->For some DDLs related to column updating, the raw types are needed.Otherwise, you may get string->varchar/char casting errors according to the underlying database. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, you can create a table with a varchar column and increase its width. But w/o this PR, you got error### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new unit tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix nullability clause for derby dialect, according to the official derby lang ref guide.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To fix bugs like:```spark-sql ()> create table src2(ID INTEGER NOT NULL, deptno INTEGER NOT NULL);spark-sql ()> alter table src2 ALTER COLUMN ID drop not null;java.sql.SQLSyntaxErrorException: Syntax error: Encountered \"NULL\" at line 1, column 42.```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, but a necessary bugfix### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Test manually.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change improves the handling of constant and generated metadata fields in `FileSourceStrategy`: instead of relying on hard-wired logic to categorize constant and generated metadata fields and process them, this change embeds the required information when creating them in `FileFormat` and uses this information in `FileSourceStrategy` to process arbitrary metadata fields.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This change is a first step towards allowing file format implementations to inject their own metadata fields into plans.  The second step will be to change `FileScanRdd`/`FileFormat` to be able to populate values of arbitrary constant and generated metadata columns.Once this is done, each file format will be able to declare and populate its own metadata field, e.g. `ParquetFileFormat` can provide the Parquet row index metadata field `ROW_INDEX` without polluting the `FileFormat`.### Does this PR introduce _any_ user-facing change?No, this is strictly a refactor without any functional change.### How was this patch tested?This change is covered by existing tests, e.p. FileMetadataStructSuite.
2	-1	### What changes were proposed in this pull request?This is a follow-up of #40526.`Project.reconcileColumnType` should use `KnownNotNull` instead of `AssertNotNull`, also only when `col.nullable`.### Why are the changes needed?There is a better expression, `KnownNotNull`, for this kind of issue.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove unused variables and method in Spark listeners### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA tests
1	-1	### What changes were proposed in this pull request?Enables tests for UDF with UDT.### Why are the changes needed?Now that UDF with UDT should work, the related tests should be enabled to see if it works.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled/modified the related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Today, `LogicalPlan.metadataOutput` is a `Seq[Attribute]`. However, it always contains `AttributeReference`, because metadata columns are \"pre-resolved\" by nodes implementing `ExposesMetadataColumns`. We can simplify a bunch of code by actually defining `metadataOutput` as a `Seq[AttributeReference]`.* `ExposesMetadataColumns` becomes simpler for a node to implement, because attribute identification and dedup can be factored out in a helper method, and implementing nodes only need a way to copy themselves with updated output. * `AddMetadataColumns` rule can be cleaned up as well with the bonus of eliminating unnecessary metadata column projections it used to impose.### Why are the changes needed?Code cleanup. Easier to reason about, easier to maintain.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing metadata column unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?NOTE: This is a stacked pull request. Ignore the bottom two commits.The work that `AddMetadataColumns` analyzer pass does for `Project` nodes can be expressed directly by `Project` itself by implementing `ExposesMetadataColumns`. ### Why are the changes needed?Simpler analysis rule, logic is more localized.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing metadata column tests.
2	-3	### What changes were proposed in this pull request?After https://github.com/apache/spark/pull/40496, run ```SPARK_ANSI_SQL_MODE=true build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite\"```There is one test faild with `spark.sql.ansi.enabled = true````[info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (11 milliseconds)[info]   timestampNTZ/datetime-special.sql_analyzer_test[info]   Expected \"...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ...\ but got \"...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ...\" Result did not match for query #1[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)[info]   org.scalatest.exceptions.TestFailedException:```The failure reason is the last parameter of function `MakeDate` is `failOnError: Boolean = SQLConf.get.ansiEnabled`.So this pr split `timestampNTZ/datetime-special.sql` into w/ and w/o ansi to mask this test difference.### Why are the changes needed?Make SQLQueryTestSuite test pass with `spark.sql.ansi.enabled = true`.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual checked `SPARK_ANSI_SQL_MODE=true build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite\"`
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add getString API for Dataset for usecases where string representation needs to be used other than stdout(println)More details in the bug### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->But there are a lot of cases where we might need to get a String representation of the show output. For example logging framework to which we need to push the representation of a df send the string over a REST call from the driversend the string to stderr instead of stdout### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Tested locally using spark-shell
1	-1	### What changes were proposed in this pull request?Backport for https://github.com/apache/spark/pull/40540This PR proposes refactoring `DistributedSequenceID` by leveraging `transformUnregisteredFunction` and remote it from proto message.### Why are the changes needed?To follow the existing structure and make proto messages simpler.### Does this PR introduce _any_ user-facing change?No, it's used for internal purpose.### How was this patch tested?Updated UTs.
2	-1	### What changes were proposed in this pull request?This PR upgrades Apache Parquet to 1.13.0. Apache Parquet [1.13.0 release notes](https://github.com/apache/parquet-mr/blob/apache-parquet-1.13.0/CHANGES.md?plain=1#L22-L78).### Why are the changes needed?1. This release includes [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160). So we no longer need [SPARK-41952](https://issues.apache.org/jira/browse/SPARK-41952).2. This release includes [Java Vector API support](https://github.com/apache/parquet-mr/blob/apache-parquet-1.13.0/README.md?plain=1#L88-L100).### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing unit test and benchmark test.TPC-DS benchmark result:Query | Parquet 1.13.0(first time) | Parquet 1.12.3(first time) | Parquet 1.13.0(second time) | Parquet 1.12.3(second time) | Parquet 1.13.0(third time) | Parquet 1.12.3(third time)-- | -- | -- | -- | -- | -- | --q1.sql | 37.819 | 37.786 | 36.322 | 37.59 | 37.772 | 36.776q2.sql | 42.132 | 41.513 | 43.189 | 42.274 | 42.859 | 42.605q3.sql | 5.933 | 6.1 | 6.082 | 6.071 | 6.128 | 6.094q4.sql | 335.051 | 319.173 | 322.396 | 320.977 | 324.464 | 326.822q5.sql | 78.41 | 76.631 | 76.841 | 76.37 | 78.257 | 76.502q6.sql | 9.006 | 9.11 | 8.737 | 8.577 | 8.729 | 9.05q7.sql | 12.881 | 12.731 | 12.685 | 12.662 | 12.606 | 12.675q8.sql | 10.122 | 10.092 | 10.035 | 10.853 | 10.277 | 10.841q9.sql | 72.562 | 71.942 | 73.649 | 73.04 | 72.899 | 72.01q10.sql | 14.127 | 13.075 | 14.276 | 13.913 | 13.281 | 13.229q11.sql | 111.334 | 111.612 | 110.952 | 110.776 | 111.686 | 112.27q12.sql | 3.138 | 3.854 | 3.187 | 3.613 | 3.437 | 3.306q13.sql | 13.131 | 12.676 | 12.516 | 12.417 | 12.739 | 12.987q14a.sql | 217.664 | 213.632 | 214.655 | 213.333 | 217.601 | 213.341q14b.sql | 191.553 | 182.775 | 184.35 | 187.004 | 188.313 | 189.876q15.sql | 10.308 | 10.46 | 10.304 | 9.901 | 10.175 | 10.307q16.sql | 81.97 | 82.059 | 82.41 | 81.263 | 83.179 | 82.042q17.sql | 28.876 | 28.905 | 30.41 | 29.573 | 29.555 | 28.837q18.sql | 14.183 | 13.929 | 14.11 | 14.466 | 13.969 | 14.022q19.sql | 6.611 | 7.593 | 6.652 | 6.659 | 6.446 | 6.533q20.sql | 3.263 | 3.701 | 3.56 | 3.503 | 3.53 | 3.627q21.sql | 2.252 | 2.188 | 2.249 | 2.128 | 2.161 | 2.252q22.sql | 14.809 | 14.715 | 14.324 | 14.266 | 14.567 | 14.123q23a.sql | 554.385 | 544.75 | 546.213 | 542.194 | 553.784 | 547.388q23b.sql | 781.236 | 768.367 | 770.584 | 776.065 | 776.502 | 776.006q24a.sql | 196.806 | 193.989 | 197.608 | 194.416 | 194.71 | 192.817q24b.sql | 176.56 | 183.084 | 177.486 | 177.936 | 177.776 | 177.389q25.sql | 22.323 | 22.089 | 22.665 | 22.049 | 22.248 | 22.317q26.sql | 8.574 | 8.356 | 8.174 | 8.753 | 8.186 | 8.302q27.sql | 9.056 | 8.252 | 8.37 | 8.319 | 8.516 | 8.38q28.sql | 102.185 | 102.382 | 102.344 | 103.058 | 102.024 | 102.786q29.sql | 75.655 | 75.604 | 75.217 | 75.532 | 75.835 | 76.024q30.sql | 12.476 | 12.966 | 13.039 | 14.108 | 12.19 | 13.143q31.sql | 26.343 | 27.632 | 26.337 | 26.791 | 26.74 | 26.098q32.sql | 3.251 | 3.41 | 3.378 | 3.333 | 3.371 | 3.516q33.sql | 7.143 | 6.125 | 6.85 | 6.718 | 7.067 | 6.615q34.sql | 8.53 | 8.656 | 8.536 | 8.866 | 8.358 | 8.589q35.sql | 35.212 | 35.571 | 35.659 | 37.631 | 36.292 | 35.603q36.sql | 9.264 | 9.166 | 9.748 | 9.488 | 9.45 | 9.469q37.sql | 36.368 | 35.881 | 37.023 | 36.578 | 35.823 | 36.7q38.sql | 74.58 | 73.472 | 72.926 | 73.823 | 71.097 | 73.329q39a.sql | 8.596 | 7.637 | 8.036 | 7.984 | 7.849 | 7.88q39b.sql | 7.233 | 6.641 | 6.278 | 7.06 | 6.595 | 6.691q40.sql | 17.34 | 16.558 | 16.448 | 16.864 | 16.432 | 16.413q41.sql | 1.223 | 1.105 | 1.103 | 1.182 | 1.232 | 1.304q42.sql | 2.464 | 2.441 | 2.554 | 2.544 | 2.314 | 2.393q43.sql | 7.477 | 7.396 | 7.394 | 7.764 | 7.381 | 7.534q44.sql | 30.228 | 30.516 | 30.859 | 31.057 | 30.372 | 29.008q45.sql | 9.93 | 10.089 | 9.874 | 10.075 | 9.802 | 9.838q46.sql | 9.544 | 9.949 | 9.503 | 9.755 | 9.395 | 9.25q47.sql | 27.322 | 26.952 | 26.974 | 26.83 | 27.087 | 26.991q48.sql | 14.266 | 14.39 | 14.517 | 14.684 | 14.471 | 14.61q49.sql | 21.279 | 21.733 | 20.286 | 20.945 | 22.388 | 21.52q50.sql | 191.416 | 194.256 | 196.701 | 194.113 | 193.354 | 191.004q51.sql | 37.552 | 37.767 | 38.317 | 37.731 | 37.369 | 38.187q52.sql | 2.206 | 2.406 | 2.235 | 2.362 | 2.337 | 2.278q53.sql | 5.282 | 5.131 | 5.465 | 5.137 | 5.142 | 5.069q54.sql | 13.039 | 12.655 | 13.047 | 12.382 | 12.992 | 12.988q55.sql | 2.534 | 2.39 | 2.375 | 2.867 | 2.623 | 2.546q56.sql | 7.365 | 7.087 | 6.902 | 7.406 | 7.586 | 7.081q57.sql | 18.064 | 17.945 | 18.699 | 17.664 | 18.362 | 18.222q58.sql | 6.198 | 6.702 | 6.109 | 6.211 | 5.9 | 6.101q59.sql | 28.266 | 28.195 | 27.876 | 28.748 | 29.027 | 28.543q60.sql | 6.847 | 7.143 | 7.322 | 7.1 | 7.207 | 7.215q61.sql | 7.258 | 7.62 | 7.317 | 7.781 | 7.616 | 7.669q62.sql | 10.334 | 11.523 | 10.389 | 10.378 | 10.072 | 10.583q63.sql | 4.631 | 4.944 | 4.947 | 5.124 | 4.61 | 4.865q64.sql | 249.694 | 252.117 | 254.359 | 254.813 | 253.236 | 250.401q65.sql | 78.742 | 79.184 | 78.559 | 78.305 | 78.985 | 78.515q66.sql | 14.98 | 14.854 | 14.794 | 14.767 | 14.781 | 14.696q67.sql | 1019.744 | 1048.439 | 987.894 | 972.062 | 927.566 | 1002.206q68.sql | 8.903 | 8.915 | 8.277 | 8.709 | 9.349 | 9.178q69.sql | 13.097 | 13.01 | 14.352 | 12.036 | 12.302 | 12.843q70.sql | 21.175 | 21.085 | 21.102 | 20.471 | 20.129 | 19.678q71.sql | 15.13 | 15.526 | 14.929 | 15.231 | 15.406 | 15.487q72.sql | 76.463 | 75.851 | 72.002 | 72.356 | 72.676 | 74.798q73.sql | 5.894 | 6.09 | 5.877 | 6.051 | 6.365 | 6.634q74.sql | 99.106 | 99.356 | 100.291 | 99.51 | 96.766 | 97.292q75.sql | 126.625 | 128.094 | 127.364 | 128.575 | 127.418 | 125.806q76.sql | 35.172 | 33.601 | 34.752 | 34.764 | 34.228 | 35.748q77.sql | 8.394 | 8.01 | 7.951 | 8.061 | 7.839 | 8.348q78.sql | 289.061 | 287.508 | 283.615 | 288.768 | 288.448 | 288.661q79.sql | 10.048 | 9.251 | 9.396 | 9.81 | 8.607 | 8.341q80.sql | 59.68 | 59.458 | 60.234 | 60.415 | 61.325 | 60.744q81.sql | 17.822 | 18.815 | 18.488 | 18.95 | 17.911 | 18.113q82.sql | 64.781 | 63.957 | 63.621 | 64.38 | 63.637 | 64.488q83.sql | 4.686 | 4.922 | 4.635 | 4.827 | 4.678 | 5.071q84.sql | 10.987 | 10.629 | 10.841 | 11.151 | 10.646 | 10.6q85.sql | 12.689 | 13.304 | 13.362 | 13.19 | 13.779 | 12.657q86.sql | 6.48 | 6.491 | 6.722 | 6.667 | 6.833 | 6.52q87.sql | 77.589 | 77.377 | 77.177 | 77.011 | 78.339 | 78.399q88.sql | 83.876 | 83.676 | 84.044 | 83.761 | 84.201 | 84.089q89.sql | 6.741 | 6.564 | 6.755 | 6.708 | 6.704 | 6.794q90.sql | 7.79 | 7.812 | 7.882 | 7.88 | 7.875 | 7.854q91.sql | 4.072 | 3.728 | 3.883 | 3.976 | 4.151 | 4.035q92.sql | 3.05 | 3.155 | 3.336 | 3.067 | 2.942 | 3.099q93.sql | 356.412 | 360.731 | 358.14 | 356 | 356.108 | 358.011q94.sql | 43.202 | 43.561 | 44.63 | 44.486 | 43.993 | 42.693q95.sql | 197.185 | 199.657 | 193.975 | 195.843 | 201.801 | 196.113q96.sql | 12.765 | 12.481 | 12.682 | 12.799 | 12.528 | 12.505q97.sql | 82.895 | 82.067 | 81.754 | 82.799 | 81.788 | 81.572q98.sql | 7.338 | 7.066 | 7.133 | 7.005 | 7.254 | 7.047q99.sql | 18.431 | 17.874 | 17.826 | 17.861 | 17.705 | 17.878total | 7105.675 | 7091.391 | 7030.209 | 7021.7 | 6992.413 | 7047.295
2	-1	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/37353 introduce `o.a.spark.util.Iterators#size` to speed up get `Iterator` size when using Scala 2.13. It will only be used by `o.a.spark.util.Utils#getIteratorSize`, and will disappear when Spark only supports Scala 2.13. It should not be public, so this pr change it access scope to `private[util]`.### Why are the changes needed? `o.a.spark.util.Iterators#size` should not public. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes the function `resolvePersistentFunctionInternal` synchronized.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make function resolution thread-safe.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UTs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The PR fixes the following bug in LCA + having resolution:```sqlselect sum(value1) as total_1, total_1from values(1, 'name', 100, 50) AS data(id, name, value1, value2)having total_1 > 0SparkException: [INTERNAL_ERROR] Found the unresolved operator: 'UnresolvedHaving (total_1#353L > cast(0 as bigint))```To trigger the issue, the having condition need to be (can be resolved by) an attribute in the select.Without the LCA `total_1`, the query works fine.#### Root cause of the issue`UnresolvedHaving` with `Aggregate` as child can use both the `Aggregate`'s output and the `Aggregate`'s child's output to resolve the having condition. If using the latter, `ResolveReferences` rule will replace the unresolved attribute with a `TempResolvedColumn`.For a `UnresolvedHaving` that actually can be resolved directly by its child `Aggregate`, there will be no `TempResolvedColumn` after the rule `ResolveReferences` applies. This  `UnresolvedHaving` still needs to be transformed to `Filter` by rule `ResolveAggregateFunctions`. This rule recognizes the shape: `UnresolvedHaving - Aggregate`.However, the current condition (the plan should not contain `TempResolvedColumn`) that prevents LCA rule to apply between `ResolveReferences` and `ResolveAggregateFunctions` does not cover the above case. It can insert `Project` in the middle and break the shape can be matched by `ResolveAggregateFunctions`.#### FixThe PR adds another condition for LCA rule to apply: the plan should not contain any `UnresolvedHaving`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->See above reasoning to fix the bug.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing and added tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->make mapInPandas / mapInArrow support \"is_barrier\"### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->feature parity.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually:`bin/pyspark --remote local`:```from pyspark.sql.functions import pandas_udfdf = spark.createDataFrame([(1, 21), (2, 30)], (\"id\ \"age\"))def filter_func(iterator):    for pdf in iterator:        yield pdf[pdf.id == 1]df.mapInPandas(filter_func, df.schema,  is_barrier=True).collect()def filter_func(iterator):    for batch in iterator:        pdf = batch.to_pandas()        yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])df.mapInArrow(filter_func, df.schema, is_barrier=True).collect()```
2	-2	### What changes were proposed in this pull request?After [SPARK-41053](https://issues.apache.org/jira/browse/SPARK-41053), Spark supports serializing/ Live UI data to RocksDB using protobuf, but these are internal implementation details, so this pr change the access scope of `ProtobufSerDe` related implementations to `private[protobuf]`.### Why are the changes needed?Weaker the access scope of Spark internal implementation details.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-3	### What changes were proposed in this pull request?This PR proposes to introduce a new API of dropDuplicates which has following different characteristics compared to existing dropDuplicates:* Weaker constraints on the subset (key)  * Does not require an event time column on the subset.* Looser semantics on deduplication  * Only guarantee to deduplicate events within watermark delay.Since the new API leverages event time, the new API has following new requirements:* The watermark must be defined in the streaming DataFrame* The event time column must be defined in the streaming DataFrame.More specifically on the semantic, once the operator processes the first arrived event, events arriving within the watermark for the first event will be deduplicated.(Technically, the expiration time should be the “event time of the first arrived event + watermark delay threshold”, to match up with future events.)Users are encouraged to set the delay threshold of watermark longer than max timestamp differences among duplicated events. (If they are unsure, they can alternatively set the delay threshold large enough, e.g. 48 hours.)For batch DataFrame, this is equivalent to the dropDuplicates.This PR also updates the SS guide doc to introduce the new feature; screenshots below:<img width=\"747\" alt=\"스크린샷 2023-04-06 오전 11 09 12\" src=\"https://user-images.githubusercontent.com/1317309/230254868-7fe76175-5883-4700-b018-d85d851799cb.png\"><img width=\"749\" alt=\"스크린샷 2023-04-06 오전 11 09 18\" src=\"https://user-images.githubusercontent.com/1317309/230254874-a754cdfd-2832-41dd-85b6-291f05eccb3d.png\"><img width=\"752\" alt=\"스크린샷 2023-04-06 오전 11 09 23\" src=\"https://user-images.githubusercontent.com/1317309/230254876-7fd7b3b1-f59d-481f-8249-5a4ae556c7cf.png\"><img width=\"751\" alt=\"스크린샷 2023-04-06 오전 11 09 29\" src=\"https://user-images.githubusercontent.com/1317309/230254880-79b158ca-3403-46a6-be4a-46618ec749db.png\">### Why are the changes needed?Existing dropDuplicates API does not address the valid use case on streaming query.There are many cases where the event time is not exact the same, although these events are same. One example is duplicated events are produced due to non-idempotent writer where event time is issued from producer/broker side. Another example is that the value of event time is unstable and users want to use alternative timestamp e.g. ingestion time.For these case, users have to exclude event time column from subset of deduplication, but then the operator is unable to evict state, leading to indefinitely growing state.To allow eviction of state while event time column is not required to be a part of subset of deduplication, we need to loose the semantic for the API, which warrants a new API.### Does this PR introduce _any_ user-facing change?Yes, this introduces a new public API, dropDuplicatesWithinWatermark.### How was this patch tested?New test suite.
2	-2	### What changes were proposed in this pull request?Update broken links.### Why are the changes needed?broken links:  https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html?highlight=best%20practices#avoid-computation-on-single-partition<img width=\"801\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26707386/227914290-842d8be5-544b-401c-962e-3aef0ca443aa.png\">### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Just docs fix
2	-1	### What changes were proposed in this pull request?Recently, Spark SQL supported `array_insert` and `array_prepend`. All implementations are individual.In fact, `array_prepend` is special case of `array_insert` and we can reuse the `array_insert` by extends `RuntimeReplaceable`.### Why are the changes needed?Simplify the implementation of `array_prepend`.### Does this PR introduce _any_ user-facing change?'No'.Just update the inner implementation.### How was this patch tested?Exists test case.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add more WriteTo tests for Spark Connect Client<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Improve Test Case, remove same todo<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new tests<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?In the PR, I propose to define Spark SQL types as keywords.### Why are the changes needed?The non-keywords types cause some inconveniences while analysing/transforming the lexer tree. For example, while forming the stable column aliases, see https://github.com/apache/spark/pull/40126.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?By running the modified test suites:```$ build/sbt \"test:testOnly *.ResolveAliasesSuite\"$ build/sbt \"test:testOnly *.ParserUtilsSuite\"```
3	-3	### What changes were proposed in this pull request?When testing `OrcEncryptionSuite` using maven, all test suites are always skipped. So this pr add `spark.hadoop.hadoop.security.key.provider.path`  to `systemProperties` of `scalatest-maven-plugin` to make `OrcEncryptionSuite` can test by maven.### Why are the changes needed?Make `OrcEncryptionSuite` can test by maven.### Does this PR introduce _any_ user-facing change?No, just for maven test### How was this patch tested?- Pass GitHub Actions- Manual testing：run ```build/mvn clean install -pl sql/core -DskipTests -ambuild/mvn test -pl sql/core -Dtest=none -DwildcardSuites=org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite ```**Before**```Discovery starting.Discovery completed in 3 seconds, 218 milliseconds.Run starting. Expected test count is: 4OrcEncryptionSuite:21:57:58.344 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable- Write and read an encrypted file !!! CANCELED !!!  [] was empty org.apache.orc.impl.NullKeyProvider@5af5d76f doesn't has the test keys. ORC shim is created with old Hadoop libraries (OrcEncryptionSuite.scala:37)- Write and read an encrypted table !!! CANCELED !!!  [] was empty org.apache.orc.impl.NullKeyProvider@5ad6cc21 doesn't has the test keys. ORC shim is created with old Hadoop libraries (OrcEncryptionSuite.scala:65)- SPARK-35325: Write and read encrypted nested columns !!! CANCELED !!!  [] was empty org.apache.orc.impl.NullKeyProvider@691124ee doesn't has the test keys. ORC shim is created with old Hadoop libraries (OrcEncryptionSuite.scala:116)- SPARK-35992: Write and read fully-encrypted columns with default masking !!! CANCELED !!!  [] was empty org.apache.orc.impl.NullKeyProvider@5403799b doesn't has the test keys. ORC shim is created with old Hadoop libraries (OrcEncryptionSuite.scala:166)21:58:00.035 WARN org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite: ===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.execution.datasources.orc.OrcEncryptionSuite, threads: rpc-boss-3-1 (daemon=true), shuffle-boss-6-1 (daemon=true) =====Run completed in 5 seconds, 41 milliseconds.Total number of tests run: 0Suites: completed 2, aborted 0Tests: succeeded 0, failed 0, canceled 4, ignored 0, pending 0No tests were executed.```**After**```Discovery starting.Discovery completed in 3 seconds, 185 milliseconds.Run starting. Expected test count is: 4OrcEncryptionSuite:21:58:46.540 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable- Write and read an encrypted file- Write and read an encrypted table- SPARK-35325: Write and read encrypted nested columns- SPARK-35992: Write and read fully-encrypted columns with default masking21:58:51.933 WARN org.apache.spark.sql.execution.datasources.orc.OrcEncryptionSuite: ===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.execution.datasources.orc.OrcEncryptionSuite, threads: rpc-boss-3-1 (daemon=true), shuffle-boss-6-1 (daemon=true) =====Run completed in 8 seconds, 708 milliseconds.Total number of tests run: 4Suites: completed 2, aborted 0Tests: succeeded 4, failed 0, canceled 0, ignored 0, pending 0All tests passed.```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?We  indroduce a new idea to optimize exchange plan when union spark plan output partitoning can't match parent plan's required distribution.1. First introduce a new RDD, it consists of parent rdds that has the same partition size. The ith parttition corresponds to ith partition of each parent rdd.2. Then push the required distribution to union plan's children. If any child output partitioning matches the required distribution , we can reduce this child shuffle operation.### Why are the changes needed?Union plan does not take full advantage of children plan output partitionings when output partitoning can't match parent plan's required distribution.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?org.apache.spark.sql.execution.PlannerSuiteorg.apache.spark.sql.execution.exchange.UnionZipRDDSuite
2	-1	### What changes were proposed in this pull request?Most uses of `Random` in spark are either in testcases or where we need a pseudo random number which is repeatable.Use `SecureRandom`, instead of `Random` for the cases where it impacts security.### Why are the changes needed?Use of `SecureRandom` in more security sensitive contexts.This was flagged in our internal scans as well.### Does this PR introduce _any_ user-facing change?Directly no.Would improve security posture of Apache Spark.### How was this patch tested?Existing unit tests
2	-3	### What changes were proposed in this pull request?Change `PlanSubqueries` to set `shouldBroadcast` to true when instantiating an `InSubqueryExec` instance.### Why are the changes needed?The below left outer join gets an error:```create or replace temp view v1 asselect * from values(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)as v1(key, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10);create or replace temp view v2 asselect * from values(1, 2),(3, 8),(7, 9)as v2(a, b);create or replace temp view v3 asselect * from values(3),(8)as v3(col1);set spark.sql.codegen.maxFields=10; -- let's make maxFields 10 instead of 100set spark.sql.adaptive.enabled=false;select *from v1left outer join v2on key = aand key in (select col1 from v3);```The join fails during predicate codegen:```23/03/27 12:24:12 WARN Predicate: Expr codegen error and falling back to interpreter modejava.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished\tat scala.Predef$.require(Predef.scala:281)\tat org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)\tat org.apache.spark.sql.execution.InSubqueryExec.doGenCode(subquery.scala:156)\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:201)\tat scala.Option.getOrElse(Option.scala:189)\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$2(CodeGenerator.scala:1278)\tat scala.collection.immutable.List.map(List.scala:293)\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1278)\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:41)\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.generate(GeneratePredicate.scala:33)\tat org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:73)\tat org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:70)\tat org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:51)\tat org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:86)\tat org.apache.spark.sql.execution.joins.HashJoin.boundCondition(HashJoin.scala:146)\tat org.apache.spark.sql.execution.joins.HashJoin.boundCondition$(HashJoin.scala:140)\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition$lzycompute(BroadcastHashJoinExec.scala:40)\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition(BroadcastHashJoinExec.scala:40)```It fails again after fallback to interpreter mode:```23/03/27 12:24:12 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 7)java.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished\tat scala.Predef$.require(Predef.scala:281)\tat org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)\tat org.apache.spark.sql.execution.InSubqueryExec.eval(subquery.scala:151)\tat org.apache.spark.sql.catalyst.expressions.InterpretedPredicate.eval(predicates.scala:52)\tat org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2(HashJoin.scala:146)\tat org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2$adapted(HashJoin.scala:146)\tat org.apache.spark.sql.execution.joins.HashJoin.$anonfun$outerJoin$1(HashJoin.scala:205)```Both the predicate codegen and the evaluation fail for the same reason: `PlanSubqueries` creates `InSubqueryExec` with `shouldBroadcast=false`. The driver waits for the subquery to finish, but it's the executor that uses the results of the subquery (for predicate codegen or evaluation). Because `shouldBroadcast` is set to false, the result is stored in a transient field (`InSubqueryExec#result`), so the result of the subquery is not serialized when the `InSubqueryExec` instance is sent to the executor.The issue occurs, as far as I can tell, only when both whole stage codegen is disabled and adaptive execution is disabled. When wholestage codegen is enabled, the predicate codegen happens on the driver, so the subquery's result is available. When adaptive execution is enabled, `PlanAdaptiveSubqueries` always sets `shouldBroadcast=true`, so the subquery's result is available on the executor, if needed.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit test.
1	-2	### What changes were proposed in this pull request?Implements `DataFrame.toLocalIterator`.The argument `prefetchPartitions` won't take effect for Spark Connect.### Why are the changes needed?Missing API.### Does this PR introduce _any_ user-facing change?`DataFrame.toLocalIterator` will be available.### How was this patch tested?Enabled the related tests.
2	-2	### What changes were proposed in this pull request?This PR is a followup of  proposes to fix:- Add `versionchanged` in its docstring.- Rename `isBarrier` to `barrier` to make it look Python friendly- Fix some wording and examples.### Why are the changes needed?For better documentation, and make it more Python friendly.### Does this PR introduce _any_ user-facing change?Yes, it renames the parameter, and fixes the documentation.### How was this patch tested?Linters in this PR should test them out.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Just remove comment.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->After https://github.com/apache/hadoop/pull/4036, unzip could keep file permissions.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No need, has been added in hadoop-client side.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Referring to https://dev.mysql.com/doc/refman/8.0/en/string-type-syntax.html, A [TEXT](https://dev.mysql.com/doc/refman/8.0/en/blob.html) column with a maximum length of 65,535 (2^16 − 1) characters.We currently convert our string to MySQL's `text` and jdbc's `CLOB`. The `text` here is insufficient. And `CLOB` is incorrect, which LONGVARCHAR should be replaced instead. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->better compatibility with MySQL and bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, you won't see MysqlDataTruncation if you store a string exceeding 65536 into a column defined by spark' string with MySQL catalog.```javaob aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (10.221.102.180 executor driver): java.sql.BatchUpdateException: Data truncation: Data too long for column 'c1' at row 1\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:816)\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:418)\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:742)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:893)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:892)\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new rule `CoalesceCachePartitions` to support coalesce partitions with `TableCacheQueryStageExec`. In order to reuse the code path with `CoalesceShufflePartitions`, this pr also does a small refactor about how we coalesce partitions.RDD cache use the RDD id and partition id as the block id, so it seems not possible to split skewd partitions like shuffle. To reduce complexity, this pr does not allow coalesce partitions with both shuffle and cache stage since shuffle read may contain skewed partition spec.For example, the follow case can not be coalesced by both `CoalesceCachePartitions` and `CoalesceShufflePartitions`.```SMJ  ShuffleQueryStage  TableCacheStage```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make AQE support coalesce table cache stage partitions.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, add a new config to control if coalesce partitions for  table cache stage.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR supports `spark.sql.pyspark.jvmStacktrace.enabled` in Spark Connect to optionally show the JVM stack trace.It also adds a new Spark Connect config ,`spark.connect.jvmStacktrace.maxSize` (default: 4096), to adjust the stack trace size. This is to prevent the HTTP header size from exceeding the maximum allowed size.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To support an existing config that works with legacy PySpark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Redact sensitive data which is nested by variable substitution#### Case 1 by SET syntax's key part```sqlspark-sql> set ${spark.ssl.keyPassword};abc    <undefined> ```#### Case 2 by SELECT as String literal ```sqlspark-sql> set spark.ssl.keyPassword;spark.ssl.keyPassword    *********(redacted)Time taken: 0.009 seconds, Fetched 1 row(s)spark-sql> select '${spark.ssl.keyPassword}';abc```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->data security### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, sensitive data can not be extracted by variable substitution### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
1	-2	### What changes were proposed in this pull request?Fixed the problem that LDAP authentication cannot be performed correctly when the domain parameter is passed in. For example, Spark Thrift Server will encounter this problem when using Active Directory and passing domain for LDAP authentication.### Why are the changes needed?When the LDAP provider has domain configuration, such as Active Directory, the principal should not be constructed according to the DN pattern, but the user containing the domain should be directly passed to the LDAP provider as the principal.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Reuse code of `streamSideKeyGenerator()` in NAAJ.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Reuse code and improve code readability.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Passing CI.
1	-2	### What changes were proposed in this pull request?This is a follow-up of #40559 and #40571.Renames `isBarrier` to `barrier` in Spark Connect, too.### Why are the changes needed?#40571 changed the argument name from `isBarrier` to `barrier`, so Spark Connect should follow it.### Does this PR introduce _any_ user-facing change?Yes, it renames the parameter.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Simplify the parameter of the following analyzer rule:* PreprocessTableCreation: use a SessionCatalog instead of passing SparkSession* DataSourceAnalysis: remove the unused `Analyzer` in the parameter and turn it from class to object.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
1	-1	### What changes were proposed in this pull request?Implemented new missing methods in the client Dataset API: filter, map, flatMap, mapPartitions.### Why are the changes needed?Missing APIs### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Integration tests.The UDF test does not work with maven.
2	-1	### What changes were proposed in this pull request?Add `YearMonthIntervalType` to PySpark and Spark Connect Python Client### Why are the changes needed?function parity**Note** the added  `YearMonthIntervalType` is not supported in `collect`/`createDataFrame`, since I cannot find a python built-in type for `YearMonthIntervalType` (like `datetime.timedelta` for `DayTimeIntervalType`), we need further discussion.### Does this PR introduce _any_ user-facing change?yes, new data type in pythonbefore this PR```In [1]: spark.sql(\"SELECT INTERVAL '10-8' YEAR TO MONTH AS interval\")Out[1]: ---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)File ~/Dev/spark/python/pyspark/sql/dataframe.py:570, in DataFrame.schema(self)    568 try:    569     self._schema = cast(--> 570         StructType, _parse_datatype_json_string(self._jdf.schema().json())    571     )    572 except Exception as e:...ValueError: Unable to parse datatype from schema. Could not parse datatype: interval year to month```after this PR```In [3]: spark.sql(\"SELECT INTERVAL '10-8' YEAR TO MONTH AS interval\")Out[3]: DataFrame[interval: interval year to month]```### How was this patch tested?added UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SparkThrowable is not necessary to be classified by JDBC Dialects and should not be wrapped with a redundantAnalysisException. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->improve error handling for JDBC catalogs### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new unit test.
2	-1	### What changes were proposed in this pull request?This pr aims to support avro functions for Scala client.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test- Checked Scala 2.13
3	-1	### What changes were proposed in this pull request?This PR aims to prevent `release-build.sh` from removing SBOM artifacts.### Why are the changes needed?According to the snapshot publishing result, we are publishing `.json` and `.xml` files successfully.- https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-core_2.12/3.4.1-SNAPSHOT/spark-core_2.12-3.4.1-20230324.001223-34-cyclonedx.json- https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-core_2.12/3.4.1-SNAPSHOT/spark-core_2.12-3.4.1-20230324.001223-34-cyclonedx.xmlHowever, `release-build.sh` removes them during release. The following is the result of Apache Spark 3.4.0 RC4.- https://repository.apache.org/content/repositories/orgapachespark-1438/org/apache/spark/spark-core_2.12/3.4.0/### Does this PR introduce _any_ user-facing change?Yes, the users will see the SBOM on released artifacts.### How was this patch tested?This should be tested during release process.
2	-2	### What changes were proposed in this pull request?This adds core streaming API support for Spark Connect. With this, we can run majority of streaming queries. All the sources and syncs are supported. Most of the aggregations are supported. Examples of features that are not yet supported: APIs that run user codes like streaming listener, `foreachBatch()` API etc. The remaining missing APIs will be added soon. How to try it in local mode (`./bin/pyspark --remote \"local[*]\"`):```>>> >>> query = ( ...   spark...     .readStream...     .format(\"rate\")...     .option(\"numPartitions\ \"1\")...     .load()...     .withWatermark(\"timestamp\ \"1 minute\")...     .groupBy(window(\"timestamp\ \"10 seconds\"))...     .count() # count for each 10 sedonds....     .writeStream...     .format(\"memory\")...     .queryName(\"rate_table\")...     .trigger(processingTime=\"10 seconds\")...     .start()... )>>>>>> query.isActiveTrue>>> >>> >>> spark.sql(\"select window.start, count from rate_table\").show()+-------------------+-----+|              start|count|+-------------------+-----+|2023-03-11 22:45:40|    6||2023-03-11 22:45:50|   10|+-------------------+-----+```### Does this PR introduce _any_ user-facing change?This is needed to run streaming queries over Spark Connect. ### How was this patch tested? - Manually tested.  - We will enable most of streaming python tests in follow up PRs. - 
1	-2	### What changes were proposed in this pull request?This PR is a follow-up of #40585 which aims to use `cyclonedx` instead of file extension.### Why are the changes needed?When we use file extensions `xml` and `json`, `maven-metadata-local.xml` are missed.```spark-rm@1ea0f8a3e397:/opt/spark-rm/output/spark/spark-repo-glCsK/org/apache/spark$ find . | grep xml./spark-core_2.13/3.4.1-SNAPSHOT/maven-metadata-local.xml./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-core_2.13/maven-metadata-local.xml```We need to use `cyclonedx` specifically.```spark-rm@1ea0f8a3e397:/opt/spark-rm/output/spark/spark-repo-glCsK/org/apache/spark$ find . -type f |grep -v \\.jar |grep -v \\.pom./spark-catalyst_2.13/3.4.1-SNAPSHOT/spark-catalyst_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-catalyst_2.13/3.4.1-SNAPSHOT/spark-catalyst_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-graphx_2.13/3.4.1-SNAPSHOT/spark-graphx_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-graphx_2.13/3.4.1-SNAPSHOT/spark-graphx_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-kvstore_2.13/3.4.1-SNAPSHOT/spark-kvstore_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-kvstore_2.13/3.4.1-SNAPSHOT/spark-kvstore_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-launcher_2.13/3.4.1-SNAPSHOT/spark-launcher_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-launcher_2.13/3.4.1-SNAPSHOT/spark-launcher_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-mllib-local_2.13/3.4.1-SNAPSHOT/spark-mllib-local_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-mllib-local_2.13/3.4.1-SNAPSHOT/spark-mllib-local_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-network-common_2.13/3.4.1-SNAPSHOT/spark-network-common_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-network-common_2.13/3.4.1-SNAPSHOT/spark-network-common_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-network-shuffle_2.13/3.4.1-SNAPSHOT/spark-network-shuffle_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-network-shuffle_2.13/3.4.1-SNAPSHOT/spark-network-shuffle_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-parent_2.13/3.4.1-SNAPSHOT/spark-parent_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-parent_2.13/3.4.1-SNAPSHOT/spark-parent_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-sketch_2.13/3.4.1-SNAPSHOT/spark-sketch_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-sketch_2.13/3.4.1-SNAPSHOT/spark-sketch_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-streaming_2.13/3.4.1-SNAPSHOT/spark-streaming_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-streaming_2.13/3.4.1-SNAPSHOT/spark-streaming_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-tags_2.13/3.4.1-SNAPSHOT/spark-tags_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-tags_2.13/3.4.1-SNAPSHOT/spark-tags_2.13-3.4.1-SNAPSHOT-cyclonedx.xml./spark-unsafe_2.13/3.4.1-SNAPSHOT/spark-unsafe_2.13-3.4.1-SNAPSHOT-cyclonedx.json./spark-unsafe_2.13/3.4.1-SNAPSHOT/spark-unsafe_2.13-3.4.1-SNAPSHOT-cyclonedx.xml```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR redirects '42P07' SQL state to table not found according to the doc - https://www.postgresql.org/docs/14/errcodes-appendix.html.Otherwise, RENAME TABLE will fail with None.get if the new table name exists.```scala23/03/29 19:33:25 ERROR SparkSQLDriver: Failed in [alter table char3 rename to char2]java.util.NoSuchElementException: None.get\tat scala.None$.get(Option.scala:529)\tat scala.None$.get(Option.scala:527)\tat org.apache.spark.sql.jdbc.PostgresDialect$.classifyException(PostgresDialect.scala:220)\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.classifyException(JdbcUtils.scala:1176)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix, avoid `None.get` after regex matching### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no, bugfix### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `injectQueryStageOptimizerRule` public method in `SparkSessionExtensions`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Provide a entrance for developers to the query stage optimizer phase of adaptive query execution. e.g., they can decide the final rdd partition with different plan.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, after this pr people can inject custom rules into query stage optimizer### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
1	-2	### What changes were proposed in this pull request?This PR is a follow-up to https://github.com/apache/spark/pull/40234, which makes it possible for extensions to create custom `Dataset`s and `Column`s. It exposes `Dataset.plan`, but unfortunately it does not expose `Column.expr`. This means that extensions cannot build custom `Column`s that provide a user provider `Column` as input.### Why are the changes needed?See above.### Does this PR introduce _any_ user-facing change?No. This only adds a change for a Developer API.### How was this patch tested?Existing tests to make sure nothing breaks.
4	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The exit code is already available in the `stop(exitCode: Int)` function of the SparkContext, it only can be propagated to the event `SparkListenerApplicationEnd` that is emitted in the `postApplicationEnd`Using an `Option[Int]` to reflect that the exit code can be missing if reading an event log of a previous version of spark### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Being able to know if a spark application succeeded or failed when using the spark listener API### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, adding a new field `exitCode` to `SparkListenerApplicationEnd`, which is marked as `@DeveloperApi`### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated the unit tests to make sure the value is correctly propagatedLaunched a spark application from this branch, and looking at the last emitted event ```{\"Event\":\"SparkListenerApplicationEnd\\"Timestamp\":1680108841738,\"Exit Code\":0}```
2	-2	### What changes were proposed in this pull request?The PR fixes a bug that SparkListenerTaskStart can have `stageAttemptId = -1` when a task is launched after the stage is cancelled. Actually, we should use the information within `Task` to update the `stageAttemptId` field.### Why are the changes needed?-1 is not a legal stageAttemptId value, thus it can lead to unexpected problem if a subscriber try to parse the stage information from the SparkListenerTaskStart event.### Does this PR introduce _any_ user-facing change?No, it's a bugfix.### How was this patch tested?Manually verified.
2	-2	### What changes were proposed in this pull request?In the PR, I propose to define literal constructors `DATE`, `TIMESTAMP`, `TIMESTAMP_NTZ`, `TIMESTAMP_LTZ`, `INTERVAL`, and `X` as Spark SQL keywords.### Why are the changes needed?The non-keywords literal constructors cause some inconveniences while analysing/transforming the lexer tree. For example, while forming the stable column aliases, see https://github.com/apache/spark/pull/40126.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?By running the affected test suites:```$ build/sbt \"test:testOnly *SQLKeywordSuite\"$ build/sbt \"test:testOnly *.ResolveAliasesSuite\"```
2	-1	### What changes were proposed in this pull request?Reuses `pyspark.sql.tests.test_arrow` test cases.### Why are the changes needed?`test_arrow` is also helpful because it contains many tests for `createDataFrame` with pandas or `toPandas`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added the tests.
2	-1	### What changes were proposed in this pull request?Reuses `pyspark.sql.tests.test_arrow` test cases.### Why are the changes needed?`test_arrow` is also helpful because it contains many tests for `createDataFrame` with pandas or `toPandas`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added the tests.
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade buf from 1.15.1 to 1.16.0### Why are the changes needed?Release Notes: https://github.com/bufbuild/buf/releases<img width=\"774\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/228708437-cd951d9d-fc27-4d15-9e87-2fcf9cad4960.png\">https://github.com/bufbuild/buf/compare/v1.15.1...v1.16.0Manually test: dev/connect-gen-protos.shThis upgrade will not change the generated files.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually test and Pass GA.
1	-1	### What changes were proposed in this pull request?This pr change to print `workdir` if `appDirs` is null when worker handle `WorkDirCleanup` event.### Why are the changes needed?Print `appDirs` ause a NPE because `appDirs` is null and from the context, what should be printed is `workdir`### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?After SPARK-39204, `Utils#createTempDir` change to use `JavaUtils#createTempDir` and always use `deleteOnExit` to cleanup tempDir. But `DeleteOnExitHook.files` will not be cleaned up until the JVM exits, even if the file has been manually deleted,  this will cause memory leak.So this pr restore `Utils#createTempDir` use `ShutdownHookManager#registerShutdownDeleteDir` to cleanup `tempDir` and the `Utils#deleteRecursively` will call `ShutdownHookManager#removeShutdownDeleteDir` to clean up registered `tempDir` .### Why are the changes needed?Restore `Utils#createTempDir` use `ShutdownHookManager#registerShutdownDeleteDir` to cleanup tempDir to avoid memory leak.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?Avro functions doctest cleanup, remove unused `print`### Why are the changes needed?those lines were just to investigate the logs in CI, sorry I forgot to remove them before merge. @HyukjinKwon ### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing UT
3	-3	### What changes were proposed in this pull request?Add option to skip commit coordinator as part of StreamingWrite API for DSv2 sources/sinks. This option was already present as part of the BatchWrite API### Why are the changes needed?Sinks such as the following are atleast-once for which we do not need to go through the commit coordinator on the driver to ensure that a single partition commits. This is even less useful for streaming use-cases where batches could be replayed from the checkpoint dir.- memory sink- console sink- no-op sink- Kafka v2 sink### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added unit test for the change```[info] ReportSinkMetricsSuite:22:23:01.276 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable22:23:03.139 WARN org.apache.spark.sql.execution.streaming.ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.[info] - test ReportSinkMetrics with useCommitCoordinator=true (2 seconds, 709 milliseconds)22:23:04.522 WARN org.apache.spark.sql.execution.streaming.ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.[info] - test ReportSinkMetrics with useCommitCoordinator=false (373 milliseconds)22:23:04.941 WARN org.apache.spark.sql.streaming.ReportSinkMetricsSuite:===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.streaming.ReportSinkMetricsSuite, threads: ForkJoinPool.commonPool-worker-19 (daemon=true), rpc-boss-3-1 (daemon=true), shuffle-boss-6-1 (daemon=true) =====[info] Run completed in 4 seconds, 934 milliseconds.[info] Total number of tests run: 2[info] Suites: completed 1, aborted 0[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```
2	-2	### What changes were proposed in this pull request?This PR makes it cast the result type of string +/- interval to timestamp type instead of string type.### Why are the changes needed?Both of them should evaluate to `true`. Otherwise, users will be confused:```sqlselect '2023-01-01' >= '2023-01-08' - interval '7' day;select '2023-01-01' >= date_sub('2023-01-08', 7);```### Does this PR introduce _any_ user-facing change?This PR changed the result type.### How was this patch tested?Unit test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix `rename a table` in derby and pg, which schema name is not allowed to qualify the new table name ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bugfix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->introduce a new error class```json  \"CANNOT_RENAME_ACROSS_SCHEMA\" : {    \"message\" : [      \"Renaming a <type> across schemas is not allowed.\"    ],    \"sqlState\" : \"0AKD0\"  },```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new unit test
2	-1	### What changes were proposed in this pull request?Instead of just showing the Scala callsite show the abbreviate version of the proto message in the Spark UI.### Why are the changes needed?Debugging### Does this PR introduce _any_ user-facing change?No### How was this patch tested?ManualBefore:![image](https://user-images.githubusercontent.com/3421/228769858-51fbc5de-c84c-4f5e-86de-bd77c45d2847.png)After:![image](https://user-images.githubusercontent.com/3421/228769955-e9db4737-6d29-4d51-9941-0cb650b73870.png)
3	-2	This reverts commit a111a02de1a814c5f335e0bcac4cffb0515557dc.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SQLMetrics is not only used in the UI, but is also a programming API as users can write a listener, get the physical plan, and read the SQLMetrics values directly.We can ask users to update their code and read SQLMetrics from the new `WriteFiles` node instead. But this is troublesome and sometimes they may need to get both write metrics and commit metrics, then they need to look at two physical plan nodes. Given that https://github.com/apache/spark/pull/39428 is mostly for cleanup and does not have many benefits, reverting is a better idea.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->avoid breaking changes.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, they can programmatically get the write command metrics as before. ### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
2	-1	### What changes were proposed in this pull request?This pr refactor `connect-jvm-client-mima-check` and `CheckConnectJvmClientCompatibility` to support mima check between `connect-client-jvm` and `avro` module.### Why are the changes needed?Do mima check between `connect-client-jvm` and `avro` module.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-1	### What changes were proposed in this pull request?Make Torch Distributor support Spark Connect### Why are the changes needed?functionality parity.**Note**, `local_mode` with `use_gpu` is not supported for now since `sc.resources` is missing in Connect### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?reused UT
3	-2	Add support for calling debugCodegen from Python & Java<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Adds `debugCodegen` to the Dataset APIs core and connector- Adds `debugCodegen` to the pyspark dataframe API- Removes the implicit `debugCodegen` from `sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala` as it is public now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To add a direct method to get debugCodegen state for Java & Python users of Dataframes.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->`debugCodegen` is now accessible from all Scala, python and Java APIs. See usage below.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added tests in:- `sql/core/src/test/scala/org/apache/spark/sql/DebugCodegenSuite.scala`.- `connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/ClientE2ETestSuite.scala`Local testing:```scala> spark.sql(\"SELECT 1\").debugCodegenFound 1 WholeStageCodegen subtrees.== Subtree 1 / 1 (maxMethodCodeSize:55; maxConstantPoolSize:99(0.15% used); numInnerClasses:0) ==*(1) Project [1 AS 1#0]+- *(1) Scan OneRowRelation[]...``````>>> spark.sql(\"select 1\").debugCodegen()Found 1 WholeStageCodegen subtrees.== Subtree 1 / 1 (maxMethodCodeSize:55; maxConstantPoolSize:99(0.15% used); numInnerClasses:0) ==*(1) Project [1 AS 1#0]+- *(1) Scan OneRowRelation[]...```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign name to _LEGACY_ERROR_TEMP_2044, \"BINARY_ARITHMETIC_OVERFLOW\".<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Assign proper name to LEGACY_ERROR_TEMP<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested? ./build/sbt \"testOnly org.apache.spark.sql.errors.QueryExecutionErrorsSuite\"<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?Add a destructive iterator to SparkResult and change `Dataset.toLocalIterator` to use the desctructive iterator.With the desctructive iterator, we will:1. Close the `ColumarBatch` once its data got consumed;2. Remove the `ColumarBatch` from `SparkResult.batches`;### Why are the changes needed?Instead of keeping everything in memory for the life time of SparkResult object, clean it up as soon as we know we are done with it. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT added.
1	-1	### What changes were proposed in this pull request?This PR adds direct serialization from user domain objects to arrow batches. This removes the need to go through catalyst.### Why are the changes needed?We want to minimalize the number of dependencies in connect. Removing catalyst and core is part of this effort.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?I added the `ArrowEncoderSuite` to test this.
2	-2	### What changes were proposed in this pull request?Fixes the comparison the result with Arrow optimization enabled/disabled.### Why are the changes needed?in `test_arrow`, there are a bunch of comparison between DataFrames with Arrow optimization enabled/disabled.These should be fixed to compare with the expected values so that it can be reusable for Spark Connect parity tests.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Updated the tests.
1	-1	### What changes were proposed in this pull request?The main change of this pr as follows:1. Make `Utils.createTempDir` and `JavaUtils.createTempDir` back to two independent implementations to restore `Utils.createTempDir` to use the `spark.util.ShutdownHookManager` mechanism.2. Use `Utils.createTempDir` or `JavaUtils.createDirectory` instead for testing where `JavaUtils.createTempDir` is used.3. Clean up `JavaUtils.createTempDir` method### Why are the changes needed?Restore `Utils.createTempDir` to use the `spark.util.ShutdownHookManager` mechanism and clean up `JavaUtils.createTempDir` method.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Correction of  code highlights in SQL protobuf documentation.old version:![image](https://user-images.githubusercontent.com/8001253/228999196-d39d62f5-992b-429b-8418-efa01300cce4.png)new version:![image](https://user-images.githubusercontent.com/8001253/228999257-54d30e09-0bab-44f6-ae6d-7794e1c4329d.png)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To help spark users to understand python, scala and java code examples.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it improve user-facing documentation### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Comparing visual structure in the old and the new version in vscode markdown preview and github markdown preview.
1	-1	### What changes were proposed in this pull request?This PR adds a new dependency on the datasketches-java project, and provides 4 new functions which utilize Datasketches HllSketch and Union instances for unique counting and intermediate storage of HLL sketches. ### Why are the changes needed?The existing approx_count_distinct provides unique counting functionality, but does not provide the ability to store HLL sketches for re-aggregation. After discussions with the Databricks team, we decided that integrating the Datasketches HllSketch implementation would make more sense than extending the existing approx_count_distinct functionality, given the wide use of the Datasketches library. See https://github.com/apache/spark/pull/39678 and https://github.com/RyanBerti/spark/pull/1 for more information.### Does this PR introduce _any_ user-facing change?Yes, this PR introduces two new aggregate functions:- hll_sketch_agg- hll_union_aggAnd two new scalar functions:- hll_sketch_estimate- hll_union### How was this patch tested?I've included various tests in the DataframeAggregateSuite and new DatasketchesHllSketchSuite test suite.
2	-2	### What changes were proposed in this pull request?1. Moved `ResolveBinaryArithmetic` from `Analyzer` to `TypeCoercion` and `AnsiTypeCoercion`.2. Update `ResolveBinaryArithmetic` in `AnsiTypeCoercion` to only support datetime and interval types +/- intervals.### Why are the changes needed?For string type  +/- internal, the string value can be date, timestamp and interval. We don't know what type of string should be casted to. For example:```sqlspark-sql (default)> select \"2023-01-08\" - interval '7' day;2023-01-01 00:00:00spark-sql (default)> select \"INTERVAL '7' DAY\" - interval '7' day; -- \"INTERVAL '7' DAY\" should be cased to interval type.NULLspark-sql (default)> select interval '7' day - interval '6' day;1 00:00:00.000000000```### Does this PR introduce _any_ user-facing change?Disable string +/- interval in ANSI mode.### How was this patch tested?Update existing test.
2	-2	### What changes were proposed in this pull request?This PR proposes to introduce new error for PySpark, and also applied it to existing RuntimeError under `python/pyspark/*.py`.### Why are the changes needed?To cover the built-in RuntimeError by PySpark error framework.### Does this PR introduce _any_ user-facing change?No, it's internal error framework improvement.### How was this patch tested?The existing CI should pass.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->fix `self.deserialized == self.deserialized` with `self.deserialized == other.deserialized`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The original expression is always True, which is likely to be a typo.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->No test added. Use GitHub Actions.
3	-2	By comparing compute.isin_limit and plotting.max_rows, `v is v` is likely to be a typo.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->fix `v is v >= 0` with `v >= 0`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->By comparing compute.isin_limit and plotting.max_rows, `v is v` is likely to be a typo.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->By GitHub Actions.
3	-2	### What changes were proposed in this pull request?the PR fix ExecutorAllocationManager cannot allocate new instances when all executors down.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->As described in the issue [SPARK-42972](https://issues.apache.org/jira/browse/SPARK-42972)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I test this in our cluster and the spark streaming app runs normally.
3	-2	vendor == vendor is always true, this is likely to be a typo.<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->fix `vendor == vendor` with `that.vendor == vendor`, and `discoveryScript == discoveryScript` with `that.discoveryScript == discoveryScript`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->vendor == vendor is always true, this is likely to be a typo.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->By GitHub Actions.
2	-2	### What changes were proposed in this pull request?In the PR, I propose to change API of parameterized SQL, and replace type of argument values from `string` to `Any` in Scala/Java/Python and `Expression.Literal` in protobuf API. Language API can accept `Any` objects from which it is possible to construct literal expressions.#### Scala/Java:```scala  def sql(sqlText: String, args: Map[String, Any]): DataFrame```values of the `args` map are wrapped by the `lit()` function which leaves `Column` as is and creates a literal from other Java/Scala objects (for more details see the `Scala` tab at https://spark.apache.org/docs/latest/sql-ref-datatypes.html).#### Python:```pythondef sql(self, sqlQuery: str, args: Optional[Dict[str, Any]] = None, **kwargs: Any) -> DataFrame:```Similarly to Scala/Java `sql`, Python's `sql()` accepts Python objects as values of the `args` dictionary (see more details about acceptable Python objects at https://spark.apache.org/docs/latest/sql-ref-datatypes.html). `sql()` converts dictionary values to `Column` literal expressions by `lit()`.#### Protobuf:```protomessage SqlCommand {  // (Required) SQL Query.  string sql = 1;  // (Optional) A map of parameter names to literal expressions.  map<string, Expression.Literal> args = 2;}```For example:```scalascala> val sqlText = \"\"\"SELECT s FROM VALUES ('Jeff /*__*/ Green'), ('E\\'Twaun Moore') AS t(s) WHERE s = :player_name\"\"\"sqlText: String = SELECT s FROM VALUES ('Jeff /*__*/ Green'), ('E\\'Twaun Moore') AS t(s) WHERE s = :player_namescala> sql(sqlText, args = Map(\"player_name\" -> lit(\"E'Twaun Moore\"))).show(false)+-------------+|s            |+-------------+|E'Twaun Moore|+-------------+```### Why are the changes needed?The current implementation the parameterized `sql()` requires arguments as string values parsed to SQL literal expressions that causes the following issues:1. SQL comments are skipped while parsing, so, some fragments of input might be skipped. For example, `'Europe -- Amsterdam'`. In this case, `-- Amsterdam` is excluded from the input.2. Special chars in string values must be escaped, for instance `'E\\'Twaun Moore'`### Does this PR introduce _any_ user-facing change?No since the parameterized SQL feature https://github.com/apache/spark/pull/38864 hasn't been released yet.### How was this patch tested?By running the affected tests:```$ build/sbt \"test:testOnly *ParametersSuite\"$ python/run-tests --parallelism=1 --testnames 'pyspark.sql.tests.connect.test_connect_basic SparkConnectBasicTests.test_sql_with_args'$ python/run-tests --parallelism=1 --testnames 'pyspark.sql.session SparkSession.sql'```
3	-2	### What changes were proposed in this pull request?This PR proposes to migrate remaining `TypeError` from `python/pyspark/sql/connect/dataframe.py` into `PySparkTypeError`.### Why are the changes needed?To migrate all user-facing errors into PySpark error framework.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, it's internal error framework improvement.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Implement access control on a database via row level security on metadata<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Implemneting this would bring access control on spark-standalone<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?using doAs for thrift-server / --proxy-user for shell would restrict access<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Created 2 users ( with row level restrictions on metadata db )- To test --proxy-user : Started a shell with --proxy-user flag and ran show tables / count rows on table (with access / without access)- To test doAs : Started a thrift server. Connected as test user via beeline and ran show tables / count rows on table (with access / without access)<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-1	### What changes were proposed in this pull request?This PR proposes to provide a 'ValidationMode' in org.apache.spark.sql.execution.ExplainMode that will only print the parsed and analysed logical plans only.### Why are the changes needed?Validate the sql query before submitting the job . We are currently using df.explain(extended=true) which generates parsed , analysed , optimised logical plan and physical plan . But generating  optimised logical plan  sometimes takes more time.### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?Pass GitHub Actions
2	-1	### What changes were proposed in this pull request?Fix `DataFrame.collect` with null struct.### Why are the changes needed?There is a behavior difference when collecting `null` struct:In Spark Connect:```py>>> df = spark.sql(\"values (1, struct('a' as x)), (2, struct(null as x)), (null, null) as t(a, b)\")>>> df.printSchema()root |-- a: integer (nullable = true) |-- b: struct (nullable = true) |    |-- x: string (nullable = true)>>> df.show()+----+------+|   a|     b|+----+------+|   1|   {a}||   2|{null}||null|  null|+----+------+>>> df.collect()[Row(a=1, b=Row(x='a')), Row(a=2, b=Row(x=None)), Row(a=None, b=<Row()>)]```whereas PySpark:```py>>> df.collect()[Row(a=1, b=Row(x='a')), Row(a=2, b=Row(x=None)), Row(a=None, b=None)]```### Does this PR introduce _any_ user-facing change?The behavior fix.### How was this patch tested?Added/modified the related tests.
1	-1	### What changes were proposed in this pull request?Implements missing methods in Dataset: foreach and foreachPartition.PR based on top of https://github.com/apache/spark/pull/40581The impl of foreachPartition is based on top of mapPartitions + count.### Why are the changes needed?Add missing methods in Dataset.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?E2E tests.
2	-2	### What changes were proposed in this pull request?The current TorrentBroadcast implementation is originally designed for large data transmission where driver might become the bottleneck and memory might also be the concern. Therefore it's kind of heavy and comes with some fixed overhead:- torrent protocol: more round traffic- disk level persistency, BlockManager, extra overheadwhich makes it inefficient for some small data transmission.We can have a lightweight broadcast implementation, e.g, SmallBroadcast, which implement star-topology broadcast(which avoids the unnecessary round traffic), and maybe in-memory only.Note:The current factory style API seem to encourage only one broadcast style on runtime, however this small broadcast implementation is a supplement to `TorrentBroadcast`. So I introduce new small broadcast APIs along the code path.### Why are the changes needed?Provide a lightweight implementation of `Broadcast`, which is suitable for situation where data size is small and latency is critical.### Does this PR introduce _any_ user-facing change?NO### How was this patch tested?Unit test. More testcases are from `BroadcastSuite`
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR fixes `TableOutputResolver` to use correct column paths in error messages for arrays and maps.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to have accurate error messages when there is a type mismatch inside arrays and maps.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
2	-1	### What changes were proposed in this pull request?This PR proposes to provide a 'ValidationMode' in org.apache.spark.sql.execution.ExplainMode that will only print the parsed and analysed logical plans only.### Why are the changes needed?Validate the sql query before submitting the job . We are currently using df.explain(extended=true) which generates parsed , analysed , optimised logical plan and physical plan . But generating optimised logical plan sometimes takes more time.### Does this PR introduce *any* user-facing change?Yes### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign name to _LEGACY_ERROR_TEMP_2132, \"CANNOT_PARSE_JSON_ARRAYS_AS_STRUCTS\".<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Assign proper name to LEGACY_ERROR_TEMP<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?./build/sbt \"testOnly org.apache.spark.sql.errors.QueryExecutionErrorsSuite\"<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-3	### What changes were proposed in this pull request?This PR updates `PromoteStrings` to not cast to double type when one side is `AnsiIntervalType` in `BinaryArithmetic`.### Why are the changes needed?1. It's already handled by `ImplicitTypeCasts`.2. The error message is incorrect. For example:   ```sql    select '2' / interval 2 second;   ```   The error message is:  the left and right operands of the binary operator have incompatible types (\"DOUBLE\" and \"INTERVAL SECOND\").   It should be:  the left and right operands of the binary operator have incompatible types (\"STRING\" and \"INTERVAL SECOND\").### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Update existing unit tests.
2	-2	### What changes were proposed in this pull request?In the PR, I propose to change the `_LEGACY_ERROR_TEMP_2004`  error to an internal error. Also this PR improves the error message.### Why are the changes needed?`_LEGACY_ERROR_TEMP_2004` cannot be triggered from user code (for instance, some SQL query), more detail:1. `_LEGACY_ERROR_TEMP_2004` error is thrown in `Literal.default` for `CharType`, `VarcharType` and other user-implemented DataType.2. `Literal.default` is called in these cases below:    1. in `org.apache.spark.sql.catalyst.expressions.aggregate.Average`, for getting initial value; but in this case, DataType would only be  DecimalType / YearMonthIntervalType / DayTimeIntervalType / DoubleType; these types are supported by `Literal.default`.    2. in `org.apache.spark.sql.catalyst.expressions.aggregate.Sum`, is same as `Average`    3. in `org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys` and `org.apache.spark.sql.catalyst.plans.logical.AsOfJoin`; In real scene, they are use for DateFrame api, but `CharType` and `VarcharType` cannot be as a part of DataFrame's schema( they would be converted to StringType); and user-defined DataType do not have a matched Encoder in `org.apache.spark.sql.catalyst.encoders.RowEncoder#encoderForDataType`so, it should be an internal error, but not `SparkRuntimeException`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?By running the modified test suites:```bash build/sbt \"sql/testOnly org.apache.spark.sql.catalyst.expressions.LiteralExpressionSuite\"```
2	-1	What changes were proposed in this pull request?This PR proposes to provide a 'ValidationMode' in org.apache.spark.sql.execution.ExplainMode that will only print the parsed and analysed logical plans only.Why are the changes needed?Validate the sql query before submitting the job . We are currently using df.explain(extended=true) which generates parsed , analysed , optimised logical plan and physical plan . But generating optimised logical plan sometimes takes more time.Does this PR introduce any user-facing change?YesHow was this patch tested?Pass GitHub Actions
2	-2	### What changes were proposed in this pull request?Add an override to BatchScanExecBase which delegates to a new default method on PartitionReaderFactory to expose vectoryTypes.### Why are the changes needed?SparkPlan's vectorType's attribute can be used to specialize codegen however BatchScanExecBase does not override this so we DSv2 sources do not get any benefit of concrete class dispatch.### Does this PR introduce *any* user-facing change?NO### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?* Added a new config property — `spark.yarn.report.loggingFrequency`* Limit the number of times the yarn application report is logged based on the number of reports processed.   <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Currently, an application report is generated every second, this tends to add a lot of noise especially for long running applications. This bloats the log file and makes it hard to navigate. With this change, we can limit the amount of times the application status report is logged based on the number of reports processed.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?The logs are now ~30s apart```31-03-2023 15:00:08 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:00:08 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:00:38 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:00:38 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:01:08 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:01:08 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:01:38 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:01:38 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:02:09 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:02:09 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: RUNNING)31-03-2023 15:02:31 PDT countByCountryFlow_countByCountry INFO - 23/03/31 22:02:31 INFO yarn.Client: Application report for application_1676870052658_5870144 (state: FINISHED)```<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tested locally to ensure the behavior was as expected<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	What changes were proposed in this pull request?Add an override to BatchScanExecBase which delegates to a new default method on PartitionReaderFactory to expose vectoryTypes.Why are the changes needed?SparkPlan's vectorType's attribute can be used to specialize codegen however BatchScanExecBase does not override this so we DSv2 sources do not get any benefit of concrete class dispatch.Does this PR introduce any user-facing change?NOHow was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This pr aims to upgrade rocksdbjni from 7.10.2 to 8.0.0.### Why are the changes needed?This version bring some bug fix about `Get` and `MultiGet `, the full release notes as follows:- https://github.com/facebook/rocksdb/releases/tag/v8.0.0### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual test `RocksDBBenchmark`:**7.10.2**```[INFO] Running org.apache.spark.util.kvstore.RocksDBBenchmark                                                count   mean    min     max     95thdbClose                                         4       0.362   0.307   0.510   0.510dbCreation                                      4       70.556  3.823   272.036 272.036naturalIndexCreateIterator                      1024    0.005   0.002   1.396   0.007naturalIndexDescendingCreateIterator            1024    0.007   0.007   0.063   0.009naturalIndexDescendingIteration                 1024    0.006   0.004   0.236   0.009naturalIndexIteration                           1024    0.006   0.004   0.054   0.010randomDeleteIndexed                             1024    0.028   0.019   0.246   0.038randomDeletesNoIndex                            1024    0.014   0.012   0.041   0.018randomUpdatesIndexed                            1024    0.084   0.033   30.028  0.095randomUpdatesNoIndex                            1024    0.033   0.029   0.759   0.037randomWritesIndexed                             1024    0.120   0.034   54.254  0.124randomWritesNoIndex                             1024    0.038   0.032   1.918   0.043refIndexCreateIterator                          1024    0.004   0.004   0.017   0.006refIndexDescendingCreateIterator                1024    0.003   0.003   0.027   0.004refIndexDescendingIteration                     1024    0.007   0.005   0.114   0.009refIndexIteration                               1024    0.007   0.005   0.045   0.010sequentialDeleteIndexed                         1024    0.024   0.018   1.944   0.028sequentialDeleteNoIndex                         1024    0.015   0.012   0.039   0.019sequentialUpdatesIndexed                        1024    0.044   0.036   0.910   0.057sequentialUpdatesNoIndex                        1024    0.037   0.032   0.868   0.046sequentialWritesIndexed                         1024    0.047   0.040   2.261   0.056sequentialWritesNoIndex                         1024    0.041   0.033   3.577   0.045```**8.0.0**```[INFO] Running org.apache.spark.util.kvstore.RocksDBBenchmark                                                count   mean    min     max     95thdbClose                                         4       0.320   0.233   0.562   0.562dbCreation                                      4       71.171  3.778   272.587 272.587naturalIndexCreateIterator                      1024    0.006   0.002   1.460   0.009naturalIndexDescendingCreateIterator            1024    0.007   0.006   0.063   0.008naturalIndexDescendingIteration                 1024    0.008   0.004   0.377   0.013naturalIndexIteration                           1024    0.006   0.004   0.060   0.010randomDeleteIndexed                             1024    0.030   0.020   0.338   0.052randomDeletesNoIndex                            1024    0.016   0.013   0.050   0.020randomUpdatesIndexed                            1024    0.087   0.032   29.873  0.096randomUpdatesNoIndex                            1024    0.036   0.032   0.592   0.041randomWritesIndexed                             1024    0.121   0.033   54.702  0.123randomWritesNoIndex                             1024    0.040   0.034   1.530   0.047refIndexCreateIterator                          1024    0.005   0.003   0.023   0.007refIndexDescendingCreateIterator                1024    0.003   0.003   0.026   0.005refIndexDescendingIteration                     1024    0.007   0.005   0.051   0.009refIndexIteration                               1024    0.007   0.005   0.052   0.010sequentialDeleteIndexed                         1024    0.021   0.017   0.133   0.025sequentialDeleteNoIndex                         1024    0.015   0.012   0.041   0.018sequentialUpdatesIndexed                        1024    0.046   0.036   2.035   0.055sequentialUpdatesNoIndex                        1024    0.040   0.028   0.798   0.050sequentialWritesIndexed                         1024    0.049   0.042   2.578   0.055sequentialWritesNoIndex                         1024    0.035   0.029   3.229   0.039```- Checked core module UTs with rocksdb live ui```export LIVE_UI_LOCAL_STORE_DIR=/${basedir}/spark-ui    build/mvn clean install -pl core -am -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest -fn```All test passed
1	-2	### What changes were proposed in this pull request?This pr aims upgrade joda-time from 2.12.2 to 2.12.5.### Why are the changes needed?New version bring a bug fix https://github.com/JodaOrg/joda-time/pull/681 and Update time zone data to version 2023cgtz.- https://www.joda.org/joda-time/changes-report.html#a2.12.5- https://github.com/JodaOrg/joda-time/compare/v2.12.2...v2.12.5 ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass Github Actions
1	-3	### What changes were proposed in this pull request?Make `array_insert` fail when input index `pos` is zero.### Why are the changes needed?see https://github.com/apache/spark/pull/40563#discussion_r1155673089### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?updated UT
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate errors from `python/pyspark/sql/column.py` into error classes.### Why are the changes needed?To leverage the PySpark error framework.### Does this PR introduce _any_ user-facing change?No, it's about the error message improvement. No API changes.### How was this patch tested?The existing CI should pass.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate `ValueError` from `python/pyspark/sql/dataframe.py` into error classes.### Why are the changes needed?To leverage the PySpark error framework.### Does this PR introduce _any_ user-facing change?No, it's about the error message improvement. No API changes.### How was this patch tested?The existing CI should pass.
2	-1	### What changes were proposed in this pull request?In the PR, I propose to update the doc page https://spark.apache.org/docs/latest/sql-ref-datatypes.html about value types in Scala and Java APIs.<img width=\"1190\" alt=\"Screenshot 2023-04-03 at 15 22 24\" src=\"https://user-images.githubusercontent.com/1580697/229509678-7f860c2b-050e-4e40-b83d-821e70d1e194.png\">### Why are the changes needed?To provide full info about supported \"external\" value types in Scala/Java APIs.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?By building docs and checking them by eyes.
2	-2	### What changes were proposed in this pull request?This PR aims to add a new configuration `spark.kubernetes.setSubmitTimeInDriver`.When an application is deployed using spark-submit in k8s cluster mode, let `spark.app.submitTime` be the time spark-submit executed.### Why are the changes needed?Currently, when an application is deployed using spark-submit in k8s cluster mode, `spark.app.submitTime` is the time driver pod started.Driver pod pending time is not included in the time between `spark.app.submitTime` and `spark.app.startTime`.### Does this PR introduce _any_ user-facing change?'No'### How was this patch tested?Add test:* \"SPARK-43014: Set `spark.app.submitTime` if missing\" * \"SPARK-43014: Overwrite `spark.app.submitTime` in k8s cluster mode driver when `spark.kubernetes.setSubmitTimeInDriver` is true\"
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Parquet has supported vector read speed up with this PR [Vectorized BytePacker decoder using Java Vector API](https://github.com/apache/parquet-mr/pull/1011)The performance gain is 4x ~ 8x according to the parquet microbenchmarkTPC-H(SF100) Q6 has 11% performance increase with Apache Spark integrating parquet vector optimization### Why are the changes needed?This PR used to support parquet vector optimization### Does this PR introduce _any_ user-facing change?Add configuration  spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.### How was this patch tested?For the test case, there are some problems to fix:1. It is necessary to Parquet-mr community release new java version to use the parquet vector optimization. 2. Parquet Vector optimization does not release default, so users have to build parquet with **mvn clean install -P vector-plugins** manually to get the parquet-encoding-vector-{VERSION}.jar and put it on the {SPARK_HOME}/jars path3. github doesn't support select runners with specific instruction set. So it is impossible (a self-hosted runner can do it) to verify the optimization on github runners machine.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?This pr just merge two `SparkVersion` related test to one.### Why are the changes needed?Merge Test Cases### Does this PR introduce _any_ user-facing change?No, just test### How was this patch tested?Pass GitHub Actions
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The Design for support async query execution<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Prepare for code async query execution<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?NO<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?This is a follow-up of #40619.Fixes `DataFrameTests.test_cache_dataframe`.### Why are the changes needed?The storage level when `df.cache()` should be `StorageLevel.MEMORY_AND_DISK_DESER` in Python.The test passed before #40619 because the difference is `deserialized`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Fixed the test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we start to move ordering to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-2	### What changes were proposed in this pull request?This PR fixes a correctness bug for INSERT commands with timestamp literals. The bug manifests when:* An INSERT command includes a user-specified column list of fewer columns than the target table.* The provided values include timestamp literals.The bug was that the long integer values stored in the rows to represent these timestamp literals were getting assigned back to `UnresolvedInlineTable` rows without the timestamp type. Then the analyzer inserted an implicit cast from `LongType` to `TimestampType` later, which incorrectly caused the value to change during execution.This PR fixes the bug by propagating the timestamp type directly to the output table instead.### Why are the changes needed?This PR fixes a correctness bug.### Does this PR introduce _any_ user-facing change?Yes, this PR fixes a correctness bug.### How was this patch tested?This PR adds a new unit test suite.
2	-1	### What changes were proposed in this pull request?This pr aims to support protobuf functions for Scala client.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Add new test- Checked Scala 2.13
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR migrates `TableOutputResolver` to use runtime NOT NULL checks instead of checking type compatibility during the analysis phase.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed per discussion that happened [here](https://github.com/apache/spark/pull/40308#discussion_r1127081206).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Nullability exceptions will be thrown at runtime (instead of analysis) but there is no API change.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
1	-1	### What changes were proposed in this pull request?After [SPARK-42519](https://issues.apache.org/jira/browse/SPARK-42519), `SimpleSparkConnectService` will start with a customized catalog named `testcat`, so this pr uses this catalog to add a testing scenario for switching catalogs for `CatalogMetadata APIs` in `CatalogSuite`### Why are the changes needed?Add more test scenario.### Does this PR introduce _any_ user-facing change?No, just for test### How was this patch tested?- Pass Github Actions- Checked Scala 2.13
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Update the error_class _LEGACY_ERROR_TEMP_2008 to INVALID_URL.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue SPARK-42844. The original name just a number, update it to a informal name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add a test case in UrlFunctionsSuite to catch the error using sql command.
3	-2	### What changes were proposed in this pull request?This PR proposes to match the behavior of pandas API on Spark to pandas 2.0.0See [What's new in 2.0.0](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html) for more detail.### Why are the changes needed?We should support latest pandas for pandas API on Spark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, there are bunch of behavior changes according to [What's new in 2.0.0](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html)### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Addressed UTs and doctests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Apply AQE with non-exchange table cache at `InsertAdaptiveSparkPlan`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`TableCacheQueryStageExec` supports to report runtime statistics, so it's possible that AQE  plans a better executed during re-optimization. Then it has benefits to apply AQE even without shuffle.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a new error class `SQL_CONF_NOT_FOUND`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error message more user-friendly when getting a non-existing SQL config. For example:```spark.conf.get(\"some.conf\")```Before this PR, it will throw this error:```java.util.NoSuchElementException: some.conf```After this PR:```[SQL_CONF_NOT_FOUND] The SQL config \"some.conf\" cannot be found. Please verify that the config exists.```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. The error message will be changed.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added a new UT.
2	-2	### What changes were proposed in this pull request?There are a lot of SQL with union multiple subquery with filter in user scenarios. Take an example,**q1**```SELECTss_item_sk,ss_ticket_number,ss_customer_skFROM store_salesLEFT OUTER JOIN store_returnsON (sr_item_sk = ss_item_sk AND sr_ticket_number = ss_ticket_number)WHERE sr_return_amt > 10000UNION ALLSELECTss_item_sk,ss_ticket_number,ss_customer_skFROM store_salesLEFT OUTER JOIN store_returnsON (sr_item_sk = ss_item_sk AND sr_ticket_number = ss_ticket_number)WHERE sr_return_amt < 1000```In fact, we can simplify this SQL as```SELECTss_item_sk,ss_ticket_number,ss_customer_skFROM store_salesLEFT OUTER JOIN store_returnsON (sr_item_sk = ss_item_sk AND sr_ticket_number = ss_ticket_number)WHERE sr_return_amt > 10000 OR sr_return_amt < 1000```**q2**```SELECT  ss_item_sk,  ss_ticket_number,  ss_customer_skFROM store_salesWHERE ss_ext_discount_amt > 1000UNION ALLSELECT  ss_item_sk,  ss_ticket_number,  ss_customer_skFROM store_salesWHERE ss_ext_discount_amt < 100```In fact, we can simplify this SQL as```SELECT  ss_item_sk,  ss_ticket_number,  ss_customer_skFROM store_salesWHERE ss_ext_discount_amt > 1000 OR ss_ext_discount_amt < 100```This PR optimizes `Union` operators if the children exists at least two `Filter` by:1. Eliminate `Union` operators if all the children are `Filter` and all the child of these `Filter`s are same. We just need merging the predicates into one single predicate by connecting these `Filter`s with `Or`.2. Combines multiple `Filter` operators into one if all the child of these `Filter`s are same. We just need merging the predicates into one single predicate by connecting these `Filter`s with `Or` too.### Why are the changes needed?Simply the SQL plan and improve the performance.### Does this PR introduce _any_ user-facing change?'No'.New feature and just update the inner implementation.### How was this patch tested?New test cases.The micro benchmark for q1 and q2.Before this PR```Java HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzTPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------q1                                               51569          52030         653          0.6        1627.9       1.0XJava HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzTPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------q2                                                4255           4287          45          6.8         147.7       1.0X```After this PR```Java HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzTPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------q1.                                              46806          47462         929          0.7        1477.5       1.0XJava HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzTPCDS Snappy:                             Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------q2.                                                2655           2674          28         10.8          92.2       1.0X```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The rule `DeduplicateRelations` finds duplicated relations and renew their output attribute IDs. For performance reasons, it uses object references to find duplicated relations. This is fine as duplicated relations only happen with DataFrame queries. For SQL query, the table look-up always returns the scan relation with fresh attribute IDs.However, with metadata columns, the analyzer will copy a scan relation with new output attributes to include metadata cols. The object references check does not work anymore as two different scan relation instances may have the same attribute IDs.This PR fixes this problem by looking at the output attribute IDs to check duplicated relations.Note that, `CTERelationRef` has the same problem as Spark always creates a new instance of it. The golden file changes are caused by the CTE fix.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new test. I also ran `TPCDSQuerySuite` locally. The runtime of rule `DeduplicateRelations` does not change much.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR fixes a data race around concurrent access to `TaskMetrics.externalAccums`. The race occurs between the `executor-heartbeater` thread and the thread executing the task. This data race is not known to cause issues on 2.12 but in 2.13 ~due this change https://github.com/scala/scala/pull/9258~ (@LuciferYang bisected this to first cause failures in scala 2.13.7 one possible reason could be https://github.com/scala/scala/pull/9786) leads to an uncaught exception in the `executor-heartbeater` thread, which means that the executor will eventually be terminated due to missing hearbeats.This fix of using of using `CopyOnWriteArrayList` is cherry picked from https://github.com/apache/spark/pull/37206 where is was suggested as a fix by @LuciferYang since `TaskMetrics.externalAccums` is also accessed from outside the class `TaskMetrics`. The old PR was closed because at that point there was no clear understanding of the race condition. @JoshRosen commented here https://github.com/apache/spark/pull/37206#issuecomment-1189930626 saying that there should be no such race based on because all accumulators should be deserialized as part of task deserialization here: https://github.com/apache/spark/blob/0cc96f76d8a4858aee09e1fa32658da3ae76d384/core/src/main/scala/org/apache/spark/executor/Executor.scala#L507-L508 and therefore no writes should occur while the hearbeat thread will read the accumulators. But my understanding is that is incorrect as accumulators will also be deserialized as part of the taskBinary here: https://github.com/apache/spark/blob/169f828b1efe10d7f21e4b71a77f68cdd1d706d6/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala#L87-L88 which will happen while the heartbeater thread is potentially reading the accumulators. This can both due to user code using accumulators (see the new test case) but also when using the Dataframe/Dataset API as  sql metrics will also be `externalAccums`. One way metrics will be sent as part of the taskBinary is when the dep is a `ShuffleDependency`: https://github.com/apache/spark/blob/fbbcf9434ac070dd4ced4fb9efe32899c6db12a9/core/src/main/scala/org/apache/spark/Dependency.scala#L85 with a ShuffleWriteProcessor that comes from https://github.com/apache/spark/blob/fbbcf9434ac070dd4ced4fb9efe32899c6db12a9/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala#L411-L422 <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?The current code has a data race.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?It will fix an uncaught exception in the `executor-hearbeater` thread when using scala 2.13.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?This patch adds a new test case, that before the fix was applied consistently produces the uncaught exception in the heartbeater thread when using scala 2.13.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?The PR proposes to upgrade pandas to 2.0.0### Why are the changes needed?Support latest pandas for pandas API on Spark### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT, tests might fail without other changes from Epic https://issues.apache.org/jira/browse/SPARK-42618
2	-1	### What changes were proposed in this pull request?Add inclusive parameter for pd.date_range to support the pandas 2.0.0### Why are the changes needed?When pandas 2.0.0 is released, we should match the behavior in pandas API on Spark.### Does this PR introduce any user-facing change?yes, the API changesBefore:ps.date_range(start='2017-01-01', end='2017-01-04', closed=None)After:ps.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")### How was this patch tested?Unit tests were updated
2	-2	### What changes were proposed in this pull request?In the PR, I propose to change API of parameterized SQL, and replace type of argument values from `string` to `Any` in Scala/Java/Python and `Expression.Literal` in protobuf API. Language API can accept `Any` objects from which it is possible to construct literal expressions.This is a backport of https://github.com/apache/spark/pull/40623#### Scala/Java:```scala  def sql(sqlText: String, args: Map[String, Any]): DataFrame```values of the `args` map are wrapped by the `lit()` function which leaves `Column` as is and creates a literal from other Java/Scala objects (for more details see the `Scala` tab at https://spark.apache.org/docs/latest/sql-ref-datatypes.html).#### Python:```pythondef sql(self, sqlQuery: str, args: Optional[Dict[str, Any]] = None, **kwargs: Any) -> DataFrame:```Similarly to Scala/Java `sql`, Python's `sql()` accepts Python objects as values of the `args` dictionary (see more details about acceptable Python objects at https://spark.apache.org/docs/latest/sql-ref-datatypes.html). `sql()` converts dictionary values to `Column` literal expressions by `lit()`.#### Protobuf:```protomessage SqlCommand {  // (Required) SQL Query.  string sql = 1;  // (Optional) A map of parameter names to literal expressions.  map<string, Expression.Literal> args = 2;}```For example:```scalascala> val sqlText = \"\"\"SELECT s FROM VALUES ('Jeff /*__*/ Green'), ('E\\'Twaun Moore') AS t(s) WHERE s = :player_name\"\"\"sqlText: String = SELECT s FROM VALUES ('Jeff /*__*/ Green'), ('E\\'Twaun Moore') AS t(s) WHERE s = :player_namescala> sql(sqlText, args = Map(\"player_name\" -> lit(\"E'Twaun Moore\"))).show(false)+-------------+|s            |+-------------+|E'Twaun Moore|+-------------+```### Why are the changes needed?The current implementation the parameterized `sql()` requires arguments as string values parsed to SQL literal expressions that causes the following issues:1. SQL comments are skipped while parsing, so, some fragments of input might be skipped. For example, `'Europe -- Amsterdam'`. In this case, `-- Amsterdam` is excluded from the input.2. Special chars in string values must be escaped, for instance `'E\\'Twaun Moore'`### Does this PR introduce _any_ user-facing change?No since the parameterized SQL feature https://github.com/apache/spark/pull/38864 hasn't been released yet.### How was this patch tested?By running the affected tests:```$ build/sbt \"test:testOnly *ParametersSuite\"$ python/run-tests --parallelism=1 --testnames 'pyspark.sql.tests.connect.test_connect_basic SparkConnectBasicTests.test_sql_with_args'$ python/run-tests --parallelism=1 --testnames 'pyspark.sql.session SparkSession.sql'```Authored-by: Max Gekk <max.gekk@gmail.com>(cherry picked from commit 156a12ec0abba8362658a58e00179a0b80f663f2)
2	-2	### What changes were proposed in this pull request?Building the project against jdk11 on IDE shows errors because `Platform.java` depends on `sun.misc` which is in `jdk.unsupported` module in jdk11. The problem goes away when we pass `java.version` as `11` to maven as system D parameters.This PR set `java.version` to `11` by detecting jdk version automatically and improves build experience.### Why are the changes needed?It makes build experience on IDE against jdk11 smoother.### Does this PR introduce _any_ user-facing change?Users won't have to specify java.version in maven parameters any more since the profile gets activated automagically based on the jdk version.### How was this patch tested?This patch was tested by building locally.
1	-2	### What changes were proposed in this pull request?Fix `createDataFrame` to handle 0-dim numpy array properly.### Why are the changes needed?When 0-dim numpy array is passed to `createDataFrame`, it raises an unexpected error:```py>>> import numpy as np>>> spark.createDataFrame(np.array(0))Traceback (most recent call last):...TypeError: len() of unsized object```The error message should be:```pyValueError: NumPy array input should be of 1 or 2 dimensions.```### Does this PR introduce _any_ user-facing change?It will show a proper error message.### How was this patch tested?Enabled/updated the related test.
2	-1	### What changes were proposed in this pull request?This PR proposes to deduplicate versionchanged directive in Catalog.### Why are the changes needed?All API is implemented so we don't need to mark individual method.### Does this PR introduce _any_ user-facing change?Yes, it changes the documentation.### How was this patch tested?Manually check with documentation build. CI in this PR should test it out too.![Screen Shot 2023-04-05 at 10 30 50 AM](https://user-images.githubusercontent.com/6477701/229958119-44b253b0-7856-44a9-8718-4be4a857fd59.png)
1	-1	### What changes were proposed in this pull request?This PR clarifies Spark Connect option to be consistent with other sections.### Why are the changes needed?To be consistent with other configuration docs, and to be clear about Spark Connect option.### Does this PR introduce _any_ user-facing change?Yes, it changes the user-facing doc in Spark scripts.### How was this patch tested?Manually tested.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds error classes, message parameters, and SQL states in the Spark Connect server's ErrorInfo. It also stores the error class and message parameters info in a `SparkConnectException` for the Python client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error class programmatically accessible for python clients.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit tests.
3	-1	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/39294 that proposes to remove the breaking change detection within `master` branch. ### Why are the changes needed?Apache Spark does not guarantee the compatibility between individual commits but between releases according to semantic versioning.It would have been great if we can use this job as a sort of warning but this feature is missing in GitHub Actions. See also:- https://github.com/orgs/community/discussions/15452- https://github.com/actions/runner/issues/2347Therefore, we cannot use this as a warning.### Does this PR introduce _any_ user-facing change?No, dev-only.### How was this patch tested?Will be tested within my fork first.
2	-1	### What changes were proposed in this pull request?This PR proposes to remove the workaround added in https://github.com/apache/spark/pull/37079.### Why are the changes needed?The security fix has landed into Ubuntu latest version, and it should be fixed now.### Does this PR introduce _any_ user-facing change?No. dev-only.### How was this patch tested?CI in this PR should test it out.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces the concept of a `ClassFinder` that is able to scrape the REPL output (either file-based or in-memory based) for generated class files.  The `ClassFinder` is registered during initialization of the REPL and aids in uploading the generated class files as artifacts to the Spark Connect server.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To run UDFs which are defined on the client side REPL, we require a mechanism that can find the local REPL classfiles and then utilise the mechanism from https://issues.apache.org/jira/browse/SPARK-42653 to transfer them to the server as artifacts.  ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, users can now run UDFs on the default (ammonite) REPL with spark connect.Input (in REPL):```class A(x: Int) { def get = x * 5 + 19 }def dummyUdf(x: Int): Int = new A(x).getval myUdf = udf(dummyUdf _)spark.range(5).select(myUdf(col(\"id\"))).as[Int].collect()```Output:```Array[Int] = Array(19, 24, 29, 34, 39)```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests + E2E tests.
2	-2	### What changes were proposed in this pull request?By default, scripts in `connector/connect/bin` rebuild the necessary JARs using SBT. Even when this build is incremental and there were no changes, the sbt rebuild takes about 4 minutes. Another issue is that launching the `connector/connect/bin/spark-connect`, `connector/connect/bin/spark-connect-shell`, `connector/connect/bin/spark-connect-scala-client` scripts in parallel causes the two sbt builds to clash. So developer has to wait for one to finish before launching the other.By supplying an env var option `BUILD=0` the build step can be skipped:```BUILD=0 ./connector/connect/bin/spark-connectBUILD=0 ./connector/connect/bin/spark-connect-shellBUILD=0 ./connector/connect/bin/spark-connect-scala-client```At the same time, add a new script `connector/connect/bin/spark-connect-build` to rebuild both the server and client part:```./connector/connect/bin/spark-connect-build```After doing that, it's still slow to have sbt retrieve the classpath of the scala client at each startup. With `connector/connect/bin/spark-connect-scala-client-classpath` and new `SCCLASSPATH` env argument, it's possible to save it to a file:```connector/connect/bin/spark-connect-scala-client-classpath > ./scclasspathBUILD=0 SCCLASSPATH=$(cat ./scclasspath) connector/connect/bin/spark-connect-scala-client```### Why are the changes needed?Improving inner dev loop.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual build, using the scripts.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Allow `FileFormat` instances to define the schema of the `_metadata` column they expose. ### Why are the changes needed?Today, the schema of the file source `_metadata` column depends on the file format (e.g. parquet file format supports `_metadata.row_index`) but this is hard-wired into the `FileFormat` itself. Not only is this an ugly design, it also prevents custom file formats from adding their own fields to the `_metadata` column.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->https://github.com/apache/spark/pull/36726 supports TimestampNTZ type in JDBC data source and https://github.com/apache/spark/pull/37013 applies a fix to pass more test cases with H2.The problem is that Java Timestamp is a poorly defined class and different JDBC drivers implement \"getTimestamp\" and \"setTimestamp\" with different expected behaviors in mind. The general conversion implementation would work with some JDBC dialects and their drivers but not others. This issue is discovered when testing with PostgreSQL database.This PR adds a `dialect` parameter to `makeGetter` for applying dialect specific conversions when reading a Java Timestamp into TimestampNTZType. `makeSetter` already has a `dialect` field and we will use that for converting back to Java Timestamp.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix TimestampNTZ support for PostgreSQL. Allows other JDBC dialects to provide dialect specific implementation forconverting between Java Timestamp and Spark TimestampNTZType.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit test.I added new test cases for `PostgresIntegrationSuite` to cover TimestampNTZ read and writes.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds back old constructors for exceptions used in the public connector API based on Spark 3.3.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed to avoid breaking connectors when consuming Spark 3.4.Here is a list of exceptions used in the connector API (`org.apache.spark.sql.connector`):```NoSuchNamespaceExceptionNoSuchTableExceptionNoSuchViewExceptionNoSuchPartitionExceptionNoSuchPartitionsException (not referenced by public Catalog API but I assume it may be related to the exception above, which is referenced)NoSuchFunctionExceptionNoSuchIndexExceptionNamespaceAlreadyExistsExceptionTableAlreadyExistsExceptionViewAlreadyExistsExceptionPartitionAlreadyExistsException (not referenced by public Catalog API but I assume it may be related to the exception below, which is referenced)PartitionsAlreadyExistExceptionIndexAlreadyExistsException```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Adds back previously released constructors.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
2	-2	### What changes were proposed in this pull request?This PR partially reverts https://github.com/apache/spark/pull/40674. Seems like we still use Ubuntu 20.04 (https://github.com/apache/spark/blob/master/dev/infra/Dockerfile#L18-L20) that has the issue. This PR brings the workaround back for the jobs we use this image### Why are the changes needed?To recover the build.### Does this PR introduce _any_ user-facing change?No, dev-only.### How was this patch tested?CI in this PR.
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade buf from 1.16.0 to 1.17.0### Why are the changes needed?Release Notes: https://github.com/bufbuild/buf/releases<img width=\"640\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/230250728-97bcbdc9-c79b-4480-a732-a84072aa16fe.png\">https://github.com/bufbuild/buf/compare/v1.16.0...v1.17.0Manually test: dev/connect-gen-protos.shThis upgrade will not change the generated files.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually test and Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use CLOB instead of VARCHAR(255) for StringType for Oracle JDBC### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->- Fix insufficient length issue when storing a spark string to oracle.- Make room for Spark VarcharType mapping ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, Using APIs, such as DDL and `df.write.jdbc`, with oracle to store string will result in CLOB columns.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests.
3	-4	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add check for operations that involve multiple data frames,  because spark do not support joining for example two data frames from different Spark Connect Sessions.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Spark do not support joining for example two data frames from different Spark Connect Sessions. To avoid exceptions, the client should clearly fail when it tries to construct such a composition.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-2	### What changes were proposed in this pull request?This PR fixes construct aggregate expressions by replacing grouping functions if a expression is part of aggregation.In the following example, the second `b` should also be replaced:<img width=\"545\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5399861/230415618-84cd6334-690e-4b0b-867b-ccc4056226a8.png\">### Why are the changes needed?Fix bug:```spark-sql (default)> SELECT CASE WHEN a IS NULL THEN count(b) WHEN b IS NULL THEN count(c) END                   > FROM grouping                   > GROUP BY GROUPING SETS (a, b, c);[MISSING_AGGREGATION] The non-aggregating expression \"b\" is based on columns which are not participating in the GROUP BY clause.```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
2	-2	### JIRAJIRA: https://issues.apache.org/jira/browse/SPARK-43051### What changes were proposed in this pull request?Currently, when deserializing protobufs using `from_protobuf`, fields that are not explicitly present in the serialized message are deserialized as null in the resulting struct. However this includes singular proto3 scalars set explicitly to their default values, as they will [not appear](https://protobuf.dev/programming-guides/field_presence/#presence-in-tag-value-stream-wire-format-serialization) in the serialized protobuf.For example, given a message format like```syntax = \"proto3\";message Person {  string name = 1;  int64 age = 2;  optional string middle_name = 3;  optional int64 salary = 4;}```and an example message like```SearchRequest(age = 0, middle_name = \"\")```the result from calling `from_protobuf` on the serialized form of the above message would be```{\"name\": null, \"age\": null, \"middle_name\": \"\ \"salary\": null}```It can be useful to deserialize these fields as their defaults, e.g.:```{\"name\": \"\ \"age\": 0, \"middle_name\": \"\ \"salary\": null}```This behavior also exists in other major libraries, e.g. `includingDefaultValues` in [jsonformat](https://protobuf.dev/reference/java/api-docs/com/google/protobuf/util/JsonFormat.Printer.html#includingDefaultValueFields--), or `emitDefaults` in [jsonpb](https://pkg.go.dev/github.com/golang/protobuf/jsonpb#Marshaler).In this PR I implemented this behavior by adding an option, `emit.default.values` that can be passed to the options map for `from_protobuf` which controls whether to materialize these defaults or not.### Why are the changes needed?Additional functionality to help control the deserialization behavior.### Does this PR introduce _any_ user-facing change?Yes, it provides a new option that can be accepted by `from_protobuf`### How was this patch tested?I added test cases that assert the deserialization behavior for unset and default values for every type in proto2 and proto3.
1	-2	### What changes were proposed in this pull request?Handle stacktrace with null file name in event log### Why are the changes needed?NPE error when handling stacktrace with null file name in event log### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added test in JsonProtocolSuite
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `CoalesceBucketsInJoin` to AQE `preprocessingRules`. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Previously optimized bucket join: 'CoalesceBucketsInJoin'` : https://github.com/apache/spark/pull/28123But when using AQE , `CoalesceBucketsInJoin` can not match beacuse the top of the spark plan is `AdaptiveSparkPlan`.The code : ```  val spark = SparkSession.builder()    .appName(\"BucketJoin\")    .master(\"local[*]\")    .config(\"spark.sql.adaptive.enabled\ true)    .config(\"spark.driver.memory\ \"4\")    .config(\"spark.sql.autoBroadcastJoinThreshold\ \"-1\")    .config(\"spark.sql.bucketing.coalesceBucketsInJoin.enabled\ true)    .enableHiveSupport()    .getOrCreate()    val df1 = (0 until 20).map(i => (i % 5, i % 13, i.toString)).toDF(\"i\ \"j\ \"k\")    val df2 = (0 until 20).map(i => (i % 7, i % 11, i.toString)).toDF(\"i\ \"j\ \"k\")    df1.write.format(\"parquet\").bucketBy(4, \"i\").saveAsTable(\"t1\")    df2.write.format(\"parquet\").bucketBy(2, \"i\").saveAsTable(\"t2\")    val t1 = spark.table(\"t1\")    val t2 = spark.table(\"t2\")    val joined = t1.join(t2, t1(\"i\") === t2(\"i\"))    joined.explain()```Before the PR```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- SortMergeJoin [i#50], [i#56], Inner   :- Sort [i#50 ASC NULLS FIRST], false, 0   :  +- Filter isnotnull(i#50)   :     +- FileScan parquet spark_catalog.default.t1[i#50,j#51,k#52] Batched: true, Bucketed: true, DataFilters: [isnotnull(i#50)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>, SelectedBucketsCount: 4 out of 4   +- Sort [i#56 ASC NULLS FIRST], false, 0      +- Exchange hashpartitioning(i#56, 4), ENSURE_REQUIREMENTS, [plan_id=78]         +- Filter isnotnull(i#56)            +- FileScan parquet spark_catalog.default.t2[i#56,j#57,k#58] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(i#56)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>```After the PR output:```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- SortMergeJoin [i#50], [i#56], Inner   :- Sort [i#50 ASC NULLS FIRST], false, 0   :  +- Filter isnotnull(i#50)   :     +- FileScan parquet spark_catalog.default.t1[i#50,j#51,k#52] Batched: true, Bucketed: true, DataFilters: [isnotnull(i#50)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>, SelectedBucketsCount: 4 out of 4 (Coalesced to 2)   +- Sort [i#56 ASC NULLS FIRST], false, 0      +- Filter isnotnull(i#56)         +- FileScan parquet spark_catalog.default.t2[i#56,j#57,k#58] Batched: true, Bucketed: true, DataFilters: [isnotnull(i#56)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/shezhiming/gh/zzzzming_new/spark/spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int,k:string>, SelectedBucketsCount: 2 out of 2```Additional Notes:We don't add CoalesceBucketsInJoin to `AdaptiveSparkPlanExec#queryStageOptimizerRules` because queryStageOptimizerRules is not applied at the beginning of the init plan. Instead, they are applied in the createQueryStages() method. And createQueryStages() is bottom-up, which causes the exchange to be eliminated to be wrapped in a layer of ShuffleQueryStage first, making CoalesceBucketsInJoin unrecognizable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds the `orc`, `parquet`, and `text` APIs in connect's DataStreamReader### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Part of Streaming Connect project.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, now the three APIs are enabled. But everything is pretty much still under developed so far.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested, unit tests will be added in SPARK-43031 as a follow-up PR https://github.com/apache/spark/pull/40691.
2	-2	### What changes were proposed in this pull request?The PR changes the implementation of MapOutputTracker.updateMapOutput() to search for the MapStatus under the help of a mapping from mapId to mapIndex, previously it was performing a linear search, which would become performance bottleneck if a large proportion of all blocks in the map are migrated.### Why are the changes needed?To avoid performance bottleneck when block decommission is enabled and a lot of blocks are migrated within a short time window.### Does this PR introduce _any_ user-facing change?No, it's pure performance improvement.### How was this patch tested?Manually test.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Enable unit tests and doctests for streaming queries. A lot are skipped and needs to be un-skipped as the development goes on. Note that I also separated the `{foreach, foreachBatch}` tests from the original test suite. Because currently they are not implemented in connect and it seems unnecessary to manually add a skip for all of them in `StreamingParityTests`. Also it doesn't hurt to separate them I think as these tests are large enough already.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->More tests is always better than less.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->It's test itself.
2	-2	### What changes were proposed in this pull request?Supports duplicated nested field names when `spark.createDataFrame` or `df.collect`.### Why are the changes needed?If there are duplicated nested field names, the following error is raised:```py>>> from pyspark.sql.types import *>>>>>> data = [Row(Row(\"a\ 1), Row(2, 3, \"b\ 4, \"c\")), Row(Row(\"x\ 6), Row(7, 8, \"y\ 9, \"z\"))]>>> schema = (...     StructType()...     .add(\"struct\ StructType().add(\"x\ StringType()).add(\"x\ IntegerType()))...     .add(...         \"struct\...         StructType()...         .add(\"a\ IntegerType())...         .add(\"x\ IntegerType())...         .add(\"x\ StringType())...         .add(\"y\ IntegerType())...         .add(\"y\ StringType()),...     )... )>>> df = spark.createDataFrame(data, schema=schema)Traceback (most recent call last):...pyarrow.lib.ArrowTypeError: Expected bytes, got a 'int' object```### Does this PR introduce _any_ user-facing change?The duplicated nested field names will be available.### How was this patch tested?Added a test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we  move Numeric and Fractional to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate Spark Connect Column errors into error class.### Why are the changes needed?Leveraging PySpark error framework.### Does this PR introduce _any_ user-facing change?No, it's error message improvements.### How was this patch tested?The existing CI should pass
2	-1	### What changes were proposed in this pull request?- Add a new proto message for `sc.resources`- PyTorch Distributor support Local Mode with GPU### Why are the changes needed?For functionality parityAfter this PR, all UTs in `test_distributor` are reused and enabled in Connect### Does this PR introduce _any_ user-facing change?Yes, new mode supported in Connect### How was this patch tested?Enabled UTs
3	-3	### What changes were proposed in this pull request?RocksDB state store commit should continue background work in finally only if its paused### Why are the changes needed?If an exception is thrown earlier in the commit sequence before background work is paused, we fail with an exception in the finally clause.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Ran unit tests```StateStoreIntegrationSuite[info] Run completed in 16 seconds, 131 milliseconds.[info] Total number of tests run: 5[info] Suites: completed 1, aborted 0[info] Tests: succeeded 5, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.``````StateStoreSuite[info] Run completed in 1 minute, 18 seconds.                                                                                                                                                                                                                                              [info] Total number of tests run: 73[info] Suites: completed 1, aborted 0[info] Tests: succeeded 73, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```With simulated exception and without the fix, the code crashes with stack trace below. With the fix, we don't see the issue in the finally block.```  org.rocksdb.RocksDBException: InvalidArgument  at org.rocksdb.RocksDB.continueBackgroundWork(Native Method)  at org.rocksdb.RocksDB.continueBackgroundWork(RocksDB.java:3611)  at org.apache.spark.sql.execution.streaming.state.RocksDB.commit(RocksDB.scala:421)```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a new API `PartitionEvaluator` to define the computing logic and requires the caller side to explicitly list what needs to be serialized and sent to executors via `PartitionEvaluatorFactory`.Two new RDD APIs are added to use `PartitionEvaluator`:```  /**   * Return a new RDD by applying an evaluator to each partition of this RDD. The given evaluator   * factory will be serialized and sent to executors, and each task will create an evaluator with   * the factory, and use the evaluator to transform the data of the input partition.   */  @DeveloperApi  @Since(\"3.5.0\")  def mapPartitionsWithEvaluator[U: ClassTag](      partitionEvaluatorFactory: PartitionEvaluatorFactory[T, U]): RDD[U] = withScope {    new MapPartitionsWithEvaluatorRDD(this, taskEvaluatorFactory)  }  /**   * Zip this RDD's partitions with another RDD and return a new RDD by applying an evaluator to   * the zipped partitions. Assumes that the two RDDs have the *same number of partitions*, but   * does *not* require them to have the same number of elements in each partition.   */  @DeveloperApi  @Since(\"3.5.0\")  def zipPartitionsWithEvaluator[U: ClassTag](      rdd2: RDD[T],      partitionEvaluatorFactory: PartitionEvaluatorFactory[T, U]): RDD[U] = withScope {    new ZippedPartitionsWithEvaluatorRDD(this, rdd2, partitionEvaluatorFactory)  }```Three SQL operators are updated to use the new API to do execution, as a showcase: Project, Filter, WholeStageCodegen. We can migrate more operators later. A config is added to still use the old code path by default.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Using lambda to define the computing logic is a bit tricky:1. it's easy to mistakenly reference objects in the closure, which increases the time to serialize the lambda and sent to executors. `ProjectExec` and `FilterExec` use `child.output` in the lambda which means the entire `child` will be serialized. There are other places trying to avoid this problem, e.g. https://github.com/apache/spark/blob/v3.3.2/sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala#L90-L922. serializing lambda is strongly discouraged by the [official Java guide](https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#serialization). We should eventually get rid of lambda during distributed execution to make Spark more robust.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
1	-1	### What changes were proposed in this pull request?Add options to `lint-python` to run each test separately.```lint-python [--compile] [--black] [--flake8] [--mypy] [--mypy-examples] [--mypy-data]```### Why are the changes needed?Running each test separately is sometimes useful during the development.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually run `lint-python` with options.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`df.show` handle null should print NULL instead of null to consistent behavior;Like as the following behavior is currently inconsistent:``` shellscala> spark.sql(\"select decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle') as result\").show(false)+------+|result|+------+|null  |+------+`````` shellspark-sql> DESC FUNCTION EXTENDED decode;function_descFunction: decodeClass: org.apache.spark.sql.catalyst.expressions.DecodeUsage:    decode(bin, charset) - Decodes the first argument using the second argument character set.    decode(expr, search, result [, search, result ] ... [, default]) - Compares expr      to each search value in order. If expr is equal to a search value, decode returns      the corresponding result. If no match is found, then it returns default. If default      is omitted, it returns null.Extended Usage:    Examples:      > SELECT decode(encode('abc', 'utf-8'), 'utf-8');       abc      > SELECT decode(2, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');       San Francisco      > SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');       Non domestic      > SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle');       NULL    Since: 3.2.0Time taken: 0.074 seconds, Fetched 4 row(s)`````` shellspark-sql> select decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle');NULL```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`df.show` keep consistent behavior when handle `null` with spark-sql CLI.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, `null` will display NULL instead of null.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
2	-1	### What changes were proposed in this pull request?Set job description for tpcds queries.Before optimization：![image](https://user-images.githubusercontent.com/94670132/230567550-9bb2842c-aecc-41a5-acb6-0ff8ea765df1.png)After optimization：![image](https://user-images.githubusercontent.com/94670132/230568132-e7fbd4a4-1ef5-4995-873a-da1c943dcf1f.png)
1	-2	### What changes were proposed in this pull request?Before <img width=\"1789\" alt=\"截屏2023-04-07 下午4 22 54\" src=\"https://user-images.githubusercontent.com/46485123/230573688-42acb9f2-6fa0-48d0-bfde-c7ceeb306aef.png\">After <img width=\"1792\" alt=\"截屏2023-04-07 下午4 24 02\" src=\"https://user-images.githubusercontent.com/46485123/230573720-2c2a7731-d776-439c-ba6f-0dad9dc87a42.png\">### Why are the changes needed?Don't need show twice, too weird### Does this PR introduce _any_ user-facing change?No### How was this patch tested?MT
1	-1	### What changes were proposed in this pull request?This PR proposes to add test for dropDuplicates in JavaDatasetSuite.### Why are the changes needed?The API dropDuplicates wasn't tested by Java test suite. It'd be better to have a sanity check to verify inter-op between Scala and Java works well.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?CI will verify.
1	-3	## What changes were proposed in this pull request?This PR update the task retry logic to not retry if the exception has an error class which means a user error.## Why are the changes needed?As discussed [here](https://github.com/apache/spark/pull/40655#discussion_r1156693696), tasks that failed because of exceptions generated by AssertNotNull should not be retried.## Does this PR introduce any user-facing change?No## How was this patch tested?This PR comes with tests.
2	-2	### What changes were proposed in this pull request?In the PR, I propose new AES mode for the `aes_encrypt()`/`aes_decrypt()` functions - `CBC` ([Cipher Block Chaining](https://www.ibm.com/docs/en/linux-on-systems?topic=operation-cipher-block-chaining-cbc-mode)) with the padding `PKCS7(5)`. The `aes_encrypt()` function returns a binary value which consists of the following fields:1. The salt magic prefix `Salted__` with the length of 8 bytes.2. A salt generated per every `aes_encrypt()` call using `java.security.SecureRandom`. Its length is 8 bytes.3. The encrypted input.The encrypt function derives the secret key and initialization vector (16 bytes) from the salt and user's key using the same algorithm as OpenSSL's `EVP_BytesToKey()` (versions >= 1.1.0c).The `aes_decrypt()` functions assumes that its input has the fields as showed above.For example:```sqlspark-sql> SELECT base64(aes_encrypt('Apache Spark', '0000111122223333', 'CBC', 'PKCS'));U2FsdGVkX1/ERGxwEOTDpDD4bQvDtQaNe+gXGudCcUk=spark-sql> SELECT aes_decrypt(unbase64('U2FsdGVkX1/ERGxwEOTDpDD4bQvDtQaNe+gXGudCcUk='), '0000111122223333', 'CBC', 'PKCS');Apache Spark```### Why are the changes needed?To achieve feature parity with other systems/frameworks, and make the migration process from them to Spark SQL easier. For example, the `CBC` mode is supported by:- BigQuery: https://cloud.google.com/bigquery/docs/reference/standard-sql/aead-encryption-concepts#block_cipher_modes- Snowflake: https://docs.snowflake.com/en/sql-reference/functions/encrypt.html### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?By running new checks:```$ build/sbt \"sql/testOnly *QueryExecutionErrorsSuite\"$ build/sbt \"sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite\"$ build/sbt \"test:testOnly org.apache.spark.sql.MiscFunctionsSuite\"$ build/sbt \"core/testOnly *SparkThrowableSuite\"```and checked compatibility with LibreSSL/OpenSSL:```$ openssl versionLibreSSL 3.3.6$ echo -n 'Apache Spark' | openssl enc -e -aes-128-cbc -pass pass:0000111122223333 -aU2FsdGVkX1+5GyAmmG7wDWWDBAuUuxjMy++cMFytpls=``````sqlspark-sql (default)> SELECT aes_decrypt(unbase64('U2FsdGVkX1+5GyAmmG7wDWWDBAuUuxjMy++cMFytpls='), '0000111122223333', 'CBC');Apache Spark```decrypt Spark's output by OpenSSL:```sqlspark-sql (default)> SELECT base64(aes_encrypt('Apache Spark', 'abcdefghijklmnop12345678ABCDEFGH', 'CBC', 'PKCS'));U2FsdGVkX1+maU2vmxrulgxXuQSyZ3ODnlHKqnt2fDA=``````$ echo 'U2FsdGVkX1+maU2vmxrulgxXuQSyZ3ODnlHKqnt2fDA=' | openssl aes-256-cbc -a -d -pass pass:abcdefghijklmnop12345678ABCDEFGHApache Spark```
1	-3	### What changes were proposed in this pull request?This PR moves the error class resource file in Kafka connector from test to src, so that error class works without test artifacts.### Why are the changes needed?Refer to the `How was this patch tested?`.### Does this PR introduce _any_ user-facing change?Yes, but the possibility of encountering this is small enough.### How was this patch tested?Ran spark-shell with Kafka connector artifacts (without test artifacts) and triggered KafkaExceptions to confirm that exception is properly raised.```scala> import org.apache.spark.sql.kafka010.KafkaExceptionsimport org.apache.spark.sql.kafka010.KafkaExceptionsscala> import org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.TopicPartitionscala> KafkaExceptions.mismatchedTopicPartitionsBetweenEndOffsetAndPrefetched(Set[TopicPartition](), Set[TopicPartition]())res1: org.apache.spark.SparkException =org.apache.spark.SparkException: Kafka data source in Trigger.AvailableNow should provide the same topic partitions in pre-fetched offset to end offset for each microbatch. The error could be transient - restart your query, and report if you still see the same issue.topic-partitions for pre-fetched offset: Set(), topic-partitions for end offset: Set().```Without the fix, triggering KafkaExceptions failed to load error class resource file and led unexpected exception. ```scala> KafkaExceptions.mismatchedTopicPartitionsBetweenEndOffsetAndPrefetched(Set[TopicPartition](), Set[TopicPartition]())java.lang.IllegalArgumentException: argument \"src\" is null  at com.fasterxml.jackson.databind.ObjectMapper._assertNotNull(ObjectMapper.java:4885)  at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3618)  at org.apache.spark.ErrorClassesJsonReader$.org$apache$spark$ErrorClassesJsonReader$$readAsMap(ErrorClassesJSONReader.scala:95)  at org.apache.spark.ErrorClassesJsonReader.$anonfun$errorInfoMap$1(ErrorClassesJSONReader.scala:44)  at scala.collection.immutable.List.map(List.scala:293)  at org.apache.spark.ErrorClassesJsonReader.<init>(ErrorClassesJSONReader.scala:44)  at org.apache.spark.sql.kafka010.KafkaExceptions$.<init>(KafkaExceptions.scala:27)  at org.apache.spark.sql.kafka010.KafkaExceptions$.<clinit>(KafkaExceptions.scala)  ... 47 elided```
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate TypeError from DataFrame(Reader|Writer) into error class### Why are the changes needed?Improve user experience for PySpark error messages.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?This existing CI should padd
1	-3	## What changes were proposed in this pull request?This PR update the task retry logic to not retry if the exception has an error class which means a user error.## Why are the changes needed?As discussed https://github.com/apache/spark/pull/40655#discussion_r1156693696, tasks that failed because of exceptions generated by AssertNotNull should not be retried.## Does this PR introduce any user-facing change?No## How was this patch tested?This PR comes with tests.
2	-2	### What changes were proposed in this pull request?This PR aims to use `sbt-eclipse` instead of `sbteclipse-plugin`.### Why are the changes needed?Thanks to SPARK-34959, Apache Spark 3.2+ uses SBT 1.5.0 and we can use `set-eclipse` instead of old `sbteclipse-plugin`.- https://github.com/sbt/sbt-eclipse/releases/tag/6.0.0  - Add support for Java Execution Environments 9, 10, 11, 12, 13### Does this PR introduce _any_ user-facing change?No, this is a dev-only plugin.### How was this patch tested?Pass the CIs and manual tests.```$ build/sbt eclipseUsing /Users/dongjoon/.jenv/versions/1.8 as default JAVA_HOME.Note, this will be overridden by -java-home if it is set.Using SPARK_LOCAL_IP=localhostAttempting to fetch sbtLaunching sbt from build/sbt-launch-1.8.2.jar[info] welcome to sbt 1.8.2 (AppleJDK-8.0.302.8.1 Java 1.8.0_302)[info] loading settings for project spark-merge-build from plugins.sbt ...[info] loading project definition from /Users/dongjoon/APACHE/spark-merge/project[info] Updatinghttps://repo1.maven.org/maven2/com/github/sbt/sbt-eclipse_2.12_1.0/6.0.0/sbt-eclipse-6.0.0.pom  100.0% [##########] 2.5 KiB (4.5 KiB / s)...```
2	-1	### What changes were proposed in this pull request?This PR aims to upgrade `sbt-unidoc` to 0.5.0 for Apache Spark 3.5.0.### Why are the changes needed?Since v0.5.0, organization has moved from `com.eed3si9n` to `com.github.sbt`- https://github.com/sbt/sbt-unidoc/releases/tag/v0.5.0  - Add support for Scala 3### Does this PR introduce _any_ user-facing change?No, this is a dev-only change.### How was this patch tested?Pass the documentation generation CIs.
3	-3	### What changes were proposed in this pull request?This PR extends column default support to allow the ORDER BY, LIMIT, and OFFSET clauses at the end of a SELECT query in the INSERT source relation.For example:```create table t1(i boolean, s bigint default 42) using parquet;insert into t1 values (true, 41), (false, default);create table t2(i boolean default true, s bigint default 42,                 t string default 'abc') using parquet;insert into t2 (i, s) select default, s from t1 order by s limit 1;select * from t2;> true, 41L, \"abc\"```### Why are the changes needed?This improves usability and helps prevent confusing error messages.### Does this PR introduce _any_ user-facing change?Yes, SQL queries that previously failed will now succeed.### How was this patch tested?This PR adds new unit test coverage.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->There are important syntax rules about Cast/Store assignment/Type precedent list in the [ANSI Compliance doc](https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html)As we are going to release [timestamp_ntz](https://issues.apache.org/jira/browse/SPARK-35662) type in Spark 3.4.0, we should update the doc page as well.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Better documentation### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual build and verify<img width=\"1183\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230692965-02ec5a6e-8b8a-48dc-8049-9a87d26b2ce5.png\"><img width=\"1068\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230692988-bd35508c-0577-44c5-8448-f8d3b0aef2ea.png\"><img width=\"764\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230693005-cb61a760-ea11-4e6d-bdcb-2738c7c507c6.png\">
1	-2	### What changes were proposed in this pull request?Add constants for un-parameterized proto data types### Why are the changes needed?avoid recreating them### Does this PR introduce _any_ user-facing change?No, dev-only### How was this patch tested?existing UT
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?#### How to support more subexpressions elimination cases* Get all common expressions from input expressions of the current physical operator to current CodeGenContext. Recursively visits all subexpressions regardless of whether the current expression is a conditional expression.* For each common expression:  * Add a new boolean variable subExprInit to indicate whether it has  already been evaluated.   * Add a new code block in CodeGenSupport trait, and reset those subExprInit variables to false before the physical operators begin to evaluate the input row.  * Add a new wrapper subExpr function for each common subexpression.```private void subExpr_n(${argList}) { if (!subExprInit_n) {   ${eval.code}   subExprInit_n = true;   subExprIsNull_n = ${eval.isNull};   subExprValue_n = ${eval.value}; }}```  * When generating the input expression code,  if the input expression is a common expression, the expression code will be replaced with the corresponding subExpr function. When the subExpr function is called for the first time, subExprInit will be set to true, and the subsequent function calls will do nothing.#### Why should we support whole-stage subexpression eliminationRight now each spark physical operator shares nothing but the input row, so the same expressions may be evaluated multiple times across different operators. For example, the expression udf(c1, c2) in plan Project [udf(c1, c2)] - Filter [udf(c1, c2) > 0] - Relation will be evaluated both in Project and Filter operators.  We can reuse the expression results across different operators such as Project and Filter.#### How to support whole-stage subexpression elimination* Add two properties in CodegenSupport trait, the reusable expressions and the the output attributes, we can reuse the expression results only if the output attributes are the same.* Visit all operators from top to bottom, bound the candidate expressions with the output attributes and add to the current candidate reusable expressions.* Visit all operators from bottom to top, collect all the common expressions to the current operator, and add the initialize code to the current operator if the common expressions have not been initialized.* Replace the common expressions code when generating codes for  the physical operators.### Why are the changes needed?Support more subexpression elimination cases, improve performance.For example, TPCDS q23b query,  we can reuse the result of projection `sum(ss_quantity * ss_sales_price)` in the if expressions:```if (isnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])))   input[0, decimal(28,2), true]else    (input[0, decimal(28,2), true] + cast(knownnotnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])) as decimal(28,2)))```q4, q62, q67 are similar to the above.| Query    | Time with PR | Time without PR | Time diff | Percentage ||----------|--------------|-----------------|-----------|------------|| q1.sql   | 36.97        | 36.623          | -0.347    | 99.06%     || q2.sql   | 42.699       | 41.741          | -0.958    | 97.76%     || q3.sql   | 6.038        | 6.111           | 0.073     | 101.21%    || q4.sql   | 273.849      | 323.799         | 49.95     | 118.24%    || q5.sql   | 76.202       | 76.353          | 0.151     | 100.20%    || q6.sql   | 8.451        | 9.011           | 0.56      | 106.63%    || q7.sql   | 13.017       | 12.954          | -0.063    | 99.52%     || q8.sql   | 10.22        | 11.027          | 0.807     | 107.90%    || q9.sql   | 72.843       | 72.019          | -0.824    | 98.87%     || q10.sql  | 13.233       | 14.028          | 0.795     | 106.01%    || q11.sql  | 112.252      | 112.983         | 0.731     | 100.65%    || q12.sql  | 3.036        | 3.488           | 0.452     | 114.89%    || q13.sql  | 12.471       | 12.911          | 0.44      | 103.53%    || q14a.sql | 210.201      | 220.12          | 9.919     | 104.72%    || q14b.sql | 185.374      | 187.57          | 2.196     | 101.18%    || q15.sql  | 10.189       | 10.338          | 0.149     | 101.46%    || q16.sql  | 80.756       | 82.503          | 1.747     | 102.16%    || q17.sql  | 28.523       | 28.567          | 0.044     | 100.15%    || q18.sql  | 13.417       | 14.271          | 0.854     | 106.37%    || q19.sql  | 6.366        | 6.53            | 0.164     | 102.58%    || q20.sql  | 3.427        | 4.939           | 1.512     | 144.12%    || q21.sql  | 2.096        | 2.16            | 0.064     | 103.05%    || q22.sql  | 14.4         | 14.01           | -0.39     | 97.29%     || q23a.sql | 507.253      | 545.185         | 37.932    | 107.48%    || q23b.sql | 707.054      | 768.148         | 61.094    | 108.64%    || q24a.sql | 193.116      | 193.793         | 0.677     | 100.35%    || q24b.sql | 177.109      | 179.54          | 2.431     | 101.37%    || q25.sql  | 22.264       | 22.949          | 0.685     | 103.08%    || q26.sql  | 8.68         | 8.973           | 0.293     | 103.38%    || q27.sql  | 8.535        | 8.558           | 0.023     | 100.27%    || q28.sql  | 101.953      | 102.713         | 0.76      | 100.75%    || q29.sql  | 75.392       | 76.211          | 0.819     | 101.09%    || q30.sql  | 12.265       | 13.508          | 1.243     | 110.13%    || q31.sql  | 26.477       | 26.965          | 0.488     | 101.84%    || q32.sql  | 3.393        | 3.507           | 0.114     | 103.36%    || q33.sql  | 6.909        | 7.277           | 0.368     | 105.33%    || q34.sql  | 8.41         | 8.572           | 0.162     | 101.93%    || q35.sql  | 34.214       | 36.822          | 2.608     | 107.62%    || q36.sql  | 9.027        | 9.79            | 0.763     | 108.45%    || q37.sql  | 36.076       | 36.753          | 0.677     | 101.88%    || q38.sql  | 71.768       | 74.473          | 2.705     | 103.77%    || q39a.sql | 7.753        | 7.617           | -0.136    | 98.25%     || q39b.sql | 6.365        | 7.229           | 0.864     | 113.57%    || q40.sql  | 16.588       | 17.164          | 0.576     | 103.47%    || q41.sql  | 1.162        | 1.188           | 0.026     | 102.24%    || q42.sql  | 2.3          | 2.561           | 0.261     | 111.35%    || q43.sql  | 7.407        | 7.605           | 0.198     | 102.67%    || q44.sql  | 28.939       | 30.473          | 1.534     | 105.30%    || q45.sql  | 9.796        | 9.634           | -0.162    | 98.35%     || q46.sql  | 9.496        | 9.692           | 0.196     | 102.06%    || q47.sql  | 27.087       | 27.151          | 0.064     | 100.24%    || q48.sql  | 14.524       | 14.889          | 0.365     | 102.51%    || q49.sql  | 21.466       | 21.572          | 0.106     | 100.49%    || q50.sql  | 194.755      | 195.052         | 0.297     | 100.15%    || q51.sql  | 37.493       | 38.56           | 1.067     | 102.85%    || q52.sql  | 2.227        | 2.28            | 0.053     | 102.38%    || q53.sql  | 5.375        | 5.437           | 0.062     | 101.15%    || q54.sql  | 12.556       | 13.015          | 0.459     | 103.66%    || q55.sql  | 2.341        | 2.809           | 0.468     | 119.99%    || q56.sql  | 7.424        | 7.207           | -0.217    | 97.08%     || q57.sql  | 17.606       | 17.797          | 0.191     | 101.08%    || q58.sql  | 6.169        | 6.374           | 0.205     | 103.32%    || q59.sql  | 27.602       | 27.744          | 0.142     | 100.51%    || q60.sql  | 7.04         | 7.459           | 0.419     | 105.95%    || q61.sql  | 7.838        | 7.816           | -0.022    | 99.72%     || q62.sql  | 9.726        | 10.762          | 1.036     | 110.65%    || q63.sql  | 4.816        | 5.176           | 0.36      | 107.48%    || q64.sql  | 253.937      | 261.034         | 7.097     | 102.79%    || q65.sql  | 78.942       | 78.373          | -0.569    | 99.28%     || q66.sql  | 15.199       | 14.73           | -0.469    | 96.91%     || q67.sql  | 926.049      | 1022.971        | 96.922    | 110.47%    || q68.sql  | 7.932        | 7.977           | 0.045     | 100.57%    || q69.sql  | 12.101       | 14.699          | 2.598     | 121.47%    || q70.sql  | 20.7         | 20.872          | 0.172     | 100.83%    || q71.sql  | 14.96        | 15.065          | 0.105     | 100.70%    || q72.sql  | 73.215       | 73.955          | 0.74      | 101.01%    || q73.sql  | 5.973        | 6.126           | 0.153     | 102.56%    || q74.sql  | 97.611       | 99.577          | 1.966     | 102.01%    || q75.sql  | 125.005      | 129.508         | 4.503     | 103.60%    || q76.sql  | 34.812       | 35.34           | 0.528     | 101.52%    || q77.sql  | 7.686        | 8.474           | 0.788     | 110.25%    || q78.sql  | 287.959      | 292.936         | 4.977     | 101.73%    || q79.sql  | 8.401        | 9.616           | 1.215     | 114.46%    || q80.sql  | 59.371       | 60.051          | 0.68      | 101.15%    || q81.sql  | 18.452       | 19.499          | 1.047     | 105.67%    || q82.sql  | 64.093       | 65.032          | 0.939     | 101.47%    || q83.sql  | 4.675        | 4.867           | 0.192     | 104.11%    || q84.sql  | 10.456       | 10.816          | 0.36      | 103.44%    || q85.sql  | 12.347       | 12.77           | 0.423     | 103.43%    || q86.sql  | 6.537        | 6.843           | 0.306     | 104.68%    || q87.sql  | 77.427       | 77.876          | 0.449     | 100.58%    || q88.sql  | 83.082       | 83.385          | 0.303     | 100.36%    || q89.sql  | 6.645        | 6.801           | 0.156     | 102.35%    || q90.sql  | 7.841        | 7.883           | 0.042     | 100.54%    || q91.sql  | 3.88         | 4.129           | 0.249     | 106.42%    || q92.sql  | 3.044        | 3.271           | 0.227     | 107.46%    || q93.sql  | 361.149      | 365.883         | 4.734     | 101.31%    || q94.sql  | 43.929       | 46.667          | 2.738     | 106.23%    || q95.sql  | 196.363      | 197.427         | 1.064     | 100.54%    || q96.sql  | 12.457       | 12.496          | 0.039     | 100.31%    || q97.sql  | 80.131       | 81.821          | 1.69      | 102.11%    || q98.sql  | 6.885        | 7.522           | 0.637     | 109.25%    || q99.sql  | 17.685       | 18.009          | 0.324     | 101.83%    ||          | 6788.707     | 7116.357        | 327.65    | 104.82%    |One of our production query which has 19 case when  expressions, it's query time changed from 1.1 hour to 42 seconds.![image](https://user-images.githubusercontent.com/3626747/227448668-8a58ff33-3296-4a95-984b-292af47532ca.png)A simplify benchmark of the above production query.```    spark.range(1, 2000000, 1, 1)      .selectExpr(        \"cast(id + 1 as decimal) as a\        \"cast(id + 2 as decimal) as b\        \"cast(id + 3 as decimal) as c\        \"cast(id + 4 as decimal) as d\")      .createOrReplaceTempView(\"tab\")    runBenchmark(\"Subexpression elimination in ProjectExec\") {      val benchmark =        new Benchmark(\"Subexpression elimination in ProjectExec\ 2000000, output = output)      benchmark.addCase(s\"Test query\") { _ =>        val query =          s\"\"\"             |SELECT a, b, c, d,             |       a * b / c as s1,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END s2,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |       ELSE 0 END s3,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s4,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                      CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |                 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s5             |FROM tab             |\"\"\".stripMargin        spark.sql(query).noop()      }      benchmark.run()```Local benchmark result:Before this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         9713           9900         263          0.2        4856.7       1.0X```After this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         1238           1307          98          8.1         123.8       1.0X```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Exists UT.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the function without constant parameters of `SessionState#executePlan`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Before this change , using pyspark to run the code will get exception because py4j can not support default arguments in scala.```df = spark.sql(\"select 1\") catalyst_plan = df._jdf.queryExecution().logical()print('catalyst_plan: ', catalyst_plan)df_size = spark._jsparkSession.sessionState().executePlan(catalyst_plan)``````py4j.protocol.Py4JError: An error occurred while calling o87.executePlan. Trace:py4j.Py4JException: Method executePlan([class org.apache.spark.sql.catalyst.plans.logical.Project]) does not exist\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\tat py4j.Gateway.invoke(Gateway.java:274)\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\tat java.lang.Thread.run(Thread.java:748)```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->user call  `spark._jsparkSession.sessionState().executePlan(catalyst_plan)`  can get result.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual testing
1	-1	### What changes were proposed in this pull request?This pr regenerate benchmark results of `StateStoreBasicOperationsBenchmark`.### Why are the changes needed?https://github.com/apache/spark/pull/40639 upgrade rocksdbjni from 7.10.2 to 8.0.0, we need check and update the relevant benchmark results.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-3	### What changes were proposed in this pull request?Change `gRPC` to `grpcio` This is ONLY in the printing, for users that haven't install `gRPC`### Why are the changes needed?Users that don't have install `gRPC` will get this error when starting connect.ModuleNotFoundError                       Traceback (most recent call last)File /opt/spark/python/pyspark/sql/connect/utils.py:45, in require_minimum_grpc_version()     44 try:---> 45     import grpc     46 except ImportError as error:ModuleNotFoundError: No module named 'grpc'The above exception was the direct cause of the following exception:ImportError                               Traceback (most recent call last)Cell In[1], line 11      9 import pyarrow     10 from pyspark import SparkConf, SparkContext---> 11 from pyspark import pandas as ps     12 from pyspark.sql import SparkSession     13 from pyspark.sql.functions import col, concat, concat_ws, expr, lit, trimFile /opt/spark/python/pyspark/pandas/__init__.py:59     50     warnings.warn(     51         \"'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to \"     52         \"set this environment variable to '1' in both driver and executor sides if you use \"   (...)     55         \"already launched.\"     56     )     57     os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"---> 59 from pyspark.pandas.frame import DataFrame     60 from pyspark.pandas.indexes.base import Index     61 from pyspark.pandas.indexes.category import CategoricalIndexFile /opt/spark/python/pyspark/pandas/frame.py:88     85 from pyspark.sql.window import Window     87 from pyspark import pandas as ps  # For running doctests and reference resolution in PyCharm.---> 88 from pyspark.pandas._typing import (     89     Axis,     90     DataFrameOrSeries,     91     Dtype,     92     Label,     93     Name,     94     Scalar,     95     T,     96     GenericColumn,     97 )     98 from pyspark.pandas.accessors import PandasOnSparkFrameMethods     99 from pyspark.pandas.config import option_context, get_optionFile /opt/spark/python/pyspark/pandas/_typing.py:25     22 from pandas.api.extensions import ExtensionDtype     24 from pyspark.sql.column import Column as PySparkColumn---> 25 from pyspark.sql.connect.column import Column as ConnectColumn     26 from pyspark.sql.dataframe import DataFrame as PySparkDataFrame     27 from pyspark.sql.connect.dataframe import DataFrame as ConnectDataFrameFile /opt/spark/python/pyspark/sql/connect/column.py:19      1 #      2 # Licensed to the Apache Software Foundation (ASF) under one or more      3 # contributor license agreements.  See the NOTICE file distributed with   (...)     15 # limitations under the License.     16 #     17 from pyspark.sql.connect.utils import check_dependencies---> 19 check_dependencies(__name__)     21 import datetime     22 import decimalFile /opt/spark/python/pyspark/sql/connect/utils.py:35, in check_dependencies(mod_name)     33 require_minimum_pandas_version()     34 require_minimum_pyarrow_version()---> 35 require_minimum_grpc_version()File /opt/spark/python/pyspark/sql/connect/utils.py:47, in require_minimum_grpc_version()     45     import grpc     46 except ImportError as error:---> 47     raise ImportError(     48         \"grpc >= %s must be installed; however, \" \"it was not found.\" % minimum_grpc_version     49     ) from error     50 if LooseVersion(grpc.__version__) < LooseVersion(minimum_grpc_version):     51     raise ImportError(     52         \"gRPC >= %s must be installed; however, \"     53         \"your version was %s.\" % (minimum_grpc_version, grpc.__version__)     54     )ImportError: grpc >= 1.48.1 must be installed; however, it was not found.The last line tells that there is a module named `grpc` that's missing.  `pip install grpc`Collecting grpc  Downloading grpc-1.0.0.tar.gz (5.2 kB)  Preparing metadata (setup.py) ... error  error: subprocess-exited-with-error    × python setup.py egg_info did not run successfully.  │ exit code: 1  ╰─> [6 lines of output]      Traceback (most recent call last):        File \"<string>\ line 2, in <module>        File \"<pip-setuptools-caller>\ line 34, in <module>        File \"/tmp/pip-install-vp4d8s4c/grpc_c0f1992ad8f7456b8ac09ecbaeb81750/setup.py\ line 33, in <module>          raise RuntimeError(HINT)      RuntimeError: Please install the official package with: pip install grpcio      [end of output]    note: This error originates from a subprocess, and is likely not a problem with pip.error: metadata-generation-failed× Encountered error while generating package metadata.╰─> See above for output.note: This is an issue with the package mentioned above, not pip.hint: See above for details.Note: you may need to restart the kernel to use updated packages.[The right way to install this is](https://grpc.io/docs/languages/python/quickstart/) `pip install grpcio`### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?Change tests to avoid replacing and dropping a temporary view that is created in `SubquerySuite#beforeAll`.### Why are the changes needed?When I added a test for SPARK-42937, it tried to use the view `t`, which is created in `beforeAll`. But because other tests would replace and drop this view, the new test would fail. As a result, that new test had to re-create `t` from scratch.This change will allow `t` to be used by new tests.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes UNRECOGNIZED_SQL_TYPE print both the column type name and the id.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->UNRECOGNIZED_SQL_TYPE prints the jdbc type id in the error message currently. This is difficult for spark users to understand the meaning of this kind of error, especially when the type id is from a vendor extension.For example, ```java org.apache.spark.SparkSQLException: Unrecognized SQL type -102```While -102 is nonstandard, it's hard to know what type it is```classOf[java.sql.JDBCType].getEnumConstants.foreach(t => println(t.getName + \"|\" + t.getVendorTypeNumber))BIT|-7TINYINT|-6SMALLINT|5INTEGER|4BIGINT|-5FLOAT|6REAL|7DOUBLE|8NUMERIC|2DECIMAL|3CHAR|1VARCHAR|12LONGVARCHAR|-1DATE|91TIME|92TIMESTAMP|93BINARY|-2VARBINARY|-3LONGVARBINARY|-4NULL|0OTHER|1111JAVA_OBJECT|2000DISTINCT|2001STRUCT|2002ARRAY|2003BLOB|2004CLOB|2005REF|2006DATALINK|70BOOLEAN|16ROWID|-8NCHAR|-15NVARCHAR|-9LONGNVARCHAR|-16NCLOB|2011SQLXML|2009REF_CURSOR|2012TIME_WITH_TIMEZONE|2013TIMESTAMP_WITH_TIMEZONE|2014```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, the unrecognized jdbc type error will also print the type name For example, ```java org.apache.spark.SparkSQLException: Unrecognized SQL type - name: TIMESTAMP WITH LOCAL TIME ZONE, id: -102```### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
2	-2	### What changes were proposed in this pull request?Parquet has supported vector read speed up with this PR https://github.com/apache/parquet-mr/pull/1011The performance gain is 4x ~ 8x according to the parquet microbenchmarkTPC-H(SF100) Q6 has 11% performance increase with Apache Spark integrating parquet vector optimization### Why are the changes needed?This PR used to support parquet vector optimization### Does this PR introduce _any_ user-facing change?Add configuration spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.### How was this patch tested?For the test case, there are some problems to fix:1. It is necessary to Parquet-mr community release new java version to use the parquet vector optimization.2. Parquet Vector optimization does not release default, so users have to build parquet with mvn clean install -P vector-plugins manually to get the parquet-encoding-vector-{VERSION}.jar and put it on the {SPARK_HOME}/jars path3. github doesn't support select runners with specific instruction set. So it is impossible (a self-hosted runner can do it) to verify the optimization on github runners machine.
1	-2	### What changes were proposed in this pull request?This pr aims upgrade `zstd-jni` from 1.5.4-2 to 1.5.5-1.### Why are the changes needed?New version includes a bug fix about `Closing ZstdOutputStream without any write produces empty files`:- https://github.com/luben/zstd-jni/issues/249  | https://github.com/luben/zstd-jni/commit/2b6f2eca3e07817f4713ec43a047100c26cc51a1Other changes as follows:- https://github.com/luben/zstd-jni/compare/v1.5.4-2...v1.5.5-1### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-1	### What changes were proposed in this pull request?This PR proposes to remove the dependency on `grpcio` when remote session is not used for pandas API on Spark.### Why are the changes needed?Pandas API on spark should be able to run without `grpcio` when using regular Spark session.### Does this PR introduce _any_ user-facing change?No API change, but now users can use pandas API on Spark without installing any additional package when using regular Spark session.### How was this patch tested?Manually test without installing `grpcio`.
2	-1	### What changes were proposed in this pull request?A minor refactor,  just move  `withTable` function from `RemoteSparkSession` to `SQLHelper`, put it together with functions like `withSQLConf`, `withTempDatabase`, etc.### Why are the changes needed?`withTable` function is more suitable in `SQLHelper`.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Added a `TorchDistributor` method API :```    def _train_on_dataframe(self, train_function, spark_dataframe, *args, **kwargs):        \"\"\"        Runs distributed training using provided spark DataFrame as input data.        You should ensure the input spark DataFrame have evenly divided partitions,        and this method starts a barrier spark job that each spark task in the job        process one partition of the input spark DataFrame.        Parameters        ----------        train_function :            Either a PyTorch function, PyTorch Lightning function that launches distributed            training. Note that inside the function, you can call            `pyspark.ml.torch.distributor.get_spark_partition_data_loader` API to get a torch            data loader, the data loader loads data from the corresponding partition of the            input spark DataFrame.        spark_dataframe :            An input spark DataFrame that can be used in PyTorch `train_function` function.            See `train_function` argument doc for details.        args :            `args` need to be the input parameters to `train_function` function. It would look like            >>> model = distributor.run(train, 1e-3, 64)            where train is a function and 1e-3 and 64 are regular numeric inputs to the function.        kwargs :            `kwargs` need to be the key-work input parameters to `train_function` function.            It would look like            >>> model = distributor.run(train, tol=1e-3, max_iter=64)            where train is a function that has 2 arguments `tol` and `max_iter`.        Returns        -------            Returns the output of `train_function` called with args inside spark rank 0 task.        \"\"\"```Added an API:```def _get_spark_partition_data_loader(num_samples, batch_size, prefetch=2):    \"\"\"    This function must be called inside the `train_function` where `train_function`    is the input argument of `TorchDistributor.train_on_dataframe`.    The function returns a pytorch data loader that loads data from    the corresponding spark partition data.    Parameters    ----------    num_samples :        Number of samples to generate per epoch. If `num_samples` is less than the number of        rows in the spark partition, it generate the first `num_samples` rows of        the spark partition, if `num_samples` is greater than the number of        rows in the spark partition, then after the iterator loaded all rows from the partition,        it wraps round back to the first row.    batch_size:        How many samples per batch to load.    prefetch:        Number of batches loaded in advance.    \"\"\"```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The added APIs are designed for spark 4.0 new ML module.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests.
2	-1	### What changes were proposed in this pull request?Implement Arrow-optimized Python UDFs in Spark Connect.Please see https://github.com/apache/spark/pull/39384 for motivation and  performance improvements of Arrow-optimized Python UDFs.### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?Yes. In Spark Connect Python Client, users can:1. Set `useArrow` parameter True to enable Arrow optimization for a specific Python UDF.```sh>>> df = spark.range(2)>>> df.select(udf(lambda x : x + 1, useArrow=True)('id')).show()+------------+                                                                  |<lambda>(id)|+------------+|           1||           2|+------------+# ArrowEvalPython indicates Arrow optimization>>> df.select(udf(lambda x : x + 1, useArrow=True)('id')).explain()== Physical Plan ==*(2) Project [pythonUDF0#18 AS <lambda>(id)#16]+- ArrowEvalPython [<lambda>(id#14L)#15], [pythonUDF0#18], 200   +- *(1) Range (0, 2, step=1, splits=1)```2. Enable `spark.sql.execution.pythonUDF.arrow.enabled` Spark Conf to make all Python UDFs Arrow-optimized.```sh>>> spark.conf.set(\"spark.sql.execution.pythonUDF.arrow.enabled\ True)>>> df.select(udf(lambda x : x + 1)('id')).show()+------------+                                                                  |<lambda>(id)|+------------+|           1||           2|+------------+# ArrowEvalPython indicates Arrow optimization>>> df.select(udf(lambda x : x + 1)('id')).explain()== Physical Plan ==*(2) Project [pythonUDF0#30 AS <lambda>(id)#28]+- ArrowEvalPython [<lambda>(id#26L)#27], [pythonUDF0#30], 200   +- *(1) Range (0, 2, step=1, splits=1)```### How was this patch tested?Parity unit tests.
1	-1	### What changes were proposed in this pull request?This pr aims upgrade `cyclonedx-maven-plugin` to 2.7.6 for Apache Spark 3.5.0.### Why are the changes needed?The release notes as follows:- https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.4- https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.5- https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.6### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions.- Manual check the `cyclonedx.xml` file can be generated normally.
1	-1	### What changes were proposed in this pull request?This PR aims to mark `StateStoreSuite` and `RocksDBStateStoreSuite` as `ExtendedSQLTest`.### Why are the changes needed?To balance GitHub Action jobs by offloading heavy tests.- `sql - other tests` took [2 hour 55 minutes](https://github.com/apache/spark/actions/runs/4641961434/jobs/8215437737)- `sql - slow tests` took [1 hour 46 minutes](https://github.com/apache/spark/actions/runs/4641961434/jobs/8215437616)```- maintenance (2 seconds, 4 milliseconds)- SPARK-40492: maintenance before unload (2 minutes)- snapshotting (1 second, 96 milliseconds)- SPARK-21145: Restarted queries create new provider instances (1 second, 261 milliseconds)```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?- Upgrade the parquet dependency version to `1.13.0` from `1.12.3` (there is already a PR for this apache/spark#40555, rebase this PR once it is merged)- Parquet version `1.13.0` has a fix for [PARQUET-2161](https://issues.apache.org/jira/browse/PARQUET-2161) which allows splitting the parquet files when row index metadata column is selected. Currently the file splitting is disabled. Enable file splitting with row index column.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Splitting parquet files allows better parallelization when row index metadata column is selected.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Uncomment the existing unittests.
2	-2	### What changes were proposed in this pull request?This PR adds the mapGroup and coGroup support for Scala client.The client exposes the same SQL API to users, but uses an internal impl of the `KeyValueGroupedDatasetImpl` to keep track of initial types of the grouping keys and values. This is needed to support `keyAs` and `mapValues` type modifications to the dataset.It adds one the sorting expression into the proto so that the sorting expressions can be passed to the server.### Why are the changes needed?API completion.### Does this PR introduce _any_ user-facing change?Add new Scala Client `KeyValueGroupedDataset` APIs.### How was this patch tested?Client E2E tests that only work for SBT due to UDFUtil not synced to the server with maven build.The maven tests will be address in a follow up [PR](https://github.com/apache/spark/pull/40762)
2	-2	### What changes were proposed in this pull request?Support bin pack task scheduling on executors. This is controlled by `spark.scheduler.binPack.enabled`### Why are the changes needed?Dynamic allocation only remove or decommission idle executors. The default task scheduler use round robin to do task assignment on executors. This leads to resource waste.For example, we have 4 tasks to run, 4 executors(each has 4 cpu cores). Default task scheduler will assign 1 task per executors. With bin pack, 1 executor will be assigned 4 tasks, then dynamic allocation could remove other 3 idle executors to reduce resource waste.In prod, we have seen 20~50 percent resource save based on executor size, the more saving with more cpu cores per executor. For the issue could be caused by bin-packed executors, it's still possible to happen when all executor cores are occupied. If it indeed happen, the better solution might be large partition num.The purpose of change is to provide one more scheduling option, customer still can use the old way.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added test in TaskSchedulerImplSuite
1	-1	### What changes were proposed in this pull request?This PR adds `CoalesceBucketsInJoin` to `AdaptiveSparkPlanExec.queryStagePreparationRules`.### Why are the changes needed?Coalesce buckets in join will not work if AQE is enabled.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
2	-2	### What changes were proposed in this pull request?This PR adds support for column DEFAULT assignment for multi-part table names.### Why are the changes needed?Spark SQL workloads that refer to tables with multi-part names may want to use column DEFAULT functionality. This PR enables this.### Does this PR introduce _any_ user-facing change?Yes, column DEFAULT assignment now works with multi-part table names.### How was this patch tested?This PR adds unit tests.
2	-1	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/40603 which redacts the debug string shown in UI.### Why are the changes needed?To leverage existing redaction feature in UI.### Does this PR introduce _any_ user-facing change?Yes, it redacts the sensitive information as configured `spark.sql.redaction.string.regex`### How was this patch tested?Manually tested:![Screenshot 2023-04-11 at 11 07 22 AM](https://user-images.githubusercontent.com/6477701/231036919-fc73b9f0-b2e6-4d95-be25-60421f99ee4f.png)
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes sure Spark satisfies distribution and ordering requirements during CTAS/RTAS by carrying the analyzed plan to exec nodes and building `AppendData` for the new table.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed for multiple reasons:- Some data sources may require a specific distribution/ordering/num partitions for **correctness**. All of that information is now being requested from Spark via `RequiresDistributionAndOrdering`, which is ignored by CTAS/RTAS commands. As a result, a data source may ask for a particular distribution or ordering and Spark may not respect it. This can cause correctness issues.- Ignoring `RequiresDistributionAndOrdering` may severely degrade the write **performance** or even fail jobs as the underlying write may produce lots of small files.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
1	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40561 introduce unimplemented `dropDuplicatesWithinWatermark`  series functions to connect module `Dataset`, this pr clean up them and add it to `ProblemFilters` of `CheckConnectJvmClientCompatibility`.### Why are the changes needed?Clean up unimplemented APIs from connect module `Dataset`.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change adds applyInPandasWithState support for Spark connect. Example (try with local mode `./bin/pyspark --remote \"local[*]\"`):```>>> from pyspark.sql.streaming.state import GroupStateTimeout, GroupState>>> from pyspark.sql.types import (...     LongType,...     StringType,...     StructType,...     StructField,...     Row,... )>>> import pandas as pd>>> output_type = StructType(...     [StructField(\"key\ StringType()), StructField(\"countAsString\ StringType())]... )>>> state_type = StructType([StructField(\"c\ LongType())])>>> def func(key, pdf_iter, state):...     total_len = 0...     for pdf in pdf_iter:...         total_len += len(pdf)...     state.update((total_len,))...     yield pd.DataFrame({\"key\": [key[0]], \"countAsString\": [str(total_len)]})...>>>>>> input_path = \"/Users/peng.zhong/tmp/applyInPandasWithState\">>> df = spark.readStream.format(\"text\").load(input_path)>>> q = (...       df.groupBy(df[\"value\"])...       .applyInPandasWithState(...           func, output_type, state_type, \"Update\ GroupStateTimeout.NoTimeout...       )...       .writeStream.queryName(\"this_query\")...       .format(\"memory\")...       .outputMode(\"update\")...       .start()...   )>>>>>> q.status{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}>>>>>> spark.sql(\"select * from this_query\").show()+-----+-------------+|  key|countAsString|+-----+-------------+|hello|            1|| this|            1|+-----+-------------+```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This change adds an API support for spark connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This change adds an API support for spark connect.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested.
2	-4	### What changes were proposed in this pull request?This pr aims to refactor `Add a directory when spark.sql.legacy.addSingleFileInAddFile set to false` in `HiveCatalogedDDLSuite` to test using a temporary directory with non-fixed root directory.### Why are the changes needed?Before this PR, this test case used a fixed directory `/tmp/spark/addDirectory/` as the root directory of the temporary directory for testing, this can lead to the following scenarios1. Execute the following command to test ```build/mvn clean install -Phadoop-3 -pl sql/hive -Dtest=none -DwildcardSuites=org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite -am```2. Execute `ls -R /tmp/spark`, we can see the permission of the `/tmp/spark/addDirectory` directory is `drwxrwxr-x` and  the content of `/tmp/spark/addDirectory`  was cleaned up, but the xx directory still exists```/tmp/spark:total 4drwxrwxr-x 2 userA groupA 4096 Apr 11 10:55 addDirectory/tmp/spark/addDirectory:total 0```3. Switch to userB to re-run the test commands```build/mvn clean install -Phadoop-3 -pl sql/hive -Dtest=none -DwildcardSuites=org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite -am```then we can see the test fail with following message:```- Add a directory when spark.sql.legacy.addSingleFileInAddFile set to false *** FAILED ***  java.io.IOException: Failed to create a temp directory (under /tmp/spark/addDirectory/) after 10 attempts!  at org.apache.spark.util.Utils$.createDirectory(Utils.scala:314)  at org.apache.spark.util.Utils$.createTempDir(Utils.scala:334)  at org.apache.spark.sql.execution.command.DDLSuite.$anonfun$new$371(DDLSuite.scala:2497)  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)  at org.scalatest.Transformer.apply(Transformer.scala:22)  at org.scalatest.Transformer.apply(Transformer.scala:20)  at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190)  ...```So this pr changes the case to use a temporary directory with non-fixed root directory for tesing to solve the bad case mentioned above, and ensure the temporary directory for testing under root dir is better cleaned up.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual checked: the temporary directory has been cleaned from the first level under root after test
2	-2	### What changes were proposed in this pull request?This PR aims to upgrade Maven to 3.8.7 from 3.9.1.These two versions have different http timeout due to the default use of `resolver transport`:- Maven 3.8.7 use Wagon as default and default timeout is 60000ms- Maven 3.9.1 use native `transport-http` as default and default timeout is 10000msSo this pr also add [`-Dmaven.resolver.transport=wagon`](https://maven.apache.org/guides/mini/guide-resolver-transport.html#how-to-upgrade-from-wagon-or-%E2%80%9Cnative-transport-does-not-work%E2%80%9D) user property to `build/mvn` script to stick with Wagon which maven 3.8.7 used.### Why are the changes needed?The release notes and as follows:- https://maven.apache.org/docs/3.9.0/release-notes.html- https://maven.apache.org/docs/3.9.1/release-notes.html- https://github.com/apache/maven/releases/tag/maven-3.9.0- https://github.com/apache/maven/releases/tag/maven-3.9.1### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual test :run `build/mvn -version` wll trigger download `apache-maven-3.9.1-bin.tar.gz````exec: curl --silent --show-error -L https://www.apache.org/dyn/closer.lua/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz?action=download```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Today, `PythonUDF` can be used as an aggregate function according to the eval type. However, this is done in a tricky way, as `PythonUDF` does not extend `AggregateFunction` and we need to add special handling of it here and there. This is pretty error-prone, and we have hit issues such as https://github.com/apache/spark/pull/39824This PR adds a new `PythonUDAF` expression which extends `AggregateFunction`. Now python udaf will be handled the same as normal aggregate functions, except for the places that we need to extract python functions. After this, we can remove most of the special handling of `PythonUDF`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
1	-2	### What changes were proposed in this pull request?Rename BUILD added in https://github.com/apache/spark/pull/40676/ to SCBUILD to avoid env var clashes.### Why are the changes needed?Avoid name clashes.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual build.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?1. add columns field on SQL/SQLCommand in connect protobuf to support send expression to server2. parser columns in server to support sql with column3. add string_formatter for connect module to support sql with dataframes.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  4. If you fix some SQL features, you can provide some references of other DBMSes.  5. If there is design documentation, please add the link.  6. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?To support  sql with dataframes and columns in spark connect client.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-3	### What changes were proposed in this pull request?This PR makes it also remove `EqualNullSafe` when removing `EqualTo` if their children are same when constructing candidate constraints in `ConstraintHelper.inferAdditionalConstraints`.For example: `l = r and l <=> r`. Before this PR, `l = r and l <=> r` can infer `l <=> l and r <=> r` which is useless. After This PR, it can't infer anything.### Why are the changes needed?Avoid Once strategy's idempotence is broken for batch: `Infer Filters`:```sqlexport SPARK_TESTING=1CREATE TABLE t1 (i INT, j INT, k STRING) USING parquet;CREATE TABLE t2 (i INT, j INT, k STRING) USING parquet;CREATE TABLE t3 (i INT, j INT, k STRING) USING parquet;SELECT *FROM   (SELECT t1.i, t1.i as t1i        FROM t1 JOIN t3 ON t1.i = t3.i) t       JOIN t2 ON t.i = t2.i;```Before this PR:```=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints === Join Inner, (i#72 = i#78)                                               Join Inner, (i#72 = i#78)!:- Project [i#72, i#72 AS t1i#71]                                       :- Filter ((i#72 <=> i#72) AND (t1i#71 <=> t1i#71))!:  +- Join Inner, (i#72 = i#75)                                         :  +- Project [i#72, i#72 AS t1i#71]!:     :- Project [i#72]                                                 :     +- Join Inner, (i#72 = i#75)!:     :  +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet   :        :- Filter isnotnull(i#72)!:     +- Project [i#75]                                                 :        :  +- Project [i#72]!:        +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet   :        :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet!+- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet            :        +- Filter isnotnull(i#75)!                                                                        :           +- Project [i#75]!                                                                        :              +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet!                                                                        +- Filter isnotnull(i#78)!                                                                           +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquetorg.apache.spark.SparkRuntimeException: Once strategy's idempotence is broken for batch Infer Filters Join Inner, (i#72 = i#78)                                                     Join Inner, (i#72 = i#78) :- Filter ((i#72 <=> i#72) AND (t1i#71 <=> t1i#71))                           :- Filter ((i#72 <=> i#72) AND (t1i#71 <=> t1i#71)) :  +- Project [i#72, i#72 AS t1i#71]                                          :  +- Project [i#72, i#72 AS t1i#71] :     +- Join Inner, (i#72 = i#75)                                            :     +- Join Inner, (i#72 = i#75) :        :- Filter isnotnull(i#72)                                            :        :- Filter isnotnull(i#72) :        :  +- Project [i#72]                                                 :        :  +- Project [i#72] :        :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet   :        :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet :        +- Filter isnotnull(i#75)                                            :        +- Filter isnotnull(i#75) :           +- Project [i#75]                                                 :           +- Project [i#75] :              +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet   :              +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet!+- Filter isnotnull(i#78)                                                     +- Filter (i#78 <=> i#78)!   +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet                  +- Filter isnotnull(i#78)!                                                                                    +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet.```After this PR:```=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints === Join Inner, (i#72 = i#78)                                               Join Inner, (i#72 = i#78) :- Project [i#72, i#72 AS t1i#71]                                       :- Project [i#72, i#72 AS t1i#71] :  +- Join Inner, (i#72 = i#75)                                         :  +- Join Inner, (i#72 = i#75)!:     :- Project [i#72]                                                 :     :- Filter isnotnull(i#72)!:     :  +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet   :     :  +- Project [i#72]!:     +- Project [i#75]                                                 :     :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet!:        +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet   :     +- Filter isnotnull(i#75)!+- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet            :        +- Project [i#75]!                                                                        :           +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet!                                                                        +- Filter isnotnull(i#78)!                                                                           +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet ```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
3	-2	### What changes were proposed in this pull request?This PR makes it only rewrite `EqualNullSafe` when the left side is non-nullable when unwrapping date type to timestamp type.### Why are the changes needed?1. The current rewrite is incorrect for `EqualNullSafe` if left side is nullable.3. Even if we fix the rewrite, it will contain `If` and can't be pushed down to the data source:   ```   EqualNullSafe(Cast(ts, DateType), date) if Cast(ts, DateType).nullable -> If(IsNull(ts), FalseLiteral, And(GreaterThanOrEqual(ts, Cast(date, TimestampType)), LessThan(ts, Cast(date + 1, TimestampType))))   ```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Update existing unit test.
2	-2	### What changes were proposed in this pull request?This PR adds recursive query feature to Spark SQL.A recursive query is defined using the `WITH RECURSIVE` keywords and referring the name of the common table expression within the query.The implementation complies with SQL standard and follows similar rules to other relational databases:- A query is made of an anchor followed by a recursive term.- The anchor terms doesn't contain self reference and it is used to initialize the query.- The recursive term contains a self reference and it is used to expand the current set of rows with new ones.- The anchor and recursive terms must be joined with each other by `UNION` or `UNION ALL` operators.- New rows can only be derived from the newly added rows of the previous iteration (or from the initial set of rows of anchor term). This limitation implies that recursive references can't be used with some of the joins, aggregations or subqueries.Please see `cte-recursive.sql` for some examples.The implemetation has the same limiation that [SPARK-36447](https://issues.apache.org/jira/browse/SPARK-36447) / https://github.com/apache/spark/pull/33671 has: > With-CTEs mixed with SQL commands or DMLs will still go through the old inline code path because of our non-standard language specs and not-unified command/DML interfaces.which means that recursive queries are not supported in SQL commands and DMLs.### Why are the changes needed?Recursive query is an ANSI SQL feature that is useful to process hierarchical data.### Does this PR introduce _any_ user-facing change?Yes, adds recursive query feature.### How was this patch tested?Added new UTs and tests in `cte-recursion.sql`. 
3	-1	### What changes were proposed in this pull request?Fixes `createDataFrame` to respect the SQL configs.### Why are the changes needed?Currently some configs for `createDataFrame` are not effective.- `spark.sql.timestampType`- `spark.sql.pyspark.inferNestedDictAsStruct.enabled`- `spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled`- `spark.sql.execution.pandas.convertToArrowArraySafely`### Does this PR introduce _any_ user-facing change?The configs will be respected.### How was this patch tested?Enabled/modified the related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use `getName` instead of `getCanonicalName` to get builder class name when registering udf to FunctionRegistrySince JDK15+, `getCanonicalName` will return null for anonymous classes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This causes the `className` field in `ExpressionInfo` to always be null. This changes the behavior when `DESCRIBE` udfs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
1	-1	### What changes were proposed in this pull request?For push based shuffle metrics, when writting out the event to log file, the field name is [\"Push Based Shuffle\"](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L548).But when parsing it out in SHS, the expected field name is [\"Shuffle Push Read Metrics\"](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L1264). This mismatch makes all the push shuffle metrics 0 from SHS rest calls.### Why are the changes needed?Without this change, all the push shuffle metrics will not be rendered correctly through SHS rest calls.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Launched Spark-shell application in Yarn client mode in our cluster, get the log file to local, and started a local Spark History Server to parse the log file. The metrics are shown correctly after this patch.
2	-1	### What changes were proposed in this pull request?To abbreviate the `BYTES` and `STRING` fields in proto message.**Note that the `repeated` and `map<...>` fields are always skipped for now**### Why are the changes needed?1, for abbreviation:```In [6]: spark.createDataFrame(range(0, 1000)).show()In [7]: query = \"SELECT /* \" + \"bla\" * 8192 + \" */ 1\"In [8]: spark.sql(query).show()```before:![image](https://user-images.githubusercontent.com/7322292/231615429-010d028f-f113-47fa-ac6a-696371cb81d0.png)after:![image](https://user-images.githubusercontent.com/7322292/231614931-dda0fc34-77d1-401a-b7c2-ffcfe9b076b8.png)2, `Message.toString` may cause OOM when the message is largeThis PR try to abbreviate the bytes and string, which are the main parts of `LocalRelation` and `PythonUDF`### Does this PR introduce _any_ user-facing change?yes, when `BYTES` and `STRING` fields are too long, abbreviate them and show the size### How was this patch tested?manually check
2	-2	### What changes were proposed in this pull request?This pr aims to upgrade Apache commons-compress from 1.22 to 1.23.0### Why are the changes needed?The new version bring some new features:- [COMPRESS-614](https://issues.apache.org/jira/browse/COMPRESS-614): Use FileTime for time fields in SevenZipArchiveEntry- [COMPRESS-621](https://issues.apache.org/jira/browse/COMPRESS-621): Fix calculation the offset of the first ZIP central directory entry- [COMPRESS-633](https://issues.apache.org/jira/browse/COMPRESS-633): Add encryption support for SevenZ- [COMPRESS-613](https://issues.apache.org/jira/browse/COMPRESS-613): Support for extra time data in Zip archives- [COMPRESS-621](https://issues.apache.org/jira/browse/COMPRESS-621): Add org.apache.commons.compress.archivers.zip.DefaultBackingStoreSupplier to write to a custom folder instead of the default temporary folder.- [COMPRESS-600](https://issues.apache.org/jira/browse/COMPRESS-600): Add capability to configure Deflater strategy in GzipCompressorOutputStream: GzipParameters.setDeflateStrategy(int).and some bugs fix:- [COMPRESS-638](https://issues.apache.org/jira/browse/COMPRESS-638): The GzipCompressorOutputStream#writeHeader() uses ISO_8859_1 to write the file name and comment.   If the strings contains non-ISO_8859_1 characters, unknown characters are displayed after decompression.   Use percent encoding for non ISO_8859_1 characters.- [COMPRESS-641](https://issues.apache.org/jira/browse/COMPRESS-641): Add TarArchiveEntry.getLinkFlag()- [COMPRESS-642](https://issues.apache.org/jira/browse/COMPRESS-642): Integer overflow ArithmeticException in TarArchiveOutputStream- [COMPRESS-642](https://issues.apache.org/jira/browse/COMPRESS-642): org.apache.commons.compress.archivers.zip.ZipFile.finalize() should not write to std err.all changes as follows:- https://commons.apache.org/proper/commons-compress/changes-report.html#a1.23.0### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we move integral to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?This pr aims to set `shadeTestJar` of `protobuf` module to `false` to skip shade `spark-protobuf_**-tests.jar` process. ### Why are the changes needed?When `shadeTestJar` is true,  `maven-shade-plugin` always try to find(sometime is downloading) some non-existent jars:```Downloading from central: https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.5/hadoop-client-api-3.3.5-tests.jar[WARNING] Could not get tests for org.apache.hadoop:hadoop-client-api:jar:3.3.5:compileDownloading from central: https://repo.maven.apache.org/maven2/org/tukaani/xz/1.9/xz-1.9-tests.jar[WARNING] Could not get tests for org.tukaani:xz:jar:1.9:compileDownloading from gcs-maven-central-mirror: https://maven-central.storage-download.googleapis.com/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7-tests.jarDownloaded from gcs-maven-central-mirror: https://maven-central.storage-download.googleapis.com/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7-tests.jar (0 B at 0 B/s)[WARNING] Could not get tests for com.google.protobuf:protobuf-java:jar:3.22.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0-tests.jar[WARNING] Could not get tests for org.apache.curator:curator-framework:jar:2.13.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-slf4j2-impl/2.20.0/log4j-slf4j2-impl-2.20.0-tests.jar[WARNING] Could not get tests for org.apache.logging.log4j:log4j-slf4j2-impl:jar:2.20.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/jetbrains/annotations/17.0.0/annotations-17.0.0-tests.jar[WARNING] Could not get tests for org.jetbrains:annotations:jar:17.0.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-1.2-api/2.20.0/log4j-1.2-api-2.20.0-tests.jar[WARNING] Could not get tests for org.apache.logging.log4j:log4j-1.2-api:jar:2.20.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/threeten/threeten-extra/1.7.1/threeten-extra-1.7.1-tests.jar[WARNING] Could not get tests for org.threeten:threeten-extra:jar:1.7.1:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/yetus/audience-annotations/0.13.0/audience-annotations-0.13.0-tests.jar[WARNING] Could not get tests for org.apache.yetus:audience-annotations:jar:0.13.0:compileDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.14.2/jackson-databind-2.14.2-tests.jar[WARNING] Could not get tests for com.fasterxml.jackson.core:jackson-databind:jar:2.14.2:compileDownloading from central: https://repo.maven.apache.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0-tests.jar[WARNING] Could not get tests for com.google.code.findbugs:jsr305:jar:3.0.0:runtimeDownloading from central: https://repo.maven.apache.org/maven2/org/xerial/snappy/snappy-java/1.1.9.1/snappy-java-1.1.9.1-tests.jar[WARNING] Could not get tests for org.xerial.snappy:snappy-java:jar:1.1.9.1:compileDownloading from central: https://repo.maven.apache.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1-tests.jar[WARNING] Could not get tests for javax.activation:activation:jar:1.1.1:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/hive/hive-storage-api/2.8.1/hive-storage-api-2.8.1-tests.jar[WARNING] Could not get tests for org.apache.hive:hive-storage-api:jar:2.8.1:compileDownloading from central: https://repo.maven.apache.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0-tests.jar[WARNING] Could not get tests for org.spark-project.spark:unused:jar:1.0.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/scala-lang/scala-library/2.12.17/scala-library-2.12.17-tests.jar[WARNING] Could not get tests for org.scala-lang:scala-library:jar:2.12.17:compileDownloading from central: https://repo.maven.apache.org/maven2/com/github/luben/zstd-jni/1.5.4-2/zstd-jni-1.5.4-2-tests.jar[WARNING] Could not get tests for com.github.luben:zstd-jni:jar:1.5.4-2:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-core/2.20.0/log4j-core-2.20.0-tests.jar[WARNING] Could not get tests for org.apache.logging.log4j:log4j-core:jar:2.20.0:runtimeDownloading from central: https://repo.maven.apache.org/maven2/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0-tests.jar[WARNING] Could not get tests for org.apache.curator:curator-recipes:jar:2.13.0:compileDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.14.2/jackson-annotations-2.14.2-tests.jar[WARNING] Could not get tests for com.fasterxml.jackson.core:jackson-annotations:jar:2.14.2:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.5/hadoop-client-runtime-3.3.5-tests.jar[WARNING] Could not get tests for org.apache.hadoop:hadoop-client-runtime:jar:3.3.5:compileDownloading from central: https://repo.maven.apache.org/maven2/com/fasterxml/jackson/core/jackson-core/2.14.2/jackson-core-2.14.2-tests.jar[WARNING] Could not get tests for com.fasterxml.jackson.core:jackson-core:jar:2.14.2:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-api/2.20.0/log4j-api-2.20.0-tests.jar[WARNING] Could not get tests for org.apache.logging.log4j:log4j-api:jar:2.20.0:compileDownloading from central: https://repo.maven.apache.org/maven2/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0-tests.jar[WARNING] Could not get tests for org.apache.curator:curator-client:jar:2.13.0:compile```But the shaded `spark-protobuf_**-tests.jar` was not used now and sbt assembly didn't do this.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Check ` build/mvn clean install -pl connector/protobuf` and all test passed
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add `numOutputRows` metric in `WindowGroupLimitExec`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`WindowGroupLimitExec`  is a kind of filter that would drop unused rows in each window group, so `numOutputRows` could help user find if it has benefits.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->When doing an outer join with joinWith on DataFrames, unmatched rows return Row objects with null fields instead of a single null value. This is not a expected behavior, and it's a regression introduced in [this commit](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59).This pull request aims to fix the regression, note this is not a full rollback of the commit, do not add back \"schema\" variable.```case class ClassData(a: String, b: Int)val left = Seq(ClassData(\"a\ 1), ClassData(\"b\ 2)).toDFval right = Seq(ClassData(\"x\ 2), ClassData(\"y\ 3)).toDFleft.joinWith(right, left(\"b\") === right(\"b\"), \"left_outer\").collect``````Wrong results (current behavior):    Array(([a,1],[null,null]), ([b,2],[x,2]))Correct results:                     Array(([a,1],null), ([b,2],[x,2]))``` ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We need to address the regression mentioned above. It results in unexpected behavior changes in the Dataframe joinWith API between versions 2.4.8 and 3.0.0+. This could potentially cause data correctness issues for users who expect the old behavior when using Spark 3.0.0+.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added unit test (use the same test in previous [closed pull request](https://github.com/apache/spark/pull/35140), credit to Clément de Groc)Run sql-core and sql-catalyst submodules locally with ./build/mvn clean package -pl sql/core,sql/catalyst
2	-1	### What changes were proposed in this pull request?This PR adds support coalesce buckets in join applied on broadcast join stream side.### Why are the changes needed?Reduce shuffle to improve query performance.Before | After-- | --<img src=\"https://user-images.githubusercontent.com/5399861/231473104-4d9bbe4e-9cfe-4473-ba17-75d09f2eb1b9.png\" width=\"400\" height=\"630\"> | <img src=\"https://user-images.githubusercontent.com/5399861/231470141-bd15031f-facb-4c59-84f2-e60f054257d6.png\" width=\"400\" height=\"630\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
1	-1	### What changes were proposed in this pull request?Make the script introduced in https://github.com/apache/spark/pull/40676 runnable.### Why are the changes needed?Somehow the chmod didn't commit with the previous PR, which only became apparent when I pulled back from master...### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual dev again...
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes that we move asIntegral to PhysicalDataType. This is to simplify the DataType class to make it become a simple interface without coupling too many internal representations.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make DataType become a simpler interface, non-public code can be moved outside of the DataType class.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-1	### What changes were proposed in this pull request?This PR aims to simplify the code by merging nested `if` statements into single `if` statements using the `and` operator. There are 7 of these according to [Sonarcloud](https://sonarcloud.io/project/issues?languages=py&resolved=false&rules=python%3AS1066&id=spark-python&open=AYQdnXXBRrJbVxW9ZDpw). And this PR fix them all. ### Why are the changes needed?The changes do not affect the functionality of the code, but they improve readability and maintainability.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	### What changes were proposed in this pull request?Fixes `createDataFrame` to respect the given schema ddl.### Why are the changes needed?Currently even if the schema is provided as a DDL string, it's not taken into account and causes the schema mismatch in the server side.For example:```py>>> import pandas as pd>>> map_data = [{\"a\": 1}, {\"b\": 2, \"c\": 3}, {}, None, {\"d\": None}]>>> pdf = pd.DataFrame({\"id\": [0, 1, 2, 3, 4], \"m\": map_data})>>> schema = \"id long, m map<string, long>\">>>>>> spark.createDataFrame(pdf, schema=schema)Traceback (most recent call last):...pyspark.errors.exceptions.connect.AnalysisException: [INVALID_COLUMN_OR_FIELD_DATA_TYPE] Column or field `col_1` is of type \"STRUCT<col_0: BIGINT, col_1: BIGINT, col_2: BIGINT, col_3: VOID>\" while it's required to be \"MAP<STRING, BIGINT>\".```### Does this PR introduce _any_ user-facing change?The schema DDL string will be taken into account.### How was this patch tested?Enabled/modified the related tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The method `resolveExprsAndAddMissingAttrs` contains redundant code: getting the `newExprs` and `newChild` shows up 4 times in different branches.This PR is to simplify the implementation of the method.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Code clean up and remove redundant code.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
1	-3	### What changes were proposed in this pull request?Moved UDFUtils to common.Fixed the client `UserDefinedFunctionE2ETestSuite` and `KeyValueGroupedDatasetE2ETestSuite` tests to be able to run on maven.Verified the code works with the following maven commands.```build/mvn clean install -pl connector/connect/server -am -DskipTestsbuild/mvn clean install -pl assembly -am -DskipTestsbuild/mvn clean install -pl connector/connect/client/jvm```### Why are the changes needed?Fix maven failing tests### Does this PR introduce _any_ user-facing change?No### How was this patch tested?TestsFixed tests for https://github.com/apache/spark/pull/40581 and https://github.com/apache/spark/pull/40729
1	-1	### What changes were proposed in this pull request?This PR adds `DayTimeIntervalType` and `YearMonthIntervalType` to `TypeCoercionSuite`.### Why are the changes needed?Increase test coverage.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?N/A.
1	-1	### What changes were proposed in this pull request?Splits `pyspark-pandas-connect` from `pyspark-connect` module.### Why are the changes needed?Now that we have pandas API on Spark Connect, the tests for `pyspark-connect` module take long time:- before pandas API: about 40 mins- after pandas API: about 2-3 hoursso we should split `pyspark-pandas-connect` from `pyspark-connect` module.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
1	-2	### What changes were proposed in this pull request?Right now only bug test. still figuring out a clean way to fix.### Why are the changes needed?the checked in bug test describes the issue. It appears to me that spark-hive column mapping especially for insert is severely broken. I am not sure why the issue has remained hidden so far.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?not tested
3	-3	### What changes were proposed in this pull request?In `JoinCodegenSupport#getJoinCondition`, evaluate any referenced stream-side variables before using them in the generated code.This patch doesn't evaluate the passed stream-side variables directly, but instead evaluates a copy (`streamVars2`). This is because `SortMergeJoin#codegenFullOuter` will want to evaluate the stream-side vars within a different scope than the condition check, so we mustn't delete the initialization code from the original `ExprCode` instances.### Why are the changes needed?When a bound condition of a full outer join references the same stream-side column more than once, wholestage codegen generates bad code.For example, the following query fails with a compilation error:```create or replace temp view v1 asselect * from values(1, 1),(2, 2),(3, 1)as v1(key, value);create or replace temp view v2 asselect * from values(1, 22, 22),(3, -1, -1),(7, null, null)as v2(a, b, c);select *from v1full outer join v2on key = aand value > band value > c;```The error is:```org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 277, Column 9: Redefinition of local variable \"smj_isNull_7\"```The same error occurs with code generated from ShuffleHashJoinExec:```select /*+ SHUFFLE_HASH(v2) */ *from v1full outer join v2on key = aand value > band value > c;```In this case, the error is:```org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 5: Redefinition of local variable \"shj_value_1\" ```Neither `SortMergeJoin#codegenFullOuter` nor `ShuffledHashJoinExec#doProduce` evaluate the stream-side variables before calling `consumeFullOuterJoinRow#getJoinCondition`. As a result, `getJoinCondition` generates definition/initialization code for each referenced stream-side variable at the point of use. If a stream-side variable is used more than once in the bound condition, the definition/initialization code is generated more than once, resulting in the \"Redefinition of local variable\" error.In the end, the query succeeds, since Spark disables wholestage codegen and tries again.(In the case other join-type/strategy pairs, either the implementations don't call `JoinCodegenSupport#getJoinCondition`, or the stream-side variables are pre-evaluated before the call is made, so no error happens in those cases).### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The assert ` assert(Thread.currentThread().isInstanceOf[UninterruptibleThread]) ` found https://github.com/apache/spark/blob/master/connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala#L239  is not needed.  The reason is the following1. This assert was put there due to some issues when the old and deprecated KafkaOffsetReaderConsumer is used.  The default offset reader implementation has been changed to KafkaOffsetReaderAdmin which no longer require it run via UninterruptedThread.2. Even if the deprecated KafkaOffsetReaderConsumer is used, there are already asserts in that impl to check if it is running via UninterruptedThread e.g. https://github.com/apache/spark/blob/master/connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderConsumer.scala#L130 thus the assert in KafkaMicroBatchStream is redundant.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->Remove unnecessary assert.  Clean up code.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->n/a
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR introduces a TVF and makes it support JDBC standard API to support getting SQL Keywords dynamically.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->1. JDBC API Compliance 2. SQL Keywords are helpful for AI-powered BI tools during prompting to generate queries3. Used by Spark SQL Highlighting### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, a new tvf sql_keywords function added### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
1	-2	### What changes were proposed in this pull request?PyTorch distributor run on barrier mode, when the resources is not enough for some reason, `test_parity_torch_distributor` can wait for hours.### Why are the changes needed?for tests### Does this PR introduce _any_ user-facing change?No### How was this patch tested?CI
3	-3	### What changes were proposed in this pull request?Add support for tracking pinned blocks memory usage for RocksDB state store### Why are the changes needed?Today we only track total memory usage for RocksDB that comprises of write buffer, block cache and index/filter blocks. Its also useful to understand usage of blocks pinned in the cache, potentially due to iterators not releasing them. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Modified unit tests```[info] Run completed in 11 seconds, 964 milliseconds.[info] Total number of tests run: 5[info] Suites: completed 1, aborted 0[info] Tests: succeeded 5, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```Also, verified that the metrics appear in the custom metrics for state store:```    \"customMetrics\" : {      \"rocksdbBytesCopied\" : 1189,      \"rocksdbCommitCheckpointLatency\" : 14,      \"rocksdbCommitCompactLatency\" : 0,      \"rocksdbCommitFileSyncLatencyMs\" : 150,      \"rocksdbCommitFlushLatency\" : 15,      \"rocksdbCommitPauseLatency\" : 0,      \"rocksdbCommitWriteBatchLatency\" : 4,      \"rocksdbFilesCopied\" : 1,      \"rocksdbFilesReused\" : 0,      \"rocksdbGetCount\" : 6,      \"rocksdbGetLatency\" : 0,      \"rocksdbPinnedBlocksMemoryUsage\" : 87,      \"rocksdbPutCount\" : 1,      \"rocksdbPutLatency\" : 4,      \"rocksdbReadBlockCacheHitCount\" : 0,      \"rocksdbReadBlockCacheMissCount\" : 0,      \"rocksdbSstFileSize\" : 1189,      \"rocksdbTotalBytesRead\" : 0,      \"rocksdbTotalBytesReadByCompaction\" : 0,      \"rocksdbTotalBytesReadThroughIterator\" : 0,      \"rocksdbTotalBytesWritten\" : 123,      \"rocksdbTotalBytesWrittenByCompaction\" : 0,      \"rocksdbTotalBytesWrittenByFlush\" : 1325,      \"rocksdbTotalCompactionLatencyMs\" : 0,      \"rocksdbWriterStallLatencyMs\" : 0,      \"rocksdbZipFileBytesUncompressed\" : 7283    }```
3	-2	Currently spark driver and exec pod request and limit memory can only be same, not able to set separately when cluster get diff memory limit and request quota. then not able to fully use cluster memory resource.E.g. cluster get total 50G memory request, 200G memory limit, but every spark pod limit memory are same as request memory, then in this cluster total memory spark can use is depends on the smaller one: 50G<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fix DistributedDataParallel model code in TorchDistributor suite### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Testing code fix.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->N/A### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
1	-1	### What changes were proposed in this pull request?https://github.com/apache/spark/blob/891a142b9141489f3872cc4c8b9ffd50a889f524/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala#L625-L632The above code comments are no longer applicable to the current Spark version , so this pr change to call `BytesWritable.copyBytes` directly.https://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BytesWritable.java#L69-L79https://github.com/apache/hadoop/blob/rel/release-2.7.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/BytesWritable.java#L69-L79### Why are the changes needed?Using existing `BytesWritable` api### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fail Spark Application when the number of executor failures reaches the threshold.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Sometimes, the executors can not launch successfully because of the wrong configuration, but in K8s, Driver does not know that, and just keep requesting new executors.This PR ports the window-based executor failure tracking mechanism to K8s(only takes effect when `spark.kubernetes.allocation.pods.allocator` is set to 'direct'), to reduce functionality gap between YARN and K8s.Note that, YARN mode also supports host-based executor allocation failure tracking and application terminating mechanism[2], this PR does not port such functionalities to Kubernetes since it's kind of an independent and big feature, and relies on some YARN features which I'm not sure if K8s has similar one.[1] [SPARK-6735](https://issues.apache.org/jira/browse/SPARK-6735)[2] [SPARK-17675](https://issues.apache.org/jira/browse/SPARK-17675)### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this PR provides two new configurations - `spark.executor.maxNumFailures`- `spark.executor.failuresValidityInterval`which takes effect on YARN, or on Kubernetes when `spark.kubernetes.allocation.pods.allocator` is set to 'direct'.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT added, and manually tested in internal K8s cluster.
2	-2	### What changes were proposed in this pull request?`TorchDistributorLocalUnitTestsIIOnConnect` was recently added in https://github.com/apache/spark/commit/f8751e2afeb5042a17d86763f965bc8f479784bf , but after that  `test_parity_torch_distributor` became unstable and sometime got stuck### Why are the changes needed?to make CI happy, I file [SPARK-43122](https://issues.apache.org/jira/browse/SPARK-43122) to track it.### Does this PR introduce _any_ user-facing change?No, test only### How was this patch tested?CI
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In Spark, we have defined some internal field metadata to help query resolution and compilation. For example, there are quite some field metadata that are related to metadata columns.However, when we create tables, these internal field metadata can be leaked. This PR updates CTAS/RTAS commands to remove these internal field metadata before creating tables. CREATE/REPLACE TABLE command is fine as users can't generate these internal field metadata via the type string.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to avoid potential issues, like mistakenly treating a data column as metadata column### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new test
1	-2	Dump of query cancellation hacking.
3	-2	init<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-2	### What changes were proposed in this pull request?`DataSet.show()` currently triggers a job for a simple `show tables` command. This is because the command output contains an `isTemporary` boolean column that needs to be casted to string when we use `show()` on the dataset.This PR converts `CommandResult` to `LocalRelation` and let `ConvertToLocalRelation` to do the casting locally to avoid triggering job execution.### Why are the changes needed?A simple `show tables` shouldn not require an executor.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added new UT.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Fix the bug when Connect Server throw Exception without message.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix bug<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Due to the recent refactor, I realized that the two Hive UDF expressions are stateful as they both keep an array to store the input arguments. This PR fix it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->to avoid issues in a muti-thread environment.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Too hard to write unit tests and the fix itself is very obvious.
1	-2	### What changes were proposed in this pull request?Operations on `LocalRelation` can mostly be done locally (without sending RPCs).We should leverage this.### Why are the changes needed?Avoid sending RPCs for `LocalRelation`.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?Exists test cases.
2	-1	### What changes were proposed in this pull request?Implements core streaming API in Scala for running streaming queries over Spark Connect. This is functionally equivalent to Python side PR #40586 There are no server side changes here since it was done earlier in Python PR.We can run most streaming queries.Notably, queries using `foreachBatch()` are not yet supported.### Why are the changes needed?This adds structured streaming support in Scala for Spark connect. ### Does this PR introduce _any_ user-facing change?Adds more streaming API to Scala Spark Connect client. ### How was this patch tested?  - Unit test  - Manual testing
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR drops `InternalType` from the logical data type and refactors to use the PhysicalDataType and InternalType.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To simplify DataType interface.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the `await_termination()` and `exception()` to streaming query class. For `exception`, only pass the `message` the same way in `SparkConnectService`, and construct the error the same way as the `convert_exception` method in `_handle_rpc_error` in `client.py`. For `await_termination`, send the command multiple times instead of waiting to prevent RPC timeout. `<-- I'm definitely open to any discussion on its implementation!`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Add missing APIs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes but part of ongoing developing of Streaming Spark Connect.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests. Note that the unit tests for them are still skipped because of 1. queryManager is not implemented. 2. Allow access to stopped query is not implemented.I was able to test them manually by 1. For `test_stream_await_termination()`, comment out the ```for q in self.spark.streams.active:    q.stop()```2. For `test_stream_exception()`, comment out unregistering terminated query: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L411
2	-1	### What changes were proposed in this pull request?This PR aims to remove `branch-3.2` from `publish_snapshot` GitHub Action job.### Why are the changes needed?We already released Apache Spark 3.2.4 as an End-Of-Life version. To save the community resource, we need to stop publishing snapshot from `branch-3.2` from Today.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual review.
1	-2	### What changes were proposed in this pull request?Skip `TorchDistributorLocalUnitTestsOnConnect`### Why are the changes needed?it seems that `TorchDistributorLocalUnitTestsOnConnect` is also unstable in Github Action, but I cannot repro the issue locally.Not sure whether it is related to resources limit, this needs more investigation, track it in https://issues.apache.org/jira/browse/SPARK-43122### Does this PR introduce _any_ user-facing change?no, test-only### How was this patch tested?CI
1	-1	### What changes were proposed in this pull request?This pr aims to remove `hadoop-2` profile from Apache Spark 3.5.0.### Why are the changes needed?Spark 3.4.0 no longer releases Hadoop2 binary distribtuion(SPARK-42447) and Hadoop 2 GitHub Action job already removed after SPARK-42447, we can remove `hadoop-2` profile from Apache Spark 3.5.0.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-3	### What changes were proposed in this pull request?Currently, Spark supports the `array_insert` and `array_prepend`. Users insert an element into the head of array is common operation. Considered, we want make `array_prepend` reuse the implementation of `array_insert`, but it seems a bit performance worse if the position is foldable and equals to zero.The reason is that always do the check for position is negative or positive, and the code is too long. Too long code will lead to JIT failed.  ### Why are the changes needed?Improve `ArrayInsert` if the position is foldable and equals to zero.### Does this PR introduce _any_ user-facing change?'No'.Just change the inner implementation.### How was this patch tested?Exists test cases.
2	-1	### What changes were proposed in this pull request?1. It should include TimestampNTZType,AnsiIntervalType.2. ArrayType/MapType/StructType to other data type seems should be true.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Some updated tests
1	-2	### What changes were proposed in this pull request?This PR makes `DummyLeafNode` override computeStats.### Why are the changes needed?To avoid `UnsupportedOperationException` if we add a rule(for example: the following `TestRule`) use statistics even the rule after the [Early Filter and Projection Push-Down](https://github.com/apache/spark/blob/0e9e34c1bd9bd16ad5efca77ce2763eb950f3103/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L194-L197).```scalaobject TestRule extends Rule[LogicalPlan] {  def apply(plan: LogicalPlan): LogicalPlan = plan.transform {    case f: Filter if f.stats.rowCount.nonEmpty =>      f  }}```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
1	-1	### What changes were proposed in this pull request?This PR excludes Java files in `core/target` when running checkstyle with SBT.### Why are the changes needed?Files such as .../spark/core/target/scala-2.12/src_managed/main/org/apache/spark/status/protobuf/StoreTypes.java are checked in checkstyle. We shouldn't check them in the linter.### Does this PR introduce _any_ user-facing change?No, dev-only.### How was this patch tested?Manually ran:```bash./dev/sbt-checkstyle```
1	-3	### What changes were proposed in this pull request?`TorchDistributorLocalUnitTestsOnConnect` and `TorchDistributorLocalUnitTestsIIOnConnect` were not stable and occasionally got stuck. However, I can not reproduce the issue locally.The two UTs were disabled, and this PR is to reenable them. I found that the all the tests for PyTorch set up the regular sessions or connect sessions in `setUp` and close them in `tearDown`, however such session operations are very expensive and should be placed into `setUpClass` and `tearDownClass` instead. After this change, the related tests seems much stable. So I think the root cause is still related to the resources, since TorchDistributor works on barrier mode, when there is not enough resources in Github Action, the tests just keep waiting.### Why are the changes needed?for test coverage### Does this PR introduce _any_ user-facing change?No, test-only### How was this patch tested?CI
3	-3	### What changes were proposed in this pull request?This PR fixes DSL expressions on attributes with special characters by making `DslAttr.attr` and `DslAttr.expr` return the implicitly wrapped attribute instead of creating a new one.### Why are the changes needed?SPARK-43142: DSL expressions on attributes with special characters don't work even if the attribute names are quoted:```scalascala> import org.apache.spark.sql.catalyst.dsl.expressions._import org.apache.spark.sql.catalyst.dsl.expressions._scala> \"`slashed/col`\".attrres0: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'slashed/colscala> \"`slashed/col`\".attr.ascorg.apache.spark.sql.catalyst.parser.ParseException:mismatched input '/' expecting {<EOF>, '.', '-'}(line 1, pos 7)== SQL ==slashed/col-------^^^```DSL expressions rely on a call to `expr` to get child of the new expression [(e.g.)](https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L149).`expr` here is a call on implicit class `DslAttr` that's wrapping the `UnresolvedAttribute` returned by `\"...\".attr` is wrapped by the implicit class `DslAttr`.`DslAttr` and its super class implement `DslAttr.expr` such that a new `UnresolvedAttribute` is created from `UnresolvedAttribute.name` of the wrapped attribute [(here)](https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L273-L280).But that's no good, because `UnresolvedAttribute.name` drops the quotes and thus the newly created `UnresolvedAttribute` parses an identifier that should be quoted but isn't:```scalascala> \"`col/slash`\".attr.nameres5: String = col/slash```### Does this PR introduce _any_ user-facing change?DSL expressions on attributes with special characters no longer fail.### How was this patch tested?I couldn't find a suite testing the implicit classes in the DSL package, but the DSL package seems used widely enough that I'm confident this doesn't break existing behavior.Locally, I was able to reproduce with this test; it was failing before and passes now:```scalatest(\"chained DSL expressions on attributes with special characters\") {  $\"`slashed/col`\".asc}```
1	-1	Temp PR for illustrating comment reformatting from     ./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false \\         -Dscalafmt.changedOnly=false \\         -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm
2	-1	### What changes were proposed in this pull request?Added the agg, reduce support in `KeyValueGroupedDataset`.Added `Dataset#reduce`Added `RelationalGroupedDataset#as`.Summary:* `KVGDS#agg`: `KVGDS#agg` and the `RelationalGroupedDS#agg` shares the exact same proto. The only difference is that the KVGDS always passing a UDF as the first grouping expression. That's also how we tell them apart in this PR.* `KVGDS#reduce`: Reduce is a special aggregation. The client uses an UnresolvedFunc \"reduce\" to mark the agg operator is a `ReduceAggregator` and calls `KVGDS#agg` directly. The server would be able to pick this func up directly and reuse the agg code path by sending in a `ReduceAggregator`.* `Dataset#reduce`: This is free after `KVGDS#reduce`.* `RelationalGroupedDS#as`: The only difference between `KVGDS` created using `ds#groupByKey` and `ds#agg#as` is the grouping expressions. The former requires one grouping func as the grouping expression, the latter uses a dummy func (to pass encoders/types to the server) + grouping expressions. Thus the server can count how many grouping expressions received and decide if the `KVGDS` should be created as `ds#groupByKey` or `ds#agg#as`.Followups: * [SPARK-43415] Support mapValues in the Agg functions. * [SPARK-43416] The tupled ProductEncoder dose not pick up the fields names from the server.### Why are the changes needed?Missing APIs in Scala Client### Does this PR introduce _any_ user-facing change?Added `KeyValueGrouppedDataset#agg, reduce`, `Dataset#reduce`, `RelationalGroupedDataset#as` methods for the Scala client.### How was this patch tested?E2E tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the table() method support in DataStreamReader in Spark Connect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of building SS Connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes now the table() API is available.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit test
3	-3	### What changes were proposed in this pull request? / Why are the changes needed?- Currently, the official Spark docker images run as the UID `185`, but no corresponding entry exists in `/etc/passwd`. This causes [issues](https://stackoverflow.com/questions/41864985/hadoop-ioexception-failure-to-login) when libraries try to fetch the current unix username. - This PR invokes the `useradd` function as part of the `Dockerfile`, to ensure that a name exists for these users.  ### Does this PR introduce _any_ user-facing change?- No. No user-facing changes are expected.### How was this patch tested?- Docker build succeeds locally.
1	-1	### What changes were proposed in this pull request?where hive table.getStorageHandler call is used, check hive table parameter \"storage_handler\" first.  purpose is that hive table.getStorageHandler initializes the storagehandler class, if not necessary can just check on hive table parameter first. the table parameter is required for storagehandler table in hive.### Why are the changes needed?for desc table, or use case where user just want to load HiveTableRelation, user do not need to provide the storagehandler jar.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?past unit tests and also local test.
1	-1	### What changes were proposed in this pull request?Implements eager evaluation for `DataFrame.__repr__` and `DataFrame._repr_html_`.### Why are the changes needed?When `spark.sql.repl.eagerEval.enabled` is `True`, DataFrames should eagerly evaluate and show the results.```py>>> spark.conf.set('spark.sql.repl.eagerEval.enabled', True)>>> spark.range(3)+---+| id|+---+|  0||  1||  2|+---+```### Does this PR introduce _any_ user-facing change?The eager evaluation will be available.### How was this patch tested?Enabled the related test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change flake8 config file### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Before changing this, when doing local python lint `./dev/lint-python`, it checks folder that shouldn't be checked. This doesn't happen in github action.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually testedBefore: ```starting flake8 test.........many more......./python/docs/source/conf.py:112:1: E265 block comment should start with '# '#source_encoding = 'utf-8-sig'^./python/docs/source/conf.py:132:1: E265 block comment should start with '# '#language = None^./python/venv/lib/python3.8/site-packages/pkg_resources/_vendor/more_itertools/more.py:1814:13: E731 do not assign a lambda expression, use a def            key_argument = lambda zipped_items: key(            ^./dev/ansible-for-test-node/roles/jenkins-worker/files/util_scripts/post_github_pr_comment.py:7:1: F401 'urllib.parse' imported but unusedimport urllib.parse^./dev/ansible-for-test-node/roles/jenkins-worker/files/util_scripts/session_lock_resource.py:130:9: F841 local variable 'f' is assigned to but never used        f = _acquire_lock(lock_filename, timeout_secs, lock_message)......many more......```And many moreAfter:```starting flake8 test...flake8 checks passed.```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove workaround(SPARK-41952) for [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[SPARK-42926](https://issues.apache.org/jira/browse/SPARK-42926) upgraded Parquet to 1.13.0, which includes [PARQUET-2160](https://issues.apache.org/jira/browse/PARQUET-2160). So we no longer need [SPARK-41952](https://issues.apache.org/jira/browse/SPARK-41952).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
1	-1	### What changes were proposed in this pull request?The pr aims to fix typos, include:- StrageLevel -> StorageLevel- DateFrame -> DataFrame### Why are the changes needed?Fix typos### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update the prerequisites for generating Python API docs:* The command should be run under the docs directory so that the input file should be `../dev/requirements.txt`* Remove `sudo` in the command* Remove the version for `torch` and `torchvision`. I was getting ```ERROR: Could not find a version that satisfies the requirement torch==1.13.1 (from versions: 2.0.0)ERROR: No matching distribution found for torch==1.13.1ERROR: Could not find a version that satisfies the requirement torchvision==0.14.1 ```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Update the prerequisites for generating Python API docs, to save troubles from other developers when generating Python Docs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually try the command and new requirements.txt
2	-2	### What changes were proposed in this pull request?It will invalidate the bucketed read if add a cast on bucket keys:```sqlset spark.sql.autoBroadcastJoinThreshold=-1;CREATE TABLE t2 USING parquet CLUSTERED BY (i) INTO 8 buckets ASSELECT CAST(v AS bigint) AS i FROM values(1), (9223372036854775807) AS data(v);CREATE TABLE t3 USING parquet CLUSTERED BY (i) INTO 4 buckets ASSELECT CAST(v AS decimal(18, 0)) AS i FROM values(1), (999999999999999999) AS data(v);EXPLAIN SELECT * FROM t2 JOIN t3 ON t2.i = t3.i;``````== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- SortMergeJoin [cast(i#6L as decimal(20,0))], [cast(i#19 as decimal(20,0))], Inner   :- Sort [cast(i#6L as decimal(20,0)) ASC NULLS FIRST], false, 0   :  +- Exchange hashpartitioning(cast(i#6L as decimal(20,0)), 5), ENSURE_REQUIREMENTS, [plan_id=128]   :     +- Filter isnotnull(i#6L)   :        +- FileScan parquet spark_catalog.default.t2[i#6L] Batched: true, Bucketed: false (disabled by query planner)   +- Sort [cast(i#19 as decimal(20,0)) ASC NULLS FIRST], false, 0      +- Exchange hashpartitioning(cast(i#19 as decimal(20,0)), 5), ENSURE_REQUIREMENTS, [plan_id=132]         +- Filter isnotnull(i#19)            +- FileScan parquet spark_catalog.default.t3[i#19] Batched: true, Bucketed: false (disabled by query planner)```This PR adds a new rule(`UnwrapCastInJoinCondition`) before `EnsureRequirements` to unwrap cast in join condition to unlock bucketed read if they are integral types. The key idea here is that casting to either of these two types will not affect the result of join for integral types join keys. For example: `a.intCol = try_cast(b.bigIntCol AS int)`, if the value of `bigIntCol` exceeds the range of int, the result of `try_cast(b.bigIntCol AS int)` is `null`, and the result of  `a.intCol = try_cast(b.bigIntCol AS int)` in the join condition is `false`. The result is consistent with `cast(a.intCol AS bigint) = b.bigIntCol`.After This PR:```== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- SortMergeJoin [i#6L], [try_cast(i#29 as bigint)], Inner   :- Sort [i#6L ASC NULLS FIRST], false, 0   :  +- Filter isnotnull(i#6L)   :     +- FileScan parquet spark_catalog.default.t2[i#6L] Batched: true, Bucketed: true, SelectedBucketsCount: 8 out of 8   +- Sort [try_cast(i#29 as bigint) ASC NULLS FIRST], false, 0      +- Exchange hashpartitioning(try_cast(i#29 as bigint), 8), ENSURE_REQUIREMENTS, [plan_id=132]         +- Filter isnotnull(i#29)            +- FileScan parquet spark_catalog.default.t3[i#29] Batched: true, Bucketed: false (disabled by query planner)```### Why are the changes needed?Reduce shuffle to improve query performance.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
1	-1	### What changes were proposed in this pull request?Skips Spark execution when the dataframe is local.### Why are the changes needed?When the built DataFrame in Spark Connect is local, we can skip Spark execution.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
1	-2	### What changes were proposed in this pull request?This PR fixes incorrect column names in [sql-ref-syntax-dml-insert-table.md](https://spark.apache.org/docs/3.4.0/sql-ref-syntax-dml-insert-table.html).### Why are the changes needed?Bug fix.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?N/A.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR fixes an unhandled ClassNotFoundException during RDD block decommissions migrations.```2023-04-08 04:15:11,791 ERROR server.TransportRequestHandler: Error while invoking RpcHandler#receive() on RPC id 6425687122551756860java.lang.ClassNotFoundException: com.class.from.user.jar.ClassName    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)    at java.base/java.lang.Class.forName0(Native Method)    at java.base/java.lang.Class.forName(Class.java:398)    at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:71)    at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2003)    at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)    at java.base/java.io.ObjectInputStream.readClass(ObjectInputStream.java:1833)    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1658)    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)    at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)    at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)    at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)    at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)    at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)    at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)    at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123)    at org.apache.spark.network.netty.NettyBlockRpcServer.deserializeMetadata(NettyBlockRpcServer.scala:180)    at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:119)    at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)    at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)    at java.base/java.lang.Thread.run(Thread.java:829)```The exception occurs if RDD block contains user defined during the serialization of a `ClassTag` for the user defined class. The problem for serialization of the `ClassTag` a instance of `JavaSerializer`(https://github.com/apache/spark/blob/ca2ddf3c2079dda93053e64070ebda1610aa1968/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala#L62) is used, but it never configured to use a class loader including user defined classes. This PR solves the issue by instead use a serializer from the SerializerManager which is configured to use the correct class loader.The reason is this does not occur during normal block replication and only during decommission is that there is a workaround/hack in `BlockManager.doPutIterator` that replaces the `ClassTag` with a `ClassTag[Any]` when replicating that block https://github.com/apache/spark/blob/ca2ddf3c2079dda93053e64070ebda1610aa1968/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L1657-L1664 But during RDD migration (and probably pro-active replication) it will use a different codepath and potentially send the correct ClassTag which leads to the unhandled exception.### Why are the changes needed?The unhandled exception means that block replication does not work properly. Specifically cases where the block contains a user class and it not replicated at creation then the block will never successfully be migrated during decommission.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?It fixes the bug. But also since it changes from a fixed `JavaSerializer` to instead use the `SerializerManager` the `NettyBlockTransferService` might now instead use `KryoSerializer` or some other user configured serializer for the metadata.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?This modifies an existing spec to correctly check that replication happens for repl defined classes while removing the hack that erases the `ClassTag`.  Additionally I tested this manually on a hadoop cluster to check that it also solves the decommission migration issue. If some can point me to some better way to add a spec using user defined classes I would also like to add a unittest for it.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign name to _LEGACY_ERROR_TEMP_2247 as \"CANNOT_MERGE_SCHEMAS\".Also proposes to display both left and right schemas in the exception so that one can compare them. Please let me know if you prefer the old error message with a single schema.This is the stack trace after the changes:```scala> spark.read.option(\"mergeSchema\ \"true\").parquet(path)org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:Initial schema:\"STRUCT<id: BIGINT>\"Schema that cannot be merged with the initial schema:\"STRUCT<id: INT>\".  at org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2355)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:104)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5$adapted(SchemaMergeUtils.scala:100)  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:100)  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:496)  at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:78)  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)  at scala.Option.orElse(Option.scala:447)  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)  at scala.Option.getOrElse(Option.scala:189)  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:548)  ... 49 elidedCaused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"BIGINT\" and \"INT\".  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotMergeIncompatibleDataTypesError(QueryExecutionErrors.scala:1326)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:610)  at scala.Option.map(Option.scala:230)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:602)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:599)  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)  at org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:599)  at org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:647)  at org.apache.spark.sql.types.StructType$.merge(StructType.scala:593)  at org.apache.spark.sql.types.StructType.merge(StructType.scala:498)  at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$5(SchemaMergeUtils.scala:102)  ... 67 more```<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?We should assign proper name to LEGACY_ERROR_TEMP*<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, the users will see an improved error message.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Changed an existing test case to test the new error class with `checkError` utility.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?Fix a correctness bug for scalar subqueries with COUNT and a GROUP BY clause, for example:```create view t1(c1, c2) as values (0, 1), (1, 2);create view t2(c1, c2) as values (0, 2), (0, 3);select c1, c2, (select count(*) from t2 where t1.c1 = t2.c1 group by c1) from t1;-- Correct answer: [(0, 1, 2), (1, 2, null)]+---+---+------------------+|c1 |c2 |scalarsubquery(c1)|+---+---+------------------+|0  |1  |2                 ||1  |2  |0                 |+---+---+------------------+```This is due to a bug in our \"COUNT bug\" handling for scalar subqueries. For a subquery with COUNT aggregate but no GROUP BY clause, 0 is the correct output on empty inputs, and we use the COUNT bug handling to construct the plan that  yields 0 when there were no matched rows.But when there is a GROUP BY clause then NULL is the correct output (i.e. there is no COUNT bug), but we still incorrectly use the COUNT bug handling and therefore incorrectly output 0. Instead, we need to only apply the COUNT bug handling when the scalar subquery had no GROUP BY clause.To fix this, we need to track whether the scalar subquery has a GROUP BY, i.e. a non-empty groupingExpressions for the Aggregate node. This need to be checked before subquery decorrelation, because that adds the correlated outer refs to the group-by list so after that the group-by is always non-empty. We save it in a boolean in the ScalarSubquery node until later when we rewrite the subquery into a join in constructLeftJoins.This is a long-standing bug. This bug affected both the current DecorrelateInnerQuery framework and the old code (with spark.sql.optimizer.decorrelateInnerQuery.enabled = false), and this PR fixes both.This bug affects scalar subqueries (in RewriteCorrelatedScalarSubquery), but lateral subqueries handle it correctly in DecorrelateInnerQuery. Related: https://issues.apache.org/jira/browse/SPARK-36113### Why are the changes needed?Fix a correctness bug.### Does this PR introduce _any_ user-facing change?Yes, fix incorrect query results.### How was this patch tested?Add SQL tests and unit tests. (Note that there were 2 existing unit tests for queries of this shape, which had the incorrect results as golden results.)
2	-2	### What changes were proposed in this pull request?This is the most narrow fix for the issue observed in SPARK-43157. It does not attempt to identify or solve all potential correctness and concurrency issues from TreeNode.tags being modified in multiple places. It solves the issue described in  SPARK-43157 by cloning the cached plan when populating `InMemoryRelation.innerChildren`. I chose to do the clone at this point to limit the scope to tree traversal used for building up the string representation of the plan, which is where we see the issue. I do not see any other uses for `TreeNode.innerChildren`. I did not clone any earlier because the caching objects have mutable state that I wanted to avoid touching to be extra safe.Another solution I tried was to modify `InMemoryRelation.clone` to create a new `CachedRDDBuilder` and pass in a cloned `cachedPlan`. I opted not to go with this approach because `CachedRDDBuilder` has mutable state that needs to be moved to the new object and I didn't want to add that complexity if not needed.### Why are the changes needed?When caching is used the cached part of the SparkPlan is leaked to new clones of the plan. This leakage is an issue because if the TreeNode.tags are modified in one plan, it impacts the other plan. This is a correctness issue and a concurrency issue if the TreeNode.tags are set in different threads for the cloned plans.See the description of [SPARK-43157](https://issues.apache.org/jira/browse/SPARK-43157) for an example of the concurrency issue.### Does this PR introduce _any_ user-facing change?Yes. It fixes a driver hanging issue the user can observe.### How was this patch tested?Unit test added and I manually verified `Dataset.explain(\"formatted\")` still had the expected output.```scalaspark.range(10).cache.filter($\"id\" > 5).explain(\"formatted\")== Physical Plan ==* Filter (4)+- InMemoryTableScan (1)      +- InMemoryRelation (2)            +- * Range (3)(1) InMemoryTableScanOutput [1]: [id#0L]Arguments: [id#0L], [(id#0L > 5)](2) InMemoryRelationArguments: [id#0L], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@418b946b,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Range (0, 10, step=1, splits=16),None), [id#0L ASC NULLS FIRST](3) Range [codegen id : 1]Output [1]: [id#0L]Arguments: Range (0, 10, step=1, splits=Some(16))(4) Filter [codegen id : 1]Input [1]: [id#0L]Condition : (id#0L > 5)```I also verified that the `InMemory.innerChildren` is cloned when the entire plan is cloned.```scalaimport org.apache.spark.sql.execution.SparkPlanimport org.apache.spark.sql.execution.columnar.InMemoryTableScanExecimport spark.implicits._def findCacheOperator(plan: SparkPlan): Option[InMemoryTableScanExec] = {  if (plan.isInstanceOf[InMemoryTableScanExec]) {    Some(plan.asInstanceOf[InMemoryTableScanExec])  } else if (plan.children.isEmpty && plan.subqueries.isEmpty) {    None  } else {    (plan.subqueries.flatMap(p => findCacheOperator(p)) ++      plan.children.flatMap(findCacheOperator)).headOption  }}val df = spark.range(10).filter($\"id\" < 100).cache()val df1 = df.limit(1)val df2 = df.limit(1)// Get the cache operator (InMemoryTableScanExec) in each planval plan1 = findCacheOperator(df1.queryExecution.executedPlan).getval plan2 = findCacheOperator(df2.queryExecution.executedPlan).get// Check if InMemoryTableScanExec references point to the same objectprintln(plan1.eq(plan2))// returns false// Check if InMemoryRelation references point to the same objectprintln(plan1.relation.eq(plan2.relation))// returns false// Check if the cached SparkPlan references point to the same objectprintln(plan1.relation.innerChildren.head.eq(plan2.relation.innerChildren.head))// returns false// This shows the issue is fixed```
1	-3	### What changes were proposed in this pull request?This PR fixes PySpark Connect quick start working (https://mybinder.org/v2/gh/apache/spark/87a5442f7e?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_connect.ipynb)### Why are the changes needed?For end users to try them out easily. Currently it fails.### Does this PR introduce _any_ user-facing change?Yes, it fixes the quickstart.### How was this patch tested?Manually tested at https://mybinder.org/v2/gh/HyukjinKwon/spark/quickstart-connect-working?labpath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_connect.ipynb
2	-1	### What changes were proposed in this pull request?This PR proposes to set the upperbound for pandas in Binder integration. We don't currently support pandas 2.0.0 properly, see also https://issues.apache.org/jira/browse/SPARK-42618### Why are the changes needed?To make the quickstarts working.### Does this PR introduce _any_ user-facing change?Yes, it fixes the quickstart.### How was this patch tested?Tested in:- https://mybinder.org/v2/gh/HyukjinKwon/spark/set-lower-bound-pandas?labpath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_connect.ipynb- https://mybinder.org/v2/gh/HyukjinKwon/spark/set-lower-bound-pandas?labpath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_df.ipynb- https://mybinder.org/v2/gh/HyukjinKwon/spark/set-lower-bound-pandas?labpath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_ps.ipynb
1	-1	### What changes were proposed in this pull request?This PR fixes Binder integration version strings in case `dev0` is specified. It should work in master branch too (when users manually build the docs and test)### Why are the changes needed?For end users to run quickstarts.### Does this PR introduce _any_ user-facing change?Yes, it fixes the user-facing quick start.### How was this patch tested?Manually tested at https://mybinder.org/v2/gh/HyukjinKwon/spark/SPARK-42475-followup-2?labpath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_connect.ipynb
1	-1	### What changes were proposed in this pull request?This is follow-up for https://github.com/apache/spark/pull/39591 based on the suggestions from comment https://github.com/apache/spark/commit/1de83500f4621d4af91f25a339dd5057c59cfc1e#r109023188.### Why are the changes needed?For backward compatibility.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?The existing CI should pass
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update the error class _LEGACY_ERROR_TEMP_2010 to InternalError.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue [SPARK-42845](https://issues.apache.org/jira/browse/SPARK-42845). The original name just a number, update it to InteralError.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add a test case in QueryExecutionErrorsSuite.
3	-1	### What changes were proposed in this pull request?Add missing `super().__init__()` in expressions### Why are the changes needed?to make IDEA happy:<img width=\"418\" alt=\"image\" src=\"https://user-images.githubusercontent.com/7322292/232402659-20e7f740-7816-495f-967f-d90c3ac7eedc.png\">### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing UT
1	-1	### Problem DescriptionFrom python 3.12 and onward or 3.13 according to the references [1](https://bugs.python.org/issue35089), [2](https://docs.python.org/3/library/typing.html#typing.IO) the typing.io namespace will be removed.```/python/3.11.1/lib/python3.11/site-packages/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.    from typing.io import BinaryIO  # type: ignore[import]```### What changes were proposed in this pull request?I am fixing the issue with the usage of typing.io### Why are the changes needed?Because typing.io namespace will be removed [1](https://bugs.python.org/issue35089), [2](https://docs.python.org/3/library/typing.html#typing.IO)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?By running the tests: https://github.com/aimtsou/spark/actions/runs/4718557460### Jira tickethttps://issues.apache.org/jira/browse/SPARK-43160
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to improve `spark.sql.files.minPartitionNum`'s doc.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve description### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, better config description.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Introduce a new parameter for defining output metadata path. If it's not set then the behavior doesn't change, the output metadata are set to `data_output_path/_spark_metadata`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->1. Separation data from metadata2. Multiple jobs can write to the same directory3. Hive partition discoveryLinks for more description are put in jira ticket: https://issues.apache.org/jira/browse/SPARK-43152### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, there is new parameter in spark-sql config.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->![image](https://user-images.githubusercontent.com/8326978/232464191-444c8e74-be01-4dfb-82d2-f1b763c0a575.png)### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The Sql Parser will try use `SSL`(faster) mode parse sqlText at first, if throw `ParseCancellationException` then try parse sqlText with `LL`(more correcter) mode. But Spark use custom `SparkParserErrorStrategy` will not throw `ParseCancellationException` but `ParseException` in some situation. So we should catch `ParseException` then to make sure parser use `LL` mode try again.Fix sql parser throw `ParseException` in `SSL` mode will not try `LL` mode.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Make some special sql can be parsed. Like `SELECT 1 UNION SELECT 1`.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?Many databases and data warehouse SQL engines support temporary tables. A temporary table, as its named implied, is a short-lived table that its life will be only for current session. * [Hive Temporary Table](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.0.1/using-hiveql/content/hive_create_a_hive_temporary_table.html)* [Teradata Volatile Table](https://docs.teradata.com/reader/rgAb27O_xRmMVc_aQq2VGw/mpJF1z_vSlpMbZYxFmRJfA) * [PostgreSQL Temporary Table](https://www.postgresql.org/docs/12/sql-createtable.html)In Spark, there is no temporary table. the DDL “CREATE TEMPORARY TABLE AS SELECT” will create a temporary view. A temporary view is totally different with a temporary table.### Why are the changes needed?A temporary view is just a VIEW. It doesn’t materialize data in storage. So it has below shortage:1. View will not give improved performance. Materialize intermediate data in temporary tables for a complex query will accurate queries, especially in an ETL pipeline.2. View which calls other views can cause severe performance issues. Even, executing a very complex view may fail in Spark.3. Temporary view has no database namespace. In some complex ETL pipelines or data warehouse applications, without database prefix is not convenient. It needs some tables which only used in current session.### Does this PR introduce _any_ user-facing change?NO### How was this patch tested?Add unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Move canWrite to DataTypeUtils.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->canWrite access SQLConf so we can move it out from DataType to make DataType as public simpiler API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->DataType is public API while we can leave PhysicalDataType as internal API/implementation thus we can remove PhysicalDataType from DataType. So DataType does not need to have a class dependency on PhysicalDataType.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify DataType.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT
2	-3	### What changes were proposed in this pull request?In the PR, I propose to transfer a local relation to the server in streaming way when it exceeds some size which is defined by the SQL config `spark.sql.session.localRelationCacheThreshold`. The config value is 64MB by default. In particular:1. The client applies the `sha256` function over the arrow form of the local relation;2. It checks presents of the relation at the server side by sending the relation hash to the server;3. If the server doesn't have the local relation, the client transfers the local relation as an artefact with the name `cache/<sha256>`;4. As soon as the relation has presented at the server already, or transferred recently, the client transform the logical plan by replacing the `LocalRelation` node by `CachedLocalRelation` with the hash.5. On another hand, the server converts `CachedLocalRelation` back to `LocalRelation` by retrieving the relation body from the local cache.#### Details of the implementationThe client sends new command `ArtifactStatusesRequest` to check either the local relation is cached at the server or not. New command comes via new RPC endpoint `ArtifactStatus`. And the server answers by new message `ArtifactStatusesResponse`, see **base.proto**.The client transfers serialized (in avro) body of local relation and its schema via the RPC endpoint `AddArtifacts`. On another hand, the server stores the received artifact in the block manager using the id `CacheId`. The last one has 3 parts:- `userId` - the identifier of the user that created the local relation,- `sessionId` - the identifier of the session which the relation belongs to,- `hash` - a `sha-256` hash over relation body.See **SparkConnectArtifactManager.addArtifact()**.The current query is blocked till the local relation is cached at the server side.When the server receives the query, it retrieves `userId`, `sessionId` and `hash` from `CachedLocalRelation`, and gets the local relation data from the block manager. See **SparkConnectPlanner.transformCachedLocalRelation()**.The occupied blocks at the block manager are removed when an user session is invalidated in `userSessionMapping`. See **SparkConnectService.RemoveSessionListener** and **BlockManager.removeCache()`**. ### Why are the changes needed?To allow creating a dataframe from a large local collection. `spark.createDataFrame(...)` fails with the following error w/o the changes:```java23/04/21 20:32:20 WARN NettyServerStream: Exception processing messageorg.sparkproject.connect.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: gRPC message exceeds maximum size 134217728: 268435456\tat org.sparkproject.connect.grpc.Status.asRuntimeException(Status.java:526)```### Does this PR introduce _any_ user-facing change?No. The changes extend the existing proto API.### How was this patch tested?By running the new tests:```$ build/sbt \"test:testOnly *.ArtifactManagerSuite\"$ build/sbt \"test:testOnly *.ClientE2ETestSuite\"$ build/sbt \"test:testOnly *.ArtifactStatusesHandlerSuite\"```
1	-1	### What changes were proposed in this pull request?Enables `ArrowParityTests.test_createDataFrame_with_single_data_type`.### Why are the changes needed?The test is already fixed by previous commits.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled/updated the related tests.
2	-3	### What changes were proposed in this pull request?Uses deduplicated field names when creating Arrow `RecordBatch`.The result pandas DataFrame will contain `dict` with suffix `_0`, `_1`, etc. if there are duplicated field names.For example:```py>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"values (1, struct(1 as a, 2 as a, 3 as b)) as t(x, y)\").toPandas()   x                             y0  1  {'a_0': 1, 'a_1': 2, 'b': 3}```### Why are the changes needed?Currently `df.toPandas()` with Arrow enabled fails when there are duplicated field names.```py>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"values (1, struct(1 as a, 2 as a, 3 as b)) as t(x, y)\").toPandas()Traceback (most recent call last):...pyarrow.lib.ArrowInvalid: Ran out of field metadata, likely malformed```### Does this PR introduce _any_ user-facing change?`df.toPandas()` with Arrow enabled fails when there are duplicated field names will work.### How was this patch tested?Added a test.
1	-2	### What changes were proposed in this pull request?The main change of this pr as follows:1. Bump MiMa's `previousSparkVersion` to 3.4.02. Clean up expired rules and case match3. Add `shaded and generated protobuf code` exclude filters to `defaultExcludes` 4. Increase the mem of mima check from 4096m to 4196m to avoid `java.lang.OutOfMemoryError: GC overhead limit exceeded` ### Why are the changes needed?To ensure that MiMa checks cover new APIs added in Spark 3.4.0.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Scala 2.12```dev/mima -Pscala-2.12```Scala 2.13```dev/change-scala-version.sh 2.13dev/mima -Pscala-2.13```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR allows the users to custom Unix username in Pod by setting env var `SPARK_USER_NAME`, which reduces the gap between Spark on YARN and K8s.Each line in `/etc/passwd` is compose of```username:password:UID:GID:comment:home_directory:shell```This PR simply changes the first item from `$myuid` to `${SPARK_USER_NAME:-$myuid}` to achieve the above ability.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In Spark on YARN, when we launch a Spark application via `spark-submit --proxy-user jack ...`, the YARN will launch containers(usually Linux processes) using Unix user \"jack\ and some components/libraries rely on the login user in default, one example is Alluxiohttps://github.com/Alluxio/alluxio/blob/da77d688bdbb0cf0c6477bed4d3187897fe2a2e1/core/common/src/main/java/alluxio/conf/PropertyKey.java#L6469-L6476```  public static final PropertyKey SECURITY_LOGIN_USERNAME =      stringBuilder(Name.SECURITY_LOGIN_USERNAME)          .setDescription(\"When alluxio.security.authentication.type is set to SIMPLE or \"              + \"CUSTOM, user application uses this property to indicate the user requesting \"              + \"Alluxio service. If it is not set explicitly, the OS login user will be used.\")          .setConsistencyCheckLevel(ConsistencyCheckLevel.ENFORCE)          .setScope(Scope.CLIENT)          .build();```To reduce the difference between Spark on YARN and Spark on K8s, we hope Spark on K8s keeps the same ability to allow to dynamically change login user on submitting Spark application.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it allows the user to custom Pod Unix username by setting env var `SPARK_USER_NAME` in K8s, reducing the gap between Spark on YARN and K8s.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New IT is added.Also manually testing in our internal K8s cluster.```spark-submit --master=k8s://xxxx \\        --conf spark.kubernetes.driverEnv.SPARK_USER_NAME=tom \\\t--conf spark.executorEnv.SPARK_USER_NAME=tom \\\t--proxy-user tom \\        ...```Then login the Pod, verify the Unix username by `id -un` is `tom` instead of `185`
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SPARK-42657 is for Spark 3.5.0.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix the wrong API version### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Passing GA.
3	-3	### What changes were proposed in this pull request?Currently, Spark supports the `array_insert` and `array_prepend`. Users insert an element into the head of array is common operation. Considered, we want make array_prepend reuse the implementation of array_insert, but it seems a bit performance worse if the position is foldable and positive.The reason is that always do the check for position is negative or positive, and the code is too long. Too long code will lead to JIT failed.### Why are the changes needed?Improve ArrayInsert if the position is foldable and positive.### Does this PR introduce _any_ user-facing change?'No'.Just change the inner implementation.### How was this patch tested?Exists test cases.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Implemented `dropDuplicatesWithinWatermark` Python API for Spark Connect. This change is based on a previous [commit](https://github.com/apache/spark/commit/0e9e34c1bd9bd16ad5efca77ce2763eb950f3103) that introduced `dropDuplicatesWithinWatermark` API in Spark.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We recently introduced dropDuplicatesWithinWatermark API in Spark ([commit link](https://github.com/apache/spark/commit/0e9e34c1bd9bd16ad5efca77ce2763eb950f3103)). We want to bring parity to the Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this introduces a new public API, dropDuplicatesWithinWatermark in Spark Connect.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added new test cases in test suites.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR follows the https://github.com/antlr/antlr4/issues/192#issuecomment-15238595 to correct the current implementation of the **two-stage parsing strategy** in `AbstractSqlParser`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This should be a long-standing issue, before [SPARK-38385](https://issues.apache.org/jira/browse/SPARK-38385), Spark uses `DefaultErrorStrategy`, and after [SPARK-38385](https://issues.apache.org/jira/browse/SPARK-38385) Spark uses class `SparkParserErrorStrategy() extends DefaultErrorStrategy`. It is not a correct implementation of the \"two-stage parsing strategy\"As mentioned in https://github.com/antlr/antlr4/issues/192#issuecomment-15238595> You can save a great deal of time on correct inputs by using a two-stage parsing strategy.>> 1. Attempt to parse the input using BailErrorStrategy and PredictionMode.SLL.>    If no exception is thrown, you know the answer is correct.> 2. If a ParseCancellationException is thrown, retry the parse using the default>    settings (DefaultErrorStrategy and PredictionMode.LL).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the Spark SQL parser becomes more powerful, SQL like `SELECT 1 UNION SELECT 2` parse succeeded after this change.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT is added.
1	-2	### What changes were proposed in this pull request?Expose the host and bearer token as properties of the`SparkConnectClient`.### Why are the changes needed?This allows for querying the connecting host and bearer tokengiven an instance of spark connect client.### Does this PR introduce _any_ user-facing change?Introduces two new properties `host` and `token` to the`SparkConnectClient` class### How was this patch tested?Unit tests
2	-3	### What changes were proposed in this pull request?`write jdbc` will test failed when test `ClientE2ETestSuite` using Java 11 & 17 without `-Phive` because the test case need `derby` in the classpath of `SimpleSparkConnectService` and `derby.jar` will only be copied to `assembly/target/scala-xxx/jars/` when build with `-Phive`.So this pr add check to ingore `write jdbc` when test without `-Phive`.### Why are the changes needed?Ingore `write jdbc` when test without `-Phive`.### Does this PR introduce _any_ user-facing change?No, just for test.### How was this patch tested?- Pass GitHub Actions- Manual test `ClientE2ETestSuite` using Java 11 & 17 without `-Phive````build/mvn clean install -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite   build/sbt \"connect-client-jvm/testOnly *ClientE2ETestSuite\"```Before```- write jdbc *** FAILED ***  io.grpc.StatusRuntimeException: INTERNAL: No suitable driver  at io.grpc.Status.asRuntimeException(Status.java:535)  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)  at scala.collection.Iterator.foreach(Iterator.scala:943)  at scala.collection.Iterator.foreach$(Iterator.scala:943)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:458)  at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:257)  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:221)  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:218)```After```[info] - write jdbc !!! CANCELED !!! (0 milliseconds)[info]   org.apache.spark.sql.connect.client.util.IntegrationTestUtils.isSparkHiveJarAvailable was false (ClientE2ETestSuite.scala:196)[info]   org.scalatest.exceptions.TestCanceledException:[info]   at org.scalatest.Assertions.newTestCanceledException(Assertions.scala:475)[info]   at org.scalatest.Assertions.newTestCanceledException$(Assertions.scala:474)[info]   at org.scalatest.Assertions$.newTestCanceledException(Assertions.scala:1231)[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssume(Assertions.scala:1310)[info]   at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$16(ClientE2ETestSuite.scala:196)```
2	-1	### What changes were proposed in this pull request?Fix `SparkSQLCLIDriver` completer:- Replace Hive UDFs with Spark SQL UDFs.- Replace Hive conf with Spark SQL conf.- Replace Hive keywords with Spark SQL keywords.### Why are the changes needed?Get better tips.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test:```sh./dev/make-distribution.sh --name SPARK-43174 --tgz  -Phive -Phive-thriftserver -Pyarn```![manual testing](https://user-images.githubusercontent.com/5399861/232989054-e68e98b8-7858-4ad1-b4dd-070f11a956d4.gif)
1	-1	### What changes were proposed in this pull request?Deduplicate imports in Connect Tests### Why are the changes needed?for simplicity### Does this PR introduce _any_ user-facing change?No, test-only### How was this patch tested?updated unittests
2	-3	Bumps [jetty-server](https://github.com/eclipse/jetty.project) from 9.4.51.v20230217 to 10.0.14.<details><summary>Release notes</summary><p><em>Sourced from <a href=\"https://github.com/eclipse/jetty.project/releases\">jetty-server's releases</a>.</em></p><blockquote><h2>10.0.14</h2><h1>Special Thanks to the following Eclipse Jetty community members</h1><ul><li><a href=\"https://github.com/pzygielo\"><code>@​pzygielo</code></a> (Piotrek Żygieło)</li><li><a href=\"https://github.com/jluehe\"><code>@​jluehe</code></a> (jluehe)</li><li><a href=\"https://github.com/dzoech\"><code>@​dzoech</code></a> (Dominik Zöchbauer)</li></ul><h1>Changelog</h1><ul><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9344\">#9344</a> - Cleanup Multipart handling for CVE-2023-26048</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9343\">#9343</a> - URI Host Mismatch with optional Compliance modes</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9339\">#9339</a> - Cleanup Cookie Cutter handling for CVE-2023-26049</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9337\">#9337</a> - LowResourceMonitor.getReasons should include detailed reason instead of hard-coded message (<a href=\"https://github.com/jluehe\"><code>@​jluehe</code></a>)</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9334\">#9334</a> - Better support for Cookie RFC 2965 compliance</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9285\">#9285</a> - ContextHandler sends redirect on BaseResponse instead of Wrapped Response object from Handler chain</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9283\">#9283</a> - Configurable Unsafe Host Header Behaviors</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9188\">#9188</a> - Log as info exceptions from server after sending stop with StopMojo.</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9183\">#9183</a> - ConnectHandler may close the connection instead of sending 200 OK</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9128\">#9128</a> - Do not execute any phase for maven plugin :start (<a href=\"https://github.com/pzygielo\"><code>@​pzygielo</code></a>)</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9119\">#9119</a> - Wrong value of javax.servlet.forward.context_path attribute</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9092\">#9092</a> - Use ASM Bom</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9059\">#9059</a> - IteratingCallback not serializing close() and failed()</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9055\">#9055</a> - PathMappings optimizations</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/7650\">#7650</a> - QueuedThreadPool: Stopped without executing or closing null (<a href=\"https://github.com/dzoech\"><code>@​dzoech</code></a>)</li></ul><h1>Dependencies</h1><ul><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9242\">#9242</a> - Bump infinispan-bom to 11.0.17.Final</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9359\">#9359</a> - Bump maven.version to 3.9.0</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9102\">#9102</a> - Bump org.apache.aries.spifly.dynamic.bundle to 1.3.6</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9098\">#9098</a> - Bump org.eclipse.osgi to 3.18.200</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9106\">#9106</a> - Bump org.eclipse.osgi.services to 3.11.100</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9097\">#9097</a> - Bump protostream to 4.6.0.Final</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9367\">#9367</a> - Bump tycho-p2-repository-plugin to 3.0.2</li></ul><h2>10.0.13</h2><h1>Special Thanks to the following Eclipse Jetty community members</h1><ul><li><a href=\"https://github.com/janvojt\"><code>@​janvojt</code></a> (Jan Vojt)</li><li><a href=\"https://github.com/joschi\"><code>@​joschi</code></a> (Jochen Schalanda)</li><li><a href=\"https://github.com/leonchen83\"><code>@​leonchen83</code></a> (Baoyi Chen)</li><li><a href=\"https://github.com/cowwoc\"><code>@​cowwoc</code></a> (Gili Tzabari)</li><li><a href=\"https://github.com/Vlatombe\"><code>@​Vlatombe</code></a> (Vincent Latombe)</li></ul><h1>Changelog</h1><ul><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9006\">#9006</a> - WebSocket Message InputStream read() returns signed byte</li><li><a href=\"https://redirect.github.com/eclipse/jetty.project/issues/8913\">#8913</a> - Review Jetty XML syntax to allow calling JDK methods</li></ul><!-- raw HTML omitted --></blockquote><p>... (truncated)</p></details><details><summary>Commits</summary><ul><li><a href=\"https://github.com/eclipse/jetty.project/commit/976721d0f3e903a243584d47870ad2f2c1bf9e55\"><code>976721d</code></a> Updating to version 10.0.14</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/b7075161d015ddce23fbf3db873d5f6b539f6a6b\"><code>b707516</code></a> Fix osgi dependencies for update to org.eclipse.osgi.services.</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/4d146412c8feac05c25d171b15c4f6ab4d42719b\"><code>4d14641</code></a> Fix <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9334\">#9334</a> Cookie Compliance (<a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9402\">#9402</a>)</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/f01d53895f8930e1ebc52c9d89944df14fe5d6f2\"><code>f01d538</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9380\">#9380</a> from eclipse/dependabot/maven/jetty-10.0.x/org.apach...</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/8b4f5eab4127bc6fd270451eac1d044e6c855b7b\"><code>8b4f5ea</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9378\">#9378</a> from eclipse/dependabot/maven/jetty-10.0.x/org.ascii...</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/840ef489223670a840189bda4c5544acdbe03405\"><code>840ef48</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9372\">#9372</a> from eclipse/dependabot/maven/jetty-10.0.x/org.apach...</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/cfb8d706250bba25a9439a91a6a173b303054261\"><code>cfb8d70</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9371\">#9371</a> from eclipse/dependabot/maven/jetty-10.0.x/maven.sur...</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/cfe6e91338293b91fc2642e92cc665d0244fa648\"><code>cfe6e91</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9369\">#9369</a> from eclipse/dependabot/maven/jetty-10.0.x/org.apach...</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/bce4fc2941ed76ea787be048cffe2f5cd45d5bb1\"><code>bce4fc2</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9367\">#9367</a> from eclipse/dependabot/maven/jetty-10.0.x/org.eclip...</li><li><a href=\"https://github.com/eclipse/jetty.project/commit/610cee13c23c9a5757eb3c2f212582f1d8f6bf97\"><code>610cee1</code></a> Merge pull request <a href=\"https://redirect.github.com/eclipse/jetty.project/issues/9359\">#9359</a> from eclipse/dependabot/maven/jetty-10.0.x/maven.ver...</li><li>Additional commits viewable in <a href=\"https://github.com/eclipse/jetty.project/compare/jetty-9.4.51.v20230217...jetty-10.0.14\">compare view</a></li></ul></details><br />[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.eclipse.jetty:jetty-server&package-manager=maven&previous-version=9.4.51.v20230217&new-version=10.0.14)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.[//]: # (dependabot-automerge-start)[//]: # (dependabot-automerge-end)---<details><summary>Dependabot commands and options</summary><br />You can trigger Dependabot actions by commenting on this PR:- `@dependabot rebase` will rebase this PR- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it- `@dependabot merge` will merge this PR after your CI passes on it- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it- `@dependabot cancel merge` will cancel a previously requested merge and block automerging- `@dependabot reopen` will reopen this PR if it is closed- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/spark/network/alerts).</details>
2	-3	### What changes were proposed in this pull request?This change allows applications to control whether their metadata gets saved in the db. For applications with higher security requirements, storing application secret in the db without any encryption is a potential security risk. While filesystem ACLs can help protect the access to the db, this level of security is not sufficient for some use cases. Such applications can chose to not save their metadata in the db. As a result, these applications may experience more failures in the event of a node restart, but we believe this trade-off is acceptable given the increased security risk.### Why are the changes needed?These modifications are necessary to reduce the likelihood of security threats for applications with elevated security requirements.### Does this PR introduce _any_ user-facing change?No. Added a configuration `spark.shuffle.server.recovery.disabled` which by default is `false`. When set to `true`, the metadata of the application will not saved in the db.### How was this patch tested?Added UTs and also verified with test applications in our test environment.
3	-2	### What changes were proposed in this pull request?The pr aims to display `the Spark WEB UI address` when spark-sql startup.### Why are the changes needed?Promoting user experience.Like `spark-shell`, it would be great if `spark-sql` show the UI information.```$ bin/spark-sqlSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/05/07 13:58:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable23/05/07 13:58:27 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist23/05/07 13:58:27 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist23/05/07 13:58:28 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.023/05/07 13:58:28 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore dongjoon@127.0.0.1Spark Web UI available at http://localhost:4040Spark master: local[*], Application Id: local-1683493106875spark-sql (default)>```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA & Manually test.
1	-3	### What changes were proposed in this pull request?This PR introduces a new callback \"onQueryIdle\" to StreamingQueryListener, which was a part of query progress update.The signature of the new callback method is below:```def onQueryIdle(event: QueryIdleEvent): Unitclass QueryIdleEvent(val id: UUID, val runId: UUID) extends Event```This PR proposes to provide a default implementation for onQueryIdle in StreamingQueryListener so that it does not break existing implementations of streaming query listener in Scala/Java. Note that it's a behavioral change as users will receive the different callback when the streaming query is being idle for configured period of time (previously they receive the callback onQueryProgress), but this is worth doing as described in the section \"Why are the changes needed?\".### Why are the changes needed?People has been having a lot of confusions about query progress event on idleness query; it’s not only the matter of understanding but also comes up with various types of complaints, because they tend to think the event only happens after the microbatch has finished. In addition, misunderstanding may also lead to data loss on monitoring - since we give the latest batch ID for update event on idleness, if the listener implementation blindly performs upsert the information to the external storage based on batch ID, they are in risk on losing data.This also complicates the logic because we have to memorize the execution for the previous batch, which is arguably not necessary.### Does this PR introduce _any_ user-facing change?Yes. After this change, users won't get query progress update event from idle query. Instead, they will get query idle event.### How was this patch tested?Modified UTs.
2	-1	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/36917 change to use using String to compare `NodeState.DECOMMISSIONING` for compatibility with hadoop-2.7.  After SPARK-42452, Spark no longer supported build&test with hadoop 2, so this pr resume using enumeration to compare `NodeState.DECOMMISSIONING`.### Why are the changes needed?No longer requires compatibility with Hadoop 2### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-1	### What changes were proposed in this pull request?SPARK-36835 introduced `hadoop-client-api.artifact`, `adoop-client-runtime.artifact` and `hadoop-client-minicluster.artifact` to be compatible with the dependency definitions of Hadoop 2 and Hadoop 3. After [SPARK-42452](https://issues.apache.org/jira/browse/SPARK-42452), Spark no longer supports Hadoop 2, so this pr inline these properties to simplify the dependency definition.### Why are the changes needed?No longer requires compatibility with Hadoop 2### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove `org.apache.spark.sql.hive.HiveShim.ShimFileSinkDesc`, which is used to address serializable issue of `org.apache.hadoop.hive.ql.plan.FileSinkDesc`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[HIVE-6171](https://issues.apache.org/jira/browse/HIVE-6171) changed `FileSinkDesc`'s property from `String dirName` to `Path dirName`, but the `Path` is not serializable until [HADOOP-13519](https://issues.apache.org/jira/browse/HADOOP-13519) (got fixed in Hadoop 3.0.0).Since SPARK-42452 removed support for Hadoop2, we can remove this workaround now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR basically reverts the SPARK-31631, which was aimed to address [HADOOP-12656](https://issues.apache.org/jira/browse/HADOOP-12656)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since [HADOOP-12656](https://issues.apache.org/jira/browse/HADOOP-12656) got fixed in Hadoop 2.8.0/3.0.0, and SPARK-42452 removed support for Hadoop2, we can remove this workaround now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Replace reflection w/ direct calling for `org.apache.hadoop.ipc.CallerContext`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`org.apache.hadoop.ipc.CallerContext` was added in [HDFS-9184](https://issues.apache.org/jira/browse/HDFS-9184) (Hadoop 2.8.0/3.0.0), previously, Spark uses reflection to invoke it for compatible w/ Hadoop 2.7, since SPARK-42452 removed support for Hadoop2, we can call it directly instead of using reflection.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update `ListQuery` to only store the number of columns of the original plan, instead of directly storing the original plan output attributes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Storing the plan output attributes is troublesome as we have to maintain them and keep them in sync with the plan. For example, `DeduplicateRelations` may change the plan output, and today we do not update `ListQuery.childOutputs` to keep sync.`ListQuery.childOutputs` was added by https://github.com/apache/spark/pull/18968 . It's only used to track the original plan output attributes as subquery de-correlation may add more columns. We can do the same thing by storing the number of columns of the plan.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, there is no user-facing bug exposed.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->a new plan test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->SPARK-19718 introduced different code branches for pre-Hadoop 2.8(w/o [HADOOP-12074](https://issues.apache.org/jira/browse/HADOOP-12074)) and Hadoop 2.8+(w/ [HADOOP-12074](https://issues.apache.org/jira/browse/HADOOP-12074))> 1. Check if the message of IOException starts with `java.lang.InterruptedException`. If so, treat it as `InterruptedException`. This is for pre-Hadoop 2.8.> 2. Treat `InterruptedIOException` as `InterruptedException`. This is for Hadoop 2.8+ and other places that may throw `InterruptedIOException` when the thread is interrupted.This PR removes the (1) since Spark no longer supports Hadoop2 now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since [SPARK-42452](https://issues.apache.org/jira/browse/SPARK-42452) removed support for Hadoop2, we can remove the workaround code for pre Hadoop 2.8 now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-2	### Why are the changes needed?The current validation on charset is restrictive. It does not allowblank space and digits. It's common for user agent strings to containthese characters.Secondly, it restricts the length to stay under 200 characters.The limit to 200 characters was mostly something that was done as asimple protection mechanism. We've looked into different specificationsfor what could be part of the user agent and longer user agents arecommon. Increase this restriction to 2KB.The server should enforce restrictions on its side, but we would still liketo keep the restriction as fallback protection, but allow large values.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove unnecessary serializable wrapper in `HadoopFSUtils`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`Path`, `FileStatus` become serializable in Hadoop3, since SPARK-42452 removed support for Hadoop2, we can remove those wrapper now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI.
2	-1	### What changes were proposed in this pull request?This pr replace reflection with direct calling for `ContainerLaunchContext#setTokensConf` in `o.a.s.deploy.yarn.Client#setTokenConf` function.### Why are the changes needed?SPARK-37205 uses reflection to call `ContainerLaunchContext#setTokensConf` for compatibility with Hadoop 2.7, since SPARK-42452 removed support for Hadoop2, we can call it directly instead of using reflection.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GA.
3	-2	### What changes were proposed in this pull request?This PR fixes `InlineCTE`'s idempotence. E.g. the following query:```WITH  x(r) AS (SELECT random()),  y(r) AS (SELECT * FROM x),  z(r) AS (SELECT * FROM x)SELECT * FROM z```currently breaks it because we take into account the reference to `x` from `y` when deciding about not inlining `x` in the first round:```=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InlineCTE === WithCTE                                                        WithCTE :- CTERelationDef 0, false                                     :- CTERelationDef 0, false :  +- Project [rand()#218 AS r#219]                            :  +- Project [rand()#218 AS r#219] :     +- Project [random(2957388522017368375) AS rand()#218]   :     +- Project [random(2957388522017368375) AS rand()#218] :        +- OneRowRelation                                     :        +- OneRowRelation!:- CTERelationDef 1, false                                     +- Project [r#222]!:  +- Project [r#219 AS r#221]                                    +- Project [r#220 AS r#222]!:     +- Project [r#219]                                             +- Project [r#220]!:        +- CTERelationRef 0, true, [r#219]                             +- CTERelationRef 0, true, [r#220]!:- CTERelationDef 2, false                                     !:  +- Project [r#220 AS r#222]                                 !:     +- Project [r#220]                                       !:        +- CTERelationRef 0, true, [r#220]                    !+- Project [r#222]                                             !   +- CTERelationRef 2, true, [r#222]    ```But in the next round we inline `x` because `y` was removed due to lack of references:```Once strategy's idempotence is broken for batch Inline CTE!WithCTE                                                        Project [r#222]!:- CTERelationDef 0, false                                     +- Project [r#220 AS r#222]!:  +- Project [rand()#218 AS r#219]                               +- Project [r#220]!:     +- Project [random(2957388522017368375) AS rand()#218]         +- Project [r#225 AS r#220]!:        +- OneRowRelation                                              +- Project [rand()#218 AS r#225]!+- Project [r#222]                                                         +- Project [random(2957388522017368375) AS rand()#218]!   +- Project [r#220 AS r#222]                                                +- OneRowRelation!      +- Project [r#220]                                       !         +- CTERelationRef 0, true, [r#220]    ```### Why are the changes needed?We use `InlineCTE` as an idempotent rule in the `Optimizer`, `CheckAnalysis` and `ProgressReporter`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added new UT.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove Hadoop 2 reference in docs.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-42452 removed support for Hadoop 2.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually review
2	-2	### What changes were proposed in this pull request?This is a pull request to port the fix from the master branch to version 3.3. [PR](https://github.com/apache/spark/pull/40755)When doing an outer join with joinWith on DataFrames, unmatched rows return Row objects with null fields instead of a single null value. This is not a expected behavior, and it's a regression introduced in [this commit](https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59). This pull request aims to fix the regression, note this is not a full rollback of the commit, do not add back \"schema\" variable.```case class ClassData(a: String, b: Int)val left = Seq(ClassData(\"a\ 1), ClassData(\"b\ 2)).toDFval right = Seq(ClassData(\"x\ 2), ClassData(\"y\ 3)).toDFleft.joinWith(right, left(\"b\") === right(\"b\"), \"left_outer\").collect``````Wrong results (current behavior):    Array(([a,1],[null,null]), ([b,2],[x,2]))Correct results:                     Array(([a,1],null), ([b,2],[x,2]))```### Why are the changes needed?We need to address the regression mentioned above. It results in unexpected behavior changes in the Dataframe joinWith API between versions 2.4.8 and 3.0.0+. This could potentially cause data correctness issues for users who expect the old behavior when using Spark 3.0.0+.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added unit test (use the same test in previous [closed pull request](https://github.com/apache/spark/pull/35140), credit to Clément de Groc) Run sql-core and sql-catalyst submodules locally with ./build/mvn clean package -pl sql/core,sql/catalystCloses #40755 from kings129/encoder_bug_fix.Authored-by: --global <xuqiang129@gmail.com>
1	-2	### What changes were proposed in this pull request?Converters between Proto and StorageLevel to avoid code duplicationIt's follow up from https://github.com/apache/spark/pull/40015### Why are the changes needed?Code deduplication### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Replace reflection w/ direct calling for YARN Resource API, including - `org.apache.hadoop.yarn.api.records.ResourceInformation`, - `org.apache.hadoop.yarn.exceptions.ResourceNotFoundException`which were added in [YARN-4081](https://issues.apache.org/jira/browse/YARN-4081) (Hadoop 2.10.0/3.0.0) ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify code. Since [SPARK-42452](https://issues.apache.org/jira/browse/SPARK-42452) removed support for Hadoop 2, we can call those API directly now.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add support of `StreamingQueryManager()` to CONNECT PYTHON client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Now users can use typical streaming query manager method by calling `session.streams`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual test and unit test```Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Dec  7 2022 01:11:58)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> q = spark.readStream.format(\"rate\").load().writeStream.format(\"memory\").queryName(\"test\").start()23/04/19 23:10:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-913e48b9-26d8-448f-899f-d9f5ae08707d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.23/04/19 23:10:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.>>> spark.streams.active[<pyspark.sql.connect.streaming.query.StreamingQuery object at 0x7f4590400d90>]>>> q1 = spark.streams.active[0]>>> q1.id == q.idTrue>>> q1.runId == q.runIdTrue>>> q1.runId == q.runIdTrue>>> q.name'test'>>> q1.name'test'>>> q == q1False>>> q1.stop()>>> q<pyspark.sql.connect.streaming.query.StreamingQuery object at 0x7f4590400b20>>>> q1<pyspark.sql.connect.streaming.query.StreamingQuery object at 0x7f4590400ee0>>>> q.isActiveFalse```
1	-2	### What changes were proposed in this pull request?This pr aims to change `-Xmx` from `4196` to `120` for dev/mima to avoid `GC overhead limit exceeded`.### Why are the changes needed?Make `dev/mina` stable on GitHub Action.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually check that `GC overhead limit exceeded` no longer exists
2	-1	### What changes were proposed in this pull request?Add helper functions for extract value from literal expression### Why are the changes needed?some logic should be reused### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?existing UTs
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Example query:```sqlcreate or replace temp view t0 (a, b)as values    (1, 1.0),    (2, 2.0);create or replace temp view t1 (c, d)as values    (2, 3.0); spark.sql(\"select *, (select (count(1)) is null from t1 where t0.a = t1.c) from t0\").collect()res6: Array[org.apache.spark.sql.Row] = Array([1,1.0,null], [2,2.0,false])  ```In this subquery, count(1) always evaluates to a non-null integer value, so count(1) is null is always false. The correct evaluation of the subquery is always false.We incorrectly evaluate it to null for empty groups. The reason is that NullPropagation rewrites `Aggregate [c] [isnull(count(1))]` to `Aggregate [c] [false]`, this rewrite would be correct normally, but in the context of a scalar subquery it breaks our count bug handling in RewriteCorrelatedScalarSubquery.constructLeftJoins . By the time we get there, the query appears to not have the count bug - it looks the same as if the original query had a subquery with `select any_value(false) from r...`, and that case is not subject to the count bug.Postgres comparison show correct always-false result: http://sqlfiddle.com/#!17/67822/5Solution:The reason are `NullPropagation` triggers the bug for `(count(1)) is null`, but the root cause is the wrong handling of literals when dealing with the count bug. The bug can be triggered when the scalar subquery has a global aggregate with a constant, which can be produced by NullPropagation or by the user query directly. We should make sure `resultWithZeroTups` return real value when use scalar subquery and return value are `Literal`. <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix `COUNT(*) is null bug` in correlated scalar subquery<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, some sql query result will be changed.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new tests <!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-2	### What changes were proposed in this pull request?This PR proposes to migrate UDF errors into PySpark error framework.### Why are the changes needed?To leverage the PySpark error framework so that we can provide more actionable and consistent errors for users.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?The existing CI should pass.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As title, close the barrier class `InputStream` after reading in `IsolatedClassLoader`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`IOUtils.toByteArray(inputStream)` is not responsible to close the `inputStream`, the caller should do closing instead.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-2	### What changes were proposed in this pull request?This PR proposes to introduce new error for PySpark, and also applied it to existing AssertionError from `python/pyspark/sql/connect/types.py`.### Why are the changes needed?To cover the built-in AssertionError by PySpark error framework.### Does this PR introduce _any_ user-facing change?No, it's internal error framework improvement.### How was this patch tested?The existing CI should pass.
1	-2	### What changes were proposed in this pull request?This PR proposes to migrate Expression errors into PySpark error framework.### Why are the changes needed?To leverage the PySpark error framework so that we can provide more actionable and consistent errors for users.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?The existing CI should pass.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove Hadoop2 support in `IsolatedClientLoader`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Clean up Hadoop2 related code since SPARK-42452 removed support for Hadoop2.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated test cases introduced in SPARK-32256, pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This reverts https://github.com/apache/spark/pull/36625 and its followup https://github.com/apache/spark/pull/38321 .### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->External table location can be arbitrary and has no connection with the database location. It can be wrong to qualify the external table location based on the database location.If a table written by old Spark versions does not have a qualified location, there is no way to restore it as the information is already lost. People can manually fix the table locations assuming they are under the same HDFS cluster with the database location, by themselves.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->N/A
1	-2	### What changes were proposed in this pull request?fix the incorrect doc### Why are the changes needed?the description of parameter `num` is incorrect, it actually describes the `num` in `DataFrame.{limit, tail}`### Does this PR introduce _any_ user-facing change?yes### How was this patch tested?existing UT
2	-1	### What changes were proposed in this pull request?Add `DataFrame.offset` to PySpark### Why are the changes needed?`DataFrame.offset` was supported in Scala side and Spark Connect since 3.4, but it is missing in vanilla PySpark.### Does this PR introduce _any_ user-facing change?yes, new API### How was this patch tested?added doctests
1	-2	### What changes were proposed in this pull request?Reduce the memory requirement in torch-related tests### Why are the changes needed?The computation in torch distributor actually happens in the external torch processes, and the Github Action resources is very limited, this PR tries to make related tests more stable### Does this PR introduce _any_ user-facing change?no, test-only### How was this patch tested?CI, let me keep merging oncoming commits from master to see whether this change is stable enough
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Since `LocalTableScan`/`CommandResultExec` may not trigger a Spark job, post the driver-side metrics even in scenarios where a Spark job is not triggered, so that we can track the metrics in the SQL UI tab.**LocalTableScanExec**before this PR:![截屏2023-04-20 下午6 36 47](https://user-images.githubusercontent.com/8537877/233342293-9d688705-550c-441c-a666-0e88254cd91f.png)after this PR:![截屏2023-04-20 下午6 35 19](https://user-images.githubusercontent.com/8537877/233342319-965f1ee3-3015-4e3b-b70b-25341ffa6090.png)**CommandResultExec**before this PR:![截屏2023-04-20 下午6 20 05](https://user-images.githubusercontent.com/8537877/233342423-3fcc41b8-563b-4d14-a5e7-ee9612abf7be.png)after this PR:![截屏2023-04-20 下午6 18 57](https://user-images.githubusercontent.com/8537877/233342466-c18a4e4c-34ba-46d1-a090-9d83fba63fda.png)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->makes metrics of `LocalTableScanExec`/`CommandResultExec` trackable on the SQL UI tab### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new UT.
2	-1	### What changes were proposed in this pull request?After [SPARK-42452](https://issues.apache.org/jira/browse/SPARK-42452), Spark no longer supported build&test with hadoop 2, then `ResourceRequestHelper.isYarnResourceTypesAvailable` will always return `true` with Hadoop 3 So this pr remove `isYarnResourceTypesAvailable` from  `ResourceRequestHelper` and clean up other usage of `isYarnResourceTypesAvailable`.### Why are the changes needed?Code clean up### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This pr added some mock conditions to make `specify a more specific type for the application` in `ClientSuite` pass in Hadoop 3### Why are the changes needed?Recovery Test `specify a more specific type for the application`### Does this PR introduce _any_ user-facing change?No, just for test### How was this patch tested?Manually checked `org.apache.spark.deploy.yarn.ClientSuite` 
2	-1	### What changes were proposed in this pull request?Upgrade google Tink from 1.7.0 to 1.9.0[Release note for 1.8.0](https://github.com/tink-crypto/tink-java/releases/tag/v1.8.0)[Release note for 1.9.0](https://github.com/tink-crypto/tink-java/releases/tag/v1.9.0)### Why are the changes needed?[SNYK-JAVA-COMGOOGLEPROTOBUF-3040284](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3040284)[SNYK-JAVA-COMGOOGLEPROTOBUF-3167772](https://security.snyk.io/vuln/SNYK-JAVA-COMGOOGLEPROTOBUF-3167772)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?This have be benchmarks tested With\"com.google.crypto.tink\" % \"tink\" % \"1.6.1\"(min, avg, max) = (75024163.500, 76331532.832, 77324718.069), stdev = 652319.870With\"com.google.crypto.tink\" % \"tink\" % \"1.9.0\"(min, avg, max) = (76279051.841, 77512667.749, 78590966.453), stdev = 632832.384Almost the same.. Think 1.9.0 is perhaps a bit slower.Pass GA
1	-2	### What changes were proposed in this pull request?[StructType.findNestedField](https://github.com/apache/spark/blob/db2625c70a8c3aff64e6a9466981c8dd49a4ca51/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala#L325) is unable to reach nested fields below two directly nested maps or arrays. Whenever it reaches a map or an array, it'll throw an `invalidFieldName` exception if the child is not a struct.This change updates `findNestedField` to correctly recurse into two or more levels of directly nested maps or arrays.In addition, use `checkError` in tests for `findNestedField` instead of manually checking error messages.### Why are the changes needed?`findNestedField` is used for example in [`ALTER TABLE` commands](https://github.com/apache/spark/blob/9e17731f46aa26348e69f42d1a96882186022b80/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3735). It is currently not possible to add or modify a nested field within two or more levels of directly nested maps or arrays.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Tests are added to `StructTypeSuite` to cover deeply nested maps and arrays with `findNestedField`
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate built-in `TypeError` and `ValueError` from Structured Streaming into PySpark error framework.### Why are the changes needed?To improve the errors### Does this PR introduce _any_ user-facing change?No API change. It's only error message improvements.### How was this patch tested?The existing CI should pass.
1	-2	### What changes were proposed in this pull request?Add a comment explaining a tricky situation involving the evaluation of stream-side variables.This is a follow-up to #40766.### Why are the changes needed?Make the code more clear.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?N/A
2	-1	### What changes were proposed in this pull request?Remove `isHadoop3` related checks from Spark code because Apache Spark 3.5.0 no longer supports Hadoop 2.### Why are the changes needed?Code clean up.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-2	[SPARK-43221][CORE] the BlockManager with the persisted block is preferred### What changes were proposed in this pull request? Fixed bug to avoid exceeding bounds by taking values from empty arrays### Why are the changes needed?This bug will occur with probability in certain scenarios under certain conditions:1. Not using remote shuffle2. Multiple executors on the same node3. These executors hold the same block4. Some executors store the block in memory, while others store the block in disk### Does this PR introduce _any_ user-facing change? No### How was this patch tested?existing UTs &&I will supplement UT for this scenario
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->We want to allow the parameterization of identifiers in SQL statements without compromising security.Right now this can only be done using ${..}  variable substitution which allows for arbitrary text substitution.The improvement proposed here is to use an IDENTIFIER(<stringliteral>) clause which produces a possibly qualified identifier. That way any text substitution from a client or using ${..} can be safely scoped.Example:SET hivevar:tab = 'mytab';SELECT * FROM IDENTIFIER(${hivevar:tab});==>SELECT * FROM mytab;Note that the IDENTIFIER(<literal) syntax originates from Snowflake.There is no other vendor I am aware of with similar capability.[Link to a more detailed specification](https://docs.google.com/document/d/14BhSZFeDoK-iZa7-66PjlHQ4nYLpdePDWNSW3kkLZAs/edit?usp=sharing)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To protect scripts that substitute table or column names from SQL injection attacks.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this is a new SQL feature.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New SQL tests have been added.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?File-source constant metadata columns are often derived indirectly from file-level metadata values rather than exposing those values directly. Add support for metadata extractors, so that we can express such columns in a generic way.### Why are the changes needed?Allows to express the existing file-source metadata columns in a generic way (previously hard-wired), and also allows to lazily derive expensive metadata values only if the query actually selects them.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit test. Plus, existing file-source metadata unit tests pass.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the table() API for scala client.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of SS Connect development.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit testI also changed `ProtoToParsedPlanTestSuite` a little to remove the memory addresses, before the change the test for streaming table would fail with:```- streaming_table_API_with_options *** FAILED *** (8 milliseconds)[info]   Expected and actual plans do not match:[info]   [info]   === Expected Plan ===[info]   SubqueryAlias primary.tempdb.myStreamingTable[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@752725d9, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@347d8e2a, tempdb.myStreamingTable[info]   [info]   [info]   === Actual Plan ===[info]   SubqueryAlias primary.tempdb.myStreamingTable[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@a88a5db, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@2c6b362e, tempdb.myStreamingTable```Because the memory address (`InMemoryTable@752725d9`) is different every time it runs. I removed these in the test suite.And verified that memory addresses doesn't exist in existing explain files:```wei.liu:~/oss-spark$ cat connector/connect/common/src/test/resources/query-tests/explain-results/* | grep @wei.liu:~/oss-spark$ ```
2	-3	### What changes were proposed in this pull request?Fixes deduplicate field names, and refactor to use the same renaming rule between `ArrowTableToRowsConversion.convert` and `LocalDataToArrowConversion.convert`.### Why are the changes needed?If there is a duplicated field name in a separate position, it fails to deduplicate and returns a wrong result.```py>>> from pyspark.sql.types import *>>> data = [...     Row(Row(\"a\ 1), Row(2, 3, \"b\ 4, \"c\ \"d\")),...     Row(Row(\"w\ 6), Row(7, 8, \"x\ 9, \"y\ \"z\")),... ]>>> schema = (...     StructType()...     .add(\"struct\ StructType().add(\"x\ StringType()).add(\"x\ IntegerType()))...     .add(...         \"struct\...         StructType()...         .add(\"a\ IntegerType())...         .add(\"x\ IntegerType())...         .add(\"x\ StringType())...         .add(\"y\ IntegerType())...         .add(\"y\ StringType())...         .add(\"x\ StringType()),...     )... )>>> df = spark.createDataFrame(data, schema=schema)>>>>>> df.collect()[Row(struct=Row(x='a', x=1), struct=Row(a=2, x=None, x=None, y=4, y='c', x=None)), Row(struct=Row(x='w', x=6), struct=Row(a=7, x=None, x=None, y=9, y='y', x=None))]```It should be:```py>>> df.collect()[Row(struct=Row(x='a', x=1), struct=Row(a=2, x=3, x='b', y=4, y='c', x='d')), Row(struct=Row(x='w', x=6), struct=Row(a=7, x=8, x='x', y=9, y='y', x='z'))]```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Updated the related test.
1	-2	### What changes were proposed in this pull request?backporting https://github.com/apache/spark/pull/39152 to 3.3### Why are the changes needed?bug fixing### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?`INSERT INTO REPLACE` statement be supported in #38404 , but can't be found in website<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add doc for feature we supported.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?![image](https://user-images.githubusercontent.com/32387433/233919892-28131da8-0b71-4203-baaf-f7ef768757d9.png)![image](https://user-images.githubusercontent.com/32387433/233919941-e9e16931-075d-4ba6-85e6-61125fa60eaa.png)<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Improve code style, use renamed import statement for Hadoop classes### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Address comment from @mridulm https://github.com/apache/spark/pull/40850/files#r1172865732> nit: Instead of using the fully qualified name (which made sense in reflection code earlier), we should have used a renamed import statement.>> Something like:>> ```> import org.apache.hadoop.ipc.{CallerContext => HadoopCallerContext}> import org.apache.hadoop.ipc.CallerContext.{Builder => HadoopCallerContextBuilder}> ```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-1	### What changes were proposed in this pull request?This pr  add support to make `recentProgress` and `lastProgress` in `RemoteStreamingQuery` return `StreamingQueryProgress` instance consistent with the native Scala Api.### Why are the changes needed?Add Spark connect jvm client api coverage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Add new check to `StreamingQuerySuite`
2	-1	### What changes were proposed in this pull request?- Remove `jackson-core-asl` from maven dependency.- Change the scope of `jackson-mapper-asl` from compile to test.- Replace all `Hive.get(conf)` with `Hive.getWithoutRegisterFns(conf)`.### Why are the changes needed?To fix CVE issue: https://github.com/apache/spark/security/dependabot/50.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?manual test.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes the ammonite REPL use the `CodeClassWrapper` mode for classfile generation (make ammonite generate classes instead of objects) and changes the UDF serialization from lazy to eager.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The changes have the following impact:- `CodeClassWrapper` change  - Fixes the `io.grpc.StatusRuntimeException: UNKNOWN: ammonite/repl/ReplBridge$` error when trying to use the `filter` method (see [jira](https://issues.apache.org/jira/browse/SPARK-43198) for reproduction)- Lazy to eager UDF serialization  - With class-based generation, UDFs defined using `def` would hit CNFE because the `ScalarUserDefinedFuntion` class gets captured during serialisation and sent over to the server (the class is a client-only class).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behaviour and the change this PR proposes - provide the console output, description and/or an example to show the behaviour difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. There are two significant changes:- Filter works as expected when using \"in-place\" lambda expressions such as in `spark.range(10).filter(n => n % 2 == 0).collectAsList()`- UDFs defined using a lambda expression which is stored in a `val` fail due to deserialisation issues on the server.  - Root cause is currently unknown but a ticket has been [filed](https://issues.apache.org/jira/browse/SPARK-43227) to address the issue.  - Example: see [this](https://github.com/apache/spark/compare/master...vicennial:spark:SPARK-43198?expand=1#diff-8d8a214eff5d2c8d523b59f2a39758ddfa84912ef7d4e0276f54e979a58f88e0R120-R129) test.  - Currently, it is a compromise to get `filter` working as expected since that bug is a higher-impact due to it impacting the \"general\" way of using the method.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Implemented StreamingQueryProgress for Spark Connect. Also added related structs under the legacy `StreamingQueryProgress`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since Spark Connect transfers streaming progress as full “json”, implemented `fromJson` method to deserialize the json string to the legacy `StreamingQueryProgress`. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added new unit test suite.
2	-3	### What changes were proposed in this pull request?This PR reverts changes from [SPARK-42896](https://issues.apache.org/jira/browse/SPARK-42896) and [SPARK-42929](https://issues.apache.org/jira/browse/SPARK-42929) which made `mapInPandas` and `mapInArrow` support barrier mode execution via adding a `is_barrier` parameter.Instead, this PR add `is_barrier` as a optional function attribute, so that the barrier python UDF can be supported in physical operators other than `MapInBatchExec`.Right now I want to narrow its usage scope because it is only used in ML:- only supported in Pandas UDF;- only supported in `MapInPandas` and `MapInArrow`;- can not be registered;This PR will not add a user-facing API or Parameter or Annotation, instead only a _private_ function attribute will be added, that is because:- Right now it is only needed to integrate external ML training frameworks like PyTorch and XGBoost, which just run `collect` after the UDF execution to obtain the model coefficients;- The limitation of existing barrier mode: many RDD operations are not supported, see [1](https://github.com/apache/spark/blob/1a6b1770c85f37982b15d261abf9cc6e4be740f4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L474-L488) and [2](https://github.com/apache/spark/blob/485145326a9c97ede260b0e267ee116f182cfd56/core/src/main/scala/org/apache/spark/scheduler/BarrierJobAllocationFailed.scala#L42-L65) . And it is non-trivial to make `RDDBarrier` fully compatible with `RDD`. A simple example to illustrate this problem:```In [1]: df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\ \"age\"))In [2]: def filter_func(iterator):   ...:     for pdf in iterator:   ...:         yield pdf[pdf.id == 1]   ...: In [3]: df.mapInPandas(filter_func, df.schema).explain()== Physical Plan ==MapInPandas filter_func(id#0L, age#1L)#4, [id#5L, age#6L], false+- *(1) Scan ExistingRDD[id#0L,age#1L]In [4]: df.mapInPandas(filter_func, df.schema).show()+---+---+                                                                       | id|age|+---+---+|  1| 21|+---+---+In [5]: df.mapInPandas(filter_func, df.schema, barrier=True).explain()== Physical Plan ==MapInPandas filter_func(id#0L, age#1L)#23, [id#24L, age#25L], true+- *(1) Scan ExistingRDD[id#0L,age#1L]In [6]: df.mapInPandas(filter_func, df.schema, barrier=True).show()23/04/26 15:39:57 WARN DAGScheduler: Creating new stage failed due to exception - job: 3org.apache.spark.scheduler.BarrierJobUnsupportedRDDChainException: [SPARK-24820][SPARK-24821]: Barrier execution mode does not allow the following pattern of RDD chain within a barrier stage:1. Ancestor RDDs that have different number of partitions from the resulting RDD (e.g. union()/coalesce()/first()/take()/PartitionPruningRDD). A workaround for first()/take() can be barrierRdd.collect().head (scala) or barrierRdd.collect()[0] (python).2. An RDD that depends on multiple barrier RDDs (e.g. barrierRdd1.zip(barrierRdd2)).        at org.apache.spark.errors.SparkCoreErrors$.barrierStageWithRDDChainPatternError(SparkCoreErrors.scala:225)        at org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithRDDChainPattern(DAGScheduler.scala:486)        at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:635)        at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1254)        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2961)        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2953)        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2942)        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)```### Why are the changes needed?To make barrier python UDF able to be supported in other dataframe operator in the future, in the mean time, do not expose it to the end users### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing UTs
2	-2	### What changes were proposed in this pull request?This PR updates `CoalesceBucketsInJoin.satisfiesOutputPartitioning` to support matching `PartitioningCollection`. A common case is that we add an alias on the join key. For example:```sqlSELECT *FROM   (SELECT /*+ BROADCAST(t3) */ t1.i AS t1i, t1.j AS t1j, t3.*        FROM   t1 JOIN t3 ON t1.i = t3.i AND t1.j = t3.j) t       JOIN t2 ON t.t1i = t2.i AND t.t1j = t2.j```The left side outputPartitioning is:```(hashpartitioning(t1i#41, t1j#42, 8) or hashpartitioning(i#46, t1j#42, 8) or hashpartitioning(t1i#41, j#47, 8) or hashpartitioning(i#46, j#47, 8))```### Why are the changes needed?Enhance `CoalesceBucketsInJoin` to support more cases.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit test.
1	-1	### What changes were proposed in this pull request?add a helper function in `DataFrameNaFunctions`### Why are the changes needed?to Simplify `DataFrameNaFunctions.fillna`### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?existing UTs
1	-1	### What changes were proposed in this pull request?This patch fixes a minor issue in the code where for SQL Commands the plan metrics are not sent to the client. In addition, it renames a method to make clear that the method does not actually send anything but only creates the response object.### Why are the changes needed?Clarity### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->It's not necessary to check Hadoop version 2.9+ or 3.0+ now.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify code and docs.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-3	### What changes were proposed in this pull request?This pr aims to fix mima check for Scala 2.13.### Why are the changes needed?Scala 2.13 daily test failed due to mima check failed:- https://github.com/apache/spark/actions/runs/4757950254/jobs/8455416083<img width=\"1668\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/233678713-30c661e6-d5d0-424a-8b7f-c8a59bb0680a.png\">### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual check with this pr```dev/change-scala-version.sh 2.13dev/mima -Pscala-2.13```check passed```[success] Total time: 39 s, completed 2023-4-21 23:41:52```
3	-3	Re-attempting #40794. #40794 tried to more safely create `AttributeReference` objects from multi-part attributes in `ImplicitAttribute`. But that broke things and we had to revert. This PR is limiting the fix to the `UnresolvedAttribute` object returned by `DslAttr.attr`, which is enough to fix the issue here.### What changes were proposed in this pull request?This PR fixes DSL expressions on attributes with special characters by making `DslAttr.attr` and `DslAttr.expr` return the implicitly wrapped attribute instead of creating a new one.### Why are the changes needed?SPARK-43142: DSL expressions on attributes with special characters don't work even if the attribute names are quoted:```scalascala> \"`slashed/col`\".attrres0: org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute = 'slashed/colscala> \"`slashed/col`\".attr.ascorg.apache.spark.sql.catalyst.parser.ParseException:mismatched input '/' expecting {<EOF>, '.', '-'}(line 1, pos 7)== SQL ==slashed/col-------^^^```DSL expressions rely on a call to `expr` to get child of the new expression [(e.g.)](https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L149).`expr` here is a call on implicit class `DslAttr` that's wrapping the `UnresolvedAttribute` returned by `\"...\".attr` is wrapped by the implicit class `DslAttr`.`DslAttr` and its super class implement `DslAttr.expr` such that a new `UnresolvedAttribute` is created from `UnresolvedAttribute.name` of the wrapped attribute [(here)](https://github.com/apache/spark/blob/87a5442f7ed96b11051d8a9333476d080054e5a0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L273-L280).But `UnresolvedAttribute.name` drops the quotes and thus the newly created `UnresolvedAttribute` parses an identifier that should be quoted but isn't:```scalascala> \"`col/slash`\".attr.nameres5: String = col/slash```### Does this PR introduce _any_ user-facing change?DSL expressions on attributes with special characters no longer fail.### How was this patch tested?I couldn't find a suite testing the implicit classes in the DSL package, but the DSL package seems used widely enough that I'm confident this doesn't break existing behavior.Locally, I was able to reproduce with this test; it was failing before and passes now:```scalatest(\"chained DSL expressions on attributes with special characters\") {  $\"`slashed/col`\".asc}```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The current implementation of AES-CBC mode called via `aes_encrypt` and `aes_decrypt` uses a key derivation function (KDF) based on OpenSSL's [EVP_BytesToKey](https://www.openssl.org/docs/man3.0/man3/EVP_BytesToKey.html). This is intended for generating keys based on passwords and OpenSSL's documents discourage its use: \"Newer applications should use a more modern algorithm\".`aes_encrypt` and `aes_decrypt` should use the key directly in CBC mode, as it does for both GCM and ECB mode. The output should then be the initialization vector (IV) prepended to the ciphertext – as is done with GCM mode:`[16-byte randomly generated IV | AES-CBC encrypted ciphertext]`### Why are the changes needed?We want to have the ciphertext output similar across different modes. OpenSSL's EVP_BytesToKey is effectively deprecated and their own documentation says not to use it. Instead, CBC mode will generate a random vector.### Does this PR introduce _any_ user-facing change?AES-CBC output generated by the previous format will be incompatible with this change. That change was recently landed and we want to land this before CBC mode is used in practice.### How was this patch tested?A new unit test in `DataFrameFunctionsSuite` was added to test both GCM and CBC modes. Also, a new standalone unit test suite was added in `ExpressionImplUtilsSuite` to test all the modes and various key lengths.CBC values can be verified with `openssl enc` using the following command:```echo -n \"[INPUT]\" | openssl enc -a -e -aes-256-cbc -iv [HEX IV] -K [HEX KEY]echo -n \"Spark\" | openssl enc -a -e -aes-256-cbc -iv f8c832cc9c61bac6151960a58e4edf86 -K 6162636465666768696a6b6c6d6e6f7031323334353637384142434445464748```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->POC for foreachbatch spark connect```>>> def foreach_batch_function(df, epoch_id):...   from pyspark.sql.functions import col, lit...   count = df.count()...   print(\"##### count is \ count)...   df.withColumn('new_column', lit(10)).write.mode('append').saveAsTable('test_foreachbatch_1')...>>> query = (...  spark...  .readStream...  .format(\"rate\")...  .option(\"numPartitions\ \"1\")...  .load()...  .writeStream...  .foreachBatch(foreach_batch_function)...  .start()... )>>>>>> query.status{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}>>>>>>>>> spark.sql(\"select * from test_foreachbatch_1\").show()+--------------------+-----+----------+|           timestamp|value|new_column|+--------------------+-----+----------+|2023-04-21 14:01:...|   29|        10||2023-04-21 14:01:...|    6|        10||2023-04-21 14:01:...|   21|        10||2023-04-21 14:01:...|   16|        10||2023-04-21 14:01:...|   11|        10||2023-04-21 14:01:...|   26|        10||2023-04-21 14:01:...|   32|        10||2023-04-21 14:01:...|   33|        10||2023-04-21 14:01:...|    4|        10||2023-04-21 14:01:...|    5|        10||2023-04-21 14:01:...|   17|        10||2023-04-21 14:01:...|   18|        10||2023-04-21 14:01:...|   24|        10||2023-04-21 14:01:...|   25|        10||2023-04-21 14:01:...|   30|        10||2023-04-21 14:01:...|   31|        10||2023-04-21 14:01:...|    7|        10||2023-04-21 14:01:...|    8|        10||2023-04-21 14:01:...|    0|        10||2023-04-21 14:01:...|    1|        10|+--------------------+-----+----------+only showing top 20 rows```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?We add a logging when creating the batch reader with task ID, topic, partition and offset range included.The log line looks like following:23/04/18 22:35:38 INFO KafkaBatchReaderFactory: Creating Kafka reader partitionId=1 partition=StreamingDustTest-KafkaToKafkaTopic-4ccf8662-c3ca-4f3b-871e-1853c0e61765-source-2 fromOffset=0 untilOffset=3 queryId=b5b806c3-ebf3-432e-a9a7-d882d474c0f5 batchId=0 taskId=1### Why are the changes needed?Right now, for structure streaming from Kafka, it's hard to finding which task handling which topic/partition and offset range. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Run KafkaMicroBatchV2SourceSuite and watch logging outputs contain information needed. Also does a small cluster test and observe logs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add StreamingQuery exception() API for JVM client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Development of SS Connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual test:```Spark session available as 'spark'.   _____                  __      ______                            __  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/ ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/    /_/@  val q = spark.readStream.format(\"rate\").load().writeStream.option(\"checkpointLocation\ \"/home/wei.liu/ckpt\").toTable(\"my_table\")  q: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.streaming.RemoteStreamingQuery@772f3a3f@ q.exception res1: Option[org.apache.spark.sql.streaming.StreamingQueryException] = None@ q.stop() ```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Override the parent `__dir__()` method on Python `DataFrame` class to include column names. Main benefit of this is that any autocomplete engine that uses `dir()` to generate autocomplete suggestions (e.g. IPython kernel, Databricks Notebooks) will suggest column names on the completion `df.|`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To keep `__dir__()` consistent with `__getattr__()`. So this is arguably a bug fix. Increases productivity for anyone who uses an autocomplete engine on pyspark code.Example of column attribute completion coming for free after this change:https://user-images.githubusercontent.com/84545946/233747057-56b2589d-d075-4d13-8349-ac5142c38c62.mov### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Will change the output of `dir(df)`. If the user chooses to use the private method `df.__dir__()`, they will also notice an output and docstring difference there.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New doctest with three assertions. Output where I only ran this test:![pyspark test passed](https://user-images.githubusercontent.com/84545946/233744674-b59191a7-08bf-4f3e-a491-945e687727b0.png)To test it in a notebook:```pythonfrom pyspark.sql.dataframe import DataFrameclass DataFrameWithColAttrs(DataFrame):    def __init__(self, df):        super().__init__(df._jdf, df._sql_ctx if df._sql_ctx else df._session)    def __dir__(self):        attrs = super().__dir__()        attrs.extend(attr for attr in self.columns if attr not in attrs)        return attrs```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?In some use cases, users have incoming dataframes with fixed column names which might differ from the canonical order. Currently there's no way to handle this easily through the INSERT INTO API - the user has to make sure the columns are in the right order as they would when inserting a tuple. We should add an optional BY NAME clause, such that:`INSERT INTO tgt BY NAME <query>`takes each column of <query> and inserts it into the column in `tgt` which has the same name according to the configured `resolver` logic.At now don't support `INSERT OVERWRITE BY NAME`<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add new feature `INSERT INTO BY NAME`<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	### What changes were proposed in this pull request?Following describes the changes made, all changes are behind respective configuration properties1. Followed the same model as driver to create svc records for executors as well. The lifecycle of the SVC record is tied to executor lifecycle. While registering with drivers, executors now supply their SVC hostname. **Controlled by a new configuration (added as part of this PR): `spark.kubernetes.executor.service`**    ![exec_service](https://user-images.githubusercontent.com/3784871/233761856-f135c726-9c90-4a44-bcac-84ce97f09b9d.png)    2. Allowed drivers and executors to bind to all IPs. **Controlled by existing properties `spark.driver.bindAddress` and `spark.executor.bindAddress`. This PR makes `0.0.0.0` a permissible value**    ![bind_address](https://user-images.githubusercontent.com/3784871/233761913-f763a0f0-bccf-4743-871c-f982b93cf7ba.png)    3. Added support for providing    1. pre start script: that would be run before driver/executor JVM gets started. This script can do any setup e.g. waiting for istio-proxy sidecar to be up.    2. post stop script: that would be run after driver/executor JVM completes. This script can do any cleanup example in our case it makes a REST call to shutdown sidecar.These scripts are not part of the PR because the onus of providing any specialized cleanup would lie with the client. In our case it is provided by Proton. **Controlled by new configurations (added as part of this PR): `spark.kubernetes.post.stop.script`, `spark.kubernetes.pre.start.script` which when set will be executed before and after the driver/executor JVM**![sidecar_termination](https://user-images.githubusercontent.com/3784871/233762111-9251aa14-87a7-4339-8549-45b4ae1e06dc.png)### Why are the changes needed?Spark allows using Kubernetes as the resource scheduler however off the shelf does not work with Kubernetes cluster using Istio service mesh in strict MTLS mode because:1. For Istio to work, it needs to know the network identity of all possible network paths. Currently network identity (through a K8s service record) is created only for the driver pod but not for executors.2. Istio adds a istio-proxy sidecar to every pod and this sidecar handles all pod to pod networking. However the sidecar binds to Pod IP and then sends ingress traffic to localhost (if PILOT_ENABLE_INBOUND_PASSTHROUGH is set to false). Therefore for ingress traffic to correctly reach application processes (like driver and executor JVMs), the processes need to bind to all IPs and not just Pod IP, as otherwise, traffic routed to localhost by the sidecar would not reach the application processes. Off the shelf Spark allows driver and executors to only bind to Pod IP and therefore does not work with Istio.3. Unlike the Istio sidecar, driver/executor containers in the pod can finish. In which case a pod would enter NotReady state (as driver/executor containers can complete) while sidecar would continue to run. Therefore once the driver/executor containers are done, they need to signal to the istio sidecar as well to terminate.### Does this PR introduce *any* user-facing change?Yes, it adds configs that can be used to run on an K8s cluster using Istio service mesh, with strict MTLS.### How was this patch tested?- Added new unit tests- Tested on a strict MTLS Istio Kubernetes cluster.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate ValueError into PySparkValueError from Spark Connect DataFrame.### Why are the changes needed?To improve the errors from Spark Connect.### Does this PR introduce _any_ user-facing change?No, it's error improvements.### How was this patch tested?The existing tests should pass
1	-2	### What changes were proposed in this pull request?Handle null exception message in event log### Why are the changes needed?NPE error when handling null exception message in event log### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added test in JsonProtocolSuite
3	-2	### What changes were proposed in this pull request?Support only decommission idle workers in standalone### Why are the changes needed?Currently, standalone master web ui supports kill/decommission workers. But when graceful decommission is enabled, running task, shuffle and rdd migration could take long time. While waiting for running task, shuffle and rdd migration, decommissioned workers can't run new executors and decommissioned executors can't run new tasks. This caused lot of resource waste.If only idle workers to be decommissioned, these workers could be shutdown and removed to save cost without waiting long decommissioning process.### Does this PR introduce _any_ user-facing change?Yes. POST http://masterWebUI:port/workers/kill/idleOnly=trueDefault: false### How was this patch tested?Added test in MasterSuite
1	-2	### What changes were proposed in this pull request?Remove `null_counts` from info()### Why are the changes needed?Pandas 2.0 _Removed deprecated null_counts argument in [DataFrame.info()](https://pandas.pydata.org/pandas-docs/version/2.0/reference/api/pandas.DataFrame.info.html#pandas.DataFrame.info). Use show_counts instead ([GH37999](https://github.com/pandas-dev/pandas/issues/37999))_### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Tested local ### Before this PR`F05.info()````TypeError                                 Traceback (most recent call last)Cell In[12], line 1----> 1 F05.info()File /opt/spark/python/pyspark/pandas/frame.py:12167, in DataFrame.info(self, verbose, buf, max_cols, null_counts)  12163     count_func = self.count  12164     self.count = (  # type: ignore[assignment]  12165         lambda: count_func()._to_pandas()  # type: ignore[assignment, misc, union-attr]  12166     )> 12167     return pd.DataFrame.info(  12168         self,  # type: ignore[arg-type]  12169         verbose=verbose,  12170         buf=buf,  12171         max_cols=max_cols,  12172         memory_usage=False,  12173         null_counts=null_counts,  12174     )  12175 finally:  12176     del self._dataTypeError: DataFrame.info() got an unexpected keyword argument 'null_counts'```### With this PR`F05.info()````<class 'pyspark.pandas.frame.DataFrame'>Int64Index: 5257 entries, 0 to 5256Data columns (total 203 columns): #    Column                                                               Non-Null Count  Dtype  ---   ------                                                               --------------  -----   0    DOFFIN_APPENDIX:EXPRESSION_OF_INTEREST_URL                           471 non-null    object(...)```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The df.describe() method will cached the RDD.  And if the cached RDD is RDD[Unsaferow], which may be released after the row is used, then the result will be wong. Here we need to copy the RDD before caching as the [TakeOrderedAndProjectExec ](https://github.com/apache/spark/blob/d68d46c9e2cec04541e2457f4778117b570d8cdb/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala#L204)operator does.### Why are the changes needed?bug fix### Does this PR introduce _any_ user-facing change?no### How was this patch tested?
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The `ObjectHashAggregateExec` has three preformance issues:- heavy overhead of scala sugar in `createNewAggregationBuffer`- unnecessary grouping key comparation after fallback to sort based aggregator- the aggregation buffer in sort based aggregator is not reused for all rest rowsThen the performance is poor with high cardinality grouping keys.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve performance### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The test should be covered by `org.apache.spark.sql.execution.aggregate.SortBasedAggregationStoreSuite`Add benchmark for high cardinality case. Note, in this case the performance with no fallback is slower than others.```sql-- 10 * 1000 * 1000 rowsdf.groupBy(\"key1\ \"key2\")  .agg(sum($\"value\"), count($\"value\"), avg($\"value\"), max($\"value\"), min($\"value\"),    collect_set($\"value\"))  .noop()```before```OpenJDK 64-Bit Server VM 1.8.0_322-b06 on Mac OS X 13.2Apple M1 Proobject agg v.s. sort agg with high cardinality:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative----------------------------------------------------------------------------------------------------------------------------------------------sort agg                                                 4997           5099         118          2.0         499.7       1.0Xobject agg w/o fallback                                 12634          12944         311          0.8        1263.4       0.4Xobject agg w/ fallback                                   8792           8871          84          1.1         879.2       0.6X```after```OpenJDK 64-Bit Server VM 1.8.0_322-b06 on Mac OS X 13.2Apple M1 Proobject agg v.s. sort agg with high cardinality:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------------sort agg                                                 4831           5060         199          2.1         483.1       1.0Xobject agg w/o fallback                                  8971           9257         259          1.1         897.1       0.5Xobject agg w/ fallback                                   5612           5706         102          1.8         561.2       0.9X```
2	-2	### What changes were proposed in this pull request?This feature parity improvement and to add **level** param to df.printSchema for Python API (PySpark & Connect)## Connect APIExamples:```root@f53642e9adb0:/home/spark# bin/pyspark --remote \"local[*]\"Python 3.9.5 (default, Nov 23 2021, 15:27:38) [GCC 9.3.0] on linuxType \"help\ \"copyright\ \"credits\" or \"license\" for more information.Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/04/23 13:31:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.5 (default, Nov 23 2021 15:27:38)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> df = spark.createDataFrame([(1, (2,2))], [\"a\ \"b\"])>>> df.printSchema(1)root |-- a: long (nullable = true) |-- b: struct (nullable = true)>>> df.printSchema(2)root |-- a: long (nullable = true) |-- b: struct (nullable = true) |    |-- _1: long (nullable = true) |    |-- _2: long (nullable = true)>>> df.printSchema(3)root |-- a: long (nullable = true) |-- b: struct (nullable = true) |    |-- _1: long (nullable = true) |    |-- _2: long (nullable = true) >>> df.printSchema()root |-- a: long (nullable = true) |-- b: struct (nullable = true) |    |-- _1: long (nullable = true) |    |-- _2: long (nullable = true)```## PySpark```root@f53642e9adb0:/home/spark# bin/pyspark                    Python 3.9.5 (default, Nov 23 2021, 15:27:38) [GCC 9.3.0] on linuxType \"help\ \"copyright\ \"credits\" or \"license\" for more information.Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/04/23 13:36:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableWelcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0-SNAPSHOT      /_/Using Python version 3.9.5 (default, Nov 23 2021 15:27:38)Spark context Web UI available at http://localhost:4040Spark context available as 'sc' (master = local[*], app id = local-1682257002957).SparkSession available as 'spark'.>>> df = spark.createDataFrame([(1, (2,2))], [\"a\ \"b\"])>>> df.printSchema(1)root |-- a: long (nullable = true) |-- b: struct (nullable = true)>>> df.printSchema(2)root |-- a: long (nullable = true) |-- b: struct (nullable = true) |    |-- _1: long (nullable = true) |    |-- _2: long (nullable = true)>>> df.printSchema(3)root |-- a: long (nullable = true) |-- b: struct (nullable = true) |    |-- _1: long (nullable = true) |    |-- _2: long (nullable = true)>>> df.printSchema(0)root |-- a: long (nullable = true) |-- b: struct (nullable = true) |    |-- _1: long (nullable = true) |    |-- _2: long (nullable = true)```### Why are the changes needed?Feature parity### Does this PR introduce _any_ user-facing change?Yes### How was this patch tested?Existing and new test cases
3	-3	### What changes were proposed in this pull request?This is a back-port of #40766 and #40881.In `JoinCodegenSupport#getJoinCondition`, evaluate any referenced stream-side variables before using them in the generated code.This patch doesn't evaluate the passed stream-side variables directly, but instead evaluates a copy (`streamVars2`). This is because `SortMergeJoin#codegenFullOuter` will want to evaluate the stream-side vars within a different scope than the condition check, so we mustn't delete the initialization code from the original `ExprCode` instances.### Why are the changes needed?When a bound condition of a full outer join references the same stream-side column more than once, wholestage codegen generates bad code.For example, the following query fails with a compilation error:```create or replace temp view v1 asselect * from values(1, 1),(2, 2),(3, 1)as v1(key, value);create or replace temp view v2 asselect * from values(1, 22, 22),(3, -1, -1),(7, null, null)as v2(a, b, c);select *from v1full outer join v2on key = aand value > band value > c;```The error is:```org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 277, Column 9: Redefinition of local variable \"smj_isNull_7\"```The same error occurs with code generated from ShuffleHashJoinExec:```select /*+ SHUFFLE_HASH(v2) */ *from v1full outer join v2on key = aand value > band value > c;```In this case, the error is:```org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 174, Column 5: Redefinition of local variable \"shj_value_1\"```Neither `SortMergeJoin#codegenFullOuter` nor `ShuffledHashJoinExec#doProduce` evaluate the stream-side variables before calling `consumeFullOuterJoinRow#getJoinCondition`. As a result, `getJoinCondition` generates definition/initialization code for each referenced stream-side variable at the point of use. If a stream-side variable is used more than once in the bound condition, the definition/initialization code is generated more than once, resulting in the \"Redefinition of local variable\" error.In the end, the query succeeds, since Spark disables wholestage codegen and tries again.(In the case other join-type/strategy pairs, either the implementations don't call `JoinCodegenSupport#getJoinCondition`, or the stream-side variables are pre-evaluated before the call is made, so no error happens in those cases).### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Add shuffle sort merge join to RDD API. <!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Shuffle sort merge join is the default join strategy in Spark SQL and the RDD API should have an equivalent method for users to use depending on workload. <!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?New methods are exposed to users on RDDs. <!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tests are added to PairRDDFunctionsSuite.scala. And all pass. <!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR extends `ResolveRowLevelCommandAssignments` to also cover MERGE assignments.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Similar to SPARK-42151, these changes are needed so that we can rewrite MERGE statements into executable plans for tables that support row-level operations. In particular, our row-level mutation framework assumes Spark is responsible for building an updated version of each affected row and that row is passed back to the data source.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove unnecessary serialize/deserialize of `Path` on parallel gather partition stats.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Simplify code, since `Path` is serializable in Hadoop3.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
1	-2	### What changes were proposed in this pull request?A minor bugfix in `ShuffleBlockFetcherIterator.diagnose`, which not handle type ShuffleBlockBatchId properly### Why are the changes needed?`.diagnose()` is used in exception handling try-catch block, throw new exception due to type mismatch (in this case, `ShuffleBlockBatchId`) will swallow original exception stack### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing tests
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/40699 to avoid changing the Cast behavior. It pulls out the cast-to-string code into a base trait, and add a new Expression `ToPrettyString` to extend this trait with a little customization.It also handles binary value inside array/struct/map to also print hex format, for `df.show` only, not `Cast`.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->avoid behavior change### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->change back the behavior of casting array/map/struct to string regarding null elements. It was `null`, then changed to `NULL` in #40699 , and is `null` again after this PR.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
3	-2	### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes to migrate the Spark SQL pandas arrow type errors into error class.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Leveraging the PySpark error framework.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass
2	-2	### What changes were proposed in this pull request?The main change of this pr as follows:1. Like `dev/mima`, use `GenerateMIMAIgnore` to generate `.generated-mima-member-excludes` and `.generated-mima-class-excludes`2. Make `CheckConnectJvmClientCompatibility` use `.generated-mima-member-excludes` and `.generated-mima-class-excludes` as default `ProblemFilters` to excludes `privateClasses` and `privateMembers` from mima check result.3. Ignore the `ProblemFilters` add by https://github.com/apache/spark/pull/40898### Why are the changes needed?Ignore privateClasses and privateMembers from connect mima check as default### Does this PR introduce _any_ user-facing change?No### How was this patch tested?`CheckConnectJvmClientCompatibility` should pass
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate `TypeError` from Spark SQL types into error class.### Why are the changes needed?To improve PySpark error### Does this PR introduce _any_ user-facing change?No API change, only error improvement.### How was this patch tested?The existing CI should pass
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is follow-up for https://github.com/apache/spark/pull/39991 to remove unused exception.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`PySparkTypeError` never raises because we checked the type already in the codes above `if type(startPos) != type(length):` and raise `PySparkTypeError` for unsupported type for `length`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No. it's minor code cleanup### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate built-in `TypeError` and `ValueError` from Spark Connect Structured Streaming into PySpark error framework.### Why are the changes needed?To leverage the PySpark error framework for Spark Connect Structured Streaming### Does this PR introduce _any_ user-facing change?No API changes, only error improvements.### How was this patch tested?The existing CI should pass.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds lazy allocation support for the backing array of ColumnVector used in Spark VectorizedReader.  The scope of this change includes:- Simplify `OnHeapColumnVector` to only use a single-byte array for various data types.- Introduced lazy loading to the `data` array, e.g.: allocating the array only on the first write. - Changed `nulls` byte array to use `BitSet` for a smaller memory footprint and faster batch operations.Out of scope of this PR:- Lazy allocation support for `OffHeapColumnVector`.### Why are the changes needed?This PR is added as a memory optimization for addressing the memory utilization issue when reading a Parquet file with large but sparse columns.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?Existing tests:* ColumnarBatchSuite* ColumnVectorSuite* ColumnVectorUtilsSuite* ConstantColumnVectorSuiteManual tests:Tested on reading a Parquet file with a large nested struct with an array with 16GB executors. This patch fixed the OutOfMemory exception.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Experimental PR in response to https://github.com/apache/spark/pull/40885#discussion_r1174277575, so that reviewers can visualize the difference between the two approaches discussed there. NOT FOR MERGE -- If this approach is chosen, I will update the other PR accordingly. Only the last commit differs from the other PR.### Why are the changes needed?N/A### Does this PR introduce _any_ user-facing change?No### How was this patch tested?N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Move Error framework to a common utils module so that we can share it between Spark and Spark Connect without introducing heavy dependencies on Spark Connect module.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Reduce Dependencies on Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Error framework is internally API so this should be fine.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT.
1	-1	### What changes were proposed in this pull request?This PR moves `MergeScalarSubqueries` from `spark-catalyst` to `sparl-sql`### Why are the changes needed?Make  SPARK-40193 / https://github.com/apache/spark/pull/37630 easier.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing UTs.
2	-2	### What changes were proposed in this pull request?Upgrade FasterXML jackson from 2.14.2 to 2.15.0### Why are the changes needed?Upgrade Snakeyaml to 2.0 (resolves CVE-2022-1471 [CVE-2022-1471 at nist](https://nvd.nist.gov/vuln/detail/CVE-2022-1471)### Does this PR introduce _any_ user-facing change?This PR introduces user-facing changes by implementing streaming read constraints in the JSONOptions class. The constraints limit the size of input constructs, improving security and efficiency when processing input data.Users working with JSON data larger than the following default settings may need to adjust the constraints accordingly:Maximum Number value length: 1000 characters (`DEFAULT_MAX_NUM_LEN`)Maximum String value length: 5,000,000 characters (`DEFAULT_MAX_STRING_LEN`)Maximum Nesting depth: 1000 levels (`DEFAULT_MAX_DEPTH`)Additionally, the maximum magnitude of scale for BigDecimal to BigInteger conversion is set to 100,000 digits (`MAX_BIGINT_SCALE_MAGNITUDE`) and cannot be changed.Users can customize the constraints as needed by providing the corresponding options in the parameters object. If not explicitly specified, default settings will be applied.### How was this patch tested?Pass GA
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR makes sure each exception affected in PR #40679 has a proper error class when constructed with an explicit message.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed per [this discussion](https://github.com/apache/spark/pull/40679#discussion_r1159264585).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add stack trace to streamingQuery's `exception()` method. Following https://github.com/apache/spark/commit/a5c8a3c976889f33595ac18f82e73e6b9fd29b57#diff-98baf452f0352c75a39f39351c5f9e656675810b6d4cfd178f1b0bae9751495bAdd to both python client and scala client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Including stack trace is helpful in debugging### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested:1. Python:```JVM stacktrace:org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (ip-10-110-19-234.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):  File \"/home/wei.liu/oss-spark/python/lib/pyspark.zip/pyspark/worker.py\ line 850, in main    process()```2. Scala: TODO
1	-2	### What changes were proposed in this pull request?Remove unnecessary creation of `planner` in `handleWriteOperation` and `handleWriteOperationV2`### Why are the changes needed?`handleWriteOperation` and `handleWriteOperationV2` themselves are the methods of planner, no need to create another planner to call `transformRelation`### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing UTs
1	-3	### What changes were proposed in this pull request?This fixes couple of important issues related to session management for streaming queries.1. Session mapping should be maintained at connect server as long as the streaming query is active, even if there are no accesses from the client side. Currently the session mapping is dropped after 1 hour of inactivity. 2. When streaming query is stopped, the Spark session drops its reference to the streaming query object. That implies it can not accessed by remote spark-connect client. It is common usage pattern for users to access a streaming query after it is is stopped (e.g. to check its metrics, any exception if failed, etc).    - This is not a problem in legacy mode since the user code in the REPL keeps the reference. This is no longer the case in Spark-Connect. *Solution*: This PR adds `SparkConnectStreamingQueryCache` that does the following:  * Each new streaming query is registered with this cache.  * It runs a periodic task that checks the status of these queries and polls session mapping in connect-server so that the session stays alive.  * When query is stopped, it cached for 1 hour more so the it can be accessed from remote client.   * The full semantics are codified in the scaladoc. See [this comment](https://github.com/apache/spark/pull/40937/files#r1176846545) for more details.  ### Why are the changes needed?  - Explained in the above description.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Unit tests- Manual testing
2	-2	### What changes were proposed in this pull request?This PR proposes to introduce `PySparkNotImplementedError`. This PR also includes migrating Spark Connect GroupedData errors into error class as well.### Why are the changes needed?To cover user-facing Python built-in error by PySpark error framework for improving error message usability.### Does this PR introduce _any_ user-facing change?No API changes.### How was this patch tested?Fixed UT, the existing CI should pass.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate Spark Connect Window errors into error class### Why are the changes needed?To improve PySpark error usability.### Does this PR introduce _any_ user-facing change?No API changes.### How was this patch tested?The existing CI should pass.
2	-2	### What changes were proposed in this pull request?`yarn` module has the following compilation warnings related to the Hadoop API:```[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:157: [deprecation @ org.apache.spark.deploy.yarn.ApplicationMaster.prepareLocalResources.setupDistributedCache | origin=org.apache.hadoop.yarn.util.ConverterUtils.getYarnUrlFromURI | version=] method getYarnUrlFromURI in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:292: [deprecation @ org.apache.spark.deploy.yarn.Client.createApplicationSubmissionContext | origin=org.apache.hadoop.yarn.api.records.Resource.setMemory | version=] method setMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:307: [deprecation @ org.apache.spark.deploy.yarn.Client.createApplicationSubmissionContext | origin=org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext.setAMContainerResourceRequest | version=] method setAMContainerResourceRequest in class ApplicationSubmissionContext is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:392: [deprecation @ org.apache.spark.deploy.yarn.Client.verifyClusterResources.maxMem | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ClientDistributedCacheManager.scala:76: [deprecation @ org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource | origin=org.apache.hadoop.yarn.util.ConverterUtils.getYarnUrlFromPath | version=] method getYarnUrlFromPath in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala:510: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocator.updateResourceRequests.$anonfun.requestContainerMessage | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala:737: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocator.runAllocatedContainers.$anonfun | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala:737: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocator.runAllocatedContainers.$anonfun | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala:202: [deprecation @ org.apache.spark.deploy.yarn.YarnSparkHadoopUtil.getContainerId | origin=org.apache.hadoop.yarn.util.ConverterUtils.toContainerId | version=] method toContainerId in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/main/scala/org/apache/spark/util/YarnContainerInfoHelper.scala:75: [deprecation @ org.apache.spark.util.YarnContainerInfoHelper.getAttributes | origin=org.apache.hadoop.yarn.util.ConverterUtils.toString | version=] method toString in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientDistributedCacheManagerSuite.scala:83: [deprecation @ org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite.<local ClientDistributedCacheManagerSuite>.$org_scalatest_assert_macro_expr.$org_scalatest_assert_macro_left | origin=org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL | version=] method getPathFromYarnURL in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientDistributedCacheManagerSuite.scala:105: [deprecation @ org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite.<local ClientDistributedCacheManagerSuite>.$org_scalatest_assert_macro_expr.$org_scalatest_assert_macro_left | origin=org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL | version=] method getPathFromYarnURL in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientDistributedCacheManagerSuite.scala:161: [deprecation @ org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite.<local ClientDistributedCacheManagerSuite>.$org_scalatest_assert_macro_expr.$org_scalatest_assert_macro_left | origin=org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL | version=] method getPathFromYarnURL in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientDistributedCacheManagerSuite.scala:190: [deprecation @ org.apache.spark.deploy.yarn.ClientDistributedCacheManagerSuite.<local ClientDistributedCacheManagerSuite>.$org_scalatest_assert_macro_expr.$org_scalatest_assert_macro_left | origin=org.apache.hadoop.yarn.util.ConverterUtils.getPathFromYarnURL | version=] method getPathFromYarnURL in class ConverterUtils is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala:180: [deprecation @ org.apache.spark.deploy.yarn.ResourceRequestHelperSuite.createResource | origin=org.apache.hadoop.yarn.api.records.Resource.setMemory | version=] method setMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala:288: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocatorSuite.<local YarnAllocatorSuite>.expectedResources | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala:726: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocatorSuite.<local YarnAllocatorSuite>.memory | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala:742: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocatorSuite.<local YarnAllocatorSuite>.memory | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala:757: [deprecation @ org.apache.spark.deploy.yarn.YarnAllocatorSuite.<local YarnAllocatorSuite>.memory | origin=org.apache.hadoop.yarn.api.records.Resource.getMemory | version=] method getMemory in class Resource is deprecated[WARNING] [Warn] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala:655: [deprecation @ org.apache.spark.deploy.yarn.YarnClusterDriver.main.expectationAttributes | origin=org.apache.hadoop.yarn.util.ConverterUtils.toString | version=] method toString in class ConverterUtils is deprecated```This pr do the following changes as follows:- Use instead `URL.fromURI` of `ConverterUtils.getYarnUrlFromURI` refer to https://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java#L77-L84- Use `Resource.setMemorySize` instead of `Resource.setMemorySize` refer tohttps://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ResourcePBImpl.java#L110-L114- Use `ApplicationSubmissionContext#setAMContainerResourceRequests` instead of `ApplicationSubmissionContext#setAMContainerResourceRequest` refer tohttps://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/ApplicationSubmissionContext.java#L482-L490- Use `Resource.getMemorySize` instead of `Resource.getMemory` refer tohttps://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ResourcePBImpl.java#L92-L96- Use `URL.fromPath` instead of `ConverterUtils.getYarnUrlFromPath` refer tohttps://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java#L68-L75- Use `ContainerId.fromString` instead of `ConverterUtils.toContainerId` refer tohttps://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java#L133-L141- Use `ContainerId.toString` instead of `ConverterUtils.toString` and introduce `YarnContainerInfoHelper.convertToString` to defense against `null`https://github.com/apache/hadoop/blob/706d88266abcee09ed78fbaa0ad5f74d818ab0e9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java#L106-L113 ### Why are the changes needed?Clean up deprecation hadoop api usage in yarn module### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?1, update the error message;2, remove the TO-DO, since this script is dedicated for `pyspark-connect` only### Why are the changes needed?```(spark_dev) ➜  spark git:(master) ls connector/connect/src/main/protobufls: connector/connect/src/main/protobuf: No such file or directory```### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?existing UTs
1	-1	### What changes were proposed in this pull request?This pr clean up the following unused members from `SparkHadoopUtil`:- `listLeafDirStatuses`: introduced by SPARK-3928 and no longer used after SPARK-7673- `listFilesSorted`: introduced by SPARK-5342 and no longer used after SPARK-23361- `getSuffixForCredentialsPath`: same as `listFilesSorted`- `SPARK_YARN_CREDS_TEMP_EXTENSION`: same as `listFilesSorted`- `SPARK_YARN_CREDS_COUNTER_DELIM`: same as `listFilesSorted`### Why are the changes needed?Code cleanup### Does this PR introduce _any_ user-facing change?No, all cleaned up are internal APIs### How was this patch tested?Pass GitHub Action
3	-3	### What changes were proposed in this pull request?Refine the script:1. rename the file name to be more specific;2. make it work in `SPARK_HOME` dir;3. add a optional `branch` parameter;### Why are the changes needed?```(spark_dev) ➜  spark git:(master) ./dev/detect_breaking_changes.sh ./dev/detect_breaking_changes.sh: line 28: pushd: connector/connect/common/src/main: No such file or directoryFailure: failed to enumerate module files: open .Trash: operation not permittedBuf detected breaking changes for your Pull Request. Please verify.Please make sure your branch is current against spark/master.```### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?manually tests:```(spark_dev) ➜  spark git:(new_breaking_change_script) ✗ ./dev/protobuf-breaking-changes-check.sh~/Dev/spark/connector/connect/common/src/main ~/Dev/sparkStart protobuf breaking changes checking against masterFinsh protobuf breaking changes checking: SUCCESS(spark_dev) ➜  spark git:(new_breaking_change_script) ✗ ./dev/protobuf-breaking-changes-check.sh master~/Dev/spark/connector/connect/common/src/main ~/Dev/sparkStart protobuf breaking changes checking against masterFinsh protobuf breaking changes checking: SUCCESS(spark_dev) ➜  spark git:(new_breaking_change_script) ✗ ./dev/protobuf-breaking-changes-check.sh branch-3.4    ~/Dev/spark/connector/connect/common/src/main ~/Dev/sparkStart protobuf breaking changes checking against branch-3.4Finsh protobuf breaking changes checking: SUCCESS(spark_dev) ➜  spark git:(new_breaking_change_script) ✗ ./dev/protobuf-breaking-changes-check.sh branch-3.4 xyzIllegal number of parameters.Usage: ./dev/protobuf-breaking-changes-check.sh [branch]the default branch is 'master', available options are 'master', 'branch-3.4', etc(spark_dev) ➜  spark git:(new_breaking_change_script) ✗ ./dev/protobuf-breaking-changes-check.sh branch-3.3    ~/Dev/spark/connector/connect/common/src/main ~/Dev/sparkStart protobuf breaking changes checking against branch-3.3Failure: no .proto target files foundBuf detected breaking changes for your Pull Request. Please verify.Please make sure your branch is current against spark/branch-3.3.```
2	-2	What changes were proposed in this pull request?Support collect partition statistics while task running and auto update partition statistics in catalog.Does this PR introduce any user-facing change?NoHow was this patch tested?unit tests and some added tests.
2	-1	### What changes were proposed in this pull request?This pr just change to directly call `fs#createFile` instead of reflection in `SparkHadoopUtil#createFile` due to Apache Spark 3.5 no longer supports Hadoop 2.### Why are the changes needed?Code clean up.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass Github Actions
2	-2	### What changes were proposed in this pull request?Extend a test to test with both the DecorrelateInnerQuery framework on and off, since this came up in both https://github.com/apache/spark/pull/40865 and https://github.com/apache/spark/pull/40811. (The default is on.)### Why are the changes needed?Improve test coverage### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Test itself
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Update `_metadata.file_path` and `_metadata.file_name` to return url-encoded strings, rather than un-encoded strings. This was a regression introduced in Spark 3.4.0.### Why are the changes needed?This was an inadvertent behavior change.### Does this PR introduce _any_ user-facing change?Yes, fix regression!### How was this patch tested?New test added to validate that the `file_path` and `path_name` are returned as encoded strings. 
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The timeout duration for the REPL has been increased from 10 -> 30 seconds (to address slow start on JDK 17 tests) and the semaphore permits are drained after each test (to avoid cascading failures, [context](https://github.com/apache/spark/pull/40675#discussion_r1174917132)). ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The GA JDK 17 tests consistently fails as described in the [jira issue](https://issues.apache.org/jira/browse/SPARK-43285).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Locally verified by installing and running tests with JDK 17 (both the failure and the subsequent fix).
2	-2	 ### What changes were proposed in this pull request?Previously, #34829 was raised which was auto closed due to staleness. This is a rework of that by addressing comments raised. This PR seeks to improve the performance of serving the application list in History Server by storing the required information of the application as part of HDFS extended attributes instead of parsing the log file each time. ### Why are the changes needed? Improves the performance of the History Server listing pageBelow is the comparison of the  time taken to complete `mergeApplicationListing` call [in FsHistoryProvider](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L584) that is called for every log file that is updated in the event log directory : | Event log size  | Extended Attributes disabled (in ms) | Extended Attributes enabled (in ms) || ------------- | ------------- | ------------- || 122MB  | 1340 | 137  || 10.2MB  | 866 | 135  || 5.5MB  | 645  | 136 ||  0.6MB | 505 | 134 || 0.8MB  | 525 | 137  |As we can see in the comparison above, irrespective of the event log size, the time to build the application listing for the app remains the same. This is hugely beneficial in clusters that have very large event log sizes. ### Does this PR introduce _any_ user-facing change? No. ### How was this patch tested? Will add unit tests
1	-2	### What changes were proposed in this pull request?In this PR I propose to replace the legacy error class `_LEGACY_ERROR_TEMP_2014`, added as an `IllegalArgumentException` to avoid non-exhaustive case-matching in #22014, with an internal error as it is not triggered by the user space.### Why are the changes needed?As the error is not triggered by the user space, the legacy error class can be replaced by an internal error.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?No new tests were added and no tests needed changing because of the nature of the updated error class.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->`DynamicPartitionDataConcurrentWriter` it uses temp file path to get file status after commit task. However, the temp file has already moved to new path during commit task.This pr calls `closeFile` before commit task.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix bug### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, after this pr the metrics is correct### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Spark SQL now doesn’t support creating data frame from a Postgres table that contains user-defined array column.This PR support it as string.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Support handle user-defined array column in SPARK SQL with Postgres<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tested in local. Can't find TestSuite for PG.```sqlCREATE DOMAIN not_null_text    AS TEXT    DEFAULT '';create table films(    code         char(5 char)     not null        constraint firstkey            primary key,    title        varchar(40 char) not null,    did          bigint           not null,    date_prod    date,    kind         varchar(10 char),    tz           timestamp with time zone,    int_arr      integer[],    column_name  not_null_text[],    column_name2 not_null_text);INSERT INTO public.films (code, title, did, date_prod, kind, tz, int_arr, column_name, column_name2) VALUES (e'2   ', 'fdas', 1, '2023-04-07 16:05:48', '2', null, null, null, null);INSERT INTO public.films (code, title, did, date_prod, kind, tz, int_arr, column_name, column_name2) VALUES (e'4   ', 'fdsa', 1, '2023-04-07 16:05:48', '4', null, null, null, null);INSERT INTO public.films (code, title, did, date_prod, kind, tz, int_arr, column_name, column_name2) VALUES ('1    ', 'dafsdf', 1, '2023-04-04 14:43:51', '1', '2023-04-25 18:53:17.467000 +00:00', '{1,2,3}', '{1,fds,fdsa}', 'fdasfasdf');```Test Case```scala  test(\"jdbc array\") {    val connectionProperties = new Properties()    connectionProperties.put(\"user\ \"system\")    connectionProperties.put(\"password\ \"system\")    spark.read.jdbc(      url = \"jdbc:postgresql://localhost:54321/test?useSSL=false&serverTimezone=UTC\" +        \"&useUnicode=true&characterEncoding=utf-8\      table = \"TEST.public.films\      connectionProperties    ).show()  }```Result<img width=\"1444\" alt=\"image\" src=\"https://user-images.githubusercontent.com/32387433/234458027-e67e410b-c417-400d-be7e-431768afc0ef.png\"><!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-3	### What changes were proposed in this pull request?#### Make the pyspark UDF support annotating python dependencies and when executing UDF, the UDF worker creates a new python environment with provided python dependencies. - Supported spark mode: spark connect model / legacy mode - Supported UDF type: All kinds of pyspark UDFs    - SQL_BATCHED_UDF    - SQL_SCALAR_PANDAS_UDF    - SQL_GROUPED_MAP_PANDAS_UDF    - SQL_GROUPED_AGG_PANDAS_UDF    - SQL_WINDOW_AGG_PANDAS_UDF    - SQL_SCALAR_PANDAS_ITER_UDF    - SQL_MAP_PANDAS_ITER_UDF    - SQL_COGROUPED_MAP_PANDAS_UDF    - SQL_MAP_ARROW_ITER_UDF    - SQL_GROUPED_MAP_PANDAS_UDF_WITH_STATE#### Implementation sketch - Before starting the pyspark UDF worker process, the python environment with provided python packages is created, and pyspark UDF worker process is spawned using the provided python environment instead of default configured pyspark python.  - Using `virtualenv` to create a python environment based on current python environment that pyspark uses, then using `pip install` to install provided python packages. - If user configures a NFS directory that is accessible by all spark nodes (readable/writable to spark driver, readable to all spark workers), then it prepares the python environment in driver side, otherwise it creates the python environment in spark worker side. - The python environment is cached in spark driver or worker side (depending on NFS directory enabled or not), we uses SHA1 over sorted pip requirements list as the python environment caching key.#### TODOs - For chained UDF functions, if they have different pip_requirements settings, we need to split them into separate PythonRunners - In frontend, the PR currently only supports annotating `pip_requirements` for `pandas_udf`, but for other types of UDFs, and for `mapInPandas` / `mapInArrow` the `pip_requirements` argument haven't been added. - Supports annotating python version for pyspark UDF, and in UDF execution, downloading python using provided python version and creating python environment using provided python version. - Using file lock during python environment creation, to avoid race conditions. - Unit tests### Why are the changes needed? - For spark connect case, the client python environment is very likely to be different with pyspark server side python environment, this causes user's UDF function execution failure in pyspark server side. - Some machine learning third-party library (e.g. MLflow) requires pyspark UDF supporting  dependencies, because in ML cases, we need to run model inference by pyspark UDF in the exactly the same python environment that trains the model. Currently MLflow supports it by creating a child python process in pyspark UDF worker, and redirecting all UDF input data to the child python process to run model inference, this way it causes significant overhead, if pyspark UDF support builtin python dependency management then we don't need such poorly performing approach.### Does this PR introduce _any_ user-facing change?```@pandas_udf(\"string\ pip_requirements=...)````pip_requirements` argument means either an iterable of pip requirement strings (e.g. ``[\"scikit-learn\ \"-r /path/to/req2.txt\ \"-c /path/to/constraints.txt\"]``) or the string path to a pip requirements file path on the local filesystem (e.g. ``\"/path/to/requirements.txt\"``) represents the pip requirements for the python UDF.### How was this patch tested?Unit tests to be added.Manually tests:```import pandas as pdfrom pyspark.sql.functions import pandas_udfsc.setLogLevel(\"INFO\")@pandas_udf(\"string\ pip_requirements=[\"PyYAML==6.0\"])def to_upper(s: pd.Series) -> pd.Series:    import yaml    return s.str.upper() + f\"yaml-version: {yaml.__version__}\"df = spark.createDataFrame([(\"John Doe\)], (\"name\))df.select(to_upper(\"name\")).show(truncate=False)```Run above code in spark legacy mode or spark connect mode.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Update the error class _LEGACY_ERROR_TEMP_2007 to REGEX_GROUP_INDEX.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue [SPARK-42843](https://issues.apache.org/jira/browse/SPARK-42843). The original name just a number, update it to an informal name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Current tests already covered it.
3	-3	### What changes were proposed in this pull request?This pr move `ExecutorClassLoader` from `repl` module to `core` module an put it into `executor` package, then `ArtifactManagerSuite` can test using maven.On the other hand, this pr removed reflection calls in the `Executor#addReplClassLoaderIfNeeded` due to `ExecutorClassLoader` and `Executor` are in the same module after this pr.### Why are the changes needed?1. `ExecutorClassLoader` only be used by `Executor`, it is more suitable for placing in the `core` module2. Make `ArtifactManagerSuite` can test using maven.### Does this PR introduce _any_ user-facing change?No, just for maven test### How was this patch tested?- Pass GitHub Actions- Manual testRun the following commands```build/mvn clean install -DskipTests -Phive build/mvn test -pl connector/connect/server ```**Before**`ArtifactManagerSuite` test failed due to:```23/04/26 16:36:57.140 ScalaTest-main-running-DiscoverySuite ERROR Executor: Could not find org.apache.spark.repl.ExecutorClassLoader on classpath!```**After** All tests passed.```Run completed in 10 seconds, 494 milliseconds.Total number of tests run: 560Suites: completed 11, aborted 0Tests: succeeded 560, failed 0, canceled 0, ignored 0, pending 0All tests passed.```
2	-2	### What changes were proposed in this pull request?Change the error class_LEGACY_ERROR_TEMP_2022 to internal error as it cannot be accessed.### Why are the changes needed?Fix jira issue [SPARK-43257](https://issues.apache.org/jira/browse/SPARK-43257). `ResolveNaturalAndUsingJoin` only handles `UsingJoin` and `NaturalJoin`,  and  all join types supported by `UsingJoin` and `NaturalJoin` are supported by natural join, so `commonNaturalJoinProcessing` will never go to the default branch and throw  `unsupportedNaturalJoinTypeError`. I think this error class could be replaced with internal error.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Already exist tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Refer [PR#40914](https://github.com/apache/spark/pull/40914)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Implemented MapGroupsWithState and FlatMapGroupsWithState APIs for Spark Connect### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To support streaming state APIs in Spark Connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added unit test
2	-1	### What changes were proposed in this pull request?This pull request addresses the upgrade of some python packages to their latest version.### Why are the changes needed?I was debugging [this](https://github.com/apache/spark/pull/40819), I opened a support ticket with mypy and they suggested upgrading to the latest version of the package. ### Does this PR introduce _any_ user-facing change?No ### How was this patch tested?This patch was tested with the current set of patches after you commit to your branch.### ExtraMaybe it would be even better not to hardcode the versions?
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/39596 to fix more corner cases. It ignores the special column flag that requires qualified access for normal output attributes, as the flag should be effective only to metadata columns.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It's very hard to make sure that we don't leak the special column flag. Since the bug has been in the Spark release for a while, there may be tables created with CTAS and the table schema contains the special flag.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new analysis test
2	-1	### What changes were proposed in this pull request?This pr aims upgrade `zstd-jni` from 1.5.5-1 to 1.5.5-2.### Why are the changes needed?New version includes one new improvement `Added support for initialising a \"dict\" from a direct ByteBuffer`:- https://github.com/luben/zstd-jni/pull/255Other changes as follows:- https://github.com/luben/zstd-jni/compare/v1.5.5-1...v1.5.5-2### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-1	### What changes were proposed in this pull request?Implement SQL command CREATE TABLE LIKE for DataSourceV2.### Why are the changes needed?There is no support for CREATE TABLE LIKE in DataSourceV2.### Does this PR introduce _any_ user-facing change?Users will be able to create a table in DataSourceV2 like another table.### How was this patch tested?New tests in DDLParserSuite and DataSourceV2SQLSuite.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate Spark Connect session errors into error class### Why are the changes needed?To improve PySpark error usability.### Does this PR introduce _any_ user-facing change?No API changes.### How was this patch tested?The existing CI should pass.
1	-2	### What changes were proposed in this pull request?This is follow-up for https://github.com/apache/spark/pull/39785 to address improper error class and error type.### Why are the changes needed?We should use proper error class and error type for PySpark errors.### Does this PR introduce _any_ user-facing change?No, it's error correction### How was this patch tested?The existing CI should pass
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add stack trace to streamingQuery's `exception()` method. Following https://github.com/apache/spark/commit/a5c8a3c976889f33595ac18f82e73e6b9fd29b57#diff-98baf452f0352c75a39f39351c5f9e656675810b6d4cfd178f1b0bae9751495bAdd to both python client and scala client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Including stack trace is helpful in debugging### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual test:Python:```Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.0.dev0      /_/Using Python version 3.9.16 (main, Dec  7 2022 01:11:58)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> from pyspark.sql.functions import col, udf>>> from pyspark.errors import StreamingQueryException>>> sdf = spark.readStream.format(\"text\").load(\"python/test_support/sql/streaming\")>>> bad_udf = udf(lambda x: 1 / 0)>>> sq = sdf.select(bad_udf(col(\"value\"))).writeStream.format(\"memory\").queryName(\"this_query\").start()>>> sq.exception()StreamingQueryException('Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-110-19-234.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\  File \"<stdin>\ line 1, in <lambda>\ZeroDivisionError: division by zero\\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:438)\\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1554)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:483)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:422)\\\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:488)\\\tat org.apache.spark.sql.execution.datasources.v2.V...\\JVM stacktrace:\org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-10-110-19-234.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\  File \"<stdin>\ line 1, in <lambda>\ZeroDivisionError: division by zero\\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\\\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\\\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\\\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\\\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:438)\\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1554)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:483)\\\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:422)\\\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:488)\\\tat org.apache.spark.sql.execution.datasources.v2.V...')```2. Scala:```Spark session available as 'spark'.   _____                  __      ______                            __  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/ ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/    /_/@ import org.apache.spark.sql.functions._ import org.apache.spark.sql.functions._@ val sdf = spark.readStream.format(\"text\").load(\"python/test_support/sql/streaming\") sdf: org.apache.spark.sql.package.DataFrame = [value: string]@ val badUdf = udf((x: String) => 1 / 0) badUdf: org.apache.spark.sql.expressions.UserDefinedFunction = ScalarUserDefinedFunction(  ammonite.$sess.cmd2$Helper$$Lambda$1913/745186412@239d9cb7,  ArrayBuffer(StringEncoder),  PrimitiveIntEncoder,  None,  true,  true)@ val sq = sdf.select(badUdf(col(\"value\"))).writeStream.format(\"memory\").queryName(\"this_query\").start() sq: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.streaming.RemoteStreamingQuery@13866865@ sq.isActive res4: Boolean = false@ sq.exception.get.toString res5: String = \"\"\"org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (ip-10-110-19-234.us-west-2.compute.internal executor driver): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.ScalaUDF.f of type scala.Function1 in instance of org.apache.spark.sql.catalyst.expressions.ScalaUDF\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2411)\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.jav...JVM stacktrace: org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (ip-10-110-19-234.us-west-2.compute.internal executor driver): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.sql.catalyst.expressions.ScalaUDF.f of type scala.Function1 in instance of org.apache.spark.sql.catalyst.expressions.ScalaUDF\tat java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2301)\tat java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1431)...```
2	-3	### What changes were proposed in this pull request?This is a followup to #39817 to handle another error condition when the input batch is a single scalar value (where the previous fix focused on a single scalar value output).### Why are the changes needed?Using `predict_batch_udf` fails when the input batch size is one.```import numpy as npfrom pyspark.ml.functions import predict_batch_udffrom pyspark.sql.types import DoubleTypedf = spark.createDataFrame([[1.0],[2.0]], schema=[\"a\"])def make_predict_fn():    def predict(inputs):        return inputs    return predictidentity = predict_batch_udf(make_predict_fn, return_type=DoubleType(), batch_size=1)preds = df.withColumn(\"preds\ identity(\"a\")).show()```fails with:```  File \"/.../spark/python/pyspark/worker.py\ line 869, in main    process()  File \"/.../spark/python/pyspark/worker.py\ line 861, in process    serializer.dump_stream(out_iter, outfile)  File \"/.../spark/python/pyspark/sql/pandas/serializers.py\ line 354, in dump_stream    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)  File \"/.../spark/python/pyspark/sql/pandas/serializers.py\ line 86, in dump_stream    for batch in iterator:  File \"/.../spark/python/pyspark/sql/pandas/serializers.py\ line 347, in init_stream_yield_batches    for series in iterator:  File \"/.../spark/python/pyspark/worker.py\ line 555, in func    for result_batch, result_type in result_iter:  File \"/.../spark/python/pyspark/ml/functions.py\ line 818, in predict    yield _validate_and_transform_prediction_result(  File \"/.../spark/python/pyspark/ml/functions.py\ line 339, in _validate_and_transform_prediction_result    if len(preds_array) != num_input_rows:TypeError: len() of unsized object```After the fix:```+---+-----+|  a|preds|+---+-----+|1.0|  1.0||2.0|  2.0|+---+-----+```### Does this PR introduce _any_ user-facing change?This fixes a bug in the feature that was released in Spark 3.4.0.### How was this patch tested?Unit test was added.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the awaitTermination() API to scala client query. Please note that currently it won't throw the exception as it would do in original method. Because the JVM client side error handling is not ready yet. Details in SPARK-43299### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SS Connect development### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes they can use that now### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->```Spark session available as 'spark'.   _____                  __      ______                            __  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/ ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/    /_/@ val q = spark.readStream.format(\"rate\").load().writeStream.format(\"memory\").queryName(\"test\").start() q: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.streaming.RemoteStreamingQuery@34eaf9c1@ q.awaitTermination(500) res1: Boolean = false```
2	-3	### What changes were proposed in this pull request?The current implementation of AES-CBC mode called via `aes_encrypt` and `aes_decrypt` uses a key derivation function (KDF) based on OpenSSL's [EVP_BytesToKey](https://www.openssl.org/docs/man3.0/man3/EVP_BytesToKey.html). This is intended for generating keys based on passwords and OpenSSL's documents discourage its use: \"Newer applications should use a more modern algorithm\".`aes_encrypt` and `aes_decrypt` should use the key directly in CBC mode, as it does for both GCM and ECB mode. The output should then be the initialization vector (IV) prepended to the ciphertext – as is done with GCM mode:`[16-byte randomly generated IV | AES-CBC encrypted ciphertext]`### Why are the changes needed?We want to have the ciphertext output similar across different modes. OpenSSL's EVP_BytesToKey is effectively deprecated and their own documentation says not to use it. Instead, CBC mode will generate a random vector.### Does this PR introduce _any_ user-facing change?AES-CBC output generated by the previous format will be incompatible with this change. That change was recently landed and we want to land this before CBC mode is used in practice.### How was this patch tested?A new unit test in `DataFrameFunctionsSuite` was added to test both GCM and CBC modes. Also, a new standalone unit test suite was added in `ExpressionImplUtilsSuite` to test all the modes and various key lengths.```build/sbt \"sql/test:testOnly org.apache.spark.sql.DataFrameFunctionsSuite\"build/sbt \"sql/test:testOnly org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite\"```CBC values can be verified with `openssl enc` using the following command:```echo -n \"[INPUT]\" | openssl enc -a -e -aes-256-cbc -iv [HEX IV] -K [HEX KEY]echo -n \"Spark\" | openssl enc -a -e -aes-256-cbc -iv f8c832cc9c61bac6151960a58e4edf86 -K 6162636465666768696a6b6c6d6e6f7031323334353637384142434445464748```
2	-2	### What changes were proposed in this pull request?This change adds support for optional IV and AAD fields to ExpressionImplUtils, which is the underlying library to support `aes_encrypt` and `aes_decrypt`. This allows callers to specify their own initialization vector values for some specific use cases, and to take advantage of AES-GCM's authenticated additional data optional input.This change does **not** add the support to the user-facing `aes_encrypt` and `aes_decrypt` yet. That will be added in a follow-up, rather than in a single complex change.### Why are the changes needed?There are some use cases where callers to ExpressionImplUtils via aes_encrypt may want to provide initialization vectors (IVs) or additional authenticated data (AAD). The most common cases will be:1. Ensuring that ciphertext matches values that have been encrypted by external tools. In those cases, the caller will need to provide an identical IV value.2. For AES-CBC mode, there are some cases where callers want to generate deterministic encrypted output.3. For AES-GCM mode, providing AAD fields allows callers to bind additional data to an encrypted ciphertext so that it can only be decrypted by a caller providing the same value. This is often used to enforce some context.### Does this PR introduce _any_ user-facing change?Not yet. This change adds support to the underlying implementation, but does not yet update the SQL support to include the new parameters.### How was this patch tested?All existing unit tests still pass and new tests in `ExpressionImplUtilsSuite` exercise the new code paths:```build/sbt \"sql/test:testOnly org.apache.spark.sql.DataFrameFunctionsSuite\"build/sbt \"catalyst/test:testOnly org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite\"```
1	-1	  "body": null,
1	-2	### What changes were proposed in this pull request?Use `CompletableFuture` to implement retry logic, and retry operations are performed asynchronously.### Why are the changes needed?`BlockStoreClient#getHostLocalDirs` RPC did not retry when `IOexception` occurred, and then `FetchFailedException` was thrown.```java23/04/24 01:24:55,158 [shuffle-client-7-1] WARN ExternalBlockStoreClient: Error while trying to get the host local dirs for [148]23/04/24 01:24:55,158 [shuffle-client-7-1] ERROR ShuffleBlockFetcherIterator: Error occurred while fetching host local blocksjava.io.IOException: Connection reset by peer\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\tat java.lang.Thread.run(Thread.java:745) ```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?local test / add UT
1	-2	### What changes were proposed in this pull request?This PR proposes to migrate `NotImplementedError` into `PySparkNotImplementedError`.### Why are the changes needed?To leverage PySpark error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?The existing CI should pass
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Invoke allocatedPages.clear() to reset the bitmap after cleaning up all pages.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The status of free pages in `allocatedPages` mismatch with `pageTable` if don't add this change.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Running UT
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate all `ValueError` from Spark SQL types into error class.### Why are the changes needed?To improve PySpark error### Does this PR introduce _any_ user-facing change?No API changes, only error improvement.### How was this patch tested?This existing CI should pass.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate all `ValueError` from pandas UDF into error class.### Why are the changes needed?To improve PySpark error### Does this PR introduce _any_ user-facing change?No API changes, only error improvement.### How was this patch tested?This existing CI should pass.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Cherry pick fix COUNT(*) is null bug in correlated scalar subquerycherry pick from #40865 and #40946<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix COUNT(*) is null bug in correlated scalar subquery in branch 3.4<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change is to extend INTERNAL_ERROR with categories, add INTERNAL_ERROR_BROADCAST as an example of it, and change exceptions created in the broadcast package to use that error class.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is to better differentiate internal errors from different categories / areas / modules.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. The exceptions created in the broadcast package will change to be of error class INTERNAL_ERROR_BROADCAST.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Updated unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When use scalar subquery, sometimes we can get result before physical plan execute. Like `select (select (count(1)) is null from r where a = c ) from l where a < 4`.The result alway be false. So we can skip unnecessary aggregate.Only work when subquery without `group by` statement.eg:```sqlcreate or replace temp view l (a, b)as values    (1, 1.0),    (2, 2.0);create or replace temp view r (c, d)as values    (2, 3.0); select a, (select (count(1)) is null from r where a = c) from l where a < 4```The plan before this PR:```== Parsed Logical Plan =='Project ['a, unresolvedalias(scalar-subquery#240 [], None)]:  +- 'Project [unresolvedalias(isnull('count(1)), None)]:     +- 'Filter ('a = 'c):        +- 'UnresolvedRelation [r], [], false+- 'Filter ('a < 4)   +- 'UnresolvedRelation [l], [], false== Analyzed Logical Plan ==a: int, scalarsubquery(a): booleanProject [a#225, scalar-subquery#240 [a#225] AS scalarsubquery(a)#243]:  +- Aggregate [isnull(count(1)) AS (count(1) IS NULL)#242]:     +- Filter (outer(a#225) = c#236):        +- SubqueryAlias r:           +- View (`r`, [c#236,d#237]):              +- Project [_1#231 AS c#236, _2#232 AS d#237]:                 +- LocalRelation [_1#231, _2#232]+- Filter (a#225 < 4)   +- SubqueryAlias l      +- View (`l`, [a#225,b#226])         +- Project [_1#220 AS a#225, _2#221 AS b#226]            +- LocalRelation [_1#220, _2#221]== Optimized Logical Plan ==Project [_1#220 AS a#225, if (isnull(alwaysTrue#246)) false else (count(1) IS NULL)#242 AS scalarsubquery(a)#243]+- Join LeftOuter, (_1#220 = c#236)   :- Project [_1#220]   :  +- Filter (isnotnull(_1#220) AND (_1#220 < 4))   :     +- LocalRelation [_1#220, _2#221]   +- Aggregate [c#236], [false AS (count(1) IS NULL)#242, c#236, true AS alwaysTrue#246]      +- Project [_1#231 AS c#236]         +- Filter ((_1#231 < 4) AND isnotnull(_1#231))            +- LocalRelation [_1#231, _2#232]== Physical Plan ==AdaptiveSparkPlan isFinalPlan=false+- Project [_1#220 AS a#225, if (isnull(alwaysTrue#246)) false else (count(1) IS NULL)#242 AS scalarsubquery(a)#243]   +- BroadcastHashJoin [_1#220], [c#236], LeftOuter, BuildRight, false      :- Project [_1#220]      :  +- Filter (isnotnull(_1#220) AND (_1#220 < 4))      :     +- LocalTableScan [_1#220, _2#221]      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [plan_id=120]         +- HashAggregate(keys=[c#236], functions=[], output=[(count(1) IS NULL)#242, c#236, alwaysTrue#246])            +- Exchange hashpartitioning(c#236, 5), ENSURE_REQUIREMENTS, [plan_id=117]               +- HashAggregate(keys=[c#236], functions=[], output=[c#236])                  +- Project [_1#231 AS c#236]                     +- Filter ((_1#231 < 4) AND isnotnull(_1#231))                        +- LocalTableScan [_1#231, _2#232]```The plan after this PR:```== Parsed Logical Plan =='Project ['a, unresolvedalias(scalar-subquery#240 [], None)]:  +- 'Project [unresolvedalias(isnull('count(1)), None)]:     +- 'Filter ('a = 'c):        +- 'UnresolvedRelation [r], [], false+- 'Filter ('a < 4)   +- 'UnresolvedRelation [l], [], false== Analyzed Logical Plan ==a: int, scalarsubquery(a): booleanProject [a#225, scalar-subquery#240 [a#225] AS scalarsubquery(a)#243]:  +- Aggregate [isnull(count(1)) AS (count(1) IS NULL)#242]:     +- Filter (outer(a#225) = c#236):        +- SubqueryAlias r:           +- View (`r`, [c#236,d#237]):              +- Project [_1#231 AS c#236, _2#232 AS d#237]:                 +- LocalRelation [_1#231, _2#232]+- Filter (a#225 < 4)   +- SubqueryAlias l      +- View (`l`, [a#225,b#226])         +- Project [_1#220 AS a#225, _2#221 AS b#226]            +- LocalRelation [_1#220, _2#221]== Optimized Logical Plan ==Project [_1#220 AS a#225, false AS scalarsubquery(a)#243]+- Filter (isnotnull(_1#220) AND (_1#220 < 4))   +- LocalRelation [_1#220, _2#221]== Physical Plan ==*(1) Project [_1#220 AS a#225, false AS scalarsubquery(a)#243]+- *(1) Filter (isnotnull(_1#220) AND (_1#220 < 4))   +- *(1) LocalTableScan [_1#220, _2#221]```<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve logic plan on scalar subquery### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Test local.The current test case is enough<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?The current impl of KeyAs for the Scala client is a purely client side encoder operation. Thus we could end up with duplicates in keys. Added the tests both for the Scala Client and for Spark dataset API. It showed the behavior is the same for server and client at this moment.### Why are the changes needed?More tests to verify the client and server behavior.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Tests
3	-3	### What changes were proposed in this pull request?Add RocksDB state store memory management enhancementsThis change does the following:- remove use of writeBatchWithIndex- move towards using Native RocksDB operations- remove use of RocksDB WAL- add support for bounding memory usage for all RocksDB state store instances on executor using the write buffer manager### Why are the changes needed?Today when RocksDB is used as a State Store provider, memory usage when writing using writeBatch is not capped. Also, a related issue is that the state store coordinator can create multiple RocksDB instances on a single node without enforcing a global limit on native memory usage. Due to these issues we could run into OOM issues and task failures. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added unit tests and fixed existing onesRocksDBStateStoreSuite```[info] Run completed in 40 seconds, 916 milliseconds.[info] Total number of tests run: 33[info] Suites: completed 1, aborted 0[info] Tests: succeeded 33, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```StateStoreSuite```[info] Run completed in 2 minutes, 33 seconds.[info] Total number of tests run: 85[info] Suites: completed 1, aborted 0[info] Tests: succeeded 85, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```
1	-4	### What changes were proposed in this pull request?Create `NonFateSharingCache` to wrap around Guava cache with a KeyLock to synchronize all requests with the same key, so they will run individually and fail as if they come one at a time.Wrap cache in `CodeGenerator` with `NonFateSharingCache` to protect it from unexpected cascade failure due to cancellation from irrelevant queries that loading the same key. Feel free to use this in other places where we used Guava cache and don't want fate-sharing behavior.Also, instead of implementing Guava Cache and LoadingCache interface, I define a subset of it so that we can control at compile time what cache operations are allowed and make sure all cache loading action go through our narrow waist code path with key lock. Feel free to add new APIs when needed.### Why are the changes needed?Guava cache is widely used in spark, however, it suffers from fate-sharing behavior: If there are multiple requests trying to access the same key in the cache at the same time when the key is not in the cache, Guava cache will block all requests and create the object only once. If the creation fails, all requests will fail immediately without retry. So we might see task failure due to irrelevant failure in other queries due to fate sharing.This fate sharing behavior leads to unexpected results in some situation(for example, in code gen).### Does this PR introduce _any_ user-facing change?No### How was this patch tested?UT
2	-2	### What changes were proposed in this pull request?This adds an option to convert Protobuf 'Any' fields to JSON. At runtime such 'Any' fields   can contain arbitrary Protobuf message serialized as binary data.                                                                                                                         By default when this option is not enabled, such field behaves like normal Protobuf message  with two fields (`STRUCT<type_url: STRING, value: BINARY>`). The binary `value` field is not interpreted. This might not be convenient in practice.                                                                                                                                    One option is to deserialize it into actual Protobuf message and convert it to Spark STRUCT. But this is not feasible since the schema for `from_protobuf()` is needed at query compile   time and can not change at run time. As a result this is not feasible.                                                                                                                    Another option is parse the binary and deserialize the Protobuf message into JSON string.    This this lot more readable than the binary data. This configuration option enables          converting Any fields to JSON. The example blow clarifies further.                                                                                                                          Consider two Protobuf types defined as follows:                                             ```   message ProtoWithAny {                                                                          string event_name = 1;                                                                       google.protobuf.Any details = 2;                                                          }                                                                                                                                                                                         message Person {                                                                               string name = 1;                                                                             int32 id = 2;                                                                             }                                                                                          ```                                                                                             With this option enabled, schema for `from_protobuf(\"col\ messageName = \"ProtoWithAny\")`    would be : `STRUCT<event_name: STRING, details: STRING>`.                                    At run time, if `details` field contains `Person` Protobuf message, the returned value looks like the this:                                                        ('click', '{\"@type\":\"type.googleapis.com/...ProtoWithAny\\"name\":\"Mario\\"id\":100}')                                                                                                    Requirements:                                                                                 - The definitions for all the possible Protobuf types that are used in Any fields should be    available in the Protobuf descriptor file passed to `from_protobuf()`. If any Protobuf       is not found, it will result in error for that record.                                     - This feature is supported with Java classes as well. But only the Protobuf types defined     in the same `proto` file as the primary Java class might be visible.                         E.g. if `ProtoWithAny` and `Person` in above example are in different proto files,           definition for `Person` may not be found.                                                 ### Why are the changes needed?Improves handling of Any fields.### Does this PR introduce _any_ user-facing change?No. Default behavior is not changed### How was this patch tested?- Unit tests
2	-2	What changes were proposed in this pull request?In the PR, I propose to assign the proper name UNRESOLVED_CUSTOM_CLASS to the legacy error class _LEGACY_ERROR_TEMP_2017 , and modify test suite to use checkError() which checks the error class name.Why are the changes needed?Proper name improves user experience with Spark.Does this PR introduce any user-facing change?Yes, the PR changes an user-facing error message.How was this patch tested?By running the modified test suites:$ build/sbt \"testOnly *ObjectExpressionsSuite\"
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate all Spark Connect client errors into error class### Why are the changes needed?To improve PySpark error### Does this PR introduce _any_ user-facing change?No API changes, only error improvement.### How was this patch tested?This existing CI should pass.
2	-2	### What changes were proposed in this pull request?This PR proposes to migrate all remaining errors from DataFrame(Reader|Writer) into error class### Why are the changes needed?To improve PySpark error### Does this PR introduce _any_ user-facing change?No API changes, only error improvement.### How was this patch tested?This existing CI should pass.
2	-2	### What changes were proposed in this pull request?This PR proposes to remove `InputValidationError` and use `PySparkTypeError` instead. ### Why are the changes needed?We should use PySpark specific errors for every errors raised from PySpark.### Does this PR introduce _any_ user-facing change?No, it's error improvement.### How was this patch tested?The existing CI should pass.
2	-2	### What changes were proposed in this pull request?Adds a config for pandas conversion how to handle struct types.- `spark.sql.execution.pandas.structHandlingMode` (default: `\"legacy\"`)The conversion mode of struct type when creating pandas DataFrame.#### When `\"legacy\"`, the behavior is the same as before, except that with Arrow and Spark Connect will raise a more readable exception when there are duplicated nested field names.```py>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()Traceback (most recent call last):...pyspark.errors.exceptions.connect.UnsupportedOperationException: [DUPLICATED_FIELD_NAME_IN_ARROW_STRUCT] Duplicated field names in Arrow Struct are not allowed, got [a, a].```#### When `\"row\"`, convert to Row object regardless of Arrow optimization.```py>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x       y0  1  (1, 2)>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()   x       y0  1  (1, 2)>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x       y0  1  (1, 2)>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()   x       y0  1  (1, 2)```#### When `\"dict\"`, convert to dict and use suffixed key names, e.g., `a_0`, `a_1`, if there are duplicated nested field names, regardless of Arrow optimization.```py>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'dict')>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x                 y0  1  {'a': 1, 'b': 2}>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()   x                     y0  1  {'a_0': 1, 'a_1': 2}>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x                 y0  1  {'a': 1, 'b': 2}>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()   x                     y0  1  {'a_0': 1, 'a_1': 2}```### Why are the changes needed?Currently there are three behaviors when `df.toPandas()` with nested struct types:- vanilla PySpark with Arrow optimization disabled```py>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x       y0  1  (1, 2)```using `Row` object for struct types.It can use duplicated field names.```py>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()   x       y0  1  (1, 2)```- vanilla PySpark with Arrow optimization enabled```py>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x                 y0  1  {'a': 1, 'b': 2}```using `dict` for struct types.It raises an Exception when there are duplicated nested field names:```py>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()Traceback (most recent call last):...pyarrow.lib.ArrowInvalid: Ran out of field metadata, likely malformed```- Spark Connect```py>>> spark.sql(\"values (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()   x                 y0  1  {'a': 1, 'b': 2}```using `dict` for struct types.If there are duplicated nested field names, the duplicated keys are suffixed:```py>>> spark.sql(\"values (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()   x                     y0  1  {'a_0': 1, 'a_1': 2}```### Does this PR introduce _any_ user-facing change?Users will be able to configure the behavior.### How was this patch tested?Modified the related tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR borrows CTE SQL test coverage from other databases (Postgres, ZetaSQL, DuckDB).<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?CTE is a hot area in terms of regression and needs more test coverage.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No user-facing change.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?This PR adds SQL tests only.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new rule in `queryStagePreparationRules`. If there have adjacent aggregation with Partial and Final mode, we can combine them to Complete mode, so that we do not need to merge aggregation buffer.For example:```HashAggregate (Final)         HashAggregate (Complete)       |                             |HashAggregate (Partial)    =>    Exchange       |   Exchange```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve performance. According to the benchmark result, it has more benefits with high cardinality case.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test```OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1036-azureIntel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHzCombine adjacent aggregation cardinality: 5:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative---------------------------------------------------------------------------------------------------------------------------no adjacent aggregation                              16383          16539         228          1.2         819.1       1.0Xcombine adjacent aggregation - disable               11224          11345         187          1.8         561.2       1.5Xcombine adjacent aggregation - enable                 9769           9836          82          2.0         488.4       1.7XCombine adjacent aggregation cardinality: 50:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative----------------------------------------------------------------------------------------------------------------------------no adjacent aggregation                                5197           5295         116          3.8         259.8       1.0Xcombine adjacent aggregation - disable                 8514           8811         258          2.3         425.7       0.6Xcombine adjacent aggregation - enable                  8500           8542          38          2.4         425.0       0.6X```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Rename _LEGACY_ERROR_TEMP_2175 as RULE_ID_NOT_FOUND- Add a test case for the error class.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We are migrating onto error classes### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, the error message will include the error class name### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->`testOnly *RuleIdCollectionSuite` and Github Actions@MaxGekk @itholic 
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add missing `assert` in integration test.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The test does not make sense w/o `assert`, it won't fail even executors don't have the env var `SPARK_DRIVER_POD_IP`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->```build/sbt -Pkubernetes -Pvolcano -Pkubernetes-integration-tests \\  -Dtest.exclude.tags=local,r \\  \"kubernetes-integration-tests/testOnly *Suite -- -z SPARK-42769\"``````...[info] KubernetesSuite:[info] - SPARK-42769: All executor pods have SPARK_DRIVER_POD_IP env variable (19 seconds, 554 milliseconds)[info] VolcanoSuite:[info] - SPARK-42769: All executor pods have SPARK_DRIVER_POD_IP env variable (27 seconds, 697 milliseconds)[info] YuniKornSuite:[info] Run completed in 1 minute, 4 seconds.[info] Total number of tests run: 2[info] Suites: completed 3, aborted 0[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.[success] Total time: 102 s (01:42), completed Apr 28, 2023 11:48:26 PM```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR adds an ability to extend `ChannelBuilder` with extra logic.### Why are the changes needed?So that it's possible to extend GRPC options like in the example below:```pythonfrom pyspark.sql.connect import SparkSession, ChannelBuilderclass CustomChannelBuilder(ChannelBuilder):    ...custom_channel_builder = CustomChannelBuilder(...)spark = SparkSession.builder().channelBuilder(custom_channel_builder).getOrCreate()```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Migrate from deprecated `DefaultKubernetesClient` to suggested `KubernetesClient`.Note: The fabric8io/kubernetes-client changes rapidly, there are still bunches of deprecated API usages in the codebase, would like to migrate them in separated PRs.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->```/** * Class for Default Kubernetes Client implementing KubernetesClient interface. * It is thread safe. * * @deprecated direct usage should no longer be needed. Please use the {@link KubernetesClientBuilder} instead. */@Deprecatedpublic class DefaultKubernetesClient ...``````public interface StorageAPIGroupDSL extends Client {  /**   * DSL entrypoint for storage.k8s.io/v1 StorageClass   *   * @deprecated Use <code>client.storage().v1().storageClasses()</code> instead   * @return {@link NonNamespaceOperation} for StorageClass   */  @Deprecated  NonNamespaceOperation<StorageClass, StorageClassList, Resource<StorageClass>> storageClasses();  ...}```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Call Hive 2.3.9 API directly instead of reflection, basically reverts SPARK-37446.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Switch to direct calling to achieve compile time check.Spark does not officially support building against Hive other than 2.3.9, for cases listed in SPARK-37446, it's the vendor's responsibility to port HIVE-21563 into their maintained Hive 2.3.8-[vender-custom-version].See full discussion in https://github.com/apache/spark/pull/40893.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-2	### What changes were proposed in this pull request?This PR updates column DEFAULT assignment to add missing values for MERGE INSERT actions. This brings the behavior to parity with non-MERGE INSERT commands.* It also adds a small convenience feature where if the provided default value is a literal of a wider type than the target column, but the literal value fits within the narrower type, just coerce it for convenience. For example, `CREATE TABLE t (col INT DEFAULT 42L)` returns an error before this PR because `42L` has a long integer type which is wider than `col`, but after this PR we just coerce it to `42` since the value fits within the short integer range.* We also add the `SupportsCustomSchemaWrite` interface which tables may implement to exclude certain pseudocolumns from consideration when resolving column DEFAULT values.### Why are the changes needed?These changes make column DEFAULT values more usable in more types of situations.### Does this PR introduce _any_ user-facing change?Yes, see above.### How was this patch tested?This PR adds new unit test coverage.
1	-1	### What changes were proposed in this pull request?Impl missing method JoinWith with Join relation operationThe JoinWith adds `left` and `right` struct info into the Join relation proto. ### Why are the changes needed?Missing Dataset API### Does this PR introduce _any_ user-facing change?Yes. Added the missing Dataset#JoinWith method### How was this patch tested?E2E tests.
2	-2	### What changes were proposed in this pull request?Fixes `DataFrame.toPandas` with Arrow enabled to handle exceptions properly.```py>>> spark.conf.set(\"spark.sql.ansi.enabled\ True)>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"select 1/0\").toPandas()...Traceback (most recent call last):...pyspark.errors.exceptions.captured.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^```### Why are the changes needed?Currently `DataFrame.toPandas` doesn't capture exceptions happened in Spark properly.```py>>> spark.conf.set(\"spark.sql.ansi.enabled\ True)>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.sql(\"select 1/0\").toPandas()...  An error occurred while calling o53.getResult.: org.apache.spark.SparkException: Exception thrown in awaitResult:\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)...```because `jsocket_auth_server.getResult()` always wraps the thrown exceptions with `SparkException` that won't be captured.Whereas without Arrow:```py>>> spark.conf.set(\"spark.sql.ansi.enabled\ True)>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)>>> spark.sql(\"select 1/0\").toPandas()Traceback (most recent call last):...pyspark.errors.exceptions.captured.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^```### Does this PR introduce _any_ user-facing change?`DataFrame.toPandas` with Arrow enabled will show a proper exception.### How was this patch tested?Added the related test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Supporting the `regexp_extract_all` DataFrame API as well as it supported in the `Spark SQL`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Using this function also in the DataFrame API### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->It tested in the `StringFunctionsSuite`
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Trigger `committer.setupJob` before plan execute in `FileFormatWriter#write`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->In this issue, the case where `outputOrdering` might not work if AQE is enabled has been resolved.https://github.com/apache/spark/pull/38358However, since it materializes the AQE plan in advance (triggers getFinalPhysicalPlan) , it may cause the committer.setupJob(job) to not execute When `AdaptiveSparkPlanExec#getFinalPhysicalPlan()` is executed with an error.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add UT
2	-1	### What changes were proposed in this pull request?This PR proposes to add the latest timestamp on no-execution trigger for QueryIdleEvent.### Why are the changes needed?This adds a \"human readable\" timestamp which is useful for user side verification as well as be consistent with other events.### Does this PR introduce _any_ user-facing change?Yes, but QueryIdleEvent is not released yet.### How was this patch tested?Existing tests.
2	-2	### What changes were proposed in this pull request?Fixed minor issues & added a simple statement to ensure the program doesn't allow killing all executors. ### Why are the changes needed?The dynamic alloc config hangs when the minexec count is 0 ### Does this PR introduce _any_ user-facing change?It does, but it does not affect the user in any way, just shows a warning message. ### How was this patch tested?This was done on a test cluster. The cluster does run without problems but I feel there are additional safety methods we could add. I'm working on finding a way to calculate and run on an optimal number of clusters so this problem is not repeated. 
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->There is a type in [`docs/sql-migration-guide.md`](https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md?plain=1#L154).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->local test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->There is a typo in [docs/sql-migration-guide.md](https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md?plain=1#L154).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->local test
2	-2	### What changes were proposed in this pull request?Currently, queries that are ran using Spark Connect cannot be interrupted. Even when the RPC connection got broken, the Spark jobs on the server continue running.This PR proposes a ```rpc Interrupt(InterruptRequest) returns (InterruptResponse) {}```server RPC API, that can be called from the client as `SparkSession.interruptAll()` to interrupt all actively running Spark Jobs from ExecutePlan executions. In most user scenarios, SparkSession is not used for multiple executions concurrently, but is used sequentially, so `interruptAll()` should serve a big chunk of user needs. It can also be used to clean up.To keep track of executions, we introduce `ExecutionHolder` to hold the execution state, and make `SessionHolder` keep track of the executions currently running in the session. In this first PR, the interrupt only interrupts running Spark Jobs. As such, it is to a degree best effort, because it will not interrupt commands that don't run Spark Jobs, or it will not interrupt anything if a Spark Job is not running when it the interrupt is received by the server, and  the command will continue running and may continue launching more Spark jobs later in that case.Future work I plan to design and work on will involve:* Interrupting any execution. This will involve moving execution from the GRPC handler thread handling ExecutePlan to launching it in a separate thread that can be interrupted. `ExecutionHolder`* Interrupting executions selectively. This will involve exposing the operationId to the user.* (Refactor) some cleanup needed around using SessionHolder. Currently, sessionId, userId, various session data is passed around separately. SessionHolder may be passed instead, and then also be extended with more useful APIs for management of the session.### Why are the changes needed?Need to have APIs to be able to interrupt running queries in Spark Connect.### Does this PR introduce _any_ user-facing change?Yes.Users of Spark Connect can now call `interruptAll()` on client `SparkSession` object, to send an interrupt RPC to the server, which will interrupt the running queries.In followup PRs, this will be extended to Python client, and to work not only for interrupting Spark Jobs.### How was this patch tested?Added E2E tests to ClientE2ETestSuite for scala connect client.Added unit tests to test_client for python connect client.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->https://github.com/apache/spark/pull/40966 introduced a unneeded change in `StreamingQueryManager` by error. This fix removes it.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests
2	-3	### What changes were proposed in this pull request?We want to allow the parameterization of identifiers in SQL statements without compromising security.Right now this can only be done using ${..} variable substitution which allows for arbitrary text substitution.The improvement proposed here is to use an IDENTIFIER() clause which produces a possibly qualified identifier based on a foldable STRING constant. That way we can use BIND PARAMETERS to safely badd (qualified) table, column and function identifiers.This PR represent:- Support for column, table, and function references in queries and DML statements- Support for table and view subjects of DDL, and utility statements such as SHOW and DESCRIBE- Support for namespace: CREATE/DROP/ALTER, USE (but not USE NAMESPACE)/SHOW/DESCRIBE- Support for function: CREATE/DROP/ALTER,  SHOW and DESCRIBEWhat is missing:- unblock support for bind parameters in DDL to support: CREATE TABLE IDENTIFIER(:tabname) (c1 INT);- TEMPORARY FUNCTIONS. Debatable whether that is needed- Column definitions in table DDL. This MAY be interesting- qualifier of * (IDENTIFIER().*) **Example:**spark.sql(\"SELECT * FROM IDENTIFIER('S.' || :tabname\ args = Map(\"tabname\" -> \"myTab\");==>SELECT * FROM s.mytab;Note that the IDENTIFIER(<literal) syntax originates from Snowflake.There is no other vendor I am aware of with similar capability.[Link to a more detailed specification](https://docs.google.com/document/d/14BhSZFeDoK-iZa7-66PjlHQ4nYLpdePDWNSW3kkLZAs/edit?usp=sharing)### Why are the changes needed?To protect scripts that substitute table or column names from SQL injection attacks.### Does this PR introduce any user-facing change?Yes, this is a new SQL feature.### How was this patch tested?New SQL tests have been added.
1	-2	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/40931 that excludes `org.apache.spark.ErrorInfo$` and `org.apache.spark.ErrorSubInfo$` for binary compatibility in Scala 2.13.### Why are the changes needed?This is broken, see https://github.com/apache/spark/actions/runs/4854026308/jobs/8650909982.This PR recovers the scheduled build.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually.
1	-1	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/40907 that implements `__dir__` in PySpark Connect DataFrame.### Why are the changes needed?For feature parity.### Does this PR introduce _any_ user-facing change?Yes, see https://github.com/apache/spark/pull/40907### How was this patch tested?Shared unittests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Casting between Timestamp and TimestampNTZ requires a timezone since the timezone id is used in the evaluation.This PR updates the method `Cast.needsTimeZone` to include the conversion between Timestamp and TimestampNTZ. As a result:* Casting between Timestamp and TimestampNTZ is considered as unresolved unless the timezone is defined* Canonicalizing cast will include the time zone### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Timezone id is used in the evaluation between Timestamp and TimestampNTZ, thus we should mark such conversion as \"needsTimeZone\"### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No. It is more like a fix for potential bugs that the casting between Timestamp and TimestampNTZ is marked as resolved before resolving the timezone.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New UT
1	-2	### What changes were proposed in this pull request? Update javascript library DataTables, used in the UI, to 1.13.2.### Why are the changes needed?DataTables 1.10.25 broke asc/desc arrow icons for sorting column. The sorting icon is not displayed when the column is clicked to sort by asc/desc. Upgrading DataTables to 1.13.2 fixes this issue. ### Does this PR introduce _any_ user-facing change? No.### How was this patch tested?Manual test.
1	-2	### What changes were proposed in this pull request?This patch adds the capability to create multiple Spark Connect Spark Sessions in one process. Previously we've already disabled joining datasets from multiple clients.This builds on https://github.com/apache/spark/pull/40684.### Why are the changes needed?Usability### Does this PR introduce _any_ user-facing change?This adds a new `SparkSession.builder.create()` method that can only be used for Spark Connect.### How was this patch tested?UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a follow-up PR for SPARK-42945 to include `user_id` and `session_id` when logging errors on the Spark Connect server side. It is to address [this comment](https://github.com/apache/spark/pull/40575#discussion_r1150223876).### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To improve debuggability.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
2	-1	### What changes were proposed in this pull request?Figuring out how to generate connect grpc proto in python was surprisingly hard to figure out for me (not knowing much about python development though), so adding it to the README.### Why are the changes needed?Improving internal documentation.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Not applicable.
2	-3	### What changes were proposed in this pull request?When converting a StructType instance containing a nested StructType column which in turn contains a column for which `nullable = false` to a DDL string using `.toDDL`, the resulting DDL string does not include this non-nullability. For example:```val testSchema = StructType(List(  StructField(\"key\ IntegerType, false),  StructField(\"value\ StringType, true),  StructField(\"nestedCols\ StructType(List(    StructField(\"nestedKey\ IntegerType, false),    StructField(\"nestedValue\ StringType, true)  )), false)))println(testSchema.toDDL)println(StructType.fromDDL(testschema.toDDL))```gives```key INT NOT NULL,value STRING,nestedCols STRUCT<nestedKey: INT, nestedValue: STRING> NOT NULLStructType(  StructField(key,IntegerType,false),  StructField(value,StringType,true),  StructField(nestedCols,StructType(    StructField(nestedKey,IntegerType,true),    StructField(nestedValue,StringType,true)  ),false))```This is due to the fact that `StructType.toDDL` calls `StructField.toDDL` for its fields, which in turn calls `.sql` for its `dataType`. If `dataType` is a StructType, the call to `.sql` in turn calls `.sql` for all the nested fields, and this last method does not include the nullability of the field in its output. The proposed solution is therefore to have `StructField.toDDL` call `dataType.toDDL` for a StructType, since this will include information about nullability of nested columns.To work around the different DDL formats of nested and non-nested structs (the former is wrapped in `\"STRUCT ...>\"` and uses `colName: dataType` for its fields instead of `colName dataType`), package-private nested-specific versions of `.toDDL` have been added for StructType and StructField.### Why are the changes needed?Currently, converting a StructType schema to a DDL string does not pass information about nullability of nested columns. This leads to a loss of information, and means converting to DDL and then back could alter the StructType schema. ### Does this PR introduce _any_ user-facing change?Yes, given the example above, the output will become:```key INT NOT NULL,value STRING,nestedCols STRUCT<nestedKey: INT NOT NULL, nestedValue: STRING> NOT NULLStructType(  StructField(key,IntegerType,false),  StructField(value,StringType,true),  StructField(nestedCols,StructType(    StructField(nestedKey,IntegerType,false),    StructField(nestedValue,StringType,true)  ),false))```### How was this patch tested?In `StructTypeSuite`, the `nestedStruct` testing value has been modified to include a non-nullable nested column. The relevant unit tests have been changed accordingly.
2	-2	 When we calculate the quantile information from the peak executor metrics values for the distribution, there is a possibility of running into an `ArrayIndexOutOfBounds` exception when the metric values are empty. This PR addresses that and fixes it by returning an empty array if the values are empty. ### Why are the changes needed? Without these changes, when the withDetails query parameter is used to query the stages REST API, we encounter a partial JSON response since the peak executor metrics distribution cannot be serialized due to the above index error. ### Does this PR introduce _any_ user-facing change? No ### How was this patch tested? Added a unit test to test this behavior
1	-2	### What changes were proposed in this pull request?Rename error classes:1. QueryCompilationErrors.cannotReadCorruptedTablePropertyError `_LEGACY_ERROR_TEMP_1091` -> `INSUFFICIENT_TABLE_PROPERTY` (with subclasses)+ add test case+ refactor CatalogTable.readLargeTableProp function2. QueryCompilationErrors.notSupportedInJDBCCatalog `_LEGACY_ERROR_TEMP_1119` -> `NOT_SUPPORTED_IN_JDBC_CATALOG` (with subclasses)+ add test case3. QueryCompilationErrors.cannotSetJDBCNamespaceWithPropertyError+ add test case4. QueryCompilationErrors.alterTableSerDePropertiesNotSupportedForV2TablesError`_LEGACY_ERROR_TEMP_1124` -> `NOT_SUPPORTED_COMMAND_FOR_V2_TABLE`+ add test case+ updated test case in `AlterTableSetSerdeSuite` test class5. QueryCompilationErrors.unsetNonExistentPropertyError rename to QueryCompilationErrors.unsetNonExistentPropertiesError+ change error message and code to include all the nonexistent keys+ add test case6. QueryCompilationErrors.cannotUnsetJDBCNamespaceWithPropertyError`_LEGACY_ERROR_TEMP_1119` -> `NOT_SUPPORTED_IN_JDBC_CATALOG`+ not able to reproduce this error class from tests### Why are the changes needed?We should assign proper name to LEGACY_ERROR_TEMP*### Does this PR introduce _any_ user-facing change?No### How was this patch tested?By existing unit tests and an additional test cases were added.
2	-1	### What changes were proposed in this pull request?Upgrade `mlflow` >=1.0 from to >=2.3.1[Release notes](https://github.com/mlflow/mlflow/releases)### Why are the changes needed?[Access Restriction Bypass](https://security.snyk.io/vuln/SNYK-PYTHON-MLFLOW-5490116)CVE NOT AVAILABLE### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR proposes to assign the proper names to the following `_LEGACY_ERROR_TEMP*` error classes:* `_LEGACY_ERROR_TEMP_0041` -> `DUPLICATE_CLAUSES`* `_LEGACY_ERROR_TEMP_1206` -> `COLUMN_NOT_DEFINED_IN_TABLE`### Why are the changes needed?Proper name improves user experience w/ Spark SQL.### Does this PR introduce _any_ user-facing change?Yes, the PR changes an user-facing error message.### How was this patch tested?By running modified test suties.
2	-2	### What changes were proposed in this pull request?This is a follow-up PR to #40615.In this PR we make sure that all sketches produces by the new expressions `hll_sketch_agg`, `hll_union_agg`, and `hll_union` are actually HLL DataSketches represented with 8-bit registers.Using 8-bit registers improves the performance of the expressions at the expense of using more memory (the current implementation uses 4-bit registers by default, which is the default choice in the DataSketches library).Note that the register size of the DataSketches HLL sketch does not have any impact on the accuracy of the sketch.### Why are the changes needed?To improve the performance of the new HLL-related expressions.### Does this PR introduce _any_ user-facing change?No in terms of SQL surface. However, the results of the `hll_sketch_agg`, `hll_union_agg`, and `hll_union` expressions can be different since we are changing the sketch representation. However, since these expressions have not been released yet we are not facing any breaking changes.### How was this patch tested?Existing tests suffice.
1	-3	### What changes were proposed in this pull request?When FileStreamSource creates a DataSource for a file, disable globbing in the option passed to DataSource.### Why are the changes needed?It is to fix following bug.For example, If a directory contains a following file:/path/abc[123]and users would load spark.readStream.format(\"text\").load(\"/path\") as stream input. It throws an exception, saying no matching path /path/abc[123]. Spark thinks abc[123] is a regex that only matches file named abc1, abc2 and abc3.The bug is due to a second glob pattern match within DataSource, against files already glob matched by FileStreamSource. This match turns real file name into file path match pattern. It is unexpected and we would like to disable it.### Does this PR introduce _any_ user-facing change?No, except fixing the unexpected buggy behavior.### How was this patch tested?Added unit test scenarios which failed before the fix.
2	-1	### What changes were proposed in this pull request?This PR aims to remove `Python 3.7` Support at Apache Spark 3.5.0.### Why are the changes needed?Python 3.7 reaches the end of support on 2023-06-27 and SPARK-39861 deprecated Python 3.7 at Apache Spark 3.4.0.- https://www.python.org/downloads/### Does this PR introduce _any_ user-facing change?Yes, but this change is expected one because SPARK-39861 deprecated Python 3.7 already.### How was this patch tested?Pass the CIs.
2	-3	### What changes were proposed in this pull request?This PR aims two goals.1. Make PySpark support Python 3.8+ with PyPy32. Upgrade PyPy3 to Python 3.8 in our GitHub Action Infra Image to enable test coverageNote that there was one failure at `test_create_dataframe_from_pandas_with_day_time_interval` test case. This PR skips the test case and SPARK-43354 will recover it after further investigation.### Why are the changes needed?Previously, PySpark fails at PyPy3 `Python 3.8` environment.```pypy3 version is: Python 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:13)[PyPy 7.3.11 with GCC 10.2.1 20210130 (Red Hat 10.2.1-11)]Starting test(pypy3): pyspark.sql.tests.pandas.test_pandas_cogrouped_map (temp output: /__w/spark/spark/python/target/f1cacde7-d369-48cf-a8ea-724c42872020/pypy3__pyspark.sql.tests.pandas.test_pandas_cogrouped_map__rxih6dqu.log)Traceback (most recent call last):  File \"/usr/local/pypy/pypy3.8/lib/pypy3.8/runpy.py\ line 188, in _run_module_as_main    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)  File \"/usr/local/pypy/pypy3.8/lib/pypy3.8/runpy.py\ line 111, in _get_module_details    __import__(pkg_name)  File \"/__w/spark/spark/python/pyspark/__init__.py\ line 59, in <module>    from pyspark.rdd import RDD, RDDBarrier  File \"/__w/spark/spark/python/pyspark/rdd.py\ line 54, in <module>    from pyspark.java_gateway import local_connect_and_auth  File \"/__w/spark/spark/python/pyspark/java_gateway.py\ line 32, in <module>    from pyspark.serializers import read_int, write_with_length, UTF8Deserializer  File \"/__w/spark/spark/python/pyspark/serializers.py\ line 69, in <module>    from pyspark import cloudpickle  File \"/__w/spark/spark/python/pyspark/cloudpickle/__init__.py\ line 1, in <module>    from pyspark.cloudpickle.cloudpickle import *  # noqa  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\ line 56, in <module>    from .compat import pickle  File \"/__w/spark/spark/python/pyspark/cloudpickle/compat.py\ line 13, in <module>    from _pickle import Pickler  # noqa: F401ModuleNotFoundError: No module named '_pickle'```To support Python 3.8 in PyPy3.- From PyPy3.8, `_pickle` is removed.  - https://github.com/cloudpipe/cloudpickle/issues/458- We need this change.  - https://github.com/cloudpipe/cloudpickle/pull/469### Does this PR introduce _any_ user-facing change?This is an additional support.### How was this patch tested?Pass the CIs.
2	-3	### What changes were proposed in this pull request?This PR proposes to fix DataFrame creating test since it's flaky failing within some envs as below:```DataFrame.index values are different (100.0 %)[left]:  Index(['Databricks', 'Hello', 'Universe'], dtype='object')[right]: Index(['Hello', 'Universe', 'Databricks'], dtype='object')Left:                 xDatabricks  2004.0Hello       2002.0Universe       NaNx    float64dtype: objectRight:                 xHello       2002.0Universe       NaNDatabricks  2004.0x    float64dtype: object```### Why are the changes needed?Fix flaky test### Does this PR introduce _any_ user-facing change?No, test-only### How was this patch tested?Manually tested.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the `foreach()` API in `DataStreamWriter` to python client### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of SS CONNECT development.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests, added parity test file
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds `RewriteUpdateTable`, similar to `RewriteDeleteFromTable`, to handle UPDATE commands for delta-based sources. Support for group-based sources will come in a follow-up PR.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->These changes are needed per SPIP SPARK-35801.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests. There are more tests in `AlignUpdateAssignmentsSuite`, which was merged earlier.
1	-2	### What changes were proposed in this pull request?The pr aims to upgrade scalafmt from 3.7.2 to 3.7.3### Why are the changes needed?A. Release note:> https://github.com/scalameta/scalafmt/releasesB. V3.7.2 VS V3.7.3> https://github.com/scalameta/scalafmt/compare/v3.7.2...v3.7.3C. Bring bug fix:<img width=\"571\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/235818482-83649ebd-a9a5-4036-8457-f3ee108d427c.png\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Inline `DepsTestsSuite#setPythonSparkConfProperties`, which was introduced in SPARK-27936.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The `setPythonSparkConfProperties` is only invoked in one place, doesn't look helping the readability much. Besides, it should use `conf` instead of `sparkAppConf`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No. Since the caller always passes the `sparkAppConf`, this bug affects nothing actually.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
1	-2	### What changes were proposed in this pull request?This PR proposes to migrate remaining Spark session errors into error class### Why are the changes needed?To leverage PySpark error framework.### Does this PR introduce _any_ user-facing change?No API changes.### How was this patch tested?The existing CI should pass.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is follow-up for https://github.com/apache/spark/pull/40694 to redeem some errors.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make errors clear### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass
2	-2	Hey there, I noticed this project is referencing older Remote Plugins. There are upcoming [Remote Plugin changes](https://buf.build/blog/remote-packages-remote-plugins-approaching-v1) and this PR fixes the references to avoid any issues.Older plugins on [buf.build](https://buf.build) will be removed in favor of Buf-managed plugins, which are managed by @bufbuild to ensure they work[^1] for the entire community.In summary, the changes in this PR are as follows:- Update older plugins to reference the new plugins in `connector/connect/common/src/main/buf.gen.yaml`- Run `./dev/connect-gen-protos.sh` to generate the files[^1]: Buf has an open-source [bufbuild/plugins](https://github.com/bufbuild/plugins) repository that builds, tests, patches and automates publishing popular Protobuf plugins. All plugins in that repository are kept up-to-date and published to [buf.build/plugins](https://buf.build/plugins)
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The release notes are available athttps://github.com/fabric8io/kubernetes-client/releases/tag/v6.6.0### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It's basically a routine work, to keep the third-party libs up-to-date.And https://github.com/fabric8io/kubernetes-client/pull/5073 simplifies SPARK-43356### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this PR the date value is quoted before being sent to the hive metastore### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Glue for example expects the dates to be quoted, without quotes we get errors.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Changed the result of unit tests, also tested by running the fix against glue
2	-1	### What changes were proposed in this pull request?This pull request is to add a small Spark Connect Go client example and prototype.JIRA: https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-43351### Why are the changes needed?Spark Connect was released in Spark 3.4.0. There is no Go client yet. Better to have a Go client so Spark Connect could be used by Go programmer.### Does this PR introduce _any_ user-facing change?Yes. User will be able to use Go to write Spark Connect application. A very simple example in Go looks like following:```func main() {\tremote := \"localhost:15002\"\tspark, _ := sql.SparkSession.Builder.Remote(remote).Build()\tdefer spark.Stop()\tdf, _ := spark.Sql(\"select 'apple' as word, 123 as count union all select 'orange' as word, 456 as count\")\tdf.Show(100, false)}```See [README](https://github.com/apache/spark/blob/b0207feed2d898617103057ac47c42d88f261236/connector/connect/client/go/README.md) for more details.### How was this patch tested?Manually tested by running the example Go code.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Some bug fix for streaming ***connect*** python SQMNote that I also changed ***non-connect***'s StreamingQueryManager `get()` API to return an `Optional[StreamingQuery]`.Before it looks like this when you get a non-exist query:```>>> a = spark.streams.get(\"00000000-0000-0001-0000-000000000001\") >>> a<pyspark.sql.streaming.query.StreamingQuery object at 0x7f86465702b0>>>> a.idTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>  File \"/home/wei.liu/oss-spark/python/pyspark/sql/streaming/query.py\ line 78, in id    return self._jsq.id().toString()AttributeError: 'NoneType' object has no attribute 'id'```But now it looks like:```>>> a = spark.streams.get(\"00000000-0000-0001-0000-000000000001\") >>> a.idTraceback (most recent call last):  File \"<stdin>\ line 1, in <module>AttributeError: 'NoneType' object has no attribute 'id'```The only difference is the return type, which is not typically honored in Python... But not very sure if that's a breaking change### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug fix### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested. Also verified that it won't throw even without this fix so it's not that urgent
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this change we determine whether unresolved identifier is multipart or not and remap list of suggested columns to fit the same pattern.- Main change is in `StringUtils.scala`. The rest is caused by method rename and test fixes.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->When determining a list of suggested columns/attributes for `UNRESOLVED_COLUMN.WITH_SUGGESTION` we sort by Levenshtein distance between unresolved identifier and list of fully qualified column names from target table. In case of a single-part identifier this might lead to poor experience especially for short(ish) identifiers, e.g. `a` and table with columns `m` and `aa` in default spark catalog => suggested list will be (`spark_catalog.default.m`, `spark_catalog.default.aa`) ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, since we don't document internals of how we generate suggestion list for this error.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add support for scala client `StreamingQueryManager`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Development of scala connect client### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes they can use SQM in scala client### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested, also unit test
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->MySQL JSON type is converted into JDBC VARCHAR type with precision of -1 on some MariaDB drivers.When receiving VARCHAR with negative precision, Spark will throw an error.This PR special cases this scenario by directly converting JSON type into StringType in MySQLDialect.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Enable reading MySQL tables that has a JSON column.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Update existing integration test
1	-1	### What changes were proposed in this pull request?Makes to call `astype` to the category type only when the arrow type is not provided.### Why are the changes needed?Now that the minimum version of pyarrow is `1.0.0`, a workaround for pandas' categorical type for pyarrow can be removed if the arrow type is provided.> Note: This can be removed once minimum pyarrow version is >= 0.16.1### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing tests.
1	-1	### What changes were proposed in this pull request?Add docs for RocksDB state store memory management### Why are the changes needed?Docs only change### Does this PR introduce _any_ user-facing change?N/A### How was this patch tested?N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->protobuf-java is licensed under the BSD 3-clause, not the 2 we claimed.And the copy should be updated via https://github.com/protocolbuffers/protobuf/commit/9e080f7ac007b75dacbd233b214e5c0cb2e48e0f### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->fix license issue### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR updates the error messages for the INVALID_CONNECT_URL error.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the error messages more user-friendly.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove the old error class _LEGACY_ERROR_TEMP_2007 since new one is already created.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To fix JIRA issue 42843 completely.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Current tests covered it.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->AQE can not reuse subquery if it is pushed into `InMemoryTableScan`. There are two issues:- `ReuseAdaptiveSubquery` can not support reuse subquery if two subquery have the same exprId-  `InMemoryTableScan` miss apply `ReuseAdaptiveSubquery` when wrap `TableCacheQueryStageExec`For example:```Seq(1).toDF(\"c1\").cache().createOrReplaceTempView(\"t1\")Seq(2).toDF(\"c2\").createOrReplaceTempView(\"t2\")spark.sql(\"SELECT * FROM t1 WHERE c1 < (SELECT c2 FROM t2)\")```There are two `subquery#27` but have no `ReusedSubquery` ```AdaptiveSparkPlan isFinalPlan=true+- == Final Plan ==   *(1) Filter (c1#14 < Subquery subquery#27, [id=#20])   :  +- Subquery subquery#27, [id=#20]   :     +- AdaptiveSparkPlan isFinalPlan=true   :        +- LocalTableScan [c2#25]   +- TableCacheQueryStage 0      +- InMemoryTableScan [c1#14], [(c1#14 < Subquery subquery#27, [id=#20])]            :- InMemoryRelation [c1#14], StorageLevel(disk, memory, deserialized, 1 replicas)            :     +- LocalTableScan [c1#14]            +- Subquery subquery#27, [id=#20]               +- AdaptiveSparkPlan isFinalPlan=true                  +- LocalTableScan [c2#25]```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve the coverage of reuse subquery.Note that, it is not a real perf issue because the subquery has been already reused (the same Java object). This pr just makes the plan clearer about subquery reuse.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This pr enables `spark.sql.thriftServer.interruptOnCancel` by default### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Address the comment https://github.com/apache/spark/pull/30481#discussion_r1181684437### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass CI
1	-2	### What changes were proposed in this pull request?This change adds a error class INTERNAL_ERROR_EXECUTOR and change exceptions created in the executor package to use that error class.### Why are the changes needed?This is to move exceptions created in package org.apache.spark.executor to error class.### Does this PR introduce _any_ user-facing change?Yes. The exceptions created in package org.apache.spark.executor will change to be of error class INTERNAL_ERROR_EXECUTOR.### How was this patch tested?Existing unit tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Fixes a that SerializerHelper.deserializeFromChunkedBuffer does not call close on the deserialization stream. For some serializers like Kryo this creates a performance regressions as the kryo instances are not returned to the pool.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?This causes a performance regression in Spark 3.4.0 for some workloads.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Existing tests.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This PR fixes a regression introduced by #36885 which broke JsonProtocol's ability to handle missing fields from exception field. old eventlogs missing a `Stack Trace` will raise a NPE.  As a result, SHS misinterprets  failed-jobs/SQLs as `Active/Incomplete` This PR solves this problem by checking the JsonNode for null. If it is null, an empty array of `StackTraceElements`### Why are the changes needed?Fix a case which prevents the history server from identifying failed jobs if the stacktrace was not set.Example eventlog```{   \"Event\":\"SparkListenerJobEnd\   \"Job ID\":31,   \"Completion Time\":1616171909785,   \"Job Result\":{      \"Result\":\"JobFailed\      \"Exception\":{         \"Message\":\"Job aborted\"      }   }} ```**Original behavior:**The job is marked as `incomplete`Error from the SHS logs:```23/05/01 21:57:16 INFO FsHistoryProvider: Parsing file:/tmp/nds_q86_fail_test to re-build UI...23/05/01 21:57:17 ERROR ReplayListenerBus: Exception parsing Spark event log: file:/tmp/nds_q86_fail_testjava.lang.NullPointerException    at org.apache.spark.util.JsonProtocol$JsonNodeImplicits.extractElements(JsonProtocol.scala:1589)    at org.apache.spark.util.JsonProtocol$.stackTraceFromJson(JsonProtocol.scala:1558)    at org.apache.spark.util.JsonProtocol$.exceptionFromJson(JsonProtocol.scala:1569)    at org.apache.spark.util.JsonProtocol$.jobResultFromJson(JsonProtocol.scala:1423)    at org.apache.spark.util.JsonProtocol$.jobEndFromJson(JsonProtocol.scala:967)    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:878)    at org.apache.spark.util.JsonProtocol$.sparkEventFromJson(JsonProtocol.scala:865)....23/05/01 21:57:17 ERROR ReplayListenerBus: Malformed line #24368: {\"Event\":\"SparkListenerJobEnd\\"Job ID\":31,\"Completion Time\":1616171909785,\"Job Result\":{\"Result\":\"JobFailed\\"Exception\":{\"Message\":\"Job aborted\"}}}```**After the fix:**Job 31 is marked as `failedJob`### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added new unit test in JsonProtocolSuite.
2	-2	### What changes were proposed in this pull request?This PR aims to deprecate old Java 8 versions prior to 8u371.Specifically, it's fixed at Java SE 8u371, 11.0.19, 17.0.7, 20.0.1.### Why are the changes needed?To avoid TLS issue- [OpenJDK: improper connection handling during TLS handshake](https://bugzilla.redhat.com/show_bug.cgi?id=2187435)- https://www.oracle.com/security-alerts/cpuapr2023.html#AppendixJAVARelease notes:- https://www.oracle.com/java/technologies/javase/8u371-relnotes.html- https://www.oracle.com/java/technologies/javase/11-0-19-relnotes.html- https://www.oracle.com/java/technologies/javase/17-0-7-relnotes.html- https://www.oracle.com/java/technologies/javase/20-0-1-relnotes.html### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual review.
1	-3	### What changes were proposed in this pull request?We introduce the SQLConf `spark.sql.legacy.avro.allowReadingWithIncompatibleSchema` to prevent reading interval types as date or timestamp types to avoid getting corrupt dates as well as reading decimal types with incorrect precision.### Why are the changes needed?We found the following issues with open source Avro:- Interval types can be read as date or timestamp types that would lead to wildly different results   For example, `Duration.ofDays(1).plusSeconds(1)` will be read as `1972-09-27`, which is weird.- Decimal types can be read with lower precision, that leads to data being read as `null` instead of suggesting that a wider decimal format should be provided### Does this PR introduce _any_ user-facing change?No### How was this patch tested?New unit test
2	-2	### What changes were proposed in this pull request?Introduce `SQL_ARROW_BATCHED_UDF` EvalType for Arrow-optimized Python UDFs.An EvalType is used to uniquely identify a UDF type in PySpark.### Why are the changes needed?We are about to improve nested non-atomic input/output support of an Arrow-optimized Python UDF.However, currently, it shares the same EvalType with a pickled Python UDF, but the same implementation with a Pandas UDF.Introducing an EvalType enables isolating the changes to Arrow-optimized Python UDFs.The PR is also a pre-requisite for registering an Arrow-optimized Python UDF.### Does this PR introduce _any_ user-facing change?No user-facing behavior/result changes for Arrow-optimized Python UDFs.An `evalType`, as an attribute mainly designed for internal use, is changed as shown below:```py>>> udf(lambda x: str(x), useArrow=True).evalType == PythonEvalType.SQL_ARROW_BATCHED_UDFTrue# whereas>>> udf(lambda x: str(x), useArrow=False).evalType == PythonEvalType.SQL_BATCHED_UDFTrue```### How was this patch tested?A new unit test `test_eval_type` and existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Return URI encoded path, and add a test### Why are the changes needed?Fix regression in spark 3.4.### Does this PR introduce _any_ user-facing change?Yes, fixes a regression in `_metadata.file_path`.### How was this patch tested?New explicit test.
2	-2	### What changes were proposed in this pull request?Make 'transformStatCov' lazy### Why are the changes needed?Existing implementation eagerly compute the `cov`, and return a local relation.Even through currently `message StatCov` is only used in 'Dataset.stat.cov' which returns a double value, it still should be a bit beneficial if we make 'transformStatCov' lazy, since the creation of local relation is skipped.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?existing UT
1	-2	### What changes were proposed in this pull request?In the PR, I propose to assign the proper name COLUMN_NOT_DEFINED  to the legacy error class _LEGACY_ERROR_TEMP_1206.### Why are the changes needed?see [SPARK-43346](https://issues.apache.org/jira/browse/SPARK-43346)### Does this PR introduce _any_ user-facing change?No. ### How was this patch tested?existing tests. 
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR reverts [SPARK-39006](https://github.com/apache/spark/pull/36374) and add a case of sharing the same PVC between the driver and multiple executors in PV testing of integration testing.  Some PV types (such as NFS) support data sharing via the same PVC for multiple Pods. However, SPARK-39006 broke this scenario, causing multiple Pods unable to share the same PVC.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is regression caused by [SPARK-39006](https://github.com/apache/spark/pull/36374).### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests, and added test scenarios
2	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40678 adds a dialect parameter to makeGetter for applying dialect specific conversions when reading a Java Timestamp into TimestampNTZType.But there exists duplicated conversion.`JdbcDialect` calls `DateTimeUtils.microsToLocalDateTime` first and return `LocalDateTime` to `makeGetter`.But `makeGetter` calls `DateTimeUtils.localDateTimeToMicros` later.Personally, I don't think it's necessary to maintain complete symmetry between `convertJavaTimestampToTimestampNTZ` and `convertTimestampNTZToJavaTimestamp`.### Why are the changes needed?Simplify code to avoid duplicated conversion.### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?Exists test cases.The micro benchmark.```Java HotSpot(TM) 64-Bit Server VM 1.8.0_311-b11 on Mac OS X 10.16Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzCompare timestamp:                          Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative--------------------------------------------------------------------------------------------------------------------------Before: convertJavaTimestampToTimestampNTZ              6              7           2         16.8          59.6       1.0XAfter: convertJavaTimestampToTimestampNTZ               1              1           0        188.3           5.3      11.2X```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Merge the error class _LEGACY_ERROR_TEMP_2006 into REGEX_GROUP_INDEX.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Fix jira issue [SPARK-42842](https://issues.apache.org/jira/browse/SPARK-42842). The original name just a number, update it to an informal name.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Current tests covered it.
2	-2	### What changes were proposed in this pull request? Remove css `!important` tag for asc/desc arrow icons in jquery.dataTables.1.10.25.min.css### Why are the changes needed?Upgrading to DataTables 1.10.25 broke asc/desc arrow icons for sorting column. The sorting icon is not displayed when the column is clicked to sort by asc/desc. This is because the new DataTables 1.10.25's jquery.dataTables.1.10.25.min.css file added `!important` rule preventing the override set in webui-dataTables.css### Does this PR introduce _any_ user-facing change? No.### How was this patch tested?Manual test.![image](https://user-images.githubusercontent.com/52679095/236394863-e0004e7b-5173-495a-af23-32c1343e0ee6.png)![image](https://user-images.githubusercontent.com/52679095/236394879-db0e5e0e-f6b3-48c3-9c79-694dd9abcb76.png)
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Change `SupportsCustomSchemaWrite` to extend `Table`- Replace `StructType` w/ `Column[]`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-42398 introduces `Column`, which should be used in the new DSv2 API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, unreleased feature### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT is changed as well.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Remove useless code in class `ScriptInputOutputSchema`. Include `getRowFormatSQL`, `inputRowFormatSQL`, `outputRowFormatSQL` method. It unused when #16869<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Clear code.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Unnecessary.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to add statistics rowCount for `LocalRelation`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Previously, statistics in `LocalRelation` were missing rowCount.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR aims to make `df.show` print a nice string for `MapType`.Let's say have an example like this:```scalaspark.sql(\"SELECT map(1,1.1, 2,2.2) AS col\").show(false)```Before, it print```log+--------------------+|col                 |+--------------------+|{1 -> 1.1, 2 -> 2.2}|+--------------------+```Now, it prints as follows, that's consistent with `spark-sql` CLI.```log+-------------+|col          |+-------------+|{1:1.1,2:2.2}|+-------------+```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make `df.show` print a nice string for `MapType`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, They will face better nice strings representation for MapType.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Exist tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Generator is an expression that produces zero or more rows given a single input row. If UserDefinedGenerator and HiveUDTF were used, the output could be N times that of the child node, resulting in a statistical error.Because of incorrect statistics, Spark may select an incorrect execution plan. For example, if `BroadcastHashJoinExec` is selected, the Job maybe fails to broadcast buildSide.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The `Generator`'s statistics should be ratio times greater than the child nodes. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, Added a configuration to determine that the udtf output data is ratio times the input data### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Add UT.
2	-1	### What changes were proposed in this pull request?Adding dedicated configuration for specifying driver/executor memory limits.### Why are the changes needed?Separate mem limit for the pod might be needed if sprak JVM launches extrernal processes.Also, this could be useful in heterogenous loads when some pods may use more memory for bursty operations.### Does this PR introduce _any_ user-facing change?Without using new configurations - no### How was this patch tested?Unit tested
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change the error code of `_LEGACY_ERROR_TEMP_1168` -> `INSERT_COLUMN_ARITY_MISMATCH`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Cleaning up legacy error codes### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes - changes error code for legacy error.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests suffice
1	-2	### What changes were proposed in this pull request?This reverts commit b065c945fe27dd5869b39bfeaad8e2b23a8835b5.### Why are the changes needed?To remove the regression from SPARK-39006.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.Closes #41057
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new method `useNullableQuerySchema` in `Table`, to allow the DataSource implementation to declare whether they need to reserve schema nullability on CTAS/RTAS.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-28837 forcibly uses the nullable schema on CTAS/RTAS, which seems too aggressive:1. The existing matured RDBMSs have different behaviors for reserving schema nullability on CTAS/RTAS, as mentioned in #25536, PostgreSQL forcibly uses the nullable schema, but MySQL respects the query's output schema nullability.2. Some OLAP systems(e.g. ClickHouse) are perf-sensitive for nullable, and have strict restrictions on table schema, e.g. the primary keys are not allowed to be nullable.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, this PR adds a new DSv2 API, but the default implementation reserves backward compatibility.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UTs are updated.
3	-2	### What changes were proposed in this pull request?Not close idle connection when closeIdleConnection is disabled### Why are the changes needed?Spark will close idle connection when there're outstanding requests but no traffic for at least `requestTimeoutMs` even with closeIdleConnection is disabled. This will cause all shuffle output on this executor being unregistered when executor has spike IO with long fetch delay.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually tested.
2	-2	### What changes were proposed in this pull request?Spark has a (long-standing) overflow bug in the `sequence` expression.Consider the following operations:```spark.sql(\"CREATE TABLE foo (l LONG);\")spark.sql(s\"INSERT INTO foo VALUES (${Long.MaxValue});\")spark.sql(\"SELECT sequence(0, l) FROM foo;\").collect()```The result of these operations will be:```Array[org.apache.spark.sql.Row] = Array([WrappedArray()])```an unintended consequence of overflow.The sequence is applied to values `0` and `Long.MaxValue` with a step size of `1` which uses a length computation defined [here](https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3451). In this calculation, with `start = 0`, `stop = Long.MaxValue`, and `step = 1`, the calculated `len` overflows to `Long.MinValue`. The computation, in binary looks like:```  0111111111111111111111111111111111111111111111111111111111111111- 0000000000000000000000000000000000000000000000000000000000000000 ------------------------------------------------------------------  0111111111111111111111111111111111111111111111111111111111111111/ 0000000000000000000000000000000000000000000000000000000000000001------------------------------------------------------------------  0111111111111111111111111111111111111111111111111111111111111111+ 0000000000000000000000000000000000000000000000000000000000000001------------------------------------------------------------------  1000000000000000000000000000000000000000000000000000000000000000```The following [check](https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3454) passes as the negative `Long.MinValue` is still `<= MAX_ROUNDED_ARRAY_LENGTH`. The following cast to `toInt` uses this representation and [truncates the upper bits](https://github.com/apache/spark/blob/16411188c7ba6cb19c46a2bd512b2485a4c03e2c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L3457) resulting in an empty length of `0`.Other overflows are similarly problematic.This PR addresses the issue by checking numeric operations in the length computation for overflow.### Why are the changes needed?There is a correctness bug from overflow in the `sequence` expression.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Tests added in `CollectionExpressionsSuite.scala`.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Upgrade Maven from 3.8.7 to 3.8.8.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Maven 3.8.8 is the latest patched version of 3.8.xhttps://maven.apache.org/docs/3.8.8/release-notes.html### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add args `--no-mac-metadata --no-xattrs --no-fflags` to `tar` on macOS in `dev/make-distribution.sh` to exclude macOS-specific extended metadata.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The binary tarball created on macOS includes extended macOS-specific metadata and xattrs, which causes warnings when unarchiving it on Linux.Step to reproduce1. create tarball on macOS (13.3.1)```➜  apache-spark git:(master) tar --versionbsdtar 3.5.3 - libarchive 3.5.3 zlib/1.2.11 liblzma/5.0.5 bz2lib/1.0.8``````➜  apache-spark git:(master) dev/make-distribution.sh --tgz```2. unarchive the binary tarball on Linux (CentOS-7)```➜  ~ tar --versiontar (GNU tar) 1.26Copyright (C) 2011 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Written by John Gilmore and Jay Fenlason.``````➜  ~ tar -xzf spark-3.5.0-SNAPSHOT-bin-3.3.5.tgztar: Ignoring unknown extended header keyword `SCHILY.fflags'tar: Ignoring unknown extended header keyword `LIBARCHIVE.xattr.com.apple.FinderInfo'```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, dev only.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Create binary tarball on macOS then unarchive on Linux, warnings disappear after this change.
2	-1	https://issues.apache.org/jira/browse/SPARK-43361### What changes were proposed in this pull request?When deserializing protobuf enum fields, the spark-protobuf library will deserialize them as string values based on the enum name in the proto. E.g. ```message Person {  enum Job {    NOTHING = 0;    ENGINEER = 1;    DOCTOR = 2;  }  Job job = 1;}```And we have a message like```Person(job=ENGINEER)```Then the deserialized value will be:```{\"job\": \"ENGINEER\"}```However it can be useful to deserialize the enum integer value rather than the name, namely:```{\"job\": 1}```This commit adds that functionality to spark protobuf via an option, \"enums.as.ints\". In addition, it also allows serializing such fields back into protobuf via `to_protobuf`.Examples in other libraries:- protobuf-java-util JsonFormat: https://javadoc.io/doc/com.google.protobuf/protobuf-java-util/3.10.0/com/google/protobuf/util/JsonFormat.Printer.html#printingEnumsAsInts--- golang/protobuf jsonpb marshaler https://pkg.go.dev/github.com/golang/protobuf/jsonpb#Marshaler### Why are the changes needed?Adds new functionality that exists in other libraries.### Does this PR introduce _any_ user-facing change?Yes, adds a new option to from_protobuf.### How was this patch tested?Added unit tests confirming this behavior.
1	-3	### What changes were proposed in this pull request?Add config `spark.decommission.maxRatio` to control max ratio of decommissioning executors of running ones### Why are the changes needed?Decommission too many executors at the same time with shuffle or rdd migration could severely hurt performance of shuffle fetch. Block manager decommissioner try to migrate shuffle or rdd as soon as possible, this will compete network and disk IO with shuffle fetch in the target executor. ### Does this PR introduce _any_ user-facing change?Yes, added new config `spark.decommission.maxRatio`### How was this patch tested?Added test in `ExecutorMonitorSuite`
1	-1	### What changes were proposed in this pull request?Log executor decommission duration### Why are the changes needed?Executor decommission duration is important info to understand resource efficiency and troubleshoot decommission issue. ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually tested.
4	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Follow up #36562 , performance improvement when Timestamp type inference with user-provided format.In the current implementation of CSV/JSON data source, the schema inference with user-provided format relies on methods that will throw exceptions if the fields can't convert as some data types . Throwing and catching exceptions can be slow. We can improve it by creating methods that return optional results instead.The optimization of `DefaultTimestampFormatter` has been implemented in #36562 , this PR adds the optimization of user-provided format. The basic logic is to prevent the formatter from throwing exceptions, and then use catch to determine whether the parsing is successful.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Performance improvement when Timestamp type inference with user-provided format.When use JSON datasource, the speed up `88%`. CSV datasource speed also up, but not obvious.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change `black` min `target-version` to py38### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Spark drops support for Python 3.7.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
1	-1	```# Build allmake # Test Allmake test# Coverage and Test make fulltest```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Correct the GitHub PR label for DSTREAM.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->SPARK-38569 renamed the folder from external to connector.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual review.
2	-2	### What changes were proposed in this pull request?Executor timeout should be max of idle, shuffle and rdd timeout### Why are the changes needed?Wrong timeout value when combining idle, shuffle and rdd timeout### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added test in `ExecutorMonitorSuite`
1	-4	### What changes were proposed in this pull request?Add the config `spark.files.fetchFailure.unRegisterOutputThreshold` to control the number of fetch failed failures needed for one specific executor to unregister map output on this executor and allow to disable unregister.### Why are the changes needed?Spark will unregister map output on the executor when fetch failed from this executor. This might be too aggressive when fetch failed is temporary and recoverable, especially when re-computation is more expensive than retry failed fetch.### Does this PR introduce _any_ user-facing change?Yes. Added the config `spark.files.fetchFailure.unRegisterOutputThreshold`### How was this patch tested?Added test in `DAGSchedulerSuite`
2	-2	### What changes were proposed in this pull request?Use `typing.BinaryIO` instead of `typing.io.BinaryIO`. The latter is deprecated and had questionable type checker support, see https://github.com/python/cpython/issues/92871### Why are the changes needed?So Spark is unaffected when `typing.io` is removed in Python 3.13### Does this PR introduce any user-facing change?No### How was this patch tested?Existing unit tests / every import of this module
1	-1	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/41024 that skips the test only with PyPy 3.8.### Why are the changes needed?To narrow the scope of testing skipped.### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?CI in this PR should verify the change.
1	-3	### What changes were proposed in this pull request?This PR adds a skip for the type check being failed in the build https://github.com/apache/spark/actions/runs/4910498665/jobs/8767736542```starting mypy annotations test...annotations failed mypy checks:python/pyspark/broadcast.py:106: error: Overloaded function implementation does not accept all possible arguments of signature 3  [misc]Found 1 error in 1 file (checked 511 source files)```### Why are the changes needed?To fix the broken build up.### Does this PR introduce _any_ user-facing change?No, test-only.### How was this patch tested?CI in this PR should verify them.
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade buf from 1.17.0 to 1.18.0### Why are the changes needed?Release Notes: https://github.com/bufbuild/buf/releases<img width=\"672\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/236719959-c23e1993-24e4-45bf-8451-32ab8308391b.png\">https://github.com/bufbuild/buf/compare/v1.17.0...v1.18.0Manually test: dev/connect-gen-protos.shThis upgrade will not change the generated files.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually test and Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Scalar subquery can be pushed down as data filter at runtime, since we always execute subquery first. Ideally, we can rewrite `ScalarSubquery` to `Literal` before pushing down filter. The main issue before we do not support that is `ReuseSubquery` is ineffective, see https://github.com/apache/spark/pull/22518. It is not a issue now.For example:```sqlSELECT * FROM t1 WHERE c1 > (SELECT min(c2) FROM t2)```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Improve peformance if data filter have scalar subquery.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
3	-3	### What changes were proposed in this pull request?Skip reusing sst file for same version of RocksDB state store to avoid id mismatch error### Why are the changes needed?In case of task retry on the same executor, its possible that the original task completed the phase of creating the SST files and uploading them to the object store. In this case, we also might have added an entry to the in-memory map for `versionToRocksDBFiles` for the given version. When the retry task creates the local checkpoint, its possible the file name and size is the same, but the metadata ID embedded within the file may be different. So, when we try to load this version on successful commit, the metadata zip file points to the old SST file which results in a RocksDB mismatch id error.```Mismatch in unique ID on table file 24220. Expected: {9692563551998415634,4655083329411385714} Actual: {9692563551998415639,10299185534092933087} in file /local_disk0/spark-f58a741d-576f-400c-9b56-53497745ac01/executor-18e08e59-20e8-4a00-bd7e-94ad4599150b/spark-5d980399-3425-4951-894a-808b943054ea/StateStoreId(opId=2147483648,partId=53,name=default)-d89e082e-4e33-4371-8efd-78d927ad3ba3/workingDir-9928750e-f648-4013-a300-ac96cb6ec139/MANIFEST-024212```This change avoids reusing files for the same version on the same host based on the map entries to reduce the chance of running into the error above.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit testRocksDBSuite```[info] Run completed in 35 seconds, 995 milliseconds.[info] Total number of tests run: 33[info] Suites: completed 1, aborted 0[info] Tests: succeeded 33, failed 0, canceled 0, ignored 0, pending 0[info] All tests passed.```
2	-2	Now spark sql cannot drop multiple partitions in one call, so I fix itWith this patch we can drop multiple partitions like this : alter table test.table_partition drop partition(dt<='2023-04-02', dt>='2023-03-31')this is my test demo with hms1、insert five data into partitioned table test.table_partitioninsert into table test.table_partition partition(dt='2023-03-30') values('chenruotao','100');insert into table test.table_partition partition(dt='2023-03-31') values('chenruotao','100');insert into table test.table_partition partition(dt='2023-04-01') values('chenruotao','100');insert into table test.table_partition partition(dt='2023-04-02') values('chenruotao','100');insert into table test.table_partition partition(dt='2023-04-03') values('chenruotao','100');<img width=\"420\" alt=\"image\" src=\"https://user-images.githubusercontent.com/20615138/236775512-d54fa998-7f0b-40d9-ae30-7c2de6e7facc.png\">2、execute sql to  drop multiple partitions：alter table test.table_partition drop partition(dt<='2023-04-02', dt>='2023-03-31');<img width=\"394\" alt=\"image\" src=\"https://user-images.githubusercontent.com/20615138/236775777-2ba14634-0a87-4fee-a819-38774273fb1e.png\">3、execute sql to  drop one partitions：alter table test.table_partition drop partition(dt='2023-03-30');<img width=\"321\" alt=\"image\" src=\"https://user-images.githubusercontent.com/20615138/236775891-ea53f3fb-e670-4cdc-8765-45d56d5d156e.png\">
4	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Follow up https://github.com/apache/spark/pull/36562 , performance improvement when Timestamp type inference with legacy format.In the current implementation of CSV/JSON data source, the schema inference with legacy format relies on methods that will throw exceptions if the fields can't convert as some data types .Throwing and catching exceptions can be slow. We can improve it by creating methods that return optional results instead.The optimization of DefaultTimestampFormatter has been implemented in https://github.com/apache/spark/pull/36562 , this PR adds the optimization of legacy format. The basic logic is to prevent the formatter from throwing exceptions, and then use catch to determine whether the parsing is successful.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Performance improvement when Timestamp type inference with legacy format.When use JSON datasource, the speed up `67%`. CSV datasource speed also up, but not obvious.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-1	### What changes were proposed in this pull request?Improve vectorized loop for Packed skipValues### Why are the changes needed?Performance improvement ### Does this PR introduce _any_ user-facing change?no### How was this patch tested?Existing tests
1	-1	Remove the dot since GitHub treats it as a part of the PR's url.[Example](https://github.com/apache/spark/actions/runs/4914465788/jobs/8775809819#step:7:5)
2	-2	### What changes were proposed in this pull request?Before this PR, IN subquery expressions are incorrectly marked as non-nullable, even when they are actually nullable. They correctly check the nullability of the left-hand-side, but the right-hand-side of a IN subquery, the ListQuery, is currently defined with nullability = false always. This is incorrect and can lead to incorrect query transformations.Example: `(non_nullable_col IN (select nullable_col)) <=> TRUE`. Here the IN expression returns NULL when the nullable_col is null, but our code marks it as non-nullable, and therefore SimplifyBinaryComparison transforms away the <=> TRUE, transforming the expression to `non_nullable_col IN (select nullable_col)`, which is an incorrect transformation because NULL values of nullable_col now cause the expression to yield NULL instead of FALSE.Fix this by calculating nullability correctly from the ListQuery child output expressions.This bug can potentially lead to wrong results, but in most cases this doesn't directly cause wrong results end-to-end, because IN subqueries are almost always transformed to semi/anti/existence joins in RewritePredicateSubquery, and this rewrite can also incorrectly discard NULLs, which is another bug. But we can observe it causing wrong behavior in unit tests at least.This is a long-standing bug that has existed at least since 2016, as long as the ListQuery class has existed.### Why are the changes needed?Fix correctness bug.### Does this PR introduce _any_ user-facing change?May change query results to fix correctness bug.### How was this patch tested?Unit tests
2	-2	### What changes were proposed in this pull request?This PR addresses a test flakiness issue in Kafka connector RDD suiteshttps://github.com/apache/spark/pull/34089#pullrequestreview-872739182 (Spark 3.4.0) upgraded Spark to Kafka 3.1.0, which requires a different configuration key for configuring the broker listening port. That PR updated the `KafkaTestUtils.scala` used in SQL tests, but there's a near-duplicate of that code in a different `KafkaTestUtils.scala` used by RDD API suites which wasn't updated. As a result, the RDD suites began using Kafka's default port 9092 and this results in flakiness as multiple concurrent suites hit port conflicts when trying to bind to that default port.This PR fixes that by simply copying the updated configuration from the SQL copy of `KafkaTestUtils.scala`. ### Why are the changes needed?Fix test flakiness due to port conflicts.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Ran 20 concurrent copies of `org.apache.spark.streaming.kafka010.JavaKafkaRDDSuite` in my CI environment and confirmed that this PR's changes resolve the test flakiness.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add streaming query listener for python client. In this version, the code runs on the driver.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation development of streaming connect### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->For now manually:client:```Using Python version 3.9.16 (main, Dec  7 2022 01:11:58)Client connected to the Spark Connect server at localhostSparkSession available as 'spark'.>>> from pyspark.sql.connect.streaming.listener import QueryStartedEvent;from pyspark.sql.connect.streaming.listener import StreamingQueryListener;from pyspark.sql.streaming.listener import (QueryProgressEvent, QueryTerminatedEvent, QueryIdleEvent)>>> class MyListener(StreamingQueryListener):...     def onQueryStarted(self, event: QueryStartedEvent) -> None:...             print(\"hi, event query id is: \" +  event.id)...     def onQueryProgress(self, event: QueryProgressEvent) -> None:...             pass...     def onQueryTerminated(self, event: QueryTerminatedEvent) -> None:...             pass...     def onQueryIdle(self, event: QueryIdleEvent) -> None:...             pass... >>> spark.streams.addListener(MyListener())>>> q = spark.readStream.format(\"rate\").load().writeStream.format(\"console\").start()>>> q.stop()```server(driver):```23/05/08 21:27:49 INFO MicroBatchExecution: Starting [id = 6042353a-dc77-436b-9ee5-d4d0653ec0a2, runId = 269288ae-3463-408c-bbb3-a72934a4bc1d]. Use file:/tmp/temporary-9ec9765b-b6d8-427e-bfe9-fa758b9f87b7 to store the query checkpoint.##### Python out for query start event is: out=###### Start running onQueryStarted ######hi, event query id is: 6042353a-dc77-436b-9ee5-d4d0653ec0a2##### Python error for query start event is: error=##### End processing query start event exitCode=0```
2	-1	### What changes were proposed in this pull request?This PR adds `SparkSession.Builder.getOrCreate()` to the scala client. `getOrCreate` is used in many existing examples so it is good to support it.Spark Connect is a bit different from the old API in that it allows you to connect to multiple servers in the same process. This means that we cannot entirely match the existing semantics (one session for all). I opted to cache a number of `SparkSessions`, and make `getOrCreate` return a cached session if they share the same client configuration.  I have also added a `create()` method for cases where you want a newly instantiated session.### Why are the changes needed?Improve compatibility with the existing code.### Does this PR introduce _any_ user-facing change?Yes, it adds API.### How was this patch tested?Added tests for this.
1	-2	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/40273 that avoids adding `file://` into Windows path.### Why are the changes needed?To fix the broken AppVeyor build```== Failed ======================================================================-- 1. Error ('test_streaming.R:55'): read.stream, write.stream, awaitTerminationError: Error in start : org.apache.hadoop.HadoopIllegalArgumentException: Unsupported name: has scheme but relative path-part...```See also https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/builds/46451196### Does this PR introduce _any_ user-facing change?No to end users because the change has not been released out yet.### How was this patch tested?AppVeyor build on windows.
2	-3	### What changes were proposed in this pull request?In order to reduce the checkpoint duration and end to end latency, we propose Changelog Based Checkpointing for RocksDB State Store Provider. Below is the mechanism.1. Changelog checkpoint: Upon each put() delete() call to local rocksdb instance, log the operation to a changelog file. During the state change commit,  sync the compressed change log of the current batch to DFS as checkpointDir/{version}.delta.2. Version reconstruction: For version j, find latest snapshot i.zip such that i <= j, load snapshot i, and replay i+1.delta ~ j.delta. This is used in loading the initial state as well as creating the latest version snapshot. Note: If a query is shutdown without exception, there won’t be changelog replay during query restart because a maintenance task is executed before the state store instance is unloaded.3. Background snapshot: A maintenance thread in executors will launch maintenance tasks periodically. Inside the maintenance task, sync the latest RocksDB local snapshot to DFS as checkpointDir/{version}.zip. Snapshot enables faster failure recovery and allows old versions to be purged.4. Garbage collection: Inside the maintenance task, delete snapshot and delta files from DFS for versions that is out of retention range(default retained version number is 100)### Why are the changes needed?We have identified state checkpointing latency as one of the major performance bottlenecks for stateful streaming queries. Currently, RocksDB state store pauses the RocksDB instances to upload a snapshot to the cloud when committing a batch, which is heavy weight and has unpredictable performance.With changelog based checkpointing, we allow the RocksDB instance to run uninterruptibly, which improves RocksDB operation performance. This also dramatically reduces the commit time and batch duration because we are uploading a smaller amount of data during state commit. With this change, stateful query with RocksDB state store will have lower and more predictable latency.### How was this patch tested?Add unit test for changelog checkpointing utility.Add unit test and integration test that check backward compatibility with existing checkpoint.Enable RocksDB state store unit test and stateful streaming query integration test to run with changelog checkpointing enabled.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->If a bucket scan has no interesting partition or contains shuffle exchange, then we would disable bucket scan. But If the bucket scan is inside table cache, the cached plan would be accessed multi-times, then we should not disable bucket scan as the bucket scan could preserve output partitioning and more likely be reused.This pr insert `TableCache` at the top of cached plan, so that it can be aware of whether it's inside table cache or not.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make `DisableUnnecessaryBucketedScan` smart with table cache.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->add test
1	-3	### What changes were proposed in this pull request?`test_gmm` is a bit flaky, I can see it fails about 1~3 times per week, for example, https://github.com/apache/spark/actions/runs/4921792416/jobs/8791985336this PR is to retry it if it fails### Why are the changes needed?to make CI more stable### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?updated tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->- Add getColumnDisplaySize API support for varchar & char- make data type mapping consistent between SQL and MetaOperation(GetColumns)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->better JDBC API support### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, java.sql.ResultSetMetaData.getColumnDisplaySize now offers the exact max length for char varchar instead of Int.Max### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Noticed this one was missing when implementing `TimestampNTZType` in Iceberg.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Able to read `TimestampNTZType` using the `ColumnarBatchRow`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->I checked for `TestColumnarBatchRow.java` but couldn't find it. So I figured that this would be okay.
1	-1	### What changes were proposed in this pull request?Maintain tags on LogicalRelation if that relation has metadata output### Why are the changes needed?Tags were lost before when the Dataframe queries for a metadata column or SchemaPruning happened during optimization.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added a new test
1	-2	### What changes were proposed in this pull request?Ensure old SparkUI in HistoryServer has been detached before loading new one.### Why are the changes needed?The error described in SPARK-43403 happened quite often when user opened SparkUIs of in-progress Apps frequently.It happened when `FsHistoryProvider#onUIDetached` run (in thread-a) right after a new SparkUI with the same appId and attemptId was loaded (in thread-b).To avoid this, we let loading of the new SparkUI wait until old SparkUI is detached.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Add test `Load new SparkUI during old one is detaching` in `ApplicationCacheSuite`
1	-2	### What changes were proposed in this pull request?In the PR, I propose to store the cached local relations in the proto format, the same as `LocalRelation`. Also I reverted `transformLocalRelation()` to the state before the commit https://github.com/apache/spark/commit/0d7618a2ca847cf9577659e50409dd5a383d66d3.### Why are the changes needed?To explicitly specify the format of cached local relations in the proto API.### Does this PR introduce _any_ user-facing change?Yes but the feature of cached local relations haven't been released yet.### How was this patch tested?By running the existing tests:```$ build/sbt \"test:testOnly *.ArtifactManagerSuite\"$ build/sbt \"test:testOnly *.ClientE2ETestSuite\"$ build/sbt \"test:testOnly *.ArtifactStatusesHandlerSuite\"```
3	-2	https://issues.apache.org/jira/browse/SPARK-43427<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?**Explanation**Protobuf supports unsigned integer types, including `uint32` and `uint64`. When deserializing protobuf values with fields of these types, the `from_protobuf` library currently transforms them to the spark type of:uint32 => `IntegerType`uint64 => `LongType``IntegerType` and `LongType` are [signed](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) integer types, so this can lead to confusing results. Namely, if a uint32 value in a stored proto is above 2^31 or a uint64 value is above 2^63, their representation in binary will contain a 1 in the highest bit, which when interpreted as a signed integer will come out as negative (I.e. overflow).I propose that we deserialize unsigned integer types into a type that can contain them correctly, e.g.uint32 => `LongType`uint64 => `Decimal(20, 0)`**Precedent**I believe that unsigned integer types in **parquet** are deserialized in a similar manner, i.e. put into a larger type so that the unsigned representation natively fits. https://issues.apache.org/jira/browse/SPARK-34817 and https://github.com/apache/spark/pull/31921** Example to reproduce **Consider a protobuf message like:```syntax = \"proto3\";message Test {  uint64 val = 1;}```Generate a protobuf with a value above 2^63. I did this in python with a small script like:```import test_pb2s = test_pb2.Test()s.val = 9223372036854775809 # 2**63 + 1serialized = s.SerializeToString()print(serialized)```This generates the binary representation:```b'\\x08\\x81\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x01'```Then, deserialize this using `from_protobuf`. I did this in a notebook so its easier to see, but could reproduce in a scala test as well:<img width=\"597\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1002986/a6c58c19-b9d3-44d4-8c2a-605991d3d5de\">**Backwards Compatibility / Default Behavior****Should we maintain backwards compatibility and add an option that allows deserializing these types differently? OR should we change change the default behavior (with an option to go back to the old way)? Would love some thoughts here!**I think by default it makes more sense to deserialize them as the larger types so that it's semantically more correct. However, there may be existing users of this library that would be affected by this behavior change. Though, maybe we can justify the change since the function is tagged as `Experimental` (and spark 3.4.0 was only released very recently).### Why are the changes needed?Improve unsigned integer deserialization behavior.### Does this PR introduce _any_ user-facing change?Yes, as written it would change the deserialization behavior of unsigned integer field types. However, please see the above section about whether we should or should not maintain backwards compatibility.### How was this patch tested?Unit Testing
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR moves some commonly used class loader/reflection utils to common/utils.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Reduce the required dependency on Spark core.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->NO### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
2	-2	### What changes were proposed in this pull request?This is an extension to the `ExecutePlanRequest` proto message to support arbitrary request options. Possible use cases are pagination or result formats.For example, if we were to support additional result serialization types the client could indicate them by adding a simple message to the request options.```{  request_options : [     request_option : {        result_format: {           type: \"csv\"        }      }  ] }```### Why are the changes needed?Extensibility### Does this PR introduce _any_ user-facing change?No
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Support ANALYZE TABLE on v2 tables.Through this PR, users can use the `ANALYZE TABLE` statement on Datasourcev2 to analyze the table. Since the API of Datasourcev2 does not support the `NOSCAN` and `PARTITION` features, currently using the Analyze table with `PARTITION`, `COLUMNS` and `NOSCAN` statements will report an error.The statistics obtained through the analysis will be stored in the `SessionState` to be used by statements of the `DESC EXTENDED`. In the future, the data in the `SessionState` can be provided to `DataSourceV2Relation` to reduce repeated statistics.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?`ANALYZE TABLE` syntax for aligning Datasourcev1 and v2<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?Noticed this one was missing when implementing `TimestampNTZType` in Iceberg.### Why are the changes needed?Able to read `TimestampNTZType` using the `ColumnarBatchRow`.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Closes #41103 from Fokko/fd-add-timestampntz.Authored-by: Fokko Driesprong <fokko@tabular.io>
1	-2	### What changes were proposed in this pull request?Disable flaky doctest `pyspark.sql.connect.dataframe.DataFrame.writeStream`### Why are the changes needed?This test is unstable, see https://github.com/apache/spark/pull/40586#issuecomment-1524690074 and https://github.com/apache/spark/pull/40586#discussion_r1182411427### Does this PR introduce _any_ user-facing change?No, dev-only### How was this patch tested?updated UT
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?How to support more subexpressions elimination cases* Get all common expressions from input expressions of the current physical operator to current CodeGenContext. Recursively visits all subexpressions regardless of whether the current expression is a conditional expression.* For each common expression:  * Add a new boolean variable subExprInit to indicate whether it has  already been evaluated.   * Add a new code block in CodeGenSupport trait, and reset those subExprInit variables to false before the physical operators begin to evaluate the input row.  * Add a new wrapper subExpr function for each common subexpression.```private void subExpr_n(${argList}) { if (!subExprInit_n) {   ${eval.code}   subExprInit_n = true;   subExprIsNull_n = ${eval.isNull};   subExprValue_n = ${eval.value}; }}```  * When generating the input expression code,  if the input expression is a common expression, the expression code will be replaced with the corresponding subExpr function. When the subExpr function is called for the first time, subExprInit will be set to true, and the subsequent function calls will do nothing.### Why are the changes needed?Support more subexpression elimination cases, improve performance.For example, TPCDS q23b query,  we can reuse the result of projection `sum(ss_quantity * ss_sales_price)` in the if expressions:```if (isnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])))   input[0, decimal(28,2), true]else    (input[0, decimal(28,2), true] + cast(knownnotnull((cast(input[2, int, true] as decimal(10,0)) * input[3, decimal(7,2), true])) as decimal(28,2)))```q4, q62, q67 are similar to the above.| Query    | Time with PR | Time without PR | Time diff | Percentage ||----------|--------------|-----------------|-----------|------------|| q1.sql   | 36.97        | 36.623          | -0.347    | 99.06%     || q2.sql   | 42.699       | 41.741          | -0.958    | 97.76%     || q3.sql   | 6.038        | 6.111           | 0.073     | 101.21%    || q4.sql   | 273.849      | 323.799         | 49.95     | 118.24%    || q5.sql   | 76.202       | 76.353          | 0.151     | 100.20%    || q6.sql   | 8.451        | 9.011           | 0.56      | 106.63%    || q7.sql   | 13.017       | 12.954          | -0.063    | 99.52%     || q8.sql   | 10.22        | 11.027          | 0.807     | 107.90%    || q9.sql   | 72.843       | 72.019          | -0.824    | 98.87%     || q10.sql  | 13.233       | 14.028          | 0.795     | 106.01%    || q11.sql  | 112.252      | 112.983         | 0.731     | 100.65%    || q12.sql  | 3.036        | 3.488           | 0.452     | 114.89%    || q13.sql  | 12.471       | 12.911          | 0.44      | 103.53%    || q14a.sql | 210.201      | 220.12          | 9.919     | 104.72%    || q14b.sql | 185.374      | 187.57          | 2.196     | 101.18%    || q15.sql  | 10.189       | 10.338          | 0.149     | 101.46%    || q16.sql  | 80.756       | 82.503          | 1.747     | 102.16%    || q17.sql  | 28.523       | 28.567          | 0.044     | 100.15%    || q18.sql  | 13.417       | 14.271          | 0.854     | 106.37%    || q19.sql  | 6.366        | 6.53            | 0.164     | 102.58%    || q20.sql  | 3.427        | 4.939           | 1.512     | 144.12%    || q21.sql  | 2.096        | 2.16            | 0.064     | 103.05%    || q22.sql  | 14.4         | 14.01           | -0.39     | 97.29%     || q23a.sql | 507.253      | 545.185         | 37.932    | 107.48%    || q23b.sql | 707.054      | 768.148         | 61.094    | 108.64%    || q24a.sql | 193.116      | 193.793         | 0.677     | 100.35%    || q24b.sql | 177.109      | 179.54          | 2.431     | 101.37%    || q25.sql  | 22.264       | 22.949          | 0.685     | 103.08%    || q26.sql  | 8.68         | 8.973           | 0.293     | 103.38%    || q27.sql  | 8.535        | 8.558           | 0.023     | 100.27%    || q28.sql  | 101.953      | 102.713         | 0.76      | 100.75%    || q29.sql  | 75.392       | 76.211          | 0.819     | 101.09%    || q30.sql  | 12.265       | 13.508          | 1.243     | 110.13%    || q31.sql  | 26.477       | 26.965          | 0.488     | 101.84%    || q32.sql  | 3.393        | 3.507           | 0.114     | 103.36%    || q33.sql  | 6.909        | 7.277           | 0.368     | 105.33%    || q34.sql  | 8.41         | 8.572           | 0.162     | 101.93%    || q35.sql  | 34.214       | 36.822          | 2.608     | 107.62%    || q36.sql  | 9.027        | 9.79            | 0.763     | 108.45%    || q37.sql  | 36.076       | 36.753          | 0.677     | 101.88%    || q38.sql  | 71.768       | 74.473          | 2.705     | 103.77%    || q39a.sql | 7.753        | 7.617           | -0.136    | 98.25%     || q39b.sql | 6.365        | 7.229           | 0.864     | 113.57%    || q40.sql  | 16.588       | 17.164          | 0.576     | 103.47%    || q41.sql  | 1.162        | 1.188           | 0.026     | 102.24%    || q42.sql  | 2.3          | 2.561           | 0.261     | 111.35%    || q43.sql  | 7.407        | 7.605           | 0.198     | 102.67%    || q44.sql  | 28.939       | 30.473          | 1.534     | 105.30%    || q45.sql  | 9.796        | 9.634           | -0.162    | 98.35%     || q46.sql  | 9.496        | 9.692           | 0.196     | 102.06%    || q47.sql  | 27.087       | 27.151          | 0.064     | 100.24%    || q48.sql  | 14.524       | 14.889          | 0.365     | 102.51%    || q49.sql  | 21.466       | 21.572          | 0.106     | 100.49%    || q50.sql  | 194.755      | 195.052         | 0.297     | 100.15%    || q51.sql  | 37.493       | 38.56           | 1.067     | 102.85%    || q52.sql  | 2.227        | 2.28            | 0.053     | 102.38%    || q53.sql  | 5.375        | 5.437           | 0.062     | 101.15%    || q54.sql  | 12.556       | 13.015          | 0.459     | 103.66%    || q55.sql  | 2.341        | 2.809           | 0.468     | 119.99%    || q56.sql  | 7.424        | 7.207           | -0.217    | 97.08%     || q57.sql  | 17.606       | 17.797          | 0.191     | 101.08%    || q58.sql  | 6.169        | 6.374           | 0.205     | 103.32%    || q59.sql  | 27.602       | 27.744          | 0.142     | 100.51%    || q60.sql  | 7.04         | 7.459           | 0.419     | 105.95%    || q61.sql  | 7.838        | 7.816           | -0.022    | 99.72%     || q62.sql  | 9.726        | 10.762          | 1.036     | 110.65%    || q63.sql  | 4.816        | 5.176           | 0.36      | 107.48%    || q64.sql  | 253.937      | 261.034         | 7.097     | 102.79%    || q65.sql  | 78.942       | 78.373          | -0.569    | 99.28%     || q66.sql  | 15.199       | 14.73           | -0.469    | 96.91%     || q67.sql  | 926.049      | 1022.971        | 96.922    | 110.47%    || q68.sql  | 7.932        | 7.977           | 0.045     | 100.57%    || q69.sql  | 12.101       | 14.699          | 2.598     | 121.47%    || q70.sql  | 20.7         | 20.872          | 0.172     | 100.83%    || q71.sql  | 14.96        | 15.065          | 0.105     | 100.70%    || q72.sql  | 73.215       | 73.955          | 0.74      | 101.01%    || q73.sql  | 5.973        | 6.126           | 0.153     | 102.56%    || q74.sql  | 97.611       | 99.577          | 1.966     | 102.01%    || q75.sql  | 125.005      | 129.508         | 4.503     | 103.60%    || q76.sql  | 34.812       | 35.34           | 0.528     | 101.52%    || q77.sql  | 7.686        | 8.474           | 0.788     | 110.25%    || q78.sql  | 287.959      | 292.936         | 4.977     | 101.73%    || q79.sql  | 8.401        | 9.616           | 1.215     | 114.46%    || q80.sql  | 59.371       | 60.051          | 0.68      | 101.15%    || q81.sql  | 18.452       | 19.499          | 1.047     | 105.67%    || q82.sql  | 64.093       | 65.032          | 0.939     | 101.47%    || q83.sql  | 4.675        | 4.867           | 0.192     | 104.11%    || q84.sql  | 10.456       | 10.816          | 0.36      | 103.44%    || q85.sql  | 12.347       | 12.77           | 0.423     | 103.43%    || q86.sql  | 6.537        | 6.843           | 0.306     | 104.68%    || q87.sql  | 77.427       | 77.876          | 0.449     | 100.58%    || q88.sql  | 83.082       | 83.385          | 0.303     | 100.36%    || q89.sql  | 6.645        | 6.801           | 0.156     | 102.35%    || q90.sql  | 7.841        | 7.883           | 0.042     | 100.54%    || q91.sql  | 3.88         | 4.129           | 0.249     | 106.42%    || q92.sql  | 3.044        | 3.271           | 0.227     | 107.46%    || q93.sql  | 361.149      | 365.883         | 4.734     | 101.31%    || q94.sql  | 43.929       | 46.667          | 2.738     | 106.23%    || q95.sql  | 196.363      | 197.427         | 1.064     | 100.54%    || q96.sql  | 12.457       | 12.496          | 0.039     | 100.31%    || q97.sql  | 80.131       | 81.821          | 1.69      | 102.11%    || q98.sql  | 6.885        | 7.522           | 0.637     | 109.25%    || q99.sql  | 17.685       | 18.009          | 0.324     | 101.83%    ||          | 6788.707     | 7116.357        | 327.65    | 104.82%    |One of our production query which has 19 case when  expressions, it's query time changed from 1.1 hour to 42 seconds.![image](https://user-images.githubusercontent.com/3626747/227448668-8a58ff33-3296-4a95-984b-292af47532ca.png)A simplify benchmark of the above production query.```    spark.range(1, 2000000, 1, 1)      .selectExpr(        \"cast(id + 1 as decimal) as a\        \"cast(id + 2 as decimal) as b\        \"cast(id + 3 as decimal) as c\        \"cast(id + 4 as decimal) as d\")      .createOrReplaceTempView(\"tab\")    runBenchmark(\"Subexpression elimination in ProjectExec\") {      val benchmark =        new Benchmark(\"Subexpression elimination in ProjectExec\ 2000000, output = output)      benchmark.addCase(s\"Test query\") { _ =>        val query =          s\"\"\"             |SELECT a, b, c, d,             |       a * b / c as s1,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END s2,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |       ELSE 0 END s3,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s4,             |       CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |            CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                 CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN             |                      CASE WHEN d = 0 THEN 0 WHEN a * b / c > 0 THEN 1 ELSE 0 END             |                 ELSE 0 END             |            ELSE 0 END             |       ELSE 0 END s5             |FROM tab             |\"\"\".stripMargin        spark.sql(query).noop()      }      benchmark.run()```Local benchmark result:Before this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         9713           9900         263          0.2        4856.7       1.0X```After this PR:```Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.16Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHzSubexpression elimination in ProjectExec:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative------------------------------------------------------------------------------------------------------------------------Test query                                         1238           1307          98          8.1         123.8       1.0X```### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Exists UT.
2	-2	### What changes were proposed in this pull request?MetricSystem picks up new metrics from sources that are added throughout execution. If you do measurements via dynamic proxies you might not want to redeclare all metrics that the proxies will create and you'd prefer them to get populated as they're being produced. Right now all sources are processed only onceat startup and metrics are picked up only if they have been registered statically at compile time. Behaviour I am proposing lets you not have to declare metrics in two places.This had been previously suggested in #18406, #29980,  #31267, #35357, #36889, #38209 and #39755.### Why are the changes needed?Currently there's no way to access MetricRegistry that MetricsSystem uses to hold its state and as such it's not possible to reprocess a source. MetricsSystem throws if any metric had already been registered previously.n.b. the MetricRegistry is added as a constructor argument to make testing easier but could as well be accessed via reflection as a private variable.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added tests
1	-2	### What changes were proposed in this pull request?This PR aims to change exceptions created in package org.apache.spark.memory to use error class. This also adds an error class INTERNAL_ERROR_MEMORY and uses that for the internal errors in the package.### Why are the changes needed?This is to move exceptions created in package org.apache.spark.memory to error class.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Existing unit tests.
1	-2	### What changes were proposed in this pull request?This pr aims to upgrade rocksdbjni from 8.0.0 to 8.1.1.1.### Why are the changes needed?The full release notes as follows:- https://github.com/facebook/rocksdb/releases/tag/v8.1.1### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manual test `RocksDBBenchmark`:**8.0.0**```[INFO] Running org.apache.spark.util.kvstore.RocksDBBenchmark                                                count   mean    min     max     95thdbClose                                         4       0.339   0.256   0.532   0.532dbCreation                                      4       69.637  4.128   268.371 268.371naturalIndexCreateIterator                      1024    0.005   0.002   1.581   0.006naturalIndexDescendingCreateIterator            1024    0.005   0.005   0.067   0.007naturalIndexDescendingIteration                 1024    0.006   0.004   0.029   0.008naturalIndexIteration                           1024    0.006   0.004   0.069   0.009randomDeleteIndexed                             1024    0.026   0.018   0.200   0.035randomDeletesNoIndex                            1024    0.015   0.012   0.050   0.018randomUpdatesIndexed                            1024    0.088   0.033   33.234  0.096randomUpdatesNoIndex                            1024    0.035   0.032   0.497   0.039randomWritesIndexed                             1024    0.129   0.035   56.362  0.121randomWritesNoIndex                             1024    0.041   0.034   1.415   0.047refIndexCreateIterator                          1024    0.005   0.004   0.029   0.006refIndexDescendingCreateIterator                1024    0.003   0.003   0.028   0.004refIndexDescendingIteration                     1024    0.007   0.005   0.044   0.008refIndexIteration                               1024    0.008   0.005   0.367   0.010sequentialDeleteIndexed                         1024    0.023   0.017   1.716   0.025sequentialDeleteNoIndex                         1024    0.015   0.012   0.043   0.017sequentialUpdatesIndexed                        1024    0.045   0.037   0.943   0.054sequentialUpdatesNoIndex                        1024    0.041   0.029   0.838   0.053sequentialWritesIndexed                         1024    0.051   0.043   2.945   0.056sequentialWritesNoIndex                         1024    0.037   0.030   2.828   0.042```**8.1.1.1**```[INFO] Running org.apache.spark.util.kvstore.RocksDBBenchmark                                                count   mean    min     max     95thdbClose                                         4       0.319   0.233   0.514   0.514dbCreation                                      4       69.826  3.767   268.894 268.894naturalIndexCreateIterator                      1024    0.005   0.002   1.409   0.007naturalIndexDescendingCreateIterator            1024    0.005   0.005   0.062   0.007naturalIndexDescendingIteration                 1024    0.006   0.004   0.026   0.008naturalIndexIteration                           1024    0.007   0.004   0.182   0.011randomDeleteIndexed                             1024    0.027   0.020   0.259   0.037randomDeletesNoIndex                            1024    0.016   0.013   0.041   0.019randomUpdatesIndexed                            1024    0.087   0.033   29.514  0.102randomUpdatesNoIndex                            1024    0.036   0.033   0.495   0.040randomWritesIndexed                             1024    0.120   0.033   53.330  0.126randomWritesNoIndex                             1024    0.041   0.035   1.403   0.047refIndexCreateIterator                          1024    0.006   0.005   0.019   0.007refIndexDescendingCreateIterator                1024    0.003   0.003   0.028   0.005refIndexDescendingIteration                     1024    0.007   0.005   0.053   0.008refIndexIteration                               1024    0.008   0.006   0.254   0.010sequentialDeleteIndexed                         1024    0.023   0.018   1.237   0.027sequentialDeleteNoIndex                         1024    0.015   0.013   0.043   0.018sequentialUpdatesIndexed                        1024    0.046   0.038   0.816   0.057sequentialUpdatesNoIndex                        1024    0.042   0.030   0.719   0.052sequentialWritesIndexed                         1024    0.050   0.042   2.089   0.057sequentialWritesNoIndex                         1024    0.036   0.030   2.030   0.042```- Checked core module UTs with rocksdb live ui```export LIVE_UI_LOCAL_STORE_DIR=/${basedir}/spark-ui    build/mvn clean install -pl core -am -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest -fn```all test passed
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->1. We can use define trait for the `SparkClassUtils` and let Spark Utils to extend that to avoid duplicate code.2. We can also define trait for the `JsonUtil` and JsonProtocol can also extend that to re-use code.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  3. If you fix a bug, you can clarify why it is a bug.-->Share more between common/utils module and Spark core.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT
1	-1	### What changes were proposed in this pull request?This PR aims to make `makeDotNode` method handle the missing `DeterministicLevel`.### Why are the changes needed?Some old Spark generated data do not have that field.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs with newly added test case.
2	-2	### What changes were proposed in this pull request?The PR proposes to provide support for the registration of an Arrow-optimized Python UDF in both vanilla PySpark and Spark Connect.### Why are the changes needed?Currently, when users register an Arrow-optimized Python UDF, it will be registered as a pickled Python UDF and thus, executed without Arrow optimization. We should support Arrow-optimized Python UDFs registration and execute them with Arrow optimization.### Does this PR introduce _any_ user-facing change?Yes. No API changes, but result differences are expected in some cases.Previously, a registered Arrow-optimized Python UDF will be executed without Arrow optimization.Now, it will be executed with Arrow optimization, as shown below.```sh>>> df = spark.range(2)>>> df.createOrReplaceTempView(\"df\")>>> from pyspark.sql.functions import udf>>> @udf(useArrow=True)... def f(x):...     return str(x)... >>> spark.udf.register('str_f', f)<pyspark.sql.udf.UserDefinedFunction object at 0x7fa1980c16a0>>>> spark.sql(\"select str_f(id) from df\").explain()  # Executed with Arrow optimization== Physical Plan ==*(2) Project [pythonUDF0#32 AS f(id)#30]+- ArrowEvalPython [f(id#27L)#29], [pythonUDF0#32], 101   +- *(1) Range (0, 2, step=1, splits=16)```Enabling or disabling Arrow optimization can produce result differences in some cases - we are working on minimizing the result differences though.### How was this patch tested?Unit test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As comment https://github.com/apache/spark/pull/41064#discussion_r1190332445, This PR amins to use the same method to compute LocalRelation's data length.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->code improve.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->GA
1	-1	### What changes were proposed in this pull request?Split test module `pyspark_pandas_connect`.Add a new module `pyspark_pandas_slow_connect` which should keep in line with `pyspark_pandas_slow`### Why are the changes needed?`pyspark_pandas_connect` may take 3~4 hours### Does this PR introduce _any_ user-facing change?No, test-only### How was this patch tested?updated CI
1	-2	### What changes were proposed in this pull request?ExpressionImplUtilsSuite was mistakenly in the `core` directory, while ExpressionImplUtilsSuite is under `catalyst`. This change moves the file to the same package the ExpressionImplUtils.### Why are the changes needed?The test suite was committed to the wrong location by mistake.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?This command runs the unit tests:```$ build/sbt \"catalyst/testOnly org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite\"```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add the scala client side `foreach` support.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Continuation of SS Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. Now they can use foreach in scala client exactly as how they used it in non-connect mode### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit testManually tested it works for both `Row` and `Int````import org.apache.spark.sql.{ForeachWriter, Row} import java.io._ val filePath = \"/home/wei.liu/test_foreach/output-row\" val writer = new ForeachWriter[Row] {    var fileWriter: FileWriter = _      def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)  // true to append      true    }      def process(row: Row): Unit = {      fileWriter.write(row.mkString(\ \"))  // separate fields with commas      fileWriter.write(\"\\")  // newline for each row    }      def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }  } val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load() val query = df .writeStream .foreach(writer) .outputMode(\"update\") .start() assert(query.exception.isEmpty)``````import org.apache.spark.sql.{ForeachWriter, Row} import java.io._ val filePath = \"/home/wei.liu/test_foreach/output-int\" val writer = new ForeachWriter[Int] {    var fileWriter: FileWriter = _      def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)  // true to append      true    }      def process(value: Int): Unit = {      fileWriter.write(value.toString)      fileWriter.write(\"\\")  // newline for each value    }      def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }  } val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load() val query = df .selectExpr(\"CAST(value AS INT)\") .as[Int] .writeStream .foreach(writer) .outputMode(\"update\") .start() ```NOTE: below __DOESN'T__ work as of now. A ticket is filed regarding this: SPARK-43796```import org.apache.spark.sql.ForeachWriter import java.io._ val filePath = \"/home/wei.liu/test_foreach/output-custom\" case class MyTestClass(value: Int) {      override def toString: String = value.toString}val writer = new ForeachWriter[MyTestClass] {    var fileWriter: FileWriter = _    def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)      true    }    def process(row: MyTestClass): Unit = {      fileWriter.write(row.toString)      fileWriter.write(\"\\")    }    def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }}val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load()val query = df .selectExpr(\"CAST(value AS INT)\") .as[MyTestClass] .writeStream .foreach(writer) .outputMode(\"update\") .start()```
1	-1	### What changes were proposed in this pull request?This PR aims to upgrade Apache Arrow to 12.0.0 for Apache Spark 3.5.0.### Why are the changes needed?To bring the latest updates- https://arrow.apache.org/release/12.0.0.html### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When we try to speed up Timestamp type inference with format (PR: #36562 #41078 #41091). There is no way to judge whether the change has improved the speed for Timestamp type inference.So we need a benchmark to measure whether our optimization of Timestamp type inference is useful, we have valid Timestamp value benchmark at now, but don't have invalid Timestamp value benchmark when use Timestamp type inference.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add new benchmark for Timestamp type inference when use invalid value, to make sure our speed up PR work normally.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?benchmarks already are test code.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-1	### What changes were proposed in this pull request?This PR aims to support R 4.3.0 officially in Apache Spark 3.5.0 by upgrading AppVeyor to 4.3.0.### Why are the changes needed?R 4.3.0 is released on April 21th, 2023.- https://stat.ethz.ch/pipermail/r-announce/2023/000691.html### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.I verified locally.```$ R --versionR version 4.3.0 (2023-04-21) -- \"Already Tomorrow\"Copyright (C) 2023 The R Foundation for Statistical ComputingPlatform: aarch64-apple-darwin20 (64-bit)R is free software and comes with ABSOLUTELY NO WARRANTY.You are welcome to redistribute it under the terms of theGNU General Public License versions 2 or 3.For more information about these matters seehttps://www.gnu.org/licenses/.$ build/sbt package -Psparkr$ R/install-dev.sh; R/run-tests.sh...Tests passed.```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Remove the dummy dependency `hadoop-openstack` from Spark binary artifacts.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[HADOOP-18442](https://issues.apache.org/jira/browse/HADOOP-18442) removed the `hadoop-openstack` and temporarily retained a dummy jar for the downstream project which consumes it.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-1	### What changes were proposed in this pull request?This PR aims the following.- Remove Daily GitHub Action job on branch-3.2 to save the community resource  - https://github.com/apache/spark/actions/workflows/build_branch32.yml- Simplify `build_and_test.yml` by removing `branch-3.2` specific code.### Why are the changes needed?Apache Spark 3.2 is EOL.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The deprecation message of `createOrReplace` indicates that we should change `createOrReplace` to `serverSideApply` instead.```@deprecated please use {@link ServerSideApplicable#serverSideApply()} or attempt a create and edit/patch operation.```The change is not fully equivalent, but I believe it's reasonable.> With the caveat that the user may choose not to use forcing if they want to know when there are conflicting changes.> > Also unlike createOrReplace if the resourceVersion is set on the resource and a replace is attempted, it will be optimistically locked.See more details at https://github.com/fabric8io/kubernetes-client/pull/5073### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Remove usage of deprecated API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Release Notes: https://github.com/fabric8io/kubernetes-client/releases/tag/v6.6.1### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It's basically a routine to keep the third-party libs up-to-date.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-1	### What changes were proposed in this pull request?Augument the user agent string sent over the service to includeoperating system and Python version.### Why are the changes needed?Including OS, Python and Spark versions in the user agent improvestracking to see how Spark Connect is used across Python versionsand platforms.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit tests attached.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->As title, Make `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` works for Spark on K8S.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Now seems `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` only work for yarn cluster mode.We need to make it works for Spark on K8s.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, `SPARK_DRIVER_LOG_URL_` and `SPARK_DRIVER_ATTRIBUTE_` works for Spark on K8S after this PR.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT.
1	-2	### What changes were proposed in this pull request?This PR aims to change exceptions created in package org.apache.spark.netrowk to use error class. This also adds an error class INTERNAL_ERROR_NETWORK and uses that for the internal errors in the package.### Why are the changes needed?This is to move exceptions created in package org.apache.spark.network to error class.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing tests.
2	-2	### What changes were proposed in this pull request?This PR add more skip properties when making distribution:- `-Dmaven.javadoc.skip=true` to skip generating javadoc- `-Dmaven.scaladoc.skip=true` to skip generating scaladoc. Please see: https://davidb.github.io/scala-maven-plugin/doc-jar-mojo.html#skip- `-Dmaven.source.skip` to skip generating sources.jar- `-Dcyclonedx.skip=true` to skip making bom. Please see: https://cyclonedx.github.io/cyclonedx-maven-plugin/makeBom-mojo.html#skip### Why are the changes needed?Reduce time spent on making distribution.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test:```sh./dev/make-distribution.sh --tgz --pip -Phadoop-3 -Phive -Phive-thriftserver -Pyarn -Phadoop-provided```Before this PR | After this PR-- | --43 min total from scheduled to completion | 23 min total from scheduled to completion
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a followup of https://github.com/apache/spark/pull/40739 to do some code cleanup1. remove the pattern `PYTHON_UDAF` as it's not used by any rule.2. add `PythonFuncExpression.evalType` for convenience: catalyst rules (including third-party extensions) may want to get the eval type of a python function, no matter it's UDF or UDAF.3. update the python profile to use `PythonUDAF.resultId` instead of `AggregateExpression.resultId`, to be consistent with `PythonUDF`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  4. If you fix a bug, you can clarify why it is a bug.-->code cleanup### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
2	-3	### What changes were proposed in this pull request?In the PR, I propose to extend the grammar rule of the `DATEADD`/`TIMESTAMPADD` and `DATEDIFF`/`TIMESTAMPDIFF`, and catch wrong type of the first argument `unit` when an user pass a string instead of an identifier like `YEAR`, ..., `MICROSECOND`. In that case, Spark raised an error of new error class `INVALID_PARAMETER_VALUE.DATETIME_UNIT`.### Why are the changes needed?To make the error message clear for the case when a literal string instead of an identifier is passed to the datetime `ADD`/`DIFF` functions:```sqlspark-sql (default)> select dateadd('MONTH', 1, date'2023-05-11');[WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `dateadd` requires 2 parameters but the actual number is 3. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix.; line 1 pos 7```### Does this PR introduce _any_ user-facing change?Yes, it changes the error class.After the changes:```sqlspark-sql (default)> select dateadd('MONTH', 1, date'2023-05-11');[INVALID_PARAMETER_VALUE.DATETIME_UNIT] The value of parameter(s) `unit` in `dateadd` is invalid: expects one of the units without quotes YEAR, QUARTER, MONTH, WEEK, DAY, DAYOFYEAR, HOUR, MINUTE, SECOND, MILLISECOND, MICROSECOND, but got the string literal 'MONTH'.(line 1, pos 7)== SQL ==select dateadd('MONTH', 1, date'2023-05-11')-------^^^```### How was this patch tested?By running the existing test suites:```$ PYSPARK_PYTHON=python3 build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite\"```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.--> ### What changes were proposed in this pull request?In a heterogeneous cluster, it is hard to debug issues from the Operating system and Java/python versions incompatibilities. Currently, the Operating system and Java/python versions details are missing in the spark application log, this PR adds that information to the application info log, which will help in troubleshooting and debugging any issues that may arise### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To troubleshoot host-specific issues in the Spark application ran on heterogeneous cluster### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manually tested
1	-1	### What changes were proposed in this pull request?This PR aims to handle a corner case where `hadoopProperties` and `metricsProperties` is null which means not loaded.### Why are the changes needed?To prevent NPE.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs with the newly added test suite.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This change adds a new spark connect relation type `CachedDataFrame`, which can represent a DataFrame that's been cached on the server side.On the server side, each (userId, sessionId) has a map to cache DataFrame. DataFrame will be removed from cache when the corresponding session expires. (The caller can also evict the DataFrame from cache earlier, depending on the logic.)On the client side, a new relation type and function is added. The new function can create a DataFrame reference given a key. The key is the id of a cached DataFrame, which is usually passed from server to the client. When transforming the DataFrame reference, the server finds the actual DataFrame from the cache and replace it.One use case of this function will be streaming foreachBatch(). Server needs to call user function for every batch which takes a DataFrame as argument. With the new function, we can cache the DataFrame on the server. Pass the id back to client which can creates the DataFrame reference. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This change is needed to support streaming foreachBatch() in Spark Connect.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Scala unit test.Manual test.(More end to end test will be added when foreachBatch() is supported. Currently there is no way to add a dataframe to the server cache using Python.)
2	-1	### What changes were proposed in this pull request?Fix nested MapType behavior in Pandas UDF (and Arrow-optimized Python UDF).Previously during Arrow-pandas conversion, only the outermost layer is converted to a dictionary; but now nested MapType will be converted to nested dictionaries.That applies to Spark Connect as well.### Why are the changes needed?Correctness and consistency (with `createDataFrame` and `toPandas` when Arrow is enabled).### Does this PR introduce _any_ user-facing change?Yes.Nested MapType type support is corrected in Pandas UDF```py>>> schema = StructType([...      StructField(\"id\ StringType(), True),...      StructField(\"attributes\ MapType(StringType(), MapType(StringType(), StringType())), True)... ])>>> >>> data = [...    (\"1\ {\"personal\": {\"name\": \"John\ \"city\": \"New York\"}}),... ]>>> df = spark.createDataFrame(data, schema)>>> @pandas_udf(StringType())... def f(s: pd.Series) -> pd.Series:...    return s.astype(str)... >>> df.select(f(df.attributes)).show(truncate=False)```The results of `df.select(f(df.attributes)).show(truncate=False)` is corrected**FROM**```py+------------------------------------------------------+                        |f(attributes)                                         |+------------------------------------------------------+|{'personal': [('name', 'John'), ('city', 'New York')]}|+------------------------------------------------------+```**TO**```py>>> df.select(f(df.attributes)).show(truncate=False)+--------------------------------------------------+|f(attributes)                                     |+--------------------------------------------------+|{'personal': {'name': 'John', 'city': 'New York'}}|+--------------------------------------------------+```**Another more obvious example:**```py>>> @pandas_udf(StringType())... def extract_name(s:pd.Series) -> pd.Series:...     return s.apply(lambda x: x['personal']['name'])...>>> df.select(extract_name(df.attributes)).show(truncate=False)````df.select(extract_name(df.attributes)).show(truncate=False)` is corrected**FROM**```pyorg.apache.spark.api.python.PythonException: Traceback (most recent call last):...TypeError: list indices must be integers or slices, not str```**TO**```py+------------------------+|extract_name(attributes)|+------------------------+|John                    |+------------------------+```### How was this patch tested?Unit tests.
2	-2	### What changes were proposed in this pull request?This is a follow-up of #40575.Disables JVM stack trace by default.```py% ./bin/pyspark --remote local...>>> spark.conf.set(\"spark.sql.ansi.enabled\ True)>>> spark.sql('select 1/0').show()...Traceback (most recent call last):...pyspark.errors.exceptions.connect.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^>>> >>> spark.conf.set(\"spark.sql.pyspark.jvmStacktrace.enabled\ True)>>> spark.sql('select 1/0').show()...Traceback (most recent call last):...pyspark.errors.exceptions.connect.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^JVM stacktrace:org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:226)\tat org.apache.spark.sql.catalyst.expressions.DivModLike.eval(arithmetic.scala:674)...```### Why are the changes needed?Currently JVM stack trace is enabled by default.```py% ./bin/pyspark --remote local...>>> spark.conf.set(\"spark.sql.ansi.enabled\ True)>>> spark.sql('select 1/0').show()...Traceback (most recent call last):...pyspark.errors.exceptions.connect.ArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^JVM stacktrace:org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.== SQL(line 1, position 8) ==select 1/0       ^^^\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:226)\tat org.apache.spark.sql.catalyst.expressions.DivModLike.eval(arithmetic.scala:674)...```### Does this PR introduce _any_ user-facing change?Users won't see the JVM stack trace by default.### How was this patch tested?Existing tests.
2	-3	### What changes were proposed in this pull request?Supports struct type in `createDataFrame` from pandas DataFrame.With Arrow optimization, it works without the fallback:```py>>> import pandas as pd>>> from pyspark.sql.types import Row>>>>>> pdf = pd.DataFrame(...     {\"a\": [Row(1, \"a\"), Row(2, \"b\")], \"b\": [{\"s\": 3, \"t\": \"x\"}, {\"s\": 4, \"t\": \"y\"}]}... )>>> schema = \"a struct<x int, y string>, b struct<s int, t string>\">>>>>> df = spark.createDataFrame(pdf, schema)>>> df.show()+------+------+|     a|     b|+------+------+|{1, a}|{3, x}||{2, b}|{4, y}|+------+------+```and Spark Connect also works.### Why are the changes needed?In vanilla PySpark without Arrow optimization, `Row` object or `dict` can be handled as struct type if the schema is provided:```py>>> import pandas as pd>>> from pyspark.sql.types import *>>>>>> pdf = pd.DataFrame(...     {\"a\": [Row(1, \"a\"), Row(2, \"b\")], \"b\": [{\"s\": 3, \"t\": \"x\"}, {\"s\": 4, \"t\": \"y\"}]}... )>>> schema = \"a struct<x int, y string>, b struct<s int, t string>\">>>>>> df = spark.createDataFrame(pdf, schema)>>> df.show()+------+------+|     a|     b|+------+------+|{1, a}|{3, x}||{2, b}|{4, y}|+------+------+```Whereas with Arrow, it uses a fallback to make it:```py>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.createDataFrame(pdf, schema).show()/.../pyspark/sql/pandas/conversion.py:329: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:  A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.  warn(msg)+------+------+|     a|     b|+------+------+|{1, a}|{3, x}||{2, b}|{4, y}|+------+------+```and Spark Connect fails:```py>>> df = spark.createDataFrame(pdf, schema)Traceback (most recent call last):...ValueError: A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>```### Does this PR introduce _any_ user-facing change?`Row` object or `dict` in pandas DataFrame works as struct type when `createDataFrame` if the schema is provided.### How was this patch tested?Added the related test.
1	-2	### What changes were proposed in this pull request?This adds a new field in QueryTerminatedEvent about \"error class\" if the information exists in the exception. Note that it doesn't retrieve the information from StreamingQueryException, because it is marked as it's own error class and we want to know the causing exception.### Why are the changes needed?Custom streaming query listeners can get the \"categorized\" error as cause of the query termination, which can be used to determine trends of the errors, aggregation, etc.### Does this PR introduce _any_ user-facing change?Yes, users having customer streaming query listener will have additional information in termination event.### How was this patch tested?Modified UTs.
2	-1	### What changes were proposed in this pull request?Spark 3.4.0 released the new syntax: `OFFSET clause`.But the SQL reference missing the description for it.### Why are the changes needed?Adds SQL reference for `OFFSET` clause.### Does this PR introduce _any_ user-facing change?'Yes'.Users could find out the SQL reference for `OFFSET` clause.### How was this patch tested?Manual verify.![image](https://github.com/apache/spark/assets/8486025/55398194-5193-45eb-ac04-10f5f0793f7f)![image](https://github.com/apache/spark/assets/8486025/fef0abc1-7dfa-44e2-b2e0-a56fa82a0817)![image](https://github.com/apache/spark/assets/8486025/5ab9dc39-6812-45b4-a758-85668ab040f1)![image](https://github.com/apache/spark/assets/8486025/b726abd4-daae-4de4-a78e-45120573e699)
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Change `hadoop-client-runtime`'s scope to `provided` in kafka/kinesis assembly modules.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->`hadoop-client-runtime` is already included in Spark binary tgz, we should not package it again into the optional assembly jar.This issue exists since SPARK-33212.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manual review.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Spark does not use protobuf 2.5.0 directly, instead, it comes from other dependencies, with the following changes, now, Spark does not require protobuf 2.5.0 (please let me know if I miss something),- SPARK-40323 upgraded ORC 1.8.0, which moved from protobuf 2.5.0 to a shaded protobuf 3- SPARK-33212 switched from Hadoop vanilla client to Hadoop shaded client, also removed the protobuf 2 dependency. SPARK-42452 removed the support for Hadoop 2.- SPARK-14421 shaded and relocated protobuf 2.6.1, which is required by the kinesis client, into the kinesis assembly jar- Spark itself's core/connect/protobuf modules use protobuf 3, also shaded and relocated all protobuf 3 deps.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Remove the obsolete dependency, which is EOL long ago, and has CVEs [CVE-2022-3510](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3510) [CVE-2022-3509](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3509) [CVE-2022-3171](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-3171) [CVE-2021-22569](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22569)### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA.
2	-2	### What changes were proposed in this pull request?The batch of errors migrated to error classes as part of spark-40540 contains an error that got mixed up with the wrong error message:[ambiguousRelationAliasNameInNestedCTEError](https://github.com/apache/spark/commit/43a6b932759865c45ccf36f3e9cf6898c1b762da#diff-744ac13f6fe074fddeab09b407404bffa2386f54abc83c501e6e1fe618f6db56R1983) uses the same error message as the following commandUnsupportedInV2TableError:WITH t AS (SELECT 1), t2 AS ( WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2;AnalysisException: t is not supported for v2 tablesThe error should be:AnalysisException: Name tis ambiguous in nested CTE.Please set spark.sql.legacy.ctePrecedencePolicy to CORRECTED so that name defined in inner CTE takes precedence. If set it to LEGACY, outer CTE definitions will take precedence. See more details in SPARK-28228.### Does this PR introduce _any_ user-facing change?Fix user-facing error message for ambiguous name in nested CTEs.### How was this patch tested?
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?The example here is multiplying Decimal(38, 10) by another Decimal(38, 10), but I think it can be reproduced with other number combinations, and possibly with divide too.```scalaSeq(\"9173594185998001607642838421.5479932913\").toDF.selectExpr(\"CAST(value as DECIMAL(38,10)) as a\").selectExpr(\"a * CAST(-12 as DECIMAL(38,10))\").show(truncate=false)```This produces an answer in Spark of `-110083130231976019291714061058.575920` But if I do the calculation in regular java BigDecimal I get `-110083130231976019291714061058.575919````javaBigDecimal l = new BigDecimal(\"9173594185998001607642838421.5479932913\");BigDecimal r = new BigDecimal(\"-12.0000000000\");BigDecimal prod = l.multiply(r);BigDecimal rounded_prod = prod.setScale(6, RoundingMode.HALF_UP);```Spark does essentially all of the same operations, but it used Decimal to do it instead of java's BigDecimal directly. Spark, by way of Decimal, will set a `MathContext` for the multiply operation that has a max precision of 38 and will do half up rounding. That means that the result of the multiply operation in Spark is `-110083130231976019291714061058.57591950`, but for the java BigDecimal code the result is `-110083130231976019291714061058.57591949560000000000`. Then Spark will call `toPrecision` to round up again. So Spark round up result twice.This PR change the code-gen and `nullSafeEval` of `Arithmetic`. To make sure when use multiply method will set custom `MathContext` with precision of 39 (meaning one more scale). Then round up twice will not affect result.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix the bug Decimal multiply produce wrong answer<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?In the PR, I propose to extend the rules of `primaryExpression` in `SqlBaseParser.g4`, and two more functions `DATE_ADD` and `DATE_DIFF` that accept 3-args in the same way as the existing expressions `DATEADD` and `DATEDIFF`.### Why are the changes needed?To do not confuse users and improve user experience with Spark SQL. There are the `DATE_ADD` and `DATE_DIFF` aliases for 2-args functions: https://github.com/apache/spark/blob/46332d985e52f76887c139f67d1471b82e85d5ca/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L606-L607but Spark SQL doesn't provide 3-args `DATE_ADD` and `DATE_DIFF`. And Spark SQL outputs the confusing error:```sqlspark-sql (default)> select date_add(MONTH, 1, date'2023-05-13');[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column or function parameter with name `MONTH` cannot be resolved. ; line 1 pos 16;```### Does this PR introduce _any_ user-facing change?No, only if user code depends on the error class. After the changes, Spark SQL outputs an result similar to `DATEADD`/`DATEDIFF`:```sqlspark-sql (default)> select date_add(MONTH, 1, date'2023-05-13');2023-06-13 00:00:00```### How was this patch tested?By running the existing test suites:```$ PYSPARK_PYTHON=python3 build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite\"```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Increase timeout to 30s for tests in StreamingQuerySuite### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To make the tests more stable### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This is a test only change.
1	-1	### What changes were proposed in this pull request?This pr aims to upgrade sbt from 1.8.2 to 1.8.3### Why are the changes needed?Release notes: https://github.com/sbt/sbt/releases/tag/v1.8.3<img width=\"1148\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/b8a5a6f9-98b4-45c3-8dbf-c4c4db3ca76a\">### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-3	### What changes were proposed in this pull request?This is a follow-up of #40988.Fixes `df.toPandas` to support empty columns.The result will be no-column, same-length dataframe.```py>>> spark.range(2).select([]).toPandas()Empty DataFrameColumns: []Index: [0, 1]```### Why are the changes needed?Currently `df.toPandas()` fails if the columns are empty:```py>>> spark.range(2).select([]).toPandas()Traceback (most recent call last):...ValueError: No objects to concatenate```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added the related test.
1	-2	### What changes were proposed in this pull request?The pr aims to fix  wrong error message used for `ambiguousRelationAliasNameInNestedCTEError`.### Why are the changes needed?Fix bug.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-2	### What changes were proposed in this pull request?See [SPARK-43491](https://issues.apache.org/jira/browse/SPARK-43491).The query results of `in ('00')` and `= '00'`  are inconsistent. ![image](https://github.com/apache/spark/assets/132866841/007cc156-625a-4d0d-a0d4-6a0442cfc8f6)We do this work to ensure when dataTypes of elements in `In` expression are the same, it will behaviour as same as BinaryComparison expression like `EqualTo` when the switch is open(`spark.sql.legacy.inExpressionCompatibleWithEqualTo.enabled=true`).```scala// test data and content: // test.json// {\"name\":\"Michael\\"age\":0}spark.read().json(\"examples/src/main/resources/test.json\").createOrReplaceTempView(\"t\");```**Before change (see Filter node in Analyzed Logical Plan)**```spark.sql(\"select * from t where age in ('00')\").explain(true);== Parsed Logical Plan =='Project [*]+- 'Filter 'age IN (00)   +- 'UnresolvedRelation [t], [], false== Analyzed Logical Plan ==age: bigint, name: stringProject [age#7L, name#8]+- Filter cast(age#7L as string) IN (cast(00 as string))   +- SubqueryAlias t\t  +- Relation[age#7L,name#8] json== Optimized Logical Plan ==Filter (isnotnull(age#7L) AND (cast(age#7L as string) = 00))+- Relation[age#7L,name#8] json== Physical Plan ==*(1) Filter (isnotnull(age#7L) AND (cast(age#7L as string) = 00))+- FileScan json [age#7L,name#8] Batched: false, DataFilters: [isnotnull(age#7L), (cast(age#7L as string) = 00)], Format: JSON, Location: InMemoryFileIndex[file:/D:/code/spark/examples/src/main/resources/test.json], PartitionFilters: [], PushedFilters: [IsNotNull(age)], ReadSchema: struct<age:bigint,name:string>+---+----+|age|name|+---+----++---+----+```**After change (see Filter node in Analyzed Logical Plan)**```spark.sql(\"select * from t where age in ('00')\").explain(true);== Parsed Logical Plan =='Project [*]+- 'Filter 'age IN (00)   +- 'UnresolvedRelation [t], [], false== Analyzed Logical Plan ==age: bigint, name: stringProject [age#7L, name#8]+- Filter cast(age#7L as bigint) IN (cast(00 as bigint))   +- SubqueryAlias t\t  +- Relation[age#7L,name#8] json== Optimized Logical Plan ==Filter (isnotnull(age#7L) AND (age#7L = 0))+- Relation[age#7L,name#8] json== Physical Plan ==*(1) Filter (isnotnull(age#7L) AND (age#7L = 0))+- FileScan json [age#7L,name#8] Batched: false, DataFilters: [isnotnull(age#7L), (age#7L = 0)], Format: JSON, Location: InMemoryFileIndex[file:/D:/code/spark/examples/src/main/resources/test.json], PartitionFilters: [], PushedFilters: [IsNotNull(age), EqualTo(age,0)], ReadSchema: struct<age:bigint,name:string>+---+-------+|age|   name|+---+-------+|  0|Michael|+---+-------+```### Why are the changes needed?The query results of Spark SQL and Hive SQL are inconsistent with same sql. Spark SQL calculates `0 in ('00') ` as false in 3.1.1, which act different from `=` keyword, but Hive calculates true in 3.1.0 and false in 2.3.3. Hive has fixed the `in` keyword in 3.1.0, but SparkSQL does not.for example, this two query sql should have same result, how ever, the query result is different:```scala> spark.sql(\"select 1 as test where 0 in ('00')\").show;+----+|test|+----++----+scala> spark.sql(\"select 1 as test where 0 = '00'\").show;+----+                                                                          |test|+----+|   1|+----+```hive 2.3.3![image](https://github.com/apache/spark/assets/132866841/417768c8-3dda-49a4-9629-aba5fca39e3e)hive 3.1.0![image](https://github.com/apache/spark/assets/132866841/65edf7fd-0c1c-457b-97f3-c2fb399156b3)### Does this PR introduce _any_ user-facing change?We add a switch to support `In` expression compatible with `EqualTo` expression with false as default value, to make sure it will not change default behavior of Spark SQL.### How was this patch tested?By set spark.sql.legacy.inExpressionCompatibleWithEqualTo.enabled=true/false, to check whether the analyzed logical plan will cast expression as expected. For true, it will generate same Cast logical plan as EqualTo, and false will keep the old Cast logical plan solution.
2	-2	### What changes were proposed in this pull request?Allow expression lineage through partial aggregation (AggregatePart) and Window operators. This enables optimizations such as DFP, bloom filters to trace down expression lineage and do necessary optimizations.Also, enhanced the logic to accept the following additional parameters to be able to customize lineage logic.noLineageOnNullPaddingSide: If true, lineage is not tracked down through the null padding side of the outer joins.stopAtSubPlan: If specified, lineage tracking stops at this specified operator.Code changes include delegating the core logic to findExpressionAndTrackLineageDownBase() and the current signature findExpressionAndTrackLineageDown() would become wrapper to avoid changes to the existing callers.### Why are the changes needed?To support optimizations such as bloom filters, DFP, etc. which depend on the expression lineage.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Unit tested
1	-1	### What changes were proposed in this pull request?As discussed in https://github.com/apache/spark/pull/40945#discussion_r1191804004 and https://github.com/apache/spark/pull/40945#discussion_r1191804004, `replicate()` is a very private method of `HdfsDataOutputStreamBuilder`, so this pr uses case match to further remove the use of reflection in `SparkHadoopUtil#createFile`.### Why are the changes needed?Code simplification: remove reflection calls used for compatibility with Hadoop2.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This pr aims upgrade `RoaringBitmap` from 0.9.39 to 0.9.44. ### Why are the changes needed?The new version brings 2 bug fix:- https://github.com/RoaringBitmap/RoaringBitmap/issues/619 | https://github.com/RoaringBitmap/RoaringBitmap/pull/620- https://github.com/RoaringBitmap/RoaringBitmap/issues/623 | https://github.com/RoaringBitmap/RoaringBitmap/pull/624The full release notes as follows:- https://github.com/RoaringBitmap/RoaringBitmap/releases/tag/0.9.40- https://github.com/RoaringBitmap/RoaringBitmap/releases/tag/0.9.41- https://github.com/RoaringBitmap/RoaringBitmap/releases/tag/0.9.44### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?At the moment we do not have any function to get value of JSON array by use specified index.I add a `json_array_get` function which will return the value of JSON array by use specified index.- This function will return value of JSON array, if JSON array is valid.```sqlselect json_array_get('[{\"a\":123},{\"b\":\"hello\"}]', 1);+--------------------------------------------+|json_array_get([{\"a\":123},{\"b\":\"hello\"}], 1)|+--------------------------------------------+|                               {\"b\":\"hello\"}|+--------------------------------------------+```- In case of any other valid JSON string, the specified index does not exist, invalid JSON string or null array or NULL input , NULL will be returned.```sqlselect json_array_get(\"[[1],[2,3],[]]\ 4);+---------------------------------+|json_array_get([[1],[2,3],[]], 4)|+---------------------------------+|                             NULL|+---------------------------------+```<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?- As mentioned in JIRA, this function is supported by presto- For better user experience and ease of use.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?Yes, now users can get value of a json array by using `json_array_get`.<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-2	### What changes were proposed in this pull request?add tests for:1, `DataFrame.drop` with empty column;2, `DataFrame.drop` with column names containing dot;### Why are the changes needed?for better test coverage, the two UTs were once broken in [SPARK-39895](https://issues.apache.org/jira/browse/SPARK-39895), and then fixed in [SPARK-42444](https://issues.apache.org/jira/browse/SPARK-42444)### Does this PR introduce _any_ user-facing change?no, test-only### How was this patch tested?added UTs
2	-2	### What changes were proposed in this pull request?1. sparkConf's getOption/get supports substitution for raw keys2. sparkConf's getAllWithPrefix supports substitution for values### Why are the changes needed?To be consistent between `sparkConf.get(CONFIG_ENTRY_A)` and `sparkConf.get(CONFIG_ENTRY_A.key)`. It's also a todo left for: https://github.com/apache/spark/pull/18394#discussion_r126241078### Does this PR introduce _any_ user-facing change?Yes. Some values contain env/sys variables, such as `${env:xxx}`, `${sys:yyy}` might be substituted by the real one. However I doubt that's rarely possibel### How was this patch tested?Added new UTs.
2	-2	### What changes were proposed in this pull request?The pr aims to add a max distance argument to the levenshtein() function.### Why are the changes needed?Currently, Spark's levenshtein(str1, str2) function can be very inefficient for long strings. Many other databases which support this type of built-in function also take a third argument which signifies a maximum distance after which it is okay to terminate the algorithm.For example something like:`levenshtein(str1, str2[, max_distance])`the function stops computing the distant once the max values is reached.See postgresql for an example of a 3 argument [levenshtein](https://www.postgresql.org/docs/current/fuzzystrmatch.html#id-1.11.7.26.7).### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Add new UT & Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to remove redundant character escape \"\\\\\" and add UT for SparkHadoopUtil.substituteHadoopVariables.### Why are the changes needed?Make code clean & remove warning.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA & Add new UT.
1	-1	### What changes were proposed in this pull request?The pr aims to replace the link related to hadoop version 2 with hadoop version 3### Why are the changes needed?Because [SPARK-40651](https://issues.apache.org/jira/browse/SPARK-40651) Drop Hadoop2 binary distribtuion from release process and [SPARK-42447](https://issues.apache.org/jira/browse/SPARK-42447) Remove Hadoop 2 GitHub Action job.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
2	-2	### What changes were proposed in this pull request?The pr aims to fix error exception about 'DELETE from Hive table'### Why are the changes needed?Proper names of error classes should improve user experience with Spark SQL.**BEFORE**```scala> sql(\"delete from t\")org.apache.spark.SparkException: [INTERNAL_ERROR] Unexpected table relation: HiveTableRelation [`spark_catalog`.`default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [a#0], Partition Cols: []]```**AFTER**```scala> sql(\"delete from t\")org.apache.spark.sql.AnalysisException: [UNSUPPORTED_FEATURE.TABLE_OPERATION] The feature is not supported: Table `spark_catalog`.`default`.`t` does not support DELETE. Please check the current catalog and namespace to make sure the qualified table name is expected, and also check the catalog implementation which is configured by \"spark.sql.catalog\".```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA & Add new UT.
1	-2	### What changes were proposed in this pull request?Keep track of completed container ids in YarnAllocator and don't update internal state of a container if it's already completed.### Why are the changes needed?YarnAllocator updates internal state adding running executors after executor launch in a separate thread. That can happen after the containers are already completed (e.g. preempted) and processed by YarnAllocator. Then YarnAllocator mistakenly thinks there are still running executors which are already lost. As a result, application hangs without any running executors.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Added UT.
1	-1	### What changes were proposed in this pull request?The `KVGDS#agg` and `reduceGroups` was not able to chain mapValues functions directly with columns. This PR add the a new unresolved func `map_values` to carry the mapValues func from the client to the server.### Why are the changes needed?Missing functionality for the KVGDS.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?E2E
1	-1	### What changes were proposed in this pull request?Update StateStoreOperationsBenchmark to reflect updates to RocksDB usage as state store provider### Why are the changes needed?Need the changes to unblock RocksDB JNI upgrade### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Ran it locally and using Github Actions. Run now takes ~40-50mins
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request? - Defines basic interfaces of Evaluator / Transformer / Model / Evaluator, these interfaces are designed to support both spark connect and legacy spark mode, and are designed to support train / transform / evaluate over either spark dataframe or pandas dataframe - Implement a feature transformer `MaxAbsScaler`, `ScalerScaler` - Implement a regressor evaluator `RegressorEvaluator` that supports MSE and R2 metric evaluation - Implement a summarizer via pure python code that can summarize array type columns on spark dataframe. - Some utility methods.### Why are the changes needed?Project: Distributed ML <> spark connect### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?Unit tests.
1	-1	### What changes were proposed in this pull request?This PR proposes to add a migration guide for https://github.com/apache/spark/pull/38700.### Why are the changes needed?To guide users about the workaround of bringing the namedtuple patch back.### Does this PR introduce _any_ user-facing change?Yes, it adds the migration guides for end-users.### How was this patch tested?CI in this PR will test it out.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Upgrade Parquet from 1.13.0 to 1.13.1### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->[Apache Parquet 1.13.1](https://parquet.apache.org/blog/2023/05/18/1.13.1/) is available, the release notes are> ### Version 1.13.1 ###> > Release Notes - Parquet - Version 1.13.1> > #### Improvement> > *   [PARQUET-2276](https://issues.apache.org/jira/browse/PARQUET-2276) - Bring back support > for Hadoop 2.7.3> *   [PARQUET-2297](https://issues.apache.org/jira/browse/PARQUET-2297) - Skip delta problem > check> *   [PARQUET-2292](https://issues.apache.org/jira/browse/PARQUET-2292) - Improve default > SpecificRecord model selection for Avro `{Write,Read}`Support> *   [PARQUET-2290](https://issues.apache.org/jira/browse/PARQUET-2290) - Add CI for Hadoop 2> *   [PARQUET-2282](https://issues.apache.org/jira/browse/PARQUET-2282) - Don't initialize HadoopCodec> *   [PARQUET-2283](https://issues.apache.org/jira/browse/PARQUET-2283) - Remove Hadoop HiddenFileFilter> *   [PARQUET-2081](https://issues.apache.org/jira/browse/PARQUET-2081) - Fix support for rewriting files without ColumnIndexes### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Pass GA
2	-2	### What changes were proposed in this pull request?The pr aims to convert _LEGACY_ERROR_TEMP_2029 to INTERNAL_ERROR.### Why are the changes needed?1. I found that it can only be triggered it with the parameter value: UP,DOWN,HALF_DOWN,UNNECESSARY, but from a user's perspective, it is impossible (the internal code limits its value to only: HALF_UP,HALF_EVEN,CEILING,FLOOR), so we should convert it to an internal error.2. The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?1. Update existed UT.2. Pass GA.
1	-2	### What changes were proposed in this pull request?Make `DataFrame.drop` accept empty column### Why are the changes needed?to be consistent with vanilla PySpark### Does this PR introduce _any_ user-facing change?yes```In [1]: df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\ \"age\"))In [2]: df.drop()```before:```In [2]: df.drop()---------------------------------------------------------------------------PySparkValueError                         Traceback (most recent call last)Cell In[2], line 1----> 1 df.drop()File ~/Dev/spark/python/pyspark/sql/connect/dataframe.py:449, in DataFrame.drop(self, *cols)    444     raise PySparkTypeError(    445         error_class=\"NOT_COLUMN_OR_STR\    446         message_parameters={\"arg_name\": \"cols\ \"arg_type\": type(cols).__name__},    447     )    448 if len(_cols) == 0:--> 449     raise PySparkValueError(    450         error_class=\"CANNOT_BE_EMPTY\    451         message_parameters={\"item\": \"cols\"},    452     )    454 return DataFrame.withPlan(    455     plan.Drop(    456         child=self._plan,   (...)    459     session=self._session,    460 )PySparkValueError: [CANNOT_BE_EMPTY] At least one cols must be specified.```after```In [2]: df.drop()Out[2]: DataFrame[id: bigint, age: bigint]```### How was this patch tested?enabled UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->In this pr, for spark on k8s, the hadoop config map will be mounted in executor side as well.Before, the  hadoop config map is only mounted in driver side.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Since [SPARK-25815](https://issues.apache.org/jira/browse/SPARK-25815) [,](https://github.com/apache/spark/pull/22911,) the hadoop config map will not be mounted in executor side.Per the  https://github.com/apache/spark/pull/22911 description:> The main two things that don't need to happen in executors anymore are:> 1. adding the Hadoop config to the executor pods: this is not needed> since the Spark driver will serialize the Hadoop config and send> it to executors when running tasks. But in fact, the executor still need the hadoop configuration.![image](https://github.com/apache/spark/assets/6757692/ff6374c9-7ebd-4472-a85c-99c75a737e2a)As shown in above picture, the driver can resolve `hdfs://zeus`, but the executor can not.so we still need to mount the hadoop config map in executor side.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, users do not need to take workarounds to make executors load the hadoop configuration.Such as:- including hadoop conf in executor image- placing hadoop conf files under `SPARK_CONF_DIR`.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
2	-3	### What changes were proposed in this pull request?The pr aims to upgrade mysql-connector-java from 8.0.32 to 8.0.33`Version 8.0.33 is the latest General Availability release of the 8.0 series of MySQL Connector/J. It is suitable for use with MySQL Server versions 8.0 and 5.7. It supports the Java Database Connectivity (JDBC) 4.2 API, and implements the X DevAPI.`### Why are the changes needed?1.This version brings some bugs fixes, the release note as follows:https://dev.mysql.com/doc/relnotes/connector-j/8.0/en/news-8-0-33.html2.Bugs Fixed(https://dev.mysql.com/doc/relnotes/connector-j/8.0/en/news-8-0-33.html#connector-j-8-0-33-bug), eg:- When connecting to MySQL Server 5.7 and earlier with Connector/J 8.0.32, Connector/J sometimes hung after the prepare phase of a server-side prepared statement. (Bug #109864, Bug #35034666)References: This issue is a regression of: Bug #33968169.- Results returned by the getPrimaryKeys() method of the DatabaseMetadata interface were not sorted by COLUMN_NAME as required by the JDBC specification. (Bug #109808, Bug #35021038)- Results returned by the getTypeInfo() method of the DatabaseMetadata interface were not sorted by DATA_TYPE as required by the JDBC specification. (Bug #109807, Bug #35021014)- Rewriting of batched statements failed when a closing parenthesis was found within a VALUES clause. It was because QueryInfo failed to parse the prepared statement properly in that case. With this fix, the parser of VALUES clauses is improved, so that Connector/J is now able to recognize rewritability of statements that contain function calls or multiple VALUES lists, and it also handles well the cases when the string \"value\" is part of a table name, a column name, or a keyword. (Bug #109377, Bug #34900156, Bug #107577, Bug #34325361)- Before executing a query with Statement.executeQuery(query), Connector/J checks if the query is going to produce results and rejects it if it cannot. The check wrongly rejected queries that had a WITH clause in which there was no space after the comma between two common table expressions. (Bug #109243, Bug #34852047)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40355 has implemented the `functions#typedlit ` and `functions#typedLit `, so this pr remove the corresponding `ProblemFilters.exclude` rule from `CheckConnectJvmClientCompatibility`### Why are the changes needed?Remove `unnecessary` `ProblemFilters.exclude` rule.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Manual check `dev/connect-jvm-client-mima-check` passed
2	-1	### What changes were proposed in this pull request?The pr aims to adjust the ImportOrderChecker rule to resolve long-standing import order issues.### Why are the changes needed?Make code style consistent.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-1	### What changes were proposed in this pull request?- The pr aims to add check rules, importing in `collection`(instead with `scala.collection`) format is not allowed.- Adjust the code import according to the above rules.### Why are the changes needed?I found that some developers sometimes write `collection.JavaConverters._` when import `JavaConverters._`, while others write `scala.JavaConverters._`Actually, they all belong to the `scala group`, so it is necessary for us to specify a clearer rule to make the code style more consistent### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?Fix `catalog.listCatalogs` in PySpark### Why are the changes needed?existing implementation outputs incorrect results### Does this PR introduce _any_ user-facing change?yesbefore this PR:```In [1]: spark.catalog.listCatalogs()Out[1]: [CatalogMetadata(name=<py4j.java_gateway.JavaMember object at 0x1031f08b0>, description=<py4j.java_gateway.JavaMember object at 0x1049ac2e0>)]```after this PR:```In [1]: spark.catalog.listCatalogs()Out[1]: [CatalogMetadata(name='spark_catalog', description=None)]```### How was this patch tested?added doctest
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?When creating a struct column in Dataframe, the code that ran without problems in version 3.3.1 does not work in version 3.4.0.In 3.3.1```scalaval testDF = Seq(\"a=b,c=d,d=f\").toDF.withColumn(\"key_value\ split('value, \\")).withColumn(\"map_entry\ transform(col(\"key_value\"), x => struct(split(x, \"=\").getItem(0), split(x, \"=\").getItem(1) ) ))testDF.show()+-----------+---------------+--------------------+ |      value|      key_value|           map_entry| +-----------+---------------+--------------------+ |a=b,c=d,d=f|[a=b, c=d, d=f]|[{a, b}, {c, d}, ...| +-----------+---------------+--------------------+```In 3.4.0```org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING] Cannot resolve \"struct(split(namedlambdavariable(), =, -1)[0], split(namedlambdavariable(), =, -1)[1])\" due to data type mismatch: Only foldable `STRING` expressions are allowed to appear at odd position, but they are [\"0\ \"1\"].;'Project [value#41, key_value#45, transform(key_value#45, lambdafunction(struct(0, split(lambda x_3#49, =, -1)[0], 1, split(lambda x_3#49, =, -1)[1]), lambda x_3#49, false)) AS map_entry#48]+- Project [value#41, split(value#41, ,, -1) AS key_value#45]   +- LocalRelation [value#41]  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.dataTypeMismatch(package.scala:73)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:269)  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)  at scala.collection.Iterator.foreach(Iterator.scala:943)  at scala.collection.Iterator.foreach$(Iterator.scala:943)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)  at scala.collection.IterableLike.foreach(IterableLike.scala:74)  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:294)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:294)  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:294)  at scala.collection.Iterator.foreach(Iterator.scala:943)  at scala.collection.Iterator.foreach$(Iterator.scala:943)  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)....```The reason is `CreateNamedStruct` will use last expr of value `Expression` as column name. And will check it must are `String`. But array `Expression`'s last expr are `Integer`. The check will failed. So we can skip match with `UnresolvedExtractValue` when last expr not `String`. Then it will when fall back to the default name.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix the bug when creating struct column name with index of array<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?Follows-up on the comment here: https://github.com/apache/spark/pull/41075#discussion_r1194138082Namely:- updates `error-classes.json` and `sql-error-conditions.md` to have the updated error name.- adds an additional test to assert that enum serialization with invalid enum values throws the correct exception.### Why are the changes needed?Improve documentation ### Does this PR introduce _any_ user-facing change?Yes, documentation.### How was this patch tested?Existing unit tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->POC to run foreachBatch() user function on client side```>>> def foreach_batch_function(df, epoch_id):...   count = df.count()...   print(\"##### The count for batch_id {} is {}\".format(epoch_id, count))...>>> query = (...  spark...  .readStream...  .format(\"rate\")...  .load()...  .writeStream...  .foreachBatch(foreach_batch_function)...  .start()... )##### client: start foreach_batch_callback thread>>> ##### The count for batch_id 0 is 0##### The count for batch_id 1 is 1##### The count for batch_id 2 is 1##### The count for batch_id 3 is 1```### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	### What changes were proposed in this pull request?Support duplicated field names in `createDataFrame` with pandas DataFrame.For with Arrow, without Arrow, and Spark Connect:```py>>> spark.createDataFrame(pdf, schema).show()+--------+---------------+|struct_0|       struct_1|+--------+---------------+|  {a, 1}|{2, 3, b, 4, c}||  {x, 6}|{7, 8, y, 9, z}|+--------+---------------+```### Why are the changes needed?If there are duplicated field names, `createDataFrame` with pandas DataFrame fallbacks to without Arrow, or fails in Spark Connect.```py>>> import pandas as pd>>> from pyspark.sql.types import *>>>>>> schema = (...     StructType()...     .add(\"struct_0\ StructType().add(\"x\ StringType()).add(\"x\ IntegerType()))...     .add(...         \"struct_1\...         StructType()...         .add(\"a\ IntegerType())...         .add(\"x\ IntegerType())...         .add(\"x\ StringType())...         .add(\"y\ IntegerType())...         .add(\"y\ StringType()),...     )... )>>>>>> data = [Row(Row(\"a\ 1), Row(2, 3, \"b\ 4, \"c\")), Row(Row(\"x\ 6), Row(7, 8, \"y\ 9, \"z\"))]>>> pdf = pd.DataFrame.from_records(data, columns=schema.names)```- Without Arrow:Works fine.```py>>> spark.createDataFrame(pdf, schema).show()+--------+---------------+|struct_0|       struct_1|+--------+---------------+|  {a, 1}|{2, 3, b, 4, c}||  {x, 6}|{7, 8, y, 9, z}|+--------+---------------+```- With Arrow:Works with fallback.```py>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)>>> spark.createDataFrame(pdf, schema).show()/.../pyspark/sql/pandas/conversion.py:347: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:  [DUPLICATED_FIELD_NAME_IN_ARROW_STRUCT] Duplicated field names in Arrow Struct are not allowed, got [x, x].Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.  warn(msg)+--------+---------------+|struct_0|       struct_1|+--------+---------------+|  {a, 1}|{2, 3, b, 4, c}||  {x, 6}|{7, 8, y, 9, z}|+--------+---------------+```- Spark ConnectFails.```py>>> spark.createDataFrame(pdf, schema).show()...Traceback (most recent call last):...pyspark.errors.exceptions.connect.IllegalArgumentException: not all nodes and buffers were consumed. ...```### Does this PR introduce _any_ user-facing change?The duplicated field names will work.### How was this patch tested?Added the related test.
2	-2	### What changes were proposed in this pull request?This PR updates the SQL compiler to support general constnat expressions in the syntax for CREATE/REPLACE TABLE OPTIONS values, rather than restricting to a few types of literals only.* The analyzer now checks that the provided expressions are in fact `foldable`, and throws an error message otherwise.* This error message that users encounter in these cases improves from a general \"syntax error at or near <location>\" to instead indicate that the syntax is valid, but only constant expressions are supported in these contexts.### Why are the changes needed?This makes it easier to provide OPTIONS lists in SQL, supporting use cases like concatenating strings with `||`.### Does this PR introduce _any_ user-facing change?Yes, the SQL syntax changes.### How was this patch tested?This PR adds new unit test coverage.
2	-2	### What changes were proposed in this pull request?Protobuf functions (`from_protobuf()` & `to_protobuf()`) take file path of a descriptor file and use that for constructing Protobuf descriptors. Main problem with how this is that the file is read many times (e.g. at each executor). This is unnecessary and error prone. E.g. file contents may be updated couple of days after a streaming query starts. That could lead to various errors. **The fix**: Use the byte content (which is serialized `FileDescritptorSet` proto). We read the content from the file once and carry the byte buffer. This also adds new API where we can pass the byte buffer directly. This is useful when the users fetch the content themselves and passes it to Protobuf functions. E.g. they could fetch it from S3, or extract it Python Protobuf classes. **Note to reviewers**: This includes a lot of updates to test files, mainly because the interface change to pass the buffer. I have left a few PR comments to help with the review.### Why are the changes needed?Described above.### Does this PR introduce _any_ user-facing change?Yes, this adds two new versions for `from_protobuf()` and `to_protobuf()` API that take Protobuf bytes rather than file path. ### How was this patch tested? - Unit tests
1	-2	### What changes were proposed in this pull request?Enables more parity tests for Pandas UDFs.### Why are the changes needed?There are still many tests disabled for some reasons.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Enabled tests.
1	-1	### What changes were proposed in this pull request?The pr aims to upgrade `jdbc` related test dependencies, include:- org.mariadb.jdbc:mariadb-java-client from 2.7.4 to 2.7.9- org.postgresql:postgresql from 42.5.1 to 42.6.0- com.ibm.db2:jcc from 11.5.6.0 to 11.5.8.0- com.microsoft.sqlserver:mssql-jdbc from 9.4.0.jre8 to 9.4.1.jre8- com.oracle.database.jdbc:ojdbc8 from 21.3.0.0 to 23.2.0.0 ### Why are the changes needed?Routine upgrade.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-1	### What changes were proposed in this pull request?This PR adds `log4j-1.2-api` and `log4j-slf4j2-impl` to classpath if active `hadoop-provided`.### Why are the changes needed?To fix log issue.How to reproduce this issue:1. Build Spark:   ```   ./dev/make-distribution.sh --name provided --tgz -Phive -Phive-thriftserver -Pyarn -Phadoop-provided   tar -zxf spark-3.5.0-SNAPSHOT-bin-provided.tgz    ```2. Copy the following jars to spark-3.5.0-SNAPSHOT-bin-provided/jars:   ```   guava-14.0.1.jar   hadoop-client-api-3.3.5.jar   hadoop-client-runtime-3.3.5.jar   hadoop-shaded-guava-1.1.1.jar   hadoop-yarn-server-web-proxy-3.3.5.jar   slf4j-api-2.0.7.jar   ```3. Add a new log4j2.properties to spark-3.5.0-SNAPSHOT-bin-provided/conf:   ```   rootLogger.level = info   rootLogger.appenderRef.file.ref = File   rootLogger.appenderRef.stderr.ref = console      appender.console.type = Console   appender.console.name = console   appender.console.target = SYSTEM_ERR   appender.console.layout.type = PatternLayout   appender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n      appender.file.type = RollingFile   appender.file.name = File   appender.file.fileName = /tmp/spark/logs/spark.log   appender.file.filePattern = /tmp/spark/logs/spark.%d{yyyyMMdd-HH}.log   appender.file.append = true   appender.file.layout.type = PatternLayout   appender.file.layout.pattern = %d{yy/MM/dd HH:mm:ss,SSS} %p [%t] %c{2}:%L : %m%n   appender.file.policies.type = Policies   appender.file.policies.time.type = TimeBasedTriggeringPolicy   appender.file.policies.time.interval = 1   appender.file.policies.time.modulate = true   appender.file.policies.size.type = SizeBasedTriggeringPolicy   appender.file.policies.size.size = 256M   appender.file.strategy.type = DefaultRolloverStrategy   appender.file.strategy.max = 100   ```4. Start Spark thriftserver: `sbin/start-thriftserver.sh`.5. The log file is empty: `cat /tmp/spark/logs/spark.log`.6. Copy the following jars to spark-3.5.0-SNAPSHOT-bin-provided/jars:   ```   log4j-1.2-api-2.20.0.jar   log4j-slf4j2-impl-2.20.0.jar   ```7. Restart Spark thriftserver: `sbin/start-thriftserver.sh`.8. The log file is not empty: `cat /tmp/spark/logs/spark.log`.This is because hadoop classpath does not contain these jars. So these jars are needed even if `hadoop-provided` is activated![image](https://github.com/apache/spark/assets/5399861/d53ff28c-05d7-40d7-891f-069449315217)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
2	-2	### What changes were proposed in this pull request?1. add a new feature step to handle special environment variables, such as `PATH=/usr/bin:$PATH`   * pull these special env variables and reorder them in a new config map   * mount the config map in driver and/or executor's container   * export these env variables by leverage sh/bash's `.` capability in the `entrypoint.sh`   * update original container's env list with special envs excluded2. support `spark.executor.exraLibraryPath` on K8S by leveraging the new feature step3. update `spark-rbac.yaml` to including more permissions, which was found when add integration tests.### Why are the changes needed?1. support `spark.executor.extraLibraryPath` on k8s2. feature parity with spark on yarn. Spark on yarn handles this environment variables that need to be substituted correctly.3. K8S itself cannot substitute variables in the container spec, we have to do some extra work at the entrypoint.sh### Does this PR introduce _any_ user-facing change?Yes. Before this PR, for spark running on K8S, `spark.executor.extraLibraryPath` doesn't take effect and cannot support variable substitution.### How was this patch tested?1. added new UTs2. added new integration test3. [WIP] verifing in the real prod cluster.
2	-1	### What changes were proposed in this pull request?reformat the docstring of  `Column.over`### Why are the changes needed?[existing doc](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.over.html#pyspark.sql.Column.over) is not properly rendered### Does this PR introduce _any_ user-facing change?doc-onlybefore:![image](https://github.com/apache/spark/assets/7322292/c73cb1ed-9291-480c-8765-475b0bc451d3)after:![image](https://github.com/apache/spark/assets/7322292/f6dee714-ad4c-4ee2-8973-3a91c105652f)### How was this patch tested?CI
2	-3	### What changes were proposed in this pull request?This pr aims upgrade ASM related dependencies in the `tools` module from version 7.1 to version 9.4 to make `GenerateMIMAIgnore` can process Java 17+ compiled code.Additionally, this pr defines `asm.version` to manage versions of ASM.### Why are the changes needed?The classpath processed by `GenerateMIMAIgnore` cannot contain Java 17+ compiled code now due to the ASM version use by `tools` module is too low, but https://github.com/bmc/classutil has not been updated for a long time, we can't solve the problem by upgrading `classutil`, so this pr make the `tools` module explicitly rely on ASM 9.4 for workaround.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Action- Manual checked `dev/mima` due to this pr upgrade the dependency of tools module```dev/mima```and```dev/change-scala-version.sh 2.13dev/mima -Pscala-2.13```- A case that can reproduce the problem: run following script with master branch:```set -o pipefailset -eFWDIR=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"cd \"$FWDIR\"export SPARK_HOME=$FWDIRecho $SPARK_HOMEif [[ -x \"$JAVA_HOME/bin/java\" ]]; then  JAVA_CMD=\"$JAVA_HOME/bin/java\"else  JAVA_CMD=javafiTOOLS_CLASSPATH=\"$(build/sbt -DcopyDependencies=false \"export tools/fullClasspath\" | grep jar | tail -n1)\"ASSEMBLY_CLASSPATH=\"$(build/sbt -DcopyDependencies=false \"export assembly/fullClasspath\" | grep jar | tail -n1)\"rm -f .generated-mima*$JAVA_CMD \\  -Xmx2g \\  -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util.jar=ALL-UNNAMED \\  -cp \"$TOOLS_CLASSPATH:$ASSEMBLY_CLASSPATH\" \\  org.apache.spark.tools.GenerateMIMAIgnorerm -f .generated-mima*```**Before**```Exception in thread \"main\" java.lang.IllegalArgumentException: Unsupported class file major version 61  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:195)  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:176)  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:162)  at org.objectweb.asm.ClassReader.<init>(ClassReader.java:283)  at org.clapper.classutil.asm.ClassFile$.load(ClassFinderImpl.scala:222)  at org.clapper.classutil.ClassFinder.classData(ClassFinder.scala:404)  at org.clapper.classutil.ClassFinder.$anonfun$processOpenZip$2(ClassFinder.scala:359)  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)  at scala.collection.Iterator.toStream(Iterator.scala:1417)  at scala.collection.Iterator.toStream$(Iterator.scala:1416)  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)  at scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1417)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)  at scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)  at scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)  at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)  at scala.collection.immutable.Stream.filterImpl(Stream.scala:506)  at scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)  at scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)  at scala.collection.generic.Growable.loop$1(Growable.scala:57)  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:61)  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)  at scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:381)  at scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:329)  at scala.collection.TraversableLike.to(TraversableLike.scala:786)  at scala.collection.TraversableLike.to$(TraversableLike.scala:783)  at scala.collection.AbstractTraversable.to(Traversable.scala:108)  at scala.collection.TraversableOnce.toSet(TraversableOnce.scala:360)  at scala.collection.TraversableOnce.toSet$(TraversableOnce.scala:360)  at scala.collection.AbstractTraversable.toSet(Traversable.scala:108)  at org.apache.spark.tools.GenerateMIMAIgnore$.getClasses(GenerateMIMAIgnore.scala:156)  at org.apache.spark.tools.GenerateMIMAIgnore$.privateWithin(GenerateMIMAIgnore.scala:57)  at org.apache.spark.tools.GenerateMIMAIgnore$.main(GenerateMIMAIgnore.scala:122)  at org.apache.spark.tools.GenerateMIMAIgnore.main(GenerateMIMAIgnore.scala)```The script run failed without this pr due to `ASSEMBLY_CLASSPATH` contains jackson-core 2.15.0 and it contains code compiled from Java 17+<img width=\"463\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/85b6b269-a182-460a-9995-7b1a517c2dfe\">**After**No more errors like `Exception in thread \"main\" java.lang.IllegalArgumentException: Unsupported class file major version 61` with this pr.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->The metrics type for spark counter metrics exported to statsD are mapped to gauge instead of counter.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Due to spark counter metrics being cumulative and statsD counter metrics being derive, there is a contract mismatch between them. This leads to aggregation both on client(spark) and server(statsD) side, which results in the value o the metrics being wrong. Since StatsD does not do aggregation on gauge metric, spark should send it cumulative metrics as gauge too, to obtain correct value of the metric.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->The counter metrics that are exported by statsD to any backends will be changed to gauge metrics instead of counter metrics. Any application that processes gauge and counter metrics differently wil need to account for this change.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT has been modified.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_0003.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Adding working directory into classpath on the driver in K8S cluster mode.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->After #37417, the spark.files, spark.jars are placed  in the working directory.But seems that the spark context classloader can not access them because they are not in the classpath by default.This pr adds the current working directory into classpath, so that the spark.files, spark.jars placed in the working directory can be accessible by the classloader.For example, the `hive-site.xml` uploaded by `spark.files`.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->yes, users do not need to add the working directory into spark classpath manually.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
2	-2	### What changes were proposed in this pull request?In case the assert for the call to ListQuery.nullable is hit, mention in the assert error message the conf flag that can be used to disable the assert. Follow-up to https://github.com/apache/spark/pull/41094#discussion_r1195438179### Why are the changes needed?Improve error message.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit tests
3	-2	### What changes were proposed in this pull request?This PR fixes a null pointer thrown when non-foldable values are provided to the lgConfigK or allowDifferentLgConfigK arguments of the hll_sketch_agg, hll_union_agg, or hll_union functions.### Why are the changes needed?We should be handling this error gracefully, rather than throwing a null pointer exception.### Does this PR introduce _any_ user-facing change?This PR introduces two new error messages, replacing what would be null pointer exceptions.### How was this patch tested?Three new negative test cases have been added to the 'SPARK-16484: hll_*_agg + hll_union negative tests' test of the DataframeAggregateSuite.
2	-2	### What changes were proposed in this pull request?In the PR, I propose to propagate all tags in a `Project` while resolving of expressions and missing columns in `ColumnResolutionHelper.resolveExprsAndAddMissingAttrs()`.### Why are the changes needed?To fix the bug reproduced by the query below:```sqlspark-sql (default)> WITH                   >   t1 AS (select key from values ('a') t(key)),                   >   t2 AS (select key from values ('a') t(key))                   > SELECT t1.key                   > FROM t1 FULL OUTER JOIN t2 USING (key)                   > WHERE t1.key NOT LIKE 'bb.%';[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `t1`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4 pos 7;```### Does this PR introduce _any_ user-facing change?No. It fixes a bug, and outputs the expected result: `a`.### How was this patch tested?By new test added to `using-join.sql`:```$ PYSPARK_PYTHON=python3 build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z using-join.sql\"```and the related test suites:```$ build/sbt -Phive-2.3 -Phive-thriftserver \"test:testOnly org.apache.spark.sql.hive.HiveContextCompatibilitySuite\"```
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->We are migrating to a new error framework in order to surface errors in a friendlier way to customers. This PR defines a new error class specifically for when there are concurrent updates to the log for the same batch ID### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This gives more information to customers, and allows them to filter in a better way### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->There is an existing test to check the error message upon this condition. Because we are only changing the error type, and not the error message, this test is sufficient.
1	-2	### What changes were proposed in this pull request?This PR is a followup of https://github.com/apache/spark/pull/41013 that sets `SPARK_CONNECT_MODE_ENABLED` when running PySpark shell via `bin/pyspark --remote=local` so it works.### Why are the changes needed?It is broken after the PR:```bash./bin/pyspark --remote local``````python...Traceback (most recent call last):  File \"/.../python/pyspark/shell.py\ line 78, in <module>    sc = spark.sparkContext  File \"/.../python/pyspark/sql/connect/session.py\ line 567, in sparkContext    raise PySparkNotImplementedError(pyspark.errors.exceptions.base.PySparkNotImplementedError: [NOT_IMPLEMENTED] sparkContext() is not implemented.```### Does this PR introduce _any_ user-facing change?No to end users.Yes to the dev as described above.### How was this patch tested?Manually tested via `./bin/pyspark --remote local`
2	-2	### What changes were proposed in this pull request?This is follow-up for https://github.com/apache/spark/pull/40459 to fix the incorrect information and to elaborate more detailed changes.- We're not fully support the pandas 2.0.0, so the information \"Pandas API on Spark follows for the pandas 2.0\" is not correct.- We should list all the APIs that no longer support `inplace` parameter.### Why are the changes needed?Correctness for migration notes.### Does this PR introduce _any_ user-facing change?No, only updating migration notes.### How was this patch tested?The existing CI should pass
3	-2	### What changes were proposed in this pull request?This PR proposes to fix [Supported pandas API](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html#supported-pandas-api)  page to point out the proper pandas version.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Currently we're supporting pandas 1.5, but it says we support latest pandas which is 2.0.1.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, it's document change### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass.
2	-1	### What changes were proposed in this pull request?The pr aims to remove workaround for HADOOP-16255.https://issues.apache.org/jira/browse/HADOOP-16255### Why are the changes needed?- Because HADOOP-16255 has been fix after hadoop version  3.1.2. Spark support hadoop version: >= 3.2.2 or >= 3.3.1- Make code clean.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-1	### What changes were proposed in this pull request?The pr aims to remove workaround for HADOOP-14067.https://issues.apache.org/jira/browse/HADOOP-14067https://issues.apache.org/jira/browse/SPARK-32256### Why are the changes needed?1.According to the PR description, this doesn't happen in Hadoop 3.1.0 and above; Now spark support hadoop version: 3.2.2+ and 3.3.1+2.Make code clean.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-1	### What changes were proposed in this pull request?This PR proposes to upgrade pandas to 2.0.0.### Why are the changes needed?To support latest pandas.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Addressed the existing UTs.
2	-3	### What changes were proposed in this pull request?Currently, Spark provides the `GenTPCDSData` to generates TPCDS table data by using tpcds-kit: https://github.com/databricks/tpcds-kit.If users want generates TPCDS table data, we can run:`build/sbt \"sql/Test/runMain <this class> --dsdgenDir <path> --location <path> --scaleFactor 1\"`If the scale factor is smaller, such as: scaleFactor < 100, `GenTPCDSData` works good. otherwise, OOM issues prevent generating data.```[info] 16:43:41.618 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.[info] 16:43:41.627 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.[info] 16:43:41.646 WARN org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete file:/home/ubuntu/tpcdsdata/test/catalog_sales/_temporary/0/_temporary/attempt_202305181633205732292221634890857_0006_m_000010_610[info] 16:43:41.647 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.[info] 16:43:41.647 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.[info] 16:43:41.656 WARN org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete file:/home/ubuntu/tpcdsdata/test/catalog_sales/_temporary/0/_temporary/attempt_202305181633205732292221634890857_0006_m_000014_614[info] 16:43:41.656 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.[info] 16:43:41.668 WARN org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete file:/home/ubuntu/tpcdsdata/test/catalog_sales/_temporary/0/_temporary/attempt_202305181633205732292221634890857_0006_m_000002_602[info] 16:43:41.668 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.[error] Exception in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 6.0 failed 1 times, most recent failure: Lost task 13.0 in stage 6.0 (TID 613) (ip-172-31-27-53.cn-northwest-1.compute.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/ubuntu/tpcdsdata/test/catalog_sales.[error]         at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)[error]         at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)[error]         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[error]         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)[error]         at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)[error]         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)[error]         at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)[error]         at org.apache.spark.scheduler.Task.run(Task.scala:139)[error]         at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)[error]         at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1487)[error]         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[error]         at java.lang.Thread.run(Thread.java:750)[error] Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded[error] Driver stacktrace:[error]         at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2815)[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2751)[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2750)[error]         at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[error]         at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[error]         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[error]         at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2750)[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1218)[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1218)[error]         at scala.Option.foreach(Option.scala:407)[error]         at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1218)[error]         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3014)[error]         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2953)[error]         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2942)[error]         at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[error]         at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:983)[error]         at org.apache.spark.SparkContext.runJob(SparkContext.scala:2285)[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)[error]         at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)[error]         at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)[error]         at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)[error]         at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)[error]         at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)[error]         at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)[error]         at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)[error]         at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)[error]         at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)[error]         at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)[error]         at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)[error]         at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)[error]         at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)[error]         at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)[error]         at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)[error]         at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)[error]         at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)[error]         at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)[error]         at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)[error]         at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)[error]         at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)[error]         at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)[error]         at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)[error]         at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)[error]         at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)[error]         at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)[error]         at org.apache.spark.sql.TPCDSTables$Table.genData(GenTPCDSData.scala:246)[error]         at org.apache.spark.sql.TPCDSTables.$anonfun$genData$10(GenTPCDSData.scala:276)[error]         at org.apache.spark.sql.TPCDSTables.$anonfun$genData$10$adapted(GenTPCDSData.scala:273)[error]         at scala.collection.immutable.List.foreach(List.scala:431)[error]         at org.apache.spark.sql.TPCDSTables.genData(GenTPCDSData.scala:273)[error]         at org.apache.spark.sql.GenTPCDSData$.main(GenTPCDSData.scala:440)[error]         at org.apache.spark.sql.GenTPCDSData.main(GenTPCDSData.scala)[error] Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/ubuntu/tpcdsdata/test/catalog_sales.[error]         at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)[error]         at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)[error]         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)[error]         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)[error]         at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)[error]         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)[error]         at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)[error]         at org.apache.spark.scheduler.Task.run(Task.scala:139)[error]         at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)[error]         at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1487)[error]         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[error]         at java.lang.Thread.run(Thread.java:750)[error] Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded```The process tree show below:```ubuntu   3272634 3206726  0 18:30 pts/1    00:00:00 bash ./build/sbt sql/Test/runMain org.apache.spark.sql.GenTPCDSData --dsdgenDir /home/ubuntu/github-fork/tpcds-kit/tools --location /home/ubuntu/tpcdsdata/1000 --scaleFactor 1000 --partitionTablesubuntu   3272647 3272634  7 18:30 pts/1    00:02:20 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xms4096m -Xmx4096m -XX:ReservedCodeCacheSize=512m -Xmx1024m -Xss2m -Xmx10G -Xms10G -jar build/sbt-launch-1.8.3.jar sql/Test/runMain org.apache.spark.sql.GenTPCDSData --dsdgenDir /home/ubuntu/github-fork/tpcds-kit/tools --location /home/ubuntu/tpcdsdata/1000 --scaleFactor 1000 --partitionTablesubuntu   3272844 3272647 99 18:30 pts/1    05:01:56 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xmx4g -XX:MaxMetaspaceSize=1300m -Djava.io.tmpdir=/home/ubuntu/github-fork/spark/target/tmp -Dspark.test.home=/home/ubuntu/github-fork/spark -Dspark.testing=1 -Dspark.port.maxRetries=100 -Dspark.master.rest.enabled=false -Dspark.memory.debugFill=true -Dspark.ui.enabled=false -Dspark.ui.showConsoleProgress=false -Dspark.unsafe.exceptionOnMemoryLeak=true -Dspark.hadoop.hadoop.security.key.provider.path=test:/// -Dsun.io.serialization.extendedDebugInfo=false -Dderby.system.durability=test -Dio.netty.tryReflectionSetAccessible=true -ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=1300m -XX:ReservedCodeCacheSize=128m -Dfile.encoding=UTF-8 -XX:+IgnoreUnrecognizedVMOptions```After my investigation, **-Xmx4g** is fixed in `project/SparkBuilder.scala`.This PR want adds a new environment variable to control the JVM heap size for test process.### Why are the changes needed?Avoid OOM for generates TPC-DS data.### Does this PR introduce _any_ user-facing change?'No'.This change is relationed to developers.### How was this patch tested?Manual test.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Add a new test for scrollable result set support, which is uncovered yet through jdbc APIs### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->test improvement### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->no### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->new test
2	-2	### What changes were proposed in this pull request?The pr aims to Convert `_LEGACY_ERROR_TEMP_0036` to INVALID_SQL_SYNTAX.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA & Update UT.
2	-1	### What changes were proposed in this pull request?This PR proposes a new configuration `spark.sql.execution.pyspark.python` that sets the Python executable on worker nodes.Note that, if the Python executable is different from the one previously ran, it will creates a new Python worker processes because we reuse Python workers but they are unique by both executable path and env variables as a key:https://github.com/apache/spark/blob/d7a8b852eaa6cc04df1eea0018a9b9de29b1c4fe/core/src/main/scala/org/apache/spark/SparkEnv.scala#L123-L124This PR is also a basework for Spark Connect to support a different set of dependencies.### Why are the changes needed?This can be useful especially when you want to run your Python with a different set of dependencies during runtime (see also https://www.databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html)### Does this PR introduce _any_ user-facing change?No, this PR adds a configuration but that's internal for now.### How was this patch tested?Manually tested as below:```pythonimport sysfrom pyspark.sql.functions import udfspark.range(1).select(udf(lambda x: sys.executable)(\"id\")).show(truncate=False)spark.conf.set(\"spark.sql.execution.pyspark.python\ \"/.../miniconda3/envs/another-python/bin/python\")spark.range(1).select(udf(lambda x: sys.executable)(\"id\")).show(truncate=False)``````+------------------------------------------+|<lambda>(id)                              |+------------------------------------------+|/.../miniconda3/envs/python3.9/bin/python3|+------------------------------------------++----------------------------------------------+|<lambda>(id)                                  |+----------------------------------------------+|/.../miniconda3/envs/another-python/bin/python|+----------------------------------------------+```
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a follow-up of https://github.com/apache/spark/pull/41064 . `LocalRelaton` is heavily used in tests and it's better to not report row count in tests to avoid the query being optimized too well which may hurt test coverage.This PR updates `LocalRelaton` to not report row count in tests, and adds a test-only config to still enable it in tests.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->keep test coverage### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->existing tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Exclude `javax.activation:activation:jar:1.1.1` and `org.apache.logging.log4j:log4j-slf4j2-impl:jar:2.20.0` from `spark-streaming-kafka-0-10-assembly_2.12-<version>.jar`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->We should not include the jar which already exists in Spark binary artifact into the assembly jar of optional component.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->```build/mvn dependency:list -pl :spark-streaming-kafka-0-10-assembly_2.12```before```[INFO] --- maven-dependency-plugin:3.5.0:list (default-cli) @ spark-streaming-kafka-0-10-assembly_2.12 ---[INFO][INFO] The following files have been resolved:[INFO]    org.apache.spark:spark-streaming-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.spark:spark-token-provider-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.kafka:kafka-clients:jar:3.4.0:compile[INFO]    org.apache.spark:spark-tags_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    javax.activation:activation:jar:1.1.1:compile[INFO]    org.apache.logging.log4j:log4j-slf4j2-impl:jar:2.20.0:compile[INFO]    org.spark-project.spark:unused:jar:1.0.0:compile```after```[INFO] --- maven-dependency-plugin:3.5.0:list (default-cli) @ spark-streaming-kafka-0-10-assembly_2.12 ---[INFO][INFO] The following files have been resolved:[INFO]    org.apache.spark:spark-streaming-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.spark:spark-token-provider-kafka-0-10_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.apache.kafka:kafka-clients:jar:3.4.0:compile[INFO]    org.apache.spark:spark-tags_2.12:jar:3.5.0-SNAPSHOT:compile[INFO]    org.spark-project.spark:unused:jar:1.0.0:compile```
1	-1	### What changes were proposed in this pull request?The pr aims to remove unused declarations from `Core` module### Why are the changes needed?Make code clean.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-3	### What changes were proposed in this pull request?This PR upgrades `cyclonedx-maven-plugin` to 2.7.9.### Why are the changes needed?1. Fix compile errors running on Java 20+:   ```   Error:  Failed to execute goal org.cyclonedx:cyclonedx-maven-plugin:2.7.6:makeBom (default) on project spark-tags_2.12: Execution default of goal org.cyclonedx:cyclonedx-maven-plugin:2.7.6:makeBom failed: Unsupported class file major version 64 -> [Help 1]   ```   The issue fixed by `cyclonedx-maven-plugin` 2.7.7. Please see: https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/326.2. 2.7.9 contains other bug fixes and improvements:   https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.9   https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.8   https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.7### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?manual test.
2	-2	### What changes were proposed in this pull request?In the PR, I propose to propagate all tags in a `Project` while resolving of expressions and missing columns in `ColumnResolutionHelper.resolveExprsAndAddMissingAttrs()`.This is a backport of https://github.com/apache/spark/pull/41204.### Why are the changes needed?To fix the bug reproduced by the query below:```sqlspark-sql (default)> WITH                   >   t1 AS (select key from values ('a') t(key)),                   >   t2 AS (select key from values ('a') t(key))                   > SELECT t1.key                   > FROM t1 FULL OUTER JOIN t2 USING (key)                   > WHERE t1.key NOT LIKE 'bb.%';[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `t1`.`key` cannot be resolved. Did you mean one of the following? [`key`].; line 4 pos 7;```### Does this PR introduce _any_ user-facing change?No. It fixes a bug, and outputs the expected result: `a`.### How was this patch tested?By new test added to `using-join.sql`:```$ PYSPARK_PYTHON=python3 build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z using-join.sql\"```and the related test suites:```$ build/sbt -Phive-2.3 -Phive-thriftserver \"test:testOnly org.apache.spark.sql.hive.HiveContextCompatibilitySuite\"```Authored-by: Max Gekk <max.gekk@gmail.com>Signed-off-by: Max Gekk <max.gekk@gmail.com>(cherry picked from commit 09d5742a8679839d0846f50e708df98663a6d64c)
1	-3	### What changes were proposed in this pull request?This PR aims to add `https://dlcdn.apache.org/` to the default mirror site list during python installation tests.### Why are the changes needed?This is a preferred mirror. So, even if `https://www.apache.org/dyn/closer.lua` is inaccessible, we will download from `https://dlcdn.apache.org/`.```$ curl https://www.apache.org/dyn/closer.lua\\?preferred\\=truehttps://dlcdn.apache.org/```Although we try to get this programmatically, sometimes `https://www.apache.org/dyn/closer.lua` seems to fail.https://github.com/apache/spark/blob/acad77d56112f2cab2ce5adca913b75ce659add5/python/pyspark/install.py#L169C2-L179### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
1	-1	### What changes were proposed in this pull request?This PR aims to upgrade `kubernetes-client` from 6.6.1 to 6.6.2### Why are the changes needed?Bring a fix, \"RequestConfig is propagated to derived HttpClient instances\".https://github.com/fabric8io/kubernetes-client/commit/8cf4804a039bc221aed2f6bae3d4c2568874fb5f### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
1	-1	### What changes were proposed in this pull request?This PR aims to upgrade `sbt-pom-reader` from 2.2.0 to 2.4.0.### Why are the changes needed?Since v2.3.0, organization has moved from `com.typesafe.sbt` to `com.github.sbt`- https://github.com/sbt/sbt-pom-reader/releases/tag/v2.4.0- https://github.com/sbt/sbt-pom-reader/releases/tag/v2.3.0### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-3	### What changes were proposed in this pull request?This change fixes a bug with push-based shuffle when encryption is enabled on the server. The meta requests for push-merged blocks for an application which doesn't have encryption enabled, fails with NPE:```java.lang.RuntimeException: java.lang.NullPointerException\tat org.apache.spark.network.server.AbstractAuthRpcHandler.getMergedBlockMetaReqHandler(AbstractAuthRpcHandler.java:110)\tat org.apache.spark.network.crypto.AuthRpcHandler.getMergedBlockMetaReqHandler(AuthRpcHandler.java:144)\tat org.apache.spark.network.server.TransportRequestHandler.processMergedBlockMetaRequest(TransportRequestHandler.java:275)\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:117)\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) ```The change here fixes this issue.### Why are the changes needed?It is a bug fix for push-based shuffle.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added a simple UT. Verified manually.
1	-1	### What changes were proposed in this pull request?The pr aims to update some sbt plugins to newest version. include:- sbt-assembly from 2.1.0 to 2.1.1- sbt-mima-plugin from 1.1.0 to 1.1.2- sbt-revolver from 0.9.1 to 0.10.0### Why are the changes needed?Routine upgrade.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to update some maven plugins to newest version. include:- exec-maven-plugin from 1.6.0 to 3.1.0- scala-maven-plugin from 4.8.0 to 4.8.1- maven-antrun-plugin from 1.8 to 3.1.0- maven-enforcer-plugin from 3.2.1 to 3.3.0- build-helper-maven-plugin from 3.3.0 to 3.4.0- maven-surefire-plugin from 3.0.0 to 3.1.0- maven-assembly-plugin from 3.1.0 to 3.6.0- maven-install-plugin from 3.1.0 to 3.1.1- maven-deploy-plugin from 3.1.0 to 3.1.1- maven-checkstyle-plugin from 3.2.1 to 3.2.2### Why are the changes needed?Routine upgrade.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-3	### What changes were proposed in this pull request?This PR aims to run `HealthTrackerIntegrationSuite` in a dedicated JVM to mitigate a flaky tests.### Why are the changes needed?`HealthTrackerIntegrationSuite` has been flaky and SPARK-25400 and SPARK-37384 increased the timeout `from 1s to 10s` and `10s to 20s`, respectively. The usual suspect of this flakiness is some unknown side-effect like GCs. In this PR, we aims to run this in a separate JVM instead of increasing the timeout more.https://github.com/apache/spark/blob/abc140263303c409f8d4b9632645c5c6cbc11d20/core/src/test/scala/org/apache/spark/scheduler/SchedulerIntegrationSuite.scala#L56-L58This is the recent failure.- https://github.com/apache/spark/actions/runs/5020505360/jobs/9002039817```[info] HealthTrackerIntegrationSuite:[info] - If preferred node is bad, without excludeOnFailure job will fail (92 milliseconds)[info] - With default settings, job can succeed despite multiple bad executors on node (3 seconds, 163 milliseconds)[info] - Bad node with multiple executors, job will still succeed with the right confs *** FAILED *** (20 seconds, 43 milliseconds)[info]   java.util.concurrent.TimeoutException: Futures timed out after [20 seconds][info]   at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)[info]   at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)[info]   at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:355)[info]   at org.apache.spark.scheduler.SchedulerIntegrationSuite.awaitJobTermination(SchedulerIntegrationSuite.scala:276)[info]   at org.apache.spark.scheduler.HealthTrackerIntegrationSuite.$anonfun$new$9(HealthTrackerIntegrationSuite.scala:92)```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
2	-2	### What changes were proposed in this pull request?This pr change `RangeExec#numSlices ` to use smaller value in `Range.numElements` and `Range.numSlices` to avoid scheduling unnecessary tasks.### Why are the changes needed?Avoid scheduling unnecessary tasks when `Range.numSlices` > `Range.numElements`.### Does this PR introduce _any_ user-facing change?Yes, when `Range.numSlices` > `Range.numElements` is, for the result of `queryExecution.debug`, the `splits` value in the printed Physical Plan will change.### How was this patch tested?- Existing UT- Manual check:start a spark-shell with ``--master \"local[100]\"``, then run ```spark.range(10).map(_ + 1).reduce(_ + _)```**Before**<img width=\"1718\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/ce8317f3-46b6-4bc0-a8f8-10d7e8f33e4f\">**After**<img width=\"1720\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/711a9c24-0f79-4714-8984-532bc8132a03\">
1	-1	### What changes were proposed in this pull request?This PR aims to upgrade ASM to 9.5.### Why are the changes needed?xbean-asm9-shaded 4.25 upgrade to use ASM 9.5 and ASM 9.5 is for Java 21:- https://asm.ow2.io/versions.html- https://issues.apache.org/jira/browse/XBEAN-339 | https://github.com/apache/geronimo-xbean/pull/36### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
2	-2	### What changes were proposed in this pull request?This PR aims to fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString` instead of shift operations.### Why are the changes needed?To avoid user confusion by giving more accurate values. For example, `maxBroadcastTableBytes` is 1GB and `dataSize` is `2GB - 1 byte`.**BEFORE**```Cannot broadcast the table that is larger than 1GB: 1 GB.```**AFTER**```Cannot broadcast the table that is larger than 1024.0 MiB: 2048.0 MiB.```### Does this PR introduce _any_ user-facing change?Yes, but only error message.### How was this patch tested?Pass the CIs with newly added test case.
1	-3	### What changes were proposed in this pull request?This is a bug fix for `CheckConnectJvmClientCompatibility`: correct the parameter passed to `checkMiMaCompatibilityWithAvroModule` from `sqlJar` to `avroJar`.In addition, this pr rename the 2rd parameter of the `checkMiMaCompatibility` from `sqlJar` to `targetJar`.### Why are the changes needed?Bug fix for `CheckConnectJvmClientCompatibility`, `checkMiMaCompatibilityWithAvroModule` should compare `clientJar` and `avroJar`### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass GitHub Actions- Manually constructed the incompatibility between `connect client` and `avro`, and the check results met expectations
2	-2	### What changes were proposed in this pull request?This is a logical backporting of #41232 This PR aims to fix `cannotBroadcastTableOverMaxTableBytesError` to use `bytesToString` instead of shift operations.### Why are the changes needed?To avoid user confusion by giving more accurate values. For example, `maxBroadcastTableBytes` is 1GB and `dataSize` is `2GB - 1 byte`.**BEFORE**```Cannot broadcast the table that is larger than 1GB: 1 GB.```**AFTER**```Cannot broadcast the table that is larger than 1024.0 MiB: 2048.0 MiB.```### Does this PR introduce _any_ user-facing change?Yes, but only error message.### How was this patch tested?Pass the CIs with newly added test case.
2	-3	### What changes were proposed in this pull request?This pr make `connect-jvm-client-mima-check`  to support mima check between `connect-client-jvm` and `protobuf` module.### Why are the changes needed?Do mima check between `connect-client-jvm` and `protobuf` module.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?- Pass Github Actions- Manually constructed the incompatibility between `connect client` and `protobuf`, and the check results met expectations
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_0013.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?If there are few distinct values in the RangePartitioner, there will be very few partitions that could be very large. We can append a random expression to increase the number of partitions.### Why are the changes needed?To optimize RangePartitioner, for example, the following query can be optimized from 2.9 hours to 3.2 mins.![range_partitioner](https://github.com/apache/spark/assets/3626747/a99f7092-d38f-43e4-8881-96ee6d6ed75d)### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Added UT
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Small change to `anyToMicros` to also accept `LocalDateTime` that's being returned when working with `TIMESTAMP_NTZ`. This simplifies some code at the Iceberg side.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-1	### What changes were proposed in this pull request?Supports nested timestamp type in `spark.createDataFrame()` with pandas DataFrame and `df.toPandas()`, and makes them return correct results.For the following schema and pandas DataFrame:```pyschema = (    StructType()    .add(\"ts\ TimestampType())    .add(\"ts_ntz\ TimestampNTZType())    .add(        \"struct\ StructType().add(\"ts\ TimestampType()).add(\"ts_ntz\ TimestampNTZType())    )    .add(\"array\ ArrayType(TimestampType()))    .add(\"array_ntz\ ArrayType(TimestampNTZType()))    .add(\"map\ MapType(StringType(), TimestampType()))    .add(\"map_ntz\ MapType(StringType(), TimestampNTZType())))data = [    Row(        datetime.datetime(2023, 1, 1, 0, 0, 0),        datetime.datetime(2023, 1, 1, 0, 0, 0),        Row(            datetime.datetime(2023, 1, 1, 0, 0, 0),            datetime.datetime(2023, 1, 1, 0, 0, 0),        ),        [datetime.datetime(2023, 1, 1, 0, 0, 0)],        [datetime.datetime(2023, 1, 1, 0, 0, 0)],        dict(ts=datetime.datetime(2023, 1, 1, 0, 0, 0)),        dict(ts_ntz=datetime.datetime(2023, 1, 1, 0, 0, 0)),    )]pdf = pd.DataFrame.from_records(data, columns=schema.names)```##### `spark.createDataFrame()`For all, return the same results:```py>>> spark.conf.set(\"spark.sql.session.timeZone\ \"America/New_York\")>>> spark.createDataFrame(pdf, schema).show(truncate=False)+-------------------+-------------------+------------------------------------------+---------------------+---------------------+---------------------------+-------------------------------+|ts                 |ts_ntz             |struct                                    |array                |array_ntz            |map                        |map_ntz                        |+-------------------+-------------------+------------------------------------------+---------------------+---------------------+---------------------------+-------------------------------+|2023-01-01 00:00:00|2023-01-01 00:00:00|{2023-01-01 00:00:00, 2023-01-01 00:00:00}|[2023-01-01 00:00:00]|[2023-01-01 00:00:00]|{ts -> 2023-01-01 00:00:00}|{ts_ntz -> 2023-01-01 00:00:00}|+-------------------+-------------------+------------------------------------------+---------------------+---------------------+---------------------------+-------------------------------+```##### `df.toPandas()````py>>> spark.conf.set(\"spark.sql.session.timeZone\ \"America/New_York\")>>> df.toPandas()                   ts     ts_ntz                                      struct                  array              array_ntz                          map                          map_ntz0 2023-01-01 03:00:00 2023-01-01  (2023-01-01 03:00:00, 2023-01-01 00:00:00)  [2023-01-01 03:00:00]  [2023-01-01 00:00:00]  {'ts': 2023-01-01 03:00:00}  {'ts_ntz': 2023-01-01 00:00:00}```### Why are the changes needed?Currently nested timestamps in `spark.createDataFrame()` with pandas DataFrame and `df.toPandas()` are not supported with `ArrayType` and `MapType`, or return different results from the top-level timestamps with `StructType`.For the following schema and pandas DataFrame:```pyschema = (    StructType()    .add(\"ts\ TimestampType())    .add(\"ts_ntz\ TimestampNTZType())    .add(        \"struct\ StructType().add(\"ts\ TimestampType()).add(\"ts_ntz\ TimestampNTZType())    ))data = [    Row(        datetime.datetime(2023, 1, 1, 0, 0, 0),        datetime.datetime(2023, 1, 1, 0, 0, 0),        Row(            datetime.datetime(2023, 1, 1, 0, 0, 0),            datetime.datetime(2023, 1, 1, 0, 0, 0),        ),    )]pdf = pd.DataFrame.from_records(data, columns=schema.names)```##### `spark.createDataFrame()`- Without Arrow```py>>> spark.conf.set(\"spark.sql.session.timeZone\ \"America/New_York\")>>> spark.createDataFrame(pdf, schema).show(truncate=False)+-------------------+-------------------+------------------------------------------+|ts                 |ts_ntz             |struct                                    |+-------------------+-------------------+------------------------------------------+|2023-01-01 00:00:00|2023-01-01 00:00:00|{2023-01-01 03:00:00, 2023-01-01 00:00:00}|+-------------------+-------------------+------------------------------------------+```- With Arrow or Spark Connect:```py>>> spark.createDataFrame(pdf, schema).show(truncate=False)+-------------------+-------------------+------------------------------------------+|ts                 |ts_ntz             |struct                                    |+-------------------+-------------------+------------------------------------------+|2023-01-01 00:00:00|2023-01-01 00:00:00|{2022-12-31 19:00:00, 2023-01-01 00:00:00}|+-------------------+-------------------+------------------------------------------+```##### `df.toPandas()`For the following DataFrame:```py>>> spark.conf.unset(\"spark.sql.session.timeZone\")>>> df = spark.createDataFrame(data, schema)>>>>>> df.show(truncate=False)+-------------------+-------------------+------------------------------------------+|ts                 |ts_ntz             |struct                                    |+-------------------+-------------------+------------------------------------------+|2023-01-01 00:00:00|2023-01-01 00:00:00|{2023-01-01 00:00:00, 2023-01-01 00:00:00}|+-------------------+-------------------+------------------------------------------+>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')```- Without Arrow```py>>> spark.conf.set(\"spark.sql.session.timeZone\ \"America/New_York\")>>> df.toPandas()                   ts     ts_ntz                                      struct0 2023-01-01 03:00:00 2023-01-01  (2023-01-01 00:00:00, 2023-01-01 00:00:00)```- With Arrow or Spark Connect:```py>>> df.toPandas()                   ts     ts_ntz                                      struct0 2023-01-01 03:00:00 2023-01-01  (2023-01-01 08:00:00, 2023-01-01 00:00:00)```### Does this PR introduce _any_ user-facing change?Users will be able to use nested timestamps.### How was this patch tested?Added/updated the related tests.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_0017.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Update existed UT.Pass GA.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_2400.### Why are the changes needed?Improve the error framework.### Does this PR introduce _any_ user-facing change?'No'.### How was this patch tested?Exists test cases.
1	-1	### What changes were proposed in this pull request?Deduplicate `scikit-learn` in Dockerfile### Why are the changes needed?has two `scikit-learn` items in `pip` command### Does this PR introduce _any_ user-facing change?no### How was this patch tested?existing CI
1	-2	### What changes were proposed in this pull request?The pr aims to fix bug for `trim`.### Why are the changes needed?- Before- After### Does this PR introduce _any_ user-facing change?Yes.### How was this patch tested?- Add new UT.- Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to fix wrong reference, include:- Hadoop doc related- Kafka doc related- Other### Why are the changes needed?Update reference.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual testing.
1	-2	### What changes were proposed in this pull request?The pr aims to upgrade buf from 1.18.0 to 1.19.0### Why are the changes needed?1.Release Notes: https://github.com/bufbuild/buf/releases, bug fixed as follow:- Fix issue in buf build and buf generate where the use of type filtering (via --type flags) would cause the resulting image to have no source code info, even when --exclude-source-info was not specified. The main impact of the bug was thatgenerated code would be missing comments.- Fix issue in buf curl when using --user or --netrc that would cause a malformed Authorization header to be sent.2.https://github.com/bufbuild/buf/compare/v1.18.0...v1.19.03.Manually test: dev/connect-gen-protos.sh, this upgrade will not change the generated files.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manually test and Pass GA.
2	-2	### What changes were proposed in this pull request?This PR aims to update K8s doc to recommend K8s 1.24+.### Why are the changes needed?**1. Default K8s Version in Public Cloud environments**As of today (2023 May), the default K8s versions of public cloud providers moved on to K8s 1.24+ already.- EKS: v1.26 (Default)- GKE: v1.24 (Stable), v1.25 (Regular), v1.27 (Rapid)**2. End Of Support**In addition, K8s 1.23 and olders are going to reach EOL when Apache Spark 3.5.0 arrives. K8s 1.24 also will reach EOL in some cloud providers.| K8s  |   AKS   |   GKE   |   EKS   || ---- | ------- | ------- | ------- || 1.23 | 2023-03 | 2023-07 | 2023-10 || 1.24 | 2023-07 | 2023-10 | 2024-01 |- [AKS EOL Schedule](https://docs.microsoft.com/en-us/azure/aks/supported-kubernetes-versions?tabs=azure-cli#aks-kubernetes-release-calendar)- [GKE EOL Schedule](https://cloud.google.com/kubernetes-engine/docs/release-schedule)- [EKS EOL Schedule](https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html)### Does this PR introduce _any_ user-facing change?No, this is a documentation-only change about K8s versions.### How was this patch tested?Manual review.
1	-3	### What changes were proposed in this pull request?Remove the upper bound of `matplotlib` in requirements### Why are the changes needed?1, actually, `matplotlib` is not pinned in CI;2, `matplotlib<3.3.0` fails `pip install -U -r dev/requirements.txt` in some cases, e.g. in `ubuntu 18.04````      gcc -pthread -B /home/ruifeng.zheng/miniconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/ruifeng.zheng/miniconda3/include -fPIC -O2 -isystem /home/ruifeng.zheng/miniconda3/include -fPIC -DFREETYPE_BUILD_TYPE=system -DPY_ARRAY_UNIQUE_SYMBOL=MPL_matplotlib_ft2font_ARRAY_API -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -D__STDC_FORMAT_MACROS=1 -Iextern/agg24-svn/include -I/home/ruifeng.zheng/miniconda3/lib/python3.10/site-packages/numpy/core/include -I/home/ruifeng.zheng/miniconda3/include/python3.10 -c src/checkdep_freetype2.c -o build/temp.linux-x86_64-cpython-310/src/checkdep_freetype2.o      src/checkdep_freetype2.c:3:6: error: #error \"FreeType version 2.3 or higher is required. You may set the MPLLOCALFREETYPE environment variable to 1 to let Matplotlib download it.\"           #error \"FreeType version 2.3 or higher is required. \\            ^~~~~      src/checkdep_freetype2.c:10:10: error: #include expects \"FILENAME\" or <FILENAME>       #include FT_FREETYPE_H                ^~~~~~~~~~~~~      src/checkdep_freetype2.c:15:9: note: #pragma message: Compiling with FreeType version FREETYPE_MAJOR.FREETYPE_MINOR.FREETYPE_PATCH.       #pragma message(\"Compiling with FreeType version \" \\               ^~~~~~~      src/checkdep_freetype2.c:18:4: error: #error \"FreeType version 2.3 or higher is required. You may set the MPLLOCALFREETYPE environment variable to 1 to let Matplotlib download it.\"         #error \"FreeType version 2.3 or higher is required. \\          ^~~~~      error: command '/usr/bin/gcc' failed with exit code 1      [end of output]  note: This error originates from a subprocess, and is likely not a problem with pip.error: legacy-install-failure× Encountered error while trying to install package.╰─> matplotlib```### Does this PR introduce _any_ user-facing change?no, dev-only### How was this patch tested?manually test
2	-1	### What changes were proposed in this pull request?This pr aims to upgrade netty from 4.1.89 to 4.1.92.### Why are the changes needed?The new version brings some bug fix like:- https://github.com/netty/netty/pull/13278- https://github.com/netty/netty/pull/13314The full release notes as follows:- https://netty.io/news/2023/03/14/4-1-90-Final.html- https://netty.io/news/2023/04/03/4-1-91-Final.html- https://netty.io/news/2023/04/25/4-1-92-Final.html### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GitHub Actions
1	-2	### What changes were proposed in this pull request?This PR implements `SparkSession.addArtifact(s)`. The logic is basically translated from Scala (https://github.com/apache/spark/pull/40256) to Python here.One difference is that, it does not support `class` files and `cache` (https://github.com/apache/spark/pull/40827) because it's not realistic for Python client to add `class` files. For `cache`, this implementation will be used as a base work.This PR is also a base work to implement sending py-files and archive files### Why are the changes needed?For feature parity w/ Scala client. In addition, this is also base work for `cache` implementation, and Python dependency management (https://www.databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html)### Does this PR introduce _any_ user-facing change?Yes, this exposes an API `SparkSession.addArtifact(s)`.### How was this patch tested?Unittests were added. Also manually tested.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Hive support CREATE TABLE LIKE FILE statement in https://issues.apache.org/jira/browse/HIVE-26395 .So this PR bring `CREATE TABLE LIKE FILE` to spark.This statement will read file schema before create table, the schema will be used for new table schema.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Add new feature `CREATE TABLE LIKE FILE` statement supported.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_240[1-3].### Why are the changes needed?Improve the error framework.### Does this PR introduce _any_ user-facing change?'No'.### How was this patch tested?Exists test cases.
2	-2	### What changes were proposed in this pull request?The pr aims to refactor `INVALID_SQL_SYNTAX` for avoiding to embed error's text in source code.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Update UT.- Pass GA
3	-3	### What changes were proposed in this pull request?This PR proposes to add comments for all pandas-on-Spark with Spark Connect failing tests, with related JIRA tickets.Created all tickets for pandas API on Spark with Spark Connect are here: SPARK-42497### Why are the changes needed?To assign appropriate tasks to efficiently fix the failing tests.### Does this PR introduce _any_ user-facing change?No, it's dev-only.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass
1	-2	### What changes were proposed in this pull request?The pr aims to generate a blank line when formatting `error-classes.json` file using `SparkThrowableSuite`.### Why are the changes needed?- When I format `error-classes.json` file using `SparkThrowableSuite`, I found the last blank line of the file will be erased, which does not comply with universal underlying code specifications, similar:python: https://www.flake8rules.com/rules/W391.html- Promote developer experience.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual testing.
1	-2	### What changes were proposed in this pull request?1. reuse SPARK_CONF_DIR config map for executor pods when the cm is already created for driver pod2. deprecated `spark.kubernetes.executor.disableConfigMap` as it may not be needed any more3. create config map for executor when running on k8s-client only### Why are the changes needed?1. avoid unnecessary config map creation and reduce overhead to K8S2. code optimize to avoid unnecessary disk file scan to create spark conf config map### Does this PR introduce _any_ user-facing change?Yes.  `spark.kubernetes.executor.disableConfigMap` is deprecated and may not be necessary.### How was this patch tested?Added UTs.
2	-2	### What changes were proposed in this pull request?This PR reverts https://github.com/apache/spark/commit/58b6535cbf76f2b26fc08b94905a57dcc4d955f6 by merging `pyspark.pandas.tests.test_dataframe` and `pyspark.pandas.tests.test_dataframe_slow`, so that developers can locally use commands like `python/run-tests --testnames 'pyspark.pandas.tests.test_dataframe DataFrameTests.test_validate_test_names` to test all the `DataFrame` APIs again.But, I remove `pyspark.pandas.tests.test_dataframe` from the CI test lists, instead I would like to introduce another method to split the `test_dataframe` in CI:1. add a dictionary `python/pyspark/pandas/tests/test_dataframe_splits`;2. divide `test_dataframe` into splits like `split_a_b` which contains a subset of `test_dataframe` by name patterns, and add those splits into CI;3. add an extra test `test_validate_test_names` to make sure no test is missing.In this way, if we need to split a large test file in the future, we can avoid large changes of moving code, instead we will only need to update those split files.### Why are the changes needed?even after https://github.com/apache/spark/commit/58b6535cbf76f2b26fc08b94905a57dcc4d955f6, `pyspark.pandas.tests.test_dataframe` and `pyspark.pandas.tests.test_dataframe_slow` are still too slow, each one normally takes >5 minutes, and sometimes >10 minutes.### Does this PR introduce _any_ user-facing change?No, test-only### How was this patch tested?updated CI
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This is a follow up PR for #40684 . Add error class `SESSION_NOT_SAME` define into `error_classes.py` with a template error message.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Unified error message<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?add new test.<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
1	-1	### What changes were proposed in this pull request?Fix typo in test - should check q2, not q1 twice.### Why are the changes needed?Fix typo.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?CI
3	-3	### What changes were proposed in this pull request?Downgrade scala-maven-plugin.version from 4.8.1 to 4.8.0and add a note <!-- dont update scala-maven-plugin to version 4.8.1 -->  ### Why are the changes needed?see https://github.com/apache/spark/pull/41228https://github.com/apache/spark/pull/40442https://github.com/apache/spark/commit/1530e8dc44d7d963791ede03d54b1c2ad5e91c99As mentioned in https://github.com/apache/spark/pull/40442,  there are some regression with the 4.8.1：- Run `./build/mvn -DskipTests clean package` with Java 17 will failed```[INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile)  spark-core_2.12 ---[INFO] Not compiling main sources[INFO][INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first)  spark-core_2.12 ---[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)[INFO] compiling 597 Scala sources and 103 Java sources to /home/bjorn/github/spark/core/target/scala-2.12/classes ...[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: value sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: value sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: value Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: value Signal[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: value Signal[ERROR] 19 errors found[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:[INFO][INFO] Spark Project Parent POM ........................... SUCCESS [ 3.848 s][INFO] Spark Project Tags ................................. SUCCESS [ 12.106 s][INFO] Spark Project Sketch ............................... SUCCESS [ 10.685 s][INFO] Spark Project Local DB ............................. SUCCESS [ 8.743 s][INFO] Spark Project Networking ........................... SUCCESS [ 9.362 s][INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 7.828 s][INFO] Spark Project Unsafe ............................... SUCCESS [ 9.071 s][INFO] Spark Project Launcher ............................. SUCCESS [ 4.776 s][INFO] Spark Project Core ................................. FAILURE [ 17.228 s]```- Run `build/mvn clean install  -DskipTests -Pscala-2.13` with Java8 + Scala 2.13There are compilation errors as `ERROR] -release is only supported on Java 9 and higher`  although it does not cause compilation failures. More, I saw https://github.com/davidB/scala-maven-plugin/issues/686, So  it seems that 4.8.1 and Java 8 are not compatible well.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR refactors the default column value resolution so that we don't need an extra DS v2 API for external v2 sources. The general idea is to split the default column value resolution into two parts:1. resolve the column \"DEFAULT\" to the column default expression. This applies to `Project`/`UnresolvedInlineTable` under `InsertIntoStatement`, and assignment expressions in `UpdateTable`/`MergeIntoTable`.2. fill missing columns with column default values for the input query. This does not apply to UPDATE and non-INSERT action of MERGE as they use the column from the target table as the default value.The first part should be done for all the data sources, as it's part of column resolution. The second part should not be applied to v2 data sources with `ACCEPT_ANY_SCHEMA`, as they are free to define how to handle missing columns.More concretely, this PR:1. put the column \"DEFAULT\" resolution logic in the rule `ResolveReferences`, with two new virtual rules. This is to follow https://github.com/apache/spark/pull/388882. put the missing column handling in `TableOutputResolver`, which is shared by both the v1 and v2 insertion resolution rule. External v2 data sources can add custom catalyst rules to deal with missing columns for themselves.3. Remove the old rule `ResolveDefaultColumns`. Note that, with the refactor, we no long need to manually look up the table. We will deal with column default values after the target table of INSERT/UPDATE/MERGE is resolved.4. Remove the rule `ResolveUserSpecifiedColumns` and merge it to `PreprocessTableInsertion`. These two rules are both to resolve v1 insertion, and it's tricky to reason about their interactions. It's clearer to resolve the insertion with one pass.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  5. If you fix a bug, you can clarify why it is a bug.-->code cleanup and remove unneeded DS v2 API.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->updated tests
2	-2	### What changes were proposed in this pull request?Introduce AvroOption \"enableStableIdentifiersForUnionType\". If it is set to true (default remains to be false), Avro's union is converted to SQL schema by naming field name \"member_\" + type name. This is to try to keep field name stable with type name.### Why are the changes needed?The purpose of this is twofold:To allow adding or removing types to the union without affecting the record names of other member types. If the new or removed type is not ordered last, then existing queries referencing \"member2\" may need to be rewritten to reference \"member1\" or \"member3\".Referencing the type name in the query is more readable than referencing \"member0\".For example, our system produces an avro schema from a Java type structure where subtyping maps to union types whose members are ordered lexicographically. Adding a subtype can therefore easily result in all references to \"member2\" needing to be updated to \"member3\".### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Add a unit test that covers all types supported in union, as well as some potential name conflict cases.
2	-3	### What changes were proposed in this pull request?Scala client fails with NPE when running the following reduce agg:```spark.range(0, 5, 1, 10).as[Long].reduce(_ + _) == 10```The reason is because the `range` will produce null partitions and the Reduce encoder will not be able to set the default value correctly for partitions that contains Scala primitives. In the example, we expect 0 but receive null. This causes the codegen wrongly assumes the input is nullable and generates wrong code.### Why are the changes needed?Bug fix### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit and Scala Client E2E tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Handle IsNull predicate when doing rewriteDomainJoins. This predicate is a result of constant folding that occurred at some point earlier in the query plan### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Bug reported in https://issues.apache.org/jira/browse/SPARK-43596### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Query test
3	-2	### What changes were proposed in this pull request?This PR aims to handle a corner case when `row.excludedInStages` field is missing.### Why are the changes needed?To fix the following type error when Spark loads some very old 2.4.x or 3.0.x logs.![missing](https://github.com/apache/spark/assets/9700541/f402df10-bf92-4c9f-8bd1-ec9b98a67966)We have two places and this PR protects both places.```$ git grep row.excludedInStagescore/src/main/resources/org/apache/spark/ui/static/executorspage.js:    if (typeof row.excludedInStages === \"undefined\" || row.excludedInStages.length == 0) {core/src/main/resources/org/apache/spark/ui/static/executorspage.js:    return \"Active (Excluded in Stages: [\" + row.excludedInStages.join(\ \") + \"])\";```### Does this PR introduce _any_ user-facing change?No, this will remove the error case only.### How was this patch tested?Manual review.
2	-2	### What changes were proposed in this pull request?In `Anaylzer#commonNaturalJoinProcessing`, set nullable correctly when adding the join keys to the Project's hidden_output tag.### Why are the changes needed?The value of the hidden_output tag will be used for resolution of attributes in parent operators, so incorrect nullabilty can cause problems.For example, assume this data:```create or replace temp view t1 as values (1), (2), (3) as (c1);create or replace temp view t2 as values (2), (3), (4) as (c1);```The following query produces incorrect results:```spark-sql (default)> select explode(array(t1.c1, t2.c1)) as x1from t1full outer join t2using (c1);1-1      <== should be null2233-1      <== should be null4Time taken: 0.663 seconds, Fetched 8 row(s)spark-sql (default)> ```Similar issues occur with right outer join and left outer join.`t1.c1` and `t2.c1` have the wrong nullability at the time the array is resolved, so the array's `containsNull` value is incorrect.`UpdateNullability` will update the nullability of `t1.c1` and `t2.c1` in the `CreateArray` arguments, but will not update `containsNull` in the function's data type.Queries that don't use arrays also can get wrong results. Assume this data:```create or replace temp view t1 as values (0), (1), (2) as (c1);create or replace temp view t2 as values (1), (2), (3) as (c1);create or replace temp view t3 as values (1, 2), (3, 4), (4, 5) as (a, b);```The following query produces incorrect results:```select t1.c1 as t1_c1, t2.c1 as t2_c1, bfrom t1full outer join t2using (c1),lateral (  select b  from t3  where a = coalesce(t2.c1, 1)) lt3;1\t1\t2NULL\t3\t4Time taken: 2.395 seconds, Fetched 2 row(s)spark-sql (default)> ```The result should be the following:```0\tNULL\t21\t1\t2NULL\t3\t4```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New tests.
1	-1	### What changes were proposed in this pull request?Complete parity tests of Pandas UDF.Specifically, parity tests are added referencing```test_pandas_udf_grouped_agg.pytest_pandas_udf_scalar.pytest_pandas_udf_window.py```### Why are the changes needed?Parity with vanilla PySpark.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit tests.
1	-2	### What changes were proposed in this pull request?This pr aims upgrade `zstd-jni` from 1.5.5-2 to 1.5.5-3.### Why are the changes needed?New version includes some improvement & bug fixed, eg- https://github.com/luben/zstd-jni/pull/258- https://github.com/luben/zstd-jni/pull/262- https://github.com/luben/zstd-jni/issues/253    Other changes as follows:- https://github.com/luben/zstd-jni/compare/v1.5.5-2...v1.5.5-3### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GA.
1	-1	### What changes were proposed in this pull request?This pr aims upgrade dropwizard metrics to 4.2.18.### Why are the changes needed?- 4.2.17 VS 4.2.18: https://github.com/dropwizard/metrics/compare/v4.2.17...v4.2.18- This version relies on jetty9 v9.4.51.v20230217 for compilation, and Spark is currently using this version as well[Update jetty9.version to v9.4.51.v20230217](https://github.com/dropwizard/metrics/commit/245c516ada98986026ebf505f8c2698737207cb2)### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to upgrade common-io from 2.11.0 to 2.12.0.### Why are the changes needed?common-io new version includes some improvement & bug fixed, eg- https://github.com/apache/commons-io/pull/450- https://github.com/apache/commons-io/pull/368- [Add PathUtils.touch(Path)](https://github.com/apache/commons-io/commit/fd7c8182d2117d01f43ccc9fe939105f834ba672) The error exception of the FileUtils.touch method has been changed from `java.io.FileNotFoundException` to `java.nio.file.NoSuchFileException`- common-io 2.11.0 VS 2.12.0https://github.com/apache/commons-io/compare/rel/commons-io-2.11.0...rel/commons-io-2.12.0### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-1	### What changes were proposed in this pull request?move unsupported functions to `__getattr__`, except `getActiveSession` (this method doesn't work for `classmethod`)### Why are the changes needed?Hide unsupported functions from auto-completionbefore:<img width=\"1464\" alt=\"image\" src=\"https://github.com/apache/spark/assets/7322292/6a3efc83-99ed-4b73-b681-13640b3de7a0\">after:<img width=\"1311\" alt=\"image\" src=\"https://github.com/apache/spark/assets/7322292/79366a32-718b-4208-ae1f-3e749971d6d2\">### Does this PR introduce _any_ user-facing change?yes### How was this patch tested?manually check in `ipython`
1	-1	### What changes were proposed in this pull request?Document the difference between `Drop(column)` and `Drop(columnName)`### Why are the changes needed?to better illustrate this difference### Does this PR introduce _any_ user-facing change?yes, new doc and example### How was this patch tested?CI and added doctest
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes to fix `SeriesDateTimeTests.test_quarter` to work properly.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Test has not been properly testing### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No, test-only.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manually tested, and the existing CI should pass
2	-1	### What changes were proposed in this pull request?This PR aims to update maven-checkstyle-plugin from 3.2.2 to 3.3.0.### Why are the changes needed?- v3.2.2 VS v3.3.0 https://github.com/apache/maven-checkstyle-plugin/compare/maven-checkstyle-plugin-3.2.2...maven-checkstyle-plugin-3.3.0- This version relies on commons-lang3 for compilation, and Spark is currently using this version as wellhttps://github.com/apache/maven-checkstyle-plugin/compare/maven-checkstyle-plugin-3.2.2...maven-checkstyle-plugin-3.3.0#diff-9c5fb3d1b7e3b0f54bc5c4182965c4fe1f9023d449017cece3005d3f90e8e4d8L199-L202### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Manual testing by: ./build/mvn -Pkinesis-asl -Pmesos -Pkubernetes -Pyarn -Phive -Phive-thriftserver -am checkstyle:checkstyle- Pass GA.
2	-1	### What changes were proposed in this pull request?This PR port [HIVE-12188](https://issues.apache.org/jira/browse/HIVE-12188)(DoAs does not work properly in non-kerberos secure HS2) to Spark.### Why are the changes needed?The case with following settings is valid but it seems still not work correctly in current HS2(Copied from HIVE-12188's description):```hive.server2.authentication=NONE (or LDAP)hive.server2.enable.doAs=truehive.metastore.sasl.enabled=true (with HMS Kerberos enabled)```### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual test.
1	-2	### What changes were proposed in this pull request?This PR aims to change exceptions created in package org.apache.spark.io to use error class.This PR also adds `toConf` and `toConfVal` in `SparkCoreErrors`.### Why are the changes needed?This is to move exceptions created in package org.apache.spark.io to error class.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Updated existing tests.
2	-1	### What changes were proposed in this pull request?This PR proposes to add the support of pyfiles (`.zip`, `.py`, `.jar`, `.egg` files) in `SparkSession.addArtifacts`.### Why are the changes needed?In order for end users to add the dependencies in Python Spark Connect client.### Does this PR introduce _any_ user-facing change?Yes, it adds the support of pyfiles (`.zip`, `.py`, `.jar`, `.egg` files) in `SparkSession.addArtifacts`.### How was this patch tested?Manually tested via `local-cluster`.```bash./sbin/start-connect-server.sh --jars `ls connector/connect/server/target/**/spark-connect*SNAPSHOT.jar` --master \"local-cluster[2,2,1024]\"./bin/pyspark --remote \"sc://localhost:15002\"``````pythonimport osimport tempfilefrom pyspark.sql.functions import udfimport shutilwith tempfile.TemporaryDirectory() as d:    package_path = os.path.join(d, \"my_zipfile\")    os.mkdir(package_path)    pyfile_path = os.path.join(package_path, \"__init__.py\")    with open(pyfile_path, \"w\") as f:        _ = f.write(\"my_func = lambda: 5\")    shutil.make_archive(package_path, 'zip', d, \"my_zipfile\")    @udf(\"long\")    def func(x):        import my_zipfile        return my_zipfile.my_func()    spark.addArtifacts(f\"{package_path}.zip\ pyfile=True)    spark.range(1).select(func(\"id\")).show()```Also added a couple of unittests.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_240[4-5].### Why are the changes needed?Improve the error framework.### Does this PR introduce _any_ user-facing change?'No'.### How was this patch tested?N/A
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->After SPARK-37820, `select unbase64(\"abcs==\")`(malformed input) always throws an exception.So, `unbase64()`'s behavior for malformed input changed silently after SPARK-37820:- before: return a best-effort result, because it uses [LENIENT](https://github.com/apache/commons-codec/blob/rel/commons-codec-1.15/src/main/java/org/apache/commons/codec/binary/Base64InputStream.java#L46) policy: any trailing bits are composed into 8-bit bytes where possible. The remainder are discarded.- after: throw an exceptionAnd there is no way to restore the previous behavior. To tolerate the malformed input, the user should migrate `unbase64(<input>)` to `try_to_binary(<input>, 'base64')` to get NULL instead of interrupting by exception. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Add the behavior change to migration guide.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Manuelly review.
1	-1	### What changes were proposed in this pull request?Upgrade FasterXML jackson from 2.15.0 to 2.15.1### Why are the changes needed?New version that fix some bugs, the full release-notes as follows:- https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1 ### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Pass GA
2	-3	### What changes were proposed in this pull request?This pr aims to added a cleaning action for the `$sparkHome/sql/hive/target/$scalaDir/classes` and `$sparkHome/sql/hive/target/$scalaDir/test-classes` directories before `SimpleSparkConnectService` starts when running test cases that inherit `RemoteSparkSession` without `-Phive` to avoid to unexpected loading of `sql/hive/target/scala-2.12/classes/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` by `ServiceLoader`. ### Why are the changes needed?When we run the test cases that inherit `RemoteSparkSession`, the classpath used to launch `SimpleSparkConnectService` will at least include the following directory, both maven and sbt:```$sparkHome/conf/$sparkHome/common/kvstore/target/scala-2.12/classes/$sparkHome/common/network-common/target/scala-2.12/classes/$sparkHome/common/network-shuffle/target/scala-2.12/classes/$sparkHome/common/network-yarn/target/scala-2.12/classes$sparkHome/common/sketch/target/scala-2.12/classes/$sparkHome/common/tags/target/scala-2.12/classes/$sparkHome/common/unsafe/target/scala-2.12/classes/$sparkHome/core/target/scala-2.12/classes/$sparkHome/examples/target/scala-2.12/classes/$sparkHome/graphx/target/scala-2.12/classes/$sparkHome/launcher/target/scala-2.12/classes/$sparkHome/mllib/target/scala-2.12/classes/$sparkHome/repl/target/scala-2.12/classes/$sparkHome/resource-managers/mesos/target/scala-2.12/classes$sparkHome/resource-managers/yarn/target/scala-2.12/classes$sparkHome/sql/catalyst/target/scala-2.12/classes/$sparkHome/sql/core/target/scala-2.12/classes/$sparkHome/sql/hive/target/scala-2.12/classes/$sparkHome/sql/hive-thriftserver/target/scala-2.12/classes/$sparkHome/streaming/target/scala-2.12/classes/$sparkHome/common/kvstore/target/scala-2.12/test-classes$sparkHome/common/network-common/target/scala-2.12/test-classes/$sparkHome/common/network-shuffle/target/scala-2.12/test-classes/$sparkHome/common/network-yarn/target/scala-2.12/test-classes$sparkHome/common/sketch/target/scala-2.12/test-classes$sparkHome/common/tags/target/scala-2.12/test-classes/$sparkHome/common/unsafe/target/scala-2.12/test-classes$sparkHome/core/target/scala-2.12/test-classes/$sparkHome/examples/target/scala-2.12/test-classes$sparkHome/graphx/target/scala-2.12/test-classes$sparkHome/launcher/target/scala-2.12/test-classes/$sparkHome/mllib/target/scala-2.12/test-classes$sparkHome/repl/target/scala-2.12/test-classes$sparkHome/resource-managers/mesos/target/scala-2.12/test-classes$sparkHome/resource-managers/yarn/target/scala-2.12/test-classes$sparkHome/sql/catalyst/target/scala-2.12/test-classes/$sparkHome/sql/core/target/scala-2.12/test-classes$sparkHome/sql/hive/target/scala-2.12/test-classes$sparkHome/sql/hive-thriftserver/target/scala-2.12/test-classes$sparkHome/streaming/target/scala-2.12/test-classes$sparkHome/connector/connect/client/jvm/target/scala-2.12/test-classes/$sparkHome/connector/connect/common/target/scala-2.12/test-classes/...```So if the test case need calls `DataSource#lookupDataSource` and the `hive` module is compiled, `sql/hive/target/scala-2.12/classes/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` will be loaded by `ServiceLoader`.After SPARK-43186 | https://github.com/apache/spark/pull/40848 merged, `org.apache.spark.sql.hive.execution.HiveFileFormat` changed to use `org.apache.hadoop.hive.ql.plan.FileSinkDesc` instead of `org.apache.spark.sql.hive.HiveShim.ShimFileSinkDesc`, it has a strong dependence on `hive-exec`. But when there is no hive related jars under `assembly/target/$scalaDir/jars/`, it will cause initialization fail of `org.apache.spark.sql.hive.execution.HiveFileFormat` and test fail.For example, when we run the following commands to test `connect-client-jvm` without `-Phive`:```build/mvn clean install -DskipTestsbuild/mvn test -pl connector/connect/client/jvm```Then hive related jars will not be copied to `assembly/target/$scalaDir/jars/`, there will be test error as:**Client side**```- read and write *** FAILED ***  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated  at io.grpc.Status.asRuntimeException(Status.java:535)  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)  at scala.collection.Iterator.toStream(Iterator.scala:1417)  at scala.collection.Iterator.toStream$(Iterator.scala:1416)  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)  ... ```**Server side**```java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated\tat java.util.ServiceLoader.fail(ServiceLoader.java:232)\tat java.util.ServiceLoader.access$100(ServiceLoader.java:185)\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)\tat java.util.ServiceLoader$1.next(ServiceLoader.java:480)\tat scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:46)\tat scala.collection.Iterator.foreach(Iterator.scala:943)\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:860)\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:559)\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2326)\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2091)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handleCommand(SparkConnectStreamHandler.scala:120)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:86)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)\tat org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)\tat org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)\tat org.sparkproject.connect.grpc.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)\tat org.sparkproject.connect.grpc.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\tat org.sparkproject.connect.grpc.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\tat java.lang.Thread.run(Thread.java:750)Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc\tat java.lang.Class.getDeclaredConstructors0(Native Method)\tat java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)\tat java.lang.Class.getConstructor0(Class.java:3075)\tat java.lang.Class.newInstance(Class.java:412)\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)\t... 40 moreCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\t... 45 more```So this PR proposal takes the initiative to clean up the `$sparkHome/sql/hive/target/$scalaDir/classes` and `$sparkHome/sql/hive/target/$scalaDir/test-classes` directories when `IntegrationTestUtils#isSparkHiveJarAvailable` is false to protect the above scenario. ### Does this PR introduce _any_ user-facing change?No, just for test.### How was this patch tested?- Pass Github Actions- Manual test:The following command can reproduce the problem without this prMaven```build/mvn clean install -DskipTestsbuild/mvn test -pl connector/connect/client/jvm```SBT```build/sbt packagebuild/sbt \"connect-client-jvm/test\"```**Before**Maven There are 13 test cases with similar failures```- recoverPartitions *** FAILED ***  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated  at io.grpc.Status.asRuntimeException(Status.java:535)  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)  at scala.collection.Iterator.toStream(Iterator.scala:1417)  at scala.collection.Iterator.toStream$(Iterator.scala:1416)  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)  ...```SBT There are similar errors of sbt, and the test will unexpectedly aborted.**After**Both Maven and SBT no longer have similar test failures
2	-1	### What changes were proposed in this pull request?Remove outdated assumptions on nested struct types### Why are the changes needed?To enable nested struct types support.### Does this PR introduce _any_ user-facing change?Yes. Nested struct types are supported now.### How was this patch tested?Unit tests.
1	-2	### What changes were proposed in this pull request?Expand the client compatibility check to include all sql APIs.### Why are the changes needed?Enhance the API compatibility coverage### Does this PR introduce _any_ user-facing change?Yes. This PR modified a few classes to make them package protected to avoid expose unwanted APIs to public.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR upgrades `snappy-java` version to 1.1.10.0 from 1.1.9.1.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The new `snappy-java` version fixes a potential issue for Graviton support when used with old GLIBC versions. See https://github.com/xerial/snappy-java/issues/417.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing tests.
1	-2	### What changes were proposed in this pull request?Exposes `TimestampNTZType` in `pyspark.sql.types`.```py>>> from pyspark.sql.types import *>>>>>> TimestampNTZType()TimestampNTZType()```### Why are the changes needed?`TimestampNTZType` is missing in `_all_` list in `pyspark.sql.types`.```py>>> from pyspark.sql.types import *>>>>>> TimestampNTZType()Traceback (most recent call last):  File \"<stdin>\ line 1, in <module>NameError: name 'TimestampNTZType' is not defined```### Does this PR introduce _any_ user-facing change?Users won't need to explicitly import `TimestampNTZType` but wildcard will work.### How was this patch tested?Existing tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Makes sure that the results of scalar subqueries are declared as nullable. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is an existing correctness bug, see https://issues.apache.org/jira/browse/SPARK-43760### Does this PR introduce _any_ user-facing change?Fixes a correctness issue, so it is user-facing.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Query tests.
2	-2	### What changes were proposed in this pull request?Change the default value of `daemonExitValue` from `null` to `None` in `eofExceptionWhileReadPortNumberError` to avoid a NPE.  This is possible `PythonWorkerFactory` can call this method without a value for `daemonExitValue` (https://github.com/apache/spark/blob/06648efddb43166577269400e73cd56210e15728/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala#L229-L230).### Why are the changes needed?If no value is provided for `daemonExitValue`, the expected behavior is to still throw an exception, but without an additional message.  Currently, this results in a NPE.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?N/A
1	-3	### What changes were proposed in this pull request?The pr aims to fix bug in AvroSuite for 'reading from invalid path throws exception'.### Why are the changes needed?- As discussed and analyzed in [41271](https://github.com/apache/spark/pull/41271#issuecomment-1560355918)- There is a problem with this UT. Its original intention was to test if there is no file with .avro extensions in the directory, and the read should fail. However, this UT triggered the error as FileUtils.touch instead of spark.read.format(\"avro\").load(dir.toString).The root cause for the failure of this case is that the parent directory was not created. When FileUtils.touch is called in version 1.11.0, it just throws java.io.FileNotFoundException, which covers the error.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_24[06-10].### Why are the changes needed?Improve the error framework.### Does this PR introduce _any_ user-facing change?'No'.### How was this patch tested?Exists test cases.
1	-2	### What changes were proposed in this pull request?This Is a follow-up of https://github.com/apache/spark/pull/41285.### Why are the changes needed?When merging to branch-3.4, `hadoop-2` dependency manifest is missed.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs.
2	-1	### What changes were proposed in this pull request?This PR proposes to add the support of archive (`.zip`, `.jar`, `.tar.gz`, `.tgz`, or `.tar` files) in `SparkSession.addArtifacts` so we can support Python dependency management in Python Spark Connect.### Why are the changes needed?In order for end users to add the dependencies and archive files in Python Spark Connect client.This PR enables the Python dependency management (https://www.databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html) usecase in Spark Connect.See below how to do this with Spark Connect Python client:#### PreconditionAssume that we have a Spark Connect server already running, e.g., by:```bash./sbin/start-connect-server.sh --jars `ls connector/connect/server/target/**/spark-connect*SNAPSHOT.jar` --master \"local-cluster[2,2,1024]\"```and assume that you already have a dev env:```bash# Notice that you should install `conda-pack`.conda create -y -n pyspark_conda_env -c conda-forge conda-pack python=3.9conda activate pyspark_conda_envpip install --upgrade -r dev/requirements.txt```#### Dependency management```python./bin/pyspark --remote \"sc://localhost:15002\"``````pythonimport conda_packimport os# Pack the current environment ('pyspark_conda_env') to 'pyspark_conda_env.tar.gz'. # Or you can run 'conda pack' in your shell.conda_pack.pack()  spark.addArtifact(f\"{os.environ.get('CONDA_DEFAULT_ENV')}.tar.gz#environment\ archive=True)spark.conf.set(\"spark.sql.execution.pyspark.python\ \"environment/bin/python\")# From now on, Python workers on executors use `pyspark_conda_env` Conda environment.```Run your Python UDFs```pythonimport pandas as pdfrom pyspark.sql.functions import pandas_udf@pandas_udf(\"long\")def plug_one(s: pd.Series) -> pd.Series:    return s + 1spark.range(10).select(plug_one(\"id\")).show()```### Does this PR introduce _any_ user-facing change?Yes, it adds the support of archive (`.zip`, `.jar`, `.tar.gz`, `.tgz`, or `.tar` files) in `SparkSession.addArtifacts`.### How was this patch tested?Manually tested as described above, and added a unittest.Also, manually tested with `local-cluster` mode with the code below:Also verified via:```pythonimport sysfrom pyspark.sql.functions import udfspark.range(1).select(udf(lambda x: sys.executable)(\"id\")).show(truncate=False)``````+----------------------------------------------------------------+|<lambda>(id)                                                    |+----------------------------------------------------------------+|/.../spark/work/app-20230524132024-0000/1/environment/bin/python|+----------------------------------------------------------------+```
1	-2	### What changes were proposed in this pull request?The pr aims to implement 'levenshtein(str1, str2[, threshold])' functions for `connect` module.### Why are the changes needed?After [Add a max distance argument to the levenshtein() function](https://issues.apache.org/jira/browse/SPARK-43493) We have already implemented it on the scala side, so we need to align it.### Does this PR introduce _any_ user-facing change?Yes, new API for Connect.### How was this patch tested?- Pass GA.- Manual testing1../build/sbt \"connect-client-jvm/testOnly *ClientE2ETestSuite*\"2.sh dev/connect-jvm-client-mima-check
1	-2	### What changes were proposed in this pull request?The pr aims to upgrade mima-core from 1.1.0 to 1.1.2.### Why are the changes needed?- New version includes bug fixed, eg:1.Handle POM-only modules by creating empty Definitions by @rossabaker in https://github.com/lightbend/mima/pull/743- Release note:1.https://github.com/lightbend/mima/releases/tag/1.1.22.https://github.com/lightbend/mima/releases/tag/1.1.1### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-1	### What changes were proposed in this pull request?The pr aims to move version configuration in `connect` module to parent.### Why are the changes needed?For better management and post maintenance, eg: upgrading some library version of a module while forgetting another one.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to implement 'levenshtein(str1, str2[, threshold])' functions in python client### Why are the changes needed?After Add a max distance argument to the levenshtein() function We have already implemented it on the scala side, so we need to align it on `pyspark`.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Manual testingpython/run-tests --testnames 'python.pyspark.sql.tests.test_functions FunctionsTests.test_levenshtein_function'- Pass GA
2	-2	### What changes were proposed in this pull request?The pr aims to Convert `_LEGACY_ERROR_TEMP_0036` to INVALID_SQL_SYNTAX.ANALYZE_TABLE_UNEXPECTED_NOSCAN### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA & Update UT.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->ParseToDate should load the EvalMode in main thread instead of loading it in a lazy val. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is because it is sometimes hard to estimate when the lazy val is executed while the SQLConf where we load the EvalMode is thread local. ### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing UT 
2	-2	This PR tries to move `ForeachWriter` into the common folder but it doesn't work.Concretely:1. Move the original `ForeachWriter` from `org.spark.sql` to `org.spark.sql.utils`.2. Create a new `ForeachWriter` that extends that one. Because customer imports it from sql package3. Create a new `ForeachWriter` in common folder that extends the util one4. Create a new `ForeachWriter` in scala client side that extend the common one5. Cast the customer created `ForeachWriter`(client, package sql) to the one in common in `DataStreamWriter`6. Cast the proto to `ForeachWriter` in common in `SparkConnectPlanner````import org.apache.spark.sql.ForeachWriterimport java.io._ val filePath = \"/home/wei.liu/test_foreach/output-custom\" case class MyTestClass(value: Int) {      override def toString: String = value.toString}val writer = new ForeachWriter[MyTestClass] {    var fileWriter: FileWriter = _    def open(partitionId: Long, version: Long): Boolean = {      fileWriter = new FileWriter(filePath, true)      true    }    def process(row: MyTestClass): Unit = {      fileWriter.write(row.toString)      fileWriter.write(\"\\")    }    def close(errorOrNull: Throwable): Unit = {      fileWriter.close()    }}val df = spark.readStream .format(\"rate\") .option(\"rowsPerSecond\ \"10\") .load()val query = df .selectExpr(\"CAST(value AS INT)\") .as[MyTestClass] .writeStream .foreach(writer) .outputMode(\"update\") .start()```Error is:```java.lang.ClassCastException: ammonite.$sess.cmd5$Helper$$anon$1 cannot be cast to org.apache.spark.sql.connect.common.ForeachWriter\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:2472)\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2112)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handleCommand(SparkConnectStreamHandler.scala:120)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:86)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)\tat org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:193)\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)\tat org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)\tat org.sparkproject.connect.grpc.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)\tat org.sparkproject.connect.grpc.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\tat org.sparkproject.connect.grpc.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\tat java.lang.Thread.run(Thread.java:748)```The hard requirement is the customer must be able to import a `ForeachWriter` in package `org.apache.spark.sql` and use it. 
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds a way for data sources to request Spark to represent updates as deletes and inserts. ### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->It may be beneficial for data sources to represent updates as deletes and inserts for delta-based implementations. Specifically, it may help to properly distribute and order records before writing.Delete records set only row ID and metadata attributes. Update records set data, row ID, metadata attributes. Insert records set only data attributes.For instance, a data source may rely on a metadata column `_row_id` (synthetic internally generated) to identify the row and may be partitioned by `bucket(product_id)`. Splitting updates into inserts and deletes would allow data sources to cluster all update and insert records in MERGE for the same partition into a single task. Otherwise, the clustering key for updates and inserts will be different (inserts will always have `_row_id` as null as it is a metadata column). The new functionality is critical to reduce the number of generated files. It also makes sense in UPDATE operations if the original and new partition of a record do not match.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->This PR adds a new method to `SupportsDelta` but the change is backward compatible. ### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->This PR comes with tests.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds support to subqueries that involve joins with correlated references in join predicates, e.g.```select * from t0 join lateral (select * from t1 join t2 on t1a = t2a and t1a = t0a);```(full example in https://issues.apache.org/jira/browse/SPARK-43780)### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is a valid SQL that is not yet supported by Spark SQL.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, previously unsupported queries become supported.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Query and unit tests
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR proposes to add a static Spark conf to configure an override to the Log4j logging level. It’s a config version of the `SparkContext.setLogLevel()` semantics, when set it’ll trigger a log level override at the beginning of `SparkContext` startup.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->This is particularly helpful in the Spark Connect scenario where there’s no way for the client to call `setLogLevel` because `SparkContext` is not yet supported in its API, and when it connects to a platform where it can't change the Log4j config file.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes, it adds a new public configuration `\"spark.log.level\"`, and by default it's unset which means there's no override.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->A new unit test in `SparkContextSuite`.
1	-1	### What changes were proposed in this pull request?The pr aims to add a test for nullability about 'levenshtein' function.### Why are the changes needed?Make testing more robust about 'levenshtein' function.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Pass GA.- Manual testing
2	-1	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/41212 added the new environment variable `HEAP_SIZE`. It let developers could fix some OOM issues.This PR want improve the document of `GenTPCDSData`, so that developers could easy to generate TPCDS table data.### Why are the changes needed?Make `GenTPCDSData` could easy to use.### Does this PR introduce _any_ user-facing change?'No'.This change is relationed to developers.### How was this patch tested?N/A
2	-1	### What changes were proposed in this pull request?This PR proposes to fix `BinaryOps` test for pandas API on Spark with Spark Connect.This includes SPARK-43666, SPARK-43667, SPARK-43668, SPARK-43669 at once, because they are all related similar modifications in single file.### Why are the changes needed?To support all features for pandas API on Spark with Spark Connect.### Does this PR introduce _any_ user-facing change?Yes, `BinaryOps.lt`,  `BinaryOps.le`, `BinaryOps.ge`, `BinaryOps.gt` are now working as expected on Spark Connect.### How was this patch tested?Uncomment the UTs, and tested manually.
2	-1	### What changes were proposed in this pull request?This PR proposes to fix `DatetimeOps` test for pandas API on Spark with Spark Connect.This includes SPARK-43676, SPARK-43677, SPARK-43678, SPARK-43679 at once, because they are all related similar modifications in single file.### Why are the changes needed?To support all features for pandas API on Spark with Spark Connect.### Does this PR introduce _any_ user-facing change?Yes, `DatetimeOps.lt`,  `DatetimeOps.le`, `DatetimeOps.ge`, `DatetimeOps.gt` are now working as expected on Spark Connect.### How was this patch tested?Uncomment the UTs, and tested manually.
1	-2	### What changes were proposed in this pull request?This PR proposes to pick a proper number of partitions when create a DataFrame from R DataFrame with Arrow.Previously, the number of partitions was always `1` if not specified.Now, it splits the input R DataFrame by `spark.sql.execution.arrow.maxRecordsPerBatch`, and pick a proper number of partitions (the number of batches).This is matched with PySpark code path:https://github.com/apache/spark/blob/46949e692e863992f4c50bdd482d5216d4fd9221/python/pyspark/sql/pandas/conversion.py#L618C11-L626### Why are the changes needed?To avoid having OOM when the R DataFrame is too large, and enables a proper distributed computing.### Does this PR introduce _any_ user-facing change?Yes, it changes the default partition number when users call `createDataFrame` with R DataFrame when Arrow optimization is enabled.The concept of the partition is subject to be internal, and by default it doesn't change its behaviour.### How was this patch tested?Manually tested with a large CSV file (3 GB).Also added a unittest.
2	-1	### What changes were proposed in this pull request?This PR proposes to fix `StringOps` test for pandas API on Spark with Spark Connect.This includes SPARK-43692, SPARK-43693, SPARK-43694, SPARK-43695 at once, because they are all related similar modifications in single file.### Why are the changes needed?To support all features for pandas API on Spark with Spark Connect.### Does this PR introduce _any_ user-facing change?Yes, `StringOps.lt`,  `StringOps.le`, `StringOps.ge`, `StringOps.gt` are now working as expected on Spark Connect.### How was this patch tested?Uncomment the UTs, and tested manually.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_1336.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Update existed UT.Pass GA.
2	-1	### What changes were proposed in this pull request?This PR proposes to fix `CategoricalOps` test for pandas API on Spark with Spark Connect.This includes SPARK-43671, SPARK-43672, SPARK-43673, SPARK-43674 at once, because they are all related similar modifications in single file.### Why are the changes needed?To support all features for pandas API on Spark with Spark Connect.### Does this PR introduce _any_ user-facing change?Yes, `CategoricalOps.lt`,  `CategoricalOps.le`, `CategoricalOps.ge`, `CategoricalOps.gt` are now working as expected on Spark Connect.### How was this patch tested?Uncomment the UTs, and tested manually.
3	-1	### What changes were proposed in this pull request?Currently, the syntax `SHOW CATALOGS LIKE pattern` supports an optional pattern, so as filtered out the expected catalogs.But the `Catalog.listCatalogs` missing the function both in Catalog API and Connect Catalog API.In fact, the optional pattern is very useful.### Why are the changes needed?This PR want add the optional pattern for `Catalog.listCatalogs`### Does this PR introduce _any_ user-facing change?'No'.New feature.### How was this patch tested?New test cases.
1	-2	### What changes were proposed in this pull request?Following `Frame` functions in https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/frame.html were in wrong categories:- pad- insert- rank- reindex- reindex_like### Why are the changes needed?should be consistent with https://pandas.pydata.org/docs/reference/frame.html#dataframe### Does this PR introduce _any_ user-facing change?yes, doc changes### How was this patch tested?existing CI
2	-1	### What changes were proposed in this pull request?Currently, `SparkConnectPlanner` have some method exists parameter not used at all!For example, `catalog.getCurrentCatalog` not carry any useful parameters.This PR want remove parameters not used for `SparkConnectPlanner`.### Why are the changes needed?Remove parameters not used for `SparkConnectPlanner`.### Does this PR introduce _any_ user-facing change?'No'.Just update the inner implementation.### How was this patch tested?Exists test cases.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_1335.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Update existed UT.Pass GA.
2	-2	### What changes were proposed in this pull request?* Move code related to execution from being done directly in the GRPC callback in SparkConnextStreamHandler, to it's own classes.* `ExecutionHolder` (renamed from `ExecutePlanHolder`) launches the execution in it's own thread.* The execution returns responses via `ExecutePlanResponseObserver`* The actual execution code is refactored into `SparkConnectPlanExecution`This allows to improve query interruption, by making `interrupt` method interrupt the execution thread. This makes `interrupt` work also when no Spark Jobs are running.The refactoring further opens the possibilities of detaching query execution from a single RPC execution. Right now `ExecutionHolder` is waiting for the execution thread to finish, and `ExecutePlanResponseObserver` is forwarding the responses directly to the RPC observer.In a followup, we can design different modes of execution, e.g.`ExecutePlanResponseObserver` buffering the responses. A client which lost connection could then reconnect and ask for the stream to be retransmitted.* `ExecutionHolder` returning the operationId to the client directly, and then client requesting results in separate RPCs, with more control over the response stream, instead of having it just pushed to it.### Why are the changes needed?* Improve the working of `interrupt`* Refactoring that opens up possibilities of detaching query execution from a single RPC.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Existing Spark Connect CI covers the execution.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This PR adds the initial support for Python user-defined table functions. It allows users to create UDTFs in PySpark and use them in PySpark and SQL.Here are examples of creating and using Python UDTFs:```python# Implement the UDTF classclass TestUDTF:  def __init__(self):    ...  def eval(self, *args):    yield \"hello\ \"world\"      def terminate(self):    ...# Create the UDTFfrom pyspark.sql.functions import udtftest_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")# Invoke the UDTFtest_udtf().show()+-----+-----+|   c1|   c2|+-----+-----+|hello|world|+-----+-----+# Register the UDTFspark.udtf.register(name=\"test_udtf\ f=test_udtf)# Invoke the UDTF in SQLspark.sql(\"SELECT * FROM test_udtf()\").show()+-----+-----+|   c1|   c2|+-----+-----+|hello|world|+-----+-----+```Please note that this is the initial PR, and there will be subsequent follow-up work to make it more user-friendly and performant.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->To support another type of user-defined function in PySpark.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes. After this PR, users can create Python user-defined table functions.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->New unit tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Fixes an error with codegen for unhex and unbase64 expression when failOnError is enabled introduced in https://github.com/apache/spark/pull/37483.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Codegen fails and Spark falls back to interpreted evaluation:```Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type \"BASE64\"```in the code block:```/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),/* 110 */             project_value_1,/* 111 */             BASE64,/* 112 */             \"try_to_binary\");/* 113 */         }```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Bug fix.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added to the existing tests so evaluate an expression with failOnError enabled to test that path of the codegen.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Streaming awaitTermination() is a long running API. Currently, it keeps running on server even if client disconnects. This change periodically checks if client has disconnected. If so, we can stop the operation and release resources.We use gRPC Context.isCancelled() to determine if client has disconnected and response cannot be returned to the client. [Here is the reference of of isCancelled()](https://grpc.github.io/grpc/cpp/classgrpc_1_1_server_context.html#af2d0f087805b4b475d01b12d73508f09): > Return whether this RPC failed before the server could provide its status back to the client.> This could be because of explicit API cancellation from the client-side or server-side, because of deadline exceeded, network connection reset, HTTP/2 parameter configuration (e.g., max message size, max connection age), etc. It does NOT include failure due to a non-OK status return from the server application's request handler, including [Status::CANCELLED](https://grpc.github.io/grpc/cpp/classgrpc_1_1_status.html#a9994ffe95a0495915d82481c2ec594ab).> IsCancelled is always safe to call when using sync or callback API. When using async API, it is only safe to call IsCancelled after the AsyncNotifyWhenDone tag has been delivered. Thread-safe.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->The change improves handling of awaitTermination(). It avoids resource waste of server side.### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Existing unit tests.Manually tested on local with:1. start spark connect 2. create a streaming query3. call query.awaitTermination()4. exit() the client to disconnect it5. check that an error (RPC context is cancelled when executing awaitTermination()) is logged on server. It proves that awaitTermination() is exited on the server side when client disconnects```>>> query = (...  spark...  .readStream...  .format(\"rate\")...  .option(\"numPartitions\ \"1\")...  .load()...  .writeStream...  .format(\"memory\")...  .queryName(\"tableName_35\")...  .start()... )>>>>>> query.awaitTermination()...>>> exit()```
2	-1	### What changes were proposed in this pull request?Add the ```to_varchar``` alias for ```to_char``` to support additional SQL functions.### Why are the changes needed?To minimize code changes for people using Spark SQL who are used to other engines that support ```to_varchar```.### Does this PR introduce _any_ user-facing change?Yes. The user can now invoke ```to_varchar``` instead of ```to_char``` in their SQL applications.### How was this patch tested?```to_varchar``` was added in ```numeric.sql``` and a new Golden file was generated to check that the query output was correct.
2	-1	### What changes were proposed in this pull request?Test on nested structs support in Pandas UDF. That support is newly enabled (compared to Spark 3.4).### Why are the changes needed?Test coverage.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Unit tests.
2	-1	### What changes were proposed in this pull request?Support StructType input in Arrow-optimized Python UDFs.### Why are the changes needed?Parity with pickled Python UDFs.### Does this PR introduce _any_ user-facing change?StructType input is supported in Arrow-optimized Python UDFs now.### How was this patch tested?TODO
1	-1	### What changes were proposed in this pull request?Delete unused file `test_parity_template.py`### Why are the changes needed?it is not used in CI (not listed in `modules.py`)### Does this PR introduce _any_ user-facing change?No, test-only### How was this patch tested?CI
1	-2	### What changes were proposed in this pull request?The PR assigns a more descriptive name to the error class _LEGACY_ERROR_TEMP_2132.### Why are the changes needed?This change Improves the error framework by making the error name more descriptive.### Does this PR introduce _any_ user-facing change?No### How was this patch tested?Tested with existing test cases.
1	-1	### What changes were proposed in this pull request?reorganize `ps.DataFrame` unit tests, to follow the categories inhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/frame.htmland https://pandas.pydata.org/docs/reference/frame.html### Why are the changes needed?1, even after https://github.com/apache/spark/commit/58b6535cbf76f2b26fc08b94905a57dcc4d955f6, `pyspark.pandas.tests.test_dataframe` and `pyspark.pandas.tests.test_dataframe_slow` are still too slow, each one normally takes >5 minutes, and sometimes >10 minutes.2, the tests were not well organized, and [tests in Pandas](https://github.com/pandas-dev/pandas/tree/main/pandas/tests) are grouped by categories, we should follow it to make PS tests more maintainable### Does this PR introduce _any_ user-facing change?no test-only### How was this patch tested?updated CI
1	-2	### What changes were proposed in this pull request?Currently, DS V1 uses `_LEGACY_ERROR_TEMP_1269` and DS V2 uses `INVALID_PARTITION_OPERATION.PARTITION_SCHEMA_IS_EMPTY` if the partition operation on non-partition table.This PR want migrate `_LEGACY_ERROR_TEMP_1269` to `PARTITION_SCHEMA_IS_EMPTY`### Why are the changes needed?Migrate `_LEGACY_ERROR_TEMP_1269` to `PARTITION_SCHEMA_IS_EMPTY`.### Does this PR introduce _any_ user-facing change?'Yes'.The error msg has a little change.### How was this patch tested?Test case updated.
3	-2	### What changes were proposed in this pull request?This PR follow-up for SPARK-43671, to refine functions to use `pyspark_column_op` util for clean-up the code.### Why are the changes needed?To avoid `is_remote` in too many places for future maintenance.### Does this PR introduce _any_ user-facing change?No, it's code cleanup### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->The existing CI should pass
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use `pyspark.cloudpickle` instead of `cloudpickle`### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->All pyspark code should use pyspark.cloudpickle instead of `cloudpickle`### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->UT.
2	-2	### What changes were proposed in this pull request?Project connect, from some perspectives, can be thought as replacing the SQL parser to generate a parsed (but the difference that is unresolved) plan, then the plan is passed to the analyzer. This means that connect should also do validation on the proto as there are many in-validate parser cases that analyzer does not expect to see, which potentially could cause problems if connect only pass through the proto (of course have it translated) to analyzer.Meanwhile I think this is a good idea to decouple the validation and transformation so that we have two stages:stage 1: proto validation. For example validate if necessary fields are populated or not.stage 2: transformation, which convert the proto to a plan with assumption that the plan is valid parsed version of the plan.This PR checks before transformation through a simple proxy pattern.Because there are a lot of job to do, this PR only given an example(e.g. `checkReadRel`). So that acilitate communication and discussion.### Why are the changes needed?Decouple plan transformation and validation on server side### Does this PR introduce _any_ user-facing change?'No'.Just update the inner implementation.### How was this patch tested?Exists test cases.
1	-1	### What changes were proposed in this pull request?https://github.com/apache/spark/pull/40615 added the `hll_union_agg` into Spark and Spark connect project. and https://github.com/apache/spark/pull/40120 added the `nth_value` into Spark connect project.But `hll_union_agg` and `nth_value` in connect functions API are defined one by one.In fact, we can simplify the `hll_union_agg` and `nth_value` by reuse functions.Another minor change is we should keep a consistent API order with functions API in SQL.### Why are the changes needed?Simplify the `hll_union_agg` by reuse functions.### Does this PR introduce _any_ user-facing change?'No'.Just update the inner implementation.### How was this patch tested?Exists test cases.
1	-2	### What changes were proposed in this pull request?The pr aims to use `checkError()` to check `Exception` in `SQLViewTestSuite`.### Why are the changes needed?Migration on checkError() will make the tests independent from the text of error messages.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- By running the modified test suites:$ build/sbt \"test:testOnly *.LocalTempViewTestSuite\"$ build/sbt \"test:testOnly *.GlobalTempViewTestSuite\"$ build/sbt \"test:testOnly *.PersistedViewTestSuite\"- Pass GA.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Support unwrap date type to string type in UnwrapCastInBinaryComparison. This is similar idea to other instances in UnwrapCastInBinaryComparison. And there is an implementation already but closed later on https://github.com/apache/spark/pull/40294.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Such that filter such as `dt = date '2023-01-01'` where dt is a string column can be pushed down to file scan operator.This will help fix issue such as https://github.com/apache/iceberg/issues/4997### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Unit test.
2	-3	### What changes were proposed in this pull request?Support `UserDefinedType` in `createDataFrame` from pandas DataFrame and `toPandas`.For the following schema and pandas DataFrame:```pyschema = (    StructType()    .add(\"point\ ExamplePointUDT())    .add(\"struct\ StructType().add(\"point\ ExamplePointUDT()))    .add(\"array\ ArrayType(ExamplePointUDT()))    .add(\"map\ MapType(StringType(), ExamplePointUDT())))data = [    Row(        ExamplePoint(1.0, 2.0),        Row(ExamplePoint(3.0, 4.0)),        [ExamplePoint(5.0, 6.0)],        dict(point=ExamplePoint(7.0, 8.0)),    )]df = spark.createDataFrame(data, schema)pdf = pd.DataFrame.from_records(data, columns=schema.names)```##### `spark.createDataFrame()`For all, return the same results:```py>>> spark.createDataFrame(pdf, schema).show(truncate=False)+----------+------------+------------+---------------------+|point     |struct      |array       |map                  |+----------+------------+------------+---------------------+|(1.0, 2.0)|{(3.0, 4.0)}|[(5.0, 6.0)]|{point -> (7.0, 8.0)}|+----------+------------+------------+---------------------+```##### `df.toPandas()````py>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')>>> df.toPandas()       point        struct        array                   map0  (1.0,2.0)  ((3.0,4.0),)  [(5.0,6.0)]  {'point': (7.0,8.0)}```### Why are the changes needed?Currently `UserDefinedType` in `spark.createDataFrame()` with pandas DataFrame and `df.toPandas()` is not supported with Arrow enabled or in Spark Connect.##### `spark.createDataFrame()`Works without Arrow:```py>>> spark.createDataFrame(pdf, schema).show(truncate=False)+----------+------------+------------+---------------------+|point     |struct      |array       |map                  |+----------+------------+------------+---------------------+|(1.0, 2.0)|{(3.0, 4.0)}|[(5.0, 6.0)]|{point -> (7.0, 8.0)}|+----------+------------+------------+---------------------+```, whereas:- With Arrow:Works with fallback:```py>>> spark.createDataFrame(pdf, schema).show(truncate=False)/.../python/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:  [UNSUPPORTED_DATA_TYPE_FOR_ARROW_CONVERSION] ExamplePointUDT() is not supported in conversion to Arrow.Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.  warn(msg)+----------+------------+------------+---------------------+|point     |struct      |array       |map                  |+----------+------------+------------+---------------------+|(1.0, 2.0)|{(3.0, 4.0)}|[(5.0, 6.0)]|{point -> (7.0, 8.0)}|+----------+------------+------------+---------------------+```- Spark Connect```py>>> spark.createDataFrame(pdf, schema).show(truncate=False)Traceback (most recent call last):...pyspark.errors.exceptions.base.PySparkTypeError: [UNSUPPORTED_DATA_TYPE_FOR_ARROW_CONVERSION] ExamplePointUDT() is not supported in conversion to Arrow.```##### `df.toPandas()`Works without Arrow:```py>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')>>> df.toPandas()       point        struct        array                   map0  (1.0,2.0)  ((3.0,4.0),)  [(5.0,6.0)]  {'point': (7.0,8.0)}```, whereas:- With ArrowWorks with fallback:```py>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')>>> df.toPandas()/.../python/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:  [UNSUPPORTED_DATA_TYPE_FOR_ARROW_CONVERSION] ExamplePointUDT() is not supported in conversion to Arrow.Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.  warn(msg)       point        struct        array                   map0  (1.0,2.0)  ((3.0,4.0),)  [(5.0,6.0)]  {'point': (7.0,8.0)}```- Spark ConnectResults with the internal type:```py>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')>>> df.toPandas()        point         struct         array                    map0  [1.0, 2.0]  ([3.0, 4.0],)  [[5.0, 6.0]]  {'point': [7.0, 8.0]}```### Does this PR introduce _any_ user-facing change?Users will be able to use `UserDefinedType`.### How was this patch tested?Added the related tests.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->This is a backport of https://github.com/apache/spark/pull/41317.Fixes an error with codegen for unhex and unbase64 expression when failOnError is enabled introduced in https://github.com/apache/spark/pull/37483.### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Codegen fails and Spark falls back to interpreted evaluation:```Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 1: Unknown variable or type \"BASE64\"```in the code block:```/* 107 */         if (!org.apache.spark.sql.catalyst.expressions.UnBase64.isValidBase64(project_value_1)) {/* 108 */           throw QueryExecutionErrors.invalidInputInConversionError(/* 109 */             ((org.apache.spark.sql.types.BinaryType$) references[1] /* to */),/* 110 */             project_value_1,/* 111 */             BASE64,/* 112 */             \"try_to_binary\");/* 113 */         }```### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Bug fix.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Added to the existing tests so evaluate an expression with failOnError enabled to test that path of the codegen.
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?This is a follow-up of #41007 .This pull request documents the IDENTIFIER clause### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->If we don't document it, it doesn't exist...### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->Yes### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->Acceptance test.
2	-3	### What changes were proposed in this pull request?The pr aims to make the prompt for `findJar` method in IntegrationTestUtils clearer.### Why are the changes needed?Friendly prompts can help locate problems.When I am running tests in ClientE2ETestSuite, I often cannot locate them through error prompts when they fail, and I can only search for specific reasons through code- Before applying this patche, the error prompt is as follows:`Exception encountered when invoking run on a nested suite - Failed to find the jar inside folder: .../spark-community/connector/connect/server/target`- After applying this patche, The error prompt is as follows:`Exception encountered when invoking run on a nested suite - Failed to find the jar: spark-connect-assembly(.).jar or spark-connect(.)3.5.0-SNAPSHOT.jar inside folder: .../spark-community/connector/connect/server/target. This file can be generated by similar to the following command: build/sbt package|assembly`Improvement in two aspects- Prompt us what files are missing- How to generate the above file### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Manual check
3	-2	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->Use pyspark.cloudpickle instead of `cloudpickle` in torch distributor### Why are the changes needed?<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->Make ser and deser code consistent### Does this PR introduce _any_ user-facing change?<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->No.### How was this patch tested?<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_128[1-2].### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Update existed UT.Pass GA.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_241[1-7].### Why are the changes needed?Improve the error framework.### Does this PR introduce _any_ user-facing change?'No'.### How was this patch tested?Exists test cases.
1	-1	### What changes were proposed in this pull request?### Why are the changes needed?### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
1	-2	### What changes were proposed in this pull request?The pr aims to update scalatest and scalatestplus related dependencies to newest version, include:This pr aims upgrade `scalatest` related test dependencies to 3.2.16: - scalatest: upgrade scalatest from 3.2.15 to 3.2.16     - mockito   - mockito-core: upgrade from 4.6.1 to 4.11.0   - mockito-inline: upgrade from 4.6.1 to 4.11.0    - selenium-java: upgrade from 4.7.2 to 4.9.1  - htmlunit-driver: upgrade from 4.7.2 to 4.9.1  - htmlunit: upgrade from 2.67.0 to 2.70.0   - scalatestplus\t - scalacheck-1-17: upgrade from 3.2.15.0 to 3.2.16.0   - mockito: upgrade from `mockito-4-6` 3.2.15.0 to `mockito-4-11` 3.2.16.0   - selenium: upgrade from `selenium-4-7` 3.2.15.0 to `selenium-4-9` 3.2.16.0     ### Why are the changes needed?The relevant release notes as follows: - scalatest:     \t - https://github.com/scalatest/scalatest/releases/tag/release-3.2.16 \t  - [mockito](https://github.com/mockito/mockito)   - https://github.com/mockito/mockito/releases/tag/v4.11.0   - https://github.com/mockito/mockito/releases/tag/v4.10.0   - https://github.com/mockito/mockito/releases/tag/v4.9.0   - https://github.com/mockito/mockito/releases/tag/v4.8.1   - https://github.com/mockito/mockito/releases/tag/v4.8.0   - https://github.com/mockito/mockito/releases/tag/v4.7.0 \t  - [selenium-java](https://github.com/SeleniumHQ/selenium) \t    - https://github.com/SeleniumHQ/selenium/releases/tag/selenium-4.9.1   - https://github.com/SeleniumHQ/selenium/releases/tag/selenium-4.9.0   - https://github.com/SeleniumHQ/selenium/releases/tag/selenium-4.8.3-java   - https://github.com/SeleniumHQ/selenium/releases/tag/selenium-4.8.2-java   - https://github.com/SeleniumHQ/selenium/releases/tag/selenium-4.8.1   - https://github.com/SeleniumHQ/selenium/releases/tag/selenium-4.8.0 \t  - [htmlunit-driver](https://github.com/SeleniumHQ/htmlunit-driver)   - https://github.com/SeleniumHQ/htmlunit-driver/releases/tag/htmlunit-driver-4.9.1   - https://github.com/SeleniumHQ/htmlunit-driver/releases/tag/htmlunit-driver-4.9.0   - https://github.com/SeleniumHQ/htmlunit-driver/releases/tag/htmlunit-driver-4.8.3   - https://github.com/SeleniumHQ/htmlunit-driver/releases/tag/htmlunit-driver-4.8.1.1   - https://github.com/SeleniumHQ/htmlunit-driver/releases/tag/4.8.1   - https://github.com/SeleniumHQ/htmlunit-driver/releases/tag/4.8.0    - [htmlunit](https://github.com/HtmlUnit/htmlunit)   - https://github.com/HtmlUnit/htmlunit/releases/tag/2.70.0    - Why this version: because the 4.9.1 version of Selenium relies on it. https://github.com/SeleniumHQ/selenium/blob/selenium-4.9.1/java/maven_deps.bzl#L83 \t  - [org.scalatestplus:scalacheck-1-17](https://github.com/scalatest/scalatestplus-scalacheck)   - https://github.com/scalatest/scalatestplus-scalacheck/releases/tag/release-3.2.16.0-for-scalacheck-1.17  - [org.scalatestplus:mockito-4-11](https://github.com/scalatest/scalatestplus-mockito)   - https://github.com/scalatest/scalatestplus-mockito/releases/tag/release-3.2.16.0-for-mockito-4.11    - [org.scalatestplus:selenium-4-9](https://github.com/scalatest/scalatestplus-selenium)   - https://github.com/scalatest/scalatestplus-selenium/releases/tag/release-3.2.16.0-for-selenium-4.9### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Pass GitHub Actions- Manual test:   - ChromeUISeleniumSuite   - RocksDBBackendChromeUIHistoryServerSuite```    build/sbt -Dguava.version=31.1-jre -Dspark.test.webdriver.chrome.driver=/path/to/chromedriver -Dtest.default.exclude.tags=\"\" -Phive -Phive-thriftserver \"core/testOnly org.apache.spark.ui.ChromeUISeleniumSuite\"    build/sbt -Dguava.version=31.1-jre -Dspark.test.webdriver.chrome.driver=/path/to/chromedriver -Dtest.default.exclude.tags=\"\" -Phive -Phive-thriftserver \"core/testOnly org.apache.spark.deploy.history.RocksDBBackendChromeUIHistoryServerSuite\"```<img width=\"856\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/73349ffb-4198-4371-a741-411712d14712\">
2	-2	### What changes were proposed in this pull request?Currently, `SparkConnectPlanner.transformRelation` always return the `LogicalPlan` of `Dataset`.The `SparkConnectStreamHandler.handlePlan` constructs a new `Dataset` for it.Sometimes, `SparkConnectStreamHandler.handlePlan` could reuse the `Dataset` created by `SparkConnectPlanner.transformRelation`.This PR creates a common method `transformRelationAsDataset` could be reused in many places.### Why are the changes needed?Improve `SparkConnectPlanner` by reuse `Dataset` and avoid construct new `Dataset`.### Does this PR introduce _any_ user-facing change?'No'.Just update inner implementation.### How was this patch tested?Exists test cases.
1	-2	### What changes were proposed in this pull request?This updated Protobuf Pyspark API to allow passing binary FileDescriptorSet rather than a file name. This is a Python follow up to feature implemented in Scala in #41192. ### Why are the changes needed?  - This allows flexibility for Pyspark users to provide binary descriptor set directly.  - Even if users are using file path, Pyspark avoids passing file name to Scala and reads the descriptor file in Python. This avoids having to read the file in Scala. ### Does this PR introduce _any_ user-facing change?  - This adds extra arg to `from_protobuf()` and `to_protobuf()` API. ### How was this patch tested?  - Doc tests  - Manual tests
2	-2	### What changes were proposed in this pull request?This PR aims to make Scala 2.13 the default Scala version in Apache Spark 3.5.### Why are the changes needed?The current releases of Scala community are `Scala 3.2.2` and `Scala 2.13.10`.- https://scala-lang.org/download/all.html Although the Apache Spark community has been using Scala 2.12 by default since Apache Spark 3.0 and Scala community will release Scala 2.12.18 for Java 21 support, we had better focus on `Scala 2.13+` more from Apache Spark 3.5 timeline to adopt Scala community's activity.Since SPARK-25075 added Scala 2.13 at Apache Spark 3.2.0, the Apache Spark community has been using it as a second Scala version. This PR aims to switch only the default Scala version from 2.12 to 2.13. Apache Spark will support both Scala 2.12 and 2.13 still.### Does this PR introduce _any_ user-facing change?Yes, but we still have Scala 2.12.### How was this patch tested?Pass the CIs.
2	-2	### What changes were proposed in this pull request?The pr aims to use error classes in the compilation errors of `ResolveDefaultColumns`.### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Update UT.- Pass GA.
2	-2	### What changes were proposed in this pull request?The pr aims to assign a name to the error class _LEGACY_ERROR_TEMP_103[1-2].### Why are the changes needed?The changes improve the error framework.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Update existed UT.- Pass GA.
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?Eg:```scalasql(\"create view t(c1, c2) as values (0, 1), (0, 2), (1, 2)\")sql(\"select c1, c2, (select count(*) cnt from t t2 where t1.c1 = t2.c1 \" +\"having cnt = 0) from t t1\").show() ```The error will throw:``` [PLAN_VALIDATION_FAILED_RULE_IN_BATCH] Rule org.apache.spark.sql.catalyst.optimizer.RewriteCorrelatedScalarSubquery in batch Operator Optimization before Inferring Filters generated an invalid plan: The plan becomes unresolved: 'Project [toprettystring(c1#224, Some(America/Los_Angeles)) AS toprettystring(c1)#238, toprettystring(c2#225, Some(America/Los_Angeles)) AS toprettystring(c2)#239, toprettystring(cnt#246L, Some(America/Los_Angeles)) AS toprettystring(scalarsubquery(c1))#240]+- 'Project [c1#224, c2#225, CASE WHEN isnull(alwaysTrue#245) THEN 0 WHEN NOT (cnt#222L = 0) THEN null ELSE cnt#222L END AS cnt#246L]   +- 'Join LeftOuter, (c1#224 = c1#224#244)      :- Project [col1#226 AS c1#224, col2#227 AS c2#225]      :  +- LocalRelation [col1#226, col2#227]      +- Project [cnt#222L, c1#224#244, cnt#222L, c1#224, true AS alwaysTrue#245]         +- Project [cnt#222L, c1#224 AS c1#224#244, cnt#222L, c1#224]            +- Aggregate [c1#224], [count(1) AS cnt#222L, c1#224]               +- Project [col1#228 AS c1#224]                  +- LocalRelation [col1#228, col2#229]The previous plan: Project [toprettystring(c1#224, Some(America/Los_Angeles)) AS toprettystring(c1)#238, toprettystring(c2#225, Some(America/Los_Angeles)) AS toprettystring(c2)#239, toprettystring(scalar-subquery#223 [c1#224 && (c1#224 = c1#224#244)], Some(America/Los_Angeles)) AS toprettystring(scalarsubquery(c1))#240]:  +- Project [cnt#222L, c1#224 AS c1#224#244]:     +- Filter (cnt#222L = 0):        +- Aggregate [c1#224], [count(1) AS cnt#222L, c1#224]:           +- Project [col1#228 AS c1#224]:              +- LocalRelation [col1#228, col2#229]+- Project [col1#226 AS c1#224, col2#227 AS c2#225]   +- LocalRelation [col1#226, col2#227] ```The reason of error is the unresolved expression in `Join` node which generate by subquery decorrelation. The `duplicateResolved` in `Join` node are false. That's meaning the `Join` left and right have same `Attribute`, in this eg is `c1#224`. The right `c1#224` `Attribute` generated by having Inputs, because there are wrong having Inputs. This problem only occurs when there contain having clause.also do some code format fix.<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  2. If you fix some SQL features, you can provide some references of other DBMSes.  3. If there is design documentation, please add the link.  4. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Fix subquery bug on single table when use having clause<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Add new test<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
3	-3	<!--Thanks for sending a pull request!  Here are some tips for you:  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.  4. Be sure to keep the PR description updated to reflect all changes.  5. Please write your PR title to summarize what this PR proposes.  6. If possible, provide a concise example to reproduce the issue for a faster review.  7. If you want to add a new configuration, please read the guideline first for naming configurations in     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.  8. If you want to add or modify an error type or message, please read the guideline first in     'core/src/main/resources/error/README.md'.-->### What changes were proposed in this pull request?In order to fix DROP table behavior in session catalog cause by #37879. Because we always invoke V1 drop logic if the identifier looks like a V1 identifier. This is a big blocker for external data sources that provide custom session catalogs.So this PR move all Drop Table case to DataSource V2 (use drop table to drop view not include). More information please check https://github.com/apache/spark/pull/37879/files#r1170501180<!--Please clarify what changes you are proposing. The purpose of this section is to outline the changes and how this PR fixes the issue. If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.  1. If you refactor some codes with changing classes, showing the class hierarchy will help reviewers.  3. If you fix some SQL features, you can provide some references of other DBMSes.  4. If there is design documentation, please add the link.  5. If there is a discussion in the mailing list, please add the link.-->### Why are the changes needed?Move Drop Table case to DataSource V2 to fix bug and prepare for remove drop table v1.<!--Please clarify why the changes are needed. For instance,  1. If you propose a new API, clarify the use case for a new API.  2. If you fix a bug, you can clarify why it is a bug.-->### Does this PR introduce _any_ user-facing change?No<!--Note that it means *any* user-facing change including all aspects such as the documentation fix.If yes, please clarify the previous behavior and the change this PR proposes - provide the console output, description and/or an example to show the behavior difference if possible.If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.If no, write 'No'.-->### How was this patch tested?Tested by:- V2 table catalog tests: `org.apache.spark.sql.execution.command.v2.DropTableSuite`- V1 table catalog tests: `org.apache.spark.sql.execution.command.v1.DropTableSuiteBase`<!--If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.If tests were not added, please describe why they were not added and/or why it was difficult to add.If benchmark tests were added, please run the benchmarks in GitHub Actions for the consistent environment, and the instructions could accord to: https://spark.apache.org/developer-tools.html#github-workflow-benchmarks.-->
2	-2	### What changes were proposed in this pull request?The pr aims to convert `_LEGACY_ERROR_TEMP_1337` to `UNSUPPORTED_FEATURE.TIME_TRAVEL` and remove `_LEGACY_ERROR_TEMP_1335`### Why are the changes needed?- The changes improve the error framework.- In the spark base code `_ LEGACY_ ERROR_ TEMP_ 1335` is no longer used anywhere.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?- Add new UT- Pass GA
2	-3	Bumps [scala-library](https://github.com/scala/scala) from 2.13.8 to 2.13.9.<details><summary>Release notes</summary><p><em>Sourced from <a href=\"https://github.com/scala/scala/releases\">scala-library's releases</a>.</em></p><blockquote><h2>Scala 2.13.9</h2><p>The following changes are highlights of this release:</p><h3>Regression</h3><p>Library maintainers should avoid publishing libraries using Scala 2.13.9. Please use <a href=\"https://github.com/scala/scala/releases/v2.13.10\">2.13.10</a> instead. 2.13.9 has a <a href=\"https://redirect.github.com/scala/bug/issues/12650\">regression</a> where binary-incompatible bytecode is emitted for case classes which are also value classes (<code>case class ... extends AnyVal</code>).</p><h3>Compatibility with Scala 3</h3><ul><li>Tasty Reader: Add support for Scala 3.2 (<a href=\"https://redirect.github.com/scala/scala/pull/10068\">#10068</a>)</li><li>Tasty Reader: Restrict access to experimental definitions (<a href=\"https://redirect.github.com/scala/scala/pull/10020\">#10020</a>)</li><li>To aid cross-building, accept and ignore <code>using</code> in method calls (<a href=\"https://redirect.github.com/scala/scala/pull/10064\">#10064</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li><li>To aid cross-building, allow <code>?</code> as a wildcard even without <code>-Xsource:3</code> (<a href=\"https://redirect.github.com/scala/scala/pull/9990\">#9990</a>)</li><li>Make Scala-3-style implicit resolution explicitly opt-in rather than bundled in <code>-Xsource:3</code> (<a href=\"https://redirect.github.com/scala/scala/pull/10012\">#10012</a> by <a href=\"https://github.com/povder\"><code>@​povder</code></a>)</li><li>Prefer type of overridden member when inferring (under <code>-Xsource:3</code>) (<a href=\"https://redirect.github.com/scala/scala/pull/9891\">#9891</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li></ul><h3>JDK version support</h3><ul><li>Make <code>-release</code> more useful, deprecate <code>-target</code>, align with Scala 3 (<a href=\"https://redirect.github.com/scala/scala/pull/9982\">#9982</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li><li>Support JDK 19 (<a href=\"https://redirect.github.com/scala/scala/pull/10001\">#10001</a> by <a href=\"https://github.com/Philippus\"><code>@​Philippus</code></a>)</li></ul><h3>Warnings and lints</h3><ul><li>Add <code>-Wnonunit-statement</code> to warn about discarded values in statement position (<a href=\"https://redirect.github.com/scala/scala/pull/9893\">#9893</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li><li>Make unused-import warnings easier to silence (support filtering by <code>origin=</code>) (<a href=\"https://redirect.github.com/scala/scala/pull/9939\">#9939</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li><li>Add <code>-Wperformance</code> lints for <code>*Ref</code> boxing and nonlocal <code>return</code> (<a href=\"https://redirect.github.com/scala/scala/pull/9889\">#9889</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li></ul><h3>Language improvements</h3><ul><li>Improve support for Unicode supplementary characters in identifiers and string interpolation (<a href=\"https://redirect.github.com/scala/scala/pull/9805\">#9805</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li></ul><h3>Compiler options</h3><ul><li>Use subcolon args to simplify optimizer options (<a href=\"https://redirect.github.com/scala/scala/pull/9810\">#9810</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li><li>For troubleshooting compiler, add <code>-Vdebug-type-error</code> (and remove <code>-Yissue-debug</code>) (<a href=\"https://redirect.github.com/scala/scala/pull/9824\">#9824</a> by <a href=\"https://github.com/tribbloid\"><code>@​tribbloid</code></a>)</li></ul><h3>Security</h3><ul><li>Error on source files with Unicode directional formatting characters (<a href=\"https://redirect.github.com/scala/scala/pull/10017\">#10017</a>)</li><li>Prevent <code>Function0</code> execution during <code>LazyList</code> deserialization (<a href=\"https://redirect.github.com/scala/scala/pull/10118\">#10118</a>)</li></ul><h3>Bugfixes</h3><ul><li>Emit all bridge methods non-final (perhaps affecting serialization compat) (<a href=\"https://redirect.github.com/scala/scala/pull/9976\">#9976</a>)</li><li>Fix null-pointer regression in <code>Vector#prependedAll</code> and <code>Vector#appendedAll</code> (<a href=\"https://redirect.github.com/scala/scala/pull/9983\">#9983</a>)</li><li>Improve concurrent behavior of Java <code>ConcurrentMap</code> wrapper(<a href=\"https://redirect.github.com/scala/scala/pull/10027\">#10027</a> by <a href=\"https://github.com/igabaydulin\"><code>@​igabaydulin</code></a>)</li><li>Preserve null policy in wrapped Java <code>Map</code>s (<a href=\"https://redirect.github.com/scala/scala/pull/10129\">#10129</a> by <a href=\"https://github.com/som-snytt\"><code>@​som-snytt</code></a>)</li></ul><p>Changes that shipped in Scala 2.12.16 and 2.12.17 are also included in this release.</p><!-- raw HTML omitted --></blockquote><p>... (truncated)</p></details><details><summary>Commits</summary><ul><li><a href=\"https://github.com/scala/scala/commit/986dcc160aab85298f6cab0bf8dd0345497cdc01\"><code>986dcc1</code></a> Merge pull request <a href=\"https://redirect.github.com/scala/scala/issues/10129\">#10129</a> from som-snytt/followup/12586-preserve-NPE</li><li><a href=\"https://github.com/scala/scala/commit/b824b84c4ea93cc1490b3172b0684539fcd99a25\"><code>b824b84</code></a> Preserve null policy in wrapped Java Map</li><li><a href=\"https://github.com/scala/scala/commit/d578a02ea6b41b662072759c82c19f9309a15176\"><code>d578a02</code></a> Merge pull request <a href=\"https://redirect.github.com/scala/scala/issues/10128\">#10128</a> from SethTisue/revert-10114-10123</li><li><a href=\"https://github.com/scala/scala/commit/e5fe9193d520f8e611244101e95a85a51067f79d\"><code>e5fe919</code></a> Revert &quot;Args files are 1 arg per line, fix -Vprint-args -&quot;</li><li><a href=\"https://github.com/scala/scala/commit/362c5d1a52060374450274ff6bcd8773af8ddd07\"><code>362c5d1</code></a> Revert &quot;Trim and filter empties in arg files&quot;</li><li><a href=\"https://github.com/scala/scala/commit/864148d9f0ffa01e835e866a9e0803bb9ea7d037\"><code>864148d</code></a> Revert &quot;process.Parser strips escaping backslash&quot;</li><li><a href=\"https://github.com/scala/scala/commit/f69fe8bc88a2cb032162be6de9a3242eed909fb2\"><code>f69fe8b</code></a> Merge pull request <a href=\"https://redirect.github.com/scala/scala/issues/10127\">#10127</a> from scalacenter/tasty/support-3.2.0-final</li><li><a href=\"https://github.com/scala/scala/commit/0aa6bd497891e1c556046ba1715de6b2a3793878\"><code>0aa6bd4</code></a> remove tasty escape hatch for 3.2.0-RC4</li><li><a href=\"https://github.com/scala/scala/commit/af56abcbe03571c92897ebfaebdfe52cf8275304\"><code>af56abc</code></a> Merge pull request <a href=\"https://redirect.github.com/scala/scala/issues/10123\">#10123</a> from som-snytt/dev/814-window-cmd-escapes</li><li><a href=\"https://github.com/scala/scala/commit/7e844a5c9ec821de378808cd3806967ec7026cc9\"><code>7e844a5</code></a> Merge pull request <a href=\"https://redirect.github.com/scala/scala/issues/10121\">#10121</a> from scala-steward/update/slf4j-nop-2.0.0</li><li>Additional commits viewable in <a href=\"https://github.com/scala/scala/compare/v2.13.8...v2.13.9\">compare view</a></li></ul></details><br />[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.scala-lang:scala-library&package-manager=maven&previous-version=2.13.8&new-version=2.13.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.[//]: # (dependabot-automerge-start)[//]: # (dependabot-automerge-end)---<details><summary>Dependabot commands and options</summary><br />You can trigger Dependabot actions by commenting on this PR:- `@dependabot rebase` will rebase this PR- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it- `@dependabot merge` will merge this PR after your CI passes on it- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it- `@dependabot cancel merge` will cancel a previously requested merge and block automerging- `@dependabot reopen` will reopen this PR if it is closed- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/spark/network/alerts).</details>
1	-2	### What changes were proposed in this pull request?This PR aims to switch `scala-213` GitHub Action Job to `scala-212`.### Why are the changes needed?Since SPARK-43836, Scala 2.13 is used by default. So, we need to test `Scala 2.12` instead of `Scala 2.13` in this additional test pipeline.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass the CIs and check the `scala-212` pipeline instead of `scala-213` pipeline.https://github.com/dongjoon-hyun/spark/actions/runs/5105828839/jobs/9177657650
2	-2	### What changes were proposed in this pull request?This PR aims to upgrade `gcs-connector` to 2.2.14.### Why are the changes needed?- https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.14- https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.13- https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.12### Does this PR introduce _any_ user-facing change?Although this is a dependency change, there is no user-facing change.### How was this patch tested?Pass the CIs and manual verification.**BUILD**```$ dev/make-distribution.sh -Phadoop-cloud```**RUN**```$ export KEYFILE=YOUR-credentials.json$ export EMAIL=$(jq -r '.client_email' < $KEYFILE)$ export PRIVATE_KEY_ID=$(jq -r '.private_key_id' < $KEYFILE)$ export PRIVATE_KEY=\"$(jq -r '.private_key' < $KEYFILE)\"$ bin/spark-shell \\    -c spark.hadoop.fs.gs.auth.service.account.email=$EMAIL \\    -c spark.hadoop.fs.gs.auth.service.account.private.key.id=$PRIVATE_KEY_ID \\    -c spark.hadoop.fs.gs.auth.service.account.private.key=\"$PRIVATE_KEY\"Setting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Welcome to      ____              __     / __/__  ___ _____/ /__    _\\ \\/ _ \\/ _ `/ __/  '_/   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.0-SNAPSHOT      /_/Using Scala version 2.13.8 (OpenJDK 64-Bit Server VM, Java 17.0.7)Type in expressions to have them evaluated.Type :help for more information.23/05/28 14:43:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSpark context Web UI available at http://localhost:4040Spark context available as 'sc' (master = local[*], app id = local-1685310212206).Spark session available as 'spark'.scala> spark.read.text(\"gs://apache-spark-bucket/README.md\").count()val res0: Long = 124scala> spark.read.orc(\"examples/src/main/resources/users.orc\").write.mode(\"overwrite\").orc(\"gs://apache-spark-bucket/users.orc\")scala> spark.read.orc(\"gs://apache-spark-bucket/users.orc\").show()+------+--------------+----------------+|  name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa|          NULL|  [3, 9, 15, 20]||   Ben|           red|              []|+------+--------------+----------------+```
2	-4	### What changes were proposed in this pull request?In `StringUtils#orderSuggestedIdentifiersBySimilarity`, handle the case where the candidate attributes have a mix of empty and non-empty prefixes.### Why are the changes needed?The following query throws a `StringIndexOutOfBoundsException`:```with v1 as ( select * from values (1, 2) as (c1, c2)),v2 as (  select * from values (2, 3) as (c1, c2))select v1.c1, v1.c2, v2.c1, v2.c2, bfrom v1full outer join v2using (c1);```The query should fail anyway, since `b` refers to a non-existent column. But it should fail with a helpful error message, not with a `StringIndexOutOfBoundsException`.`StringUtils#orderSuggestedIdentifiersBySimilarity` assumes that a list of suggested attributes with a mix of prefixes will never have an attribute name with an empty prefix. But in this case it does (`c1` from the `coalesce` has no prefix, since it is not associated with any relation or subquery):```+- 'Project [c1#5, c2#6, c1#7, c2#8, 'b]   +- Project [coalesce(c1#5, c1#7) AS c1#9, c2#6, c2#8] <== c1#9 has no prefix, unlike c2#6 (v1.c2) or c2#8 (v2.c2)      +- Join FullOuter, (c1#5 = c1#7)         :- SubqueryAlias v1         :  +- CTERelationRef 0, true, [c1#5, c2#6]         +- SubqueryAlias v2            +- CTERelationRef 1, true, [c1#7, c2#8]```Because of this, `orderSuggestedIdentifiersBySimilarity` returns a sorted list of suggestions like this:```ArrayBuffer(.c1, v1.c2, v2.c2)````UnresolvedAttribute.parseAttributeName` chokes on an attribute name that starts with a '.'.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?New unit tests.
1	-1	### What changes were proposed in this pull request?This PR aims to setup Scala 2.12 Daily GitHub Action Job by converting Scala 2.13 Daily Job.- Rename `build_scala213.yml` file to `.github/workflows/build_scala212.yml`.- Rename job name```-name: \"Build (master, Scala 2.13, Hadoop 3, JDK 8)\"+name: \"Build (master, Scala 2.12, Hadoop 3, JDK 8)\"```- Switch SCALA_PROFILE from `scala2.13` to `scala2.12`.### Why are the changes needed?To keep the Scala 2.12 test coverage### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?This is a daily job. We need to check after merging.
1	-2	### What changes were proposed in this pull request?The pr aims to use `checkError()` to check Exception in `SessionCatalogSuite`.### Why are the changes needed?Migration on `checkError()` will make the tests independent from the text of error messages.### Does this PR introduce _any_ user-facing change?No.### How was this patch tested?Pass GA.
