# -*- coding: utf-8 -*-
id,user,created_at,version,body,score,positive,negative,0
1428890417,6477701,2023/2/14,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1430794230,1097932,2023/2/14,v3.3.2,Merging to master/3.4,"1,-1    ",1,-1,0
1429074678,6477701,2023/2/14,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1429368248,7322292,2023/2/14,v3.3.2,merged into master/3.4,"1,-1    ",1,-1,0
1429283022,6477701,2023/2/14,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1430262898,1097932,2023/2/14,v3.3.2,Merging to master/3.4,"1,-1    ",1,-1,0
1429264303,7322292,2023/2/14,v3.3.2,merged into master/3.4.  Thank you @ulysses-you ,"1,-1    ",1,-1,0
1429234279,7322292,2023/2/14,v3.3.2,cc @HyukjinKwon @ueshin ,"2,-1    ",2,-1,1
1429261134,7322292,2023/2/14,v3.3.2,close this one in favor of https://github.com/apache/spark/pull/40006,"1,-1    ",1,-1,0
1429707666,6477701,2023/2/14,v3.3.2,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1430025642,1938382,2023/2/14,v3.3.2,late LGTM,"1,-1    ",1,-1,0
1429346503,16032294,2023/2/14,v3.3.2,cc @JoshRosen  @liuzqt  ,"1,-1    ",1,-1,0
1563656660,41898282,2023/2/14,v3.3.2,"We're closing this PR because it hasn't been updated in a while. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.
If you'd like to revive this PR, please reopen it and ask a committer to remove the Stale tag!","1,-1    ",1,-1,0
1429709353,6477701,2023/2/14,v3.3.2,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1429328683,502522,2023/2/14,v3.3.2,"@HeartSaVioR , @HyukjinKwon , @SandishKumarHN PTAL. I would like to get this fix asap to make it into 3.4.x in OSS and 12.2 RC cut in DBR. ","3,-1    ",3,-1,2
1429341925,502522,2023/2/14,v3.3.2,cc: @gengliangwang ,"1,-1    ",1,-1,0
1430514890,1097932,2023/2/14,v3.3.2,"Thanks, merging to master/3.4","2,-1    ",2,-1,1
1429707086,6477701,2023/2/14,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1429607135,7322292,2023/2/14,v3.3.2,"`DataFrame.drop` in PySpark seems problematic when there are duplicated column names, let me address it first.","1,-1    ",1,-1,0
1436042026,9616802,2023/2/14,v3.3.2,@zhengruifeng what is the current problem with drop?,"1,-1    ",1,-1,0
1436177852,7322292,2023/2/14,v3.3.2,"> @zhengruifeng what is the current problem with drop?

existing implement in Python Client follows the behavior in PySpark that always convert column name to column, then it has this problem https://issues.apache.org/jira/browse/SPARK-42444
 ","1,-2    ",1,-2,-1
1442776708,7322292,2023/2/14,v3.3.2,cc @hvanhovell @HyukjinKwon @xinrong-meng @amaliujia ,"1,-2    ",1,-2,-1
1445910983,7322292,2023/2/15,v3.3.2,cc @HyukjinKwon @xinrong-meng would you mind taking a look at this one?,"1,-2    ",1,-2,-1
1447516956,7322292,2023/2/15,v3.3.2,merged into master/3.4,"1,-1    ",1,-1,0
1429908839,7253827,2023/2/15,v3.3.2,"cc @srowen, @dongjoon-hyun, @gengliangwang, @wangyum ","1,-2    ",1,-2,-1
1430664412,6477701,2023/2/15,v3.3.2,cc @gengliangwang FYI,"1,-1    ",1,-1,0
1430968381,1097932,2023/2/15,v3.3.2,"I did a quick test and the UI looks ok. Also I tried git grep and all the old version `1.10.25` is gone.
@peter-toth Thanks for the work. Merging to master. ","1,-1    ",1,-1,0
1430969480,7253827,2023/2/15,v3.3.2,"> LGTM if the UI works

I played a bit with the UI while making this PR and it looked ok to me.

Actually, I'm not sure that anything from our customizations in `core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css` is still needed after this version update, but I removed only the up/dpwn arrows related part that caused issues...
E.g. the executor ""show ... entries"" box (https://github.com/apache/spark/pull/36226#issuecomment-1100810575) looked ok without this last fix: https://github.com/apache/spark/pull/36226/files#diff-e96c8dc10974b7da0c5dd4f675702462124b66c4bf2f7a0682f2ec1482a2eee3","1,-1    ",1,-1,0
1430973181,7253827,2023/2/15,v3.3.2,"> I did a quick test and the UI looks ok. Also I tried git grep and all the old version `1.10.25` is gone. @peter-toth Thanks for the work. Merging to master.

Thanks everyone for the review!","1,-1    ",1,-1,0
1431992781,1938382,2023/2/15,v3.3.2,cc @zhengruifeng IIRC there were some open questions to support storage level in Connect?,"1,-1    ",1,-1,0
1435639551,11574708,2023/2/15,v3.3.2,@HyukjinKwon can you please check if it looks Ok?,"1,-1    ",1,-1,0
1438674170,11574708,2023/2/15,v3.3.2,"@HyukjinKwon @amaliujia can you please review? 
If you think implementation of StorageLevel is not right I will close and can open a new one for the param only (when there's alternative implementation is available). The problem is you cant just add something to PySpark without touching Connect. I guess it's intentional.","1,-1    ",1,-1,0
1441644663,11574708,2023/2/15,v3.3.2,Closing as no traction ,"1,-1    ",1,-1,0
1442772377,6477701,2023/2/15,v3.3.2,Sorry for late responses. We should better have this feature parity. But I need to check w/ the concern about storage level (raised internally). I will ping the guy to comment here.,"1,-1    ",1,-1,0
1444167507,11574708,2023/2/15,v3.3.2,Thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1494921787,506656,2023/2/15,v3.3.2,"Hi @khalidmammadov, now that `Catalog` in Scala client including protobuf definition has been implemented at #40438, do you want to continue working on this?
Otherwise, I can take this over.
Thanks.","1,-1    ",1,-1,0
1495403951,11574708,2023/2/15,v3.3.2,"Hi, thanks for letting me know. I will look at it

On Mon, 3 Apr 2023, 21:15 Takuya UESHIN, ***@***.***> wrote:

> Hi @khalidmammadov <https://github.com/khalidmammadov>, now that Catalog
> in Scala client including protobuf definition has been implemented, do you
> want to continue working on this?
> Otherwise, I can take this over.
> Thanks.
>
> ï¿½?> Reply to this email directly, view it on GitHub
> <https://github.com/apache/spark/pull/40015#issuecomment-1494921787>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACYJ3NHUQOOWBY4SFT7D3XDW7MVVDANCNFSM6AAAAAAU3TDHCU>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
","1,-1    ",1,-1,0
1501103732,11574708,2023/2/15,v3.3.2,"The build was failing due to https://github.com/apache/spark/pull/40674 and now fixed by https://github.com/apache/spark/pull/40681

Rebased.","1,-2    ",1,-2,-1
1505864295,506656,2023/2/15,v3.3.2,Thanks! merging to master.,"1,-1    ",1,-1,0
1505902017,11574708,2023/2/15,v3.3.2,Thanks @ueshin!,"1,-1    ",1,-1,0
1429912568,7253827,2023/2/15,v3.3.2,cc @cloud-fan ,"1,-1    ",1,-1,0
1431326036,3182036,2023/2/15,v3.3.2,"thanks, merging to master/3.4! (`multiTransform` is a to-be-released developer API)","3,-1    ",3,-1,2
1431362488,7253827,2023/2/15,v3.3.2,Thanks @cloud-fan for the review!,"2,-1    ",2,-1,1
1432886862,18014953,2023/2/15,v3.3.2,Maybe @gengliangwang ?,"2,-1    ",2,-1,1
1430123971,9616802,2023/2/15,v3.3.2,cc @zhenlineo ,"2,-1    ",2,-1,1
1430127246,9616802,2023/2/15,v3.3.2,For the reviewers. This is a mostly mechanical PR; the size is large but the complexity is low. All implemented documentation and function signatures were copies from Dataset. ,"1,-2    ",1,-2,-1
1431494777,9616802,2023/2/15,v3.3.2,merging this!,"1,-1    ",1,-1,0
1430570256,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1430570875,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1430529818,1097932,2023/2/15,v3.3.2,Sorry about getting back and forth about the schema inference. I am trying to tell a good story about the timestamp without time zone. This should be the final version before 3.4.0 release. ,"1,-1    ",1,-1,0
1431233094,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1430567928,1938382,2023/2/15,v3.3.2,@cloud-fan ,"2,-1    ",2,-1,1
1430571854,6477701,2023/2/15,v3.3.2,Let's do this kind of cleaning when we touch the codes around here next time.,"2,-1    ",2,-1,1
1430572295,6477701,2023/2/15,v3.3.2,Merged to master.,"1,-1    ",1,-1,0
1432285477,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1430678447,9616802,2023/2/15,v3.3.2,@zhenlineo can you make the PR description a bit more descriptive?,"1,-1    ",1,-1,0
1430678848,9616802,2023/2/15,v3.3.2,Oh and can you add the ticket to the title?,"1,-1    ",1,-1,0
1432056014,1938382,2023/2/15,v3.3.2,So it is ok to not have e2e tests for read API (similarly for the write side)?,"1,-1    ",1,-1,0
1432517546,1938382,2023/2/15,v3.3.2,"LGTM

Looks like you only need `./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm` to fix the style.","1,-1    ",1,-1,0
1431231926,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1431838287,9616802,2023/2/15,v3.3.2,merging.,"1,-1    ",1,-1,0
1432288033,6477701,2023/2/15,v3.3.2,"FYI, bunch of Scala 2.13 tests fail (https://github.com/apache/spark/actions/runs/4187249065/jobs/7256849374#step:9:19944) should be fixed before cutting RC cc @xinrong-meng ","1,-1    ",1,-1,0
1432574039,9700541,2023/2/15,v3.3.2,"Hi, @hvanhovell .
Since we are close to RC1 cut, could you test Scala 2.13 before merging your PR, please?
If there is the best person to do that, it's you, @hvanhovell . :) ","1,-1    ",1,-1,0
1431223398,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1431286013,7322292,2023/2/15,v3.3.2,Thank you @HyukjinKwon @dongjoon-hyun ,"1,-1    ",1,-1,0
1431230658,6477701,2023/2/15,v3.3.2,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1430853190,9700541,2023/2/15,v3.3.2,"No, it was broken since Spark 3.3.0.","1,-1    ",1,-1,0
1430858753,68855,2023/2/15,v3.3.2,"> cc @HyukjinKwon and @viirya Note that I don't think this is a blocker for 3.3.2 RC1 vote.

Hmm, I think you will back port it to branch-3.3? But we don't need to cut RC2 for it? Do you mean that?","1,-1    ",1,-1,0
1430861413,9700541,2023/2/15,v3.3.2,"Yes, I'll land this after your release, @viirya .","1,-1    ",1,-1,0
1430889715,9700541,2023/2/15,v3.3.2,"Thank you, @viirya . Let me open this until Tomorrow. :)","1,-1    ",1,-1,0
1430892112,9700541,2023/2/15,v3.3.2,"Also, cc @xinrong-meng ","1,-1    ",1,-1,0
1431745003,9700541,2023/2/15,v3.3.2,"Thank you, @viirya and @HyukjinKwon .

For the record, Apache Spark 3.3.2 RC1 vote passed. This patch will be delivered via Spark 3.4.0/3.3.3.
- https://lists.apache.org/thread/krnpf0tv1jdxy6dlssho1cn7ckfjwk2m","1,-3    ",1,-3,-2
1430935707,112507318,2023/2/16,v3.3.2,"Attaching screenshots of the updated pages.

<img width=""1160"" alt=""getting_started-install rst"" src=""https://user-images.githubusercontent.com/112507318/218973521-6206edc3-abc5-481b-8c90-183fe0d82b7b.png"">

<img width=""1160"" alt=""getting_started-quickstart_df ipynb"" src=""https://user-images.githubusercontent.com/112507318/218973623-1d9e4e02-0b4a-42fa-aa87-b5dda72e64bc.png"">

<img width=""1160"" alt=""reference-pyspark streaming rst"" src=""https://user-images.githubusercontent.com/112507318/218973697-be6f786b-7c90-4bfb-a86f-1b870987a2b7.png"">

<img width=""1160"" alt=""user_guide-index rst"" src=""https://user-images.githubusercontent.com/112507318/218973758-e2d9b81a-63f9-43b0-8f72-1fb96f594ed7.png"">
","1,-1    ",1,-1,0
1431229494,6477701,2023/2/16,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1431303122,7322292,2023/2/16,v3.3.2,"Late LGTM, thank you @allanf-db !","3,-1    ",3,-1,2
1432821635,1580697,2023/2/16,v3.3.2,"@haoyanzhang It is better to create a separate branch per every PR:
<img width=""700"" alt=""Screenshot 2023-02-16 at 12 54 49"" src=""https://user-images.githubusercontent.com/1580697/219332108-0c909f7d-907c-46c3-b290-b70029675ea1.png"">

see ""Pull request"" at https://spark.apache.org/contributing.html","3,-1    ",3,-1,2
1433173840,1580697,2023/2/16,v3.3.2,"@haoyanzhang Do you have an account at OSS JIRA: https://issues.apache.org/jira ? If not, I'll create it for you.","1,-1    ",1,-1,0
1434149215,42863974,2023/2/16,v3.3.2,"> @haoyanzhang It is better to create a separate branch per every PR: <img alt=""Screenshot 2023-02-16 at 12 54 49"" width=""700"" src=""https://user-images.githubusercontent.com/1580697/219332108-0c909f7d-907c-46c3-b290-b70029675ea1.png"">
> 
> see ""Pull request"" at https://spark.apache.org/contributing.html

got it,  will do it next time","2,-1    ",2,-1,1
1434149874,42863974,2023/2/16,v3.3.2,"> @haoyanzhang Do you have an account at OSS JIRA: https://issues.apache.org/jira ? If not, I'll create it for you.

I have one, thanks","3,-1    ",3,-1,2
1434159868,1580697,2023/2/16,v3.3.2,"+1, LGTM. Merging to master.
Thank you, @haoyanzhang and @HyukjinKwon for review.","1,-1    ",1,-1,0
1434161152,1580697,2023/2/16,v3.3.2,@haoyanzhang Congratulations with the first contribution to Apache Spark!,"1,-1    ",1,-1,0
1430947347,9700541,2023/2/16,v3.3.2,cc @HyukjinKwon ,"1,-2    ",1,-2,-1
1431225319,6477701,2023/2/16,v3.3.2,Merged to master.,"2,-1    ",2,-1,1
1431450365,1475305,2023/2/16,v3.3.2,late LGTM,"1,-1    ",1,-1,0
1431692588,9700541,2023/2/16,v3.3.2,"Thank you, @yaooqinn , @HyukjinKwon , @LuciferYang .","1,-1    ",1,-1,0
1433551293,47577197,2023/2/16,v3.3.2,So now we can upgrade guava?,"2,-2    ",2,-2,0
1433788143,9700541,2023/2/16,v3.3.2,"Not yet, @bjornjorgensen ~ :) Please hold on any significant changes until Apache Spark 3.4 is released. We still need to backport many bug fixes during Apache Spark 3.4 RC period. Here I just removed the broken CI which no one will take care.","1,-1    ",1,-1,0
1478796828,6477701,2023/2/16,v3.3.2,I remember Guava upgrade is also blocked by Hive .. IIRC .. ,"1,-1    ",1,-1,0
1430981352,79476540,2023/2/16,v3.3.2,cc @olaky ,"2,-1    ",2,-1,1
1431265668,1825975,2023/2/16,v3.3.2,"Thanks, this is a helpful clarification","1,-1    ",1,-1,0
1433941303,6477701,2023/2/16,v3.3.2,cc @cloud-fan ,"1,-1    ",1,-1,0
1437769149,79476540,2023/2/16,v3.3.2,"@cloud-fan @olaky Added more clarifications, let me know WDYT! Thanks","1,-1    ",1,-1,0
1438036639,79476540,2023/2/16,v3.3.2,Thanks! updated,"1,-1    ",1,-1,0
1439619002,79476540,2023/2/16,v3.3.2,"cc @cloud-fan Fixed the style, and passed all checks, thanks!","1,-1    ",1,-1,0
1439883459,3182036,2023/2/16,v3.3.2,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1431227079,6477701,2023/2/16,v3.3.2,cc @juliuszsompolski @cloud-fan FYI,"1,-1    ",1,-1,0
1436361416,8326978,2023/2/16,v3.3.2,kindly ping @HyukjinKwon @HyukjinKwon ,"2,-1    ",2,-1,1
1441387689,8326978,2023/2/16,v3.3.2,"thanks, merged to master and 3.4","1,-1    ",1,-1,0
1431299312,7322292,2023/2/16,v3.3.2,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1431427227,6477701,2023/2/16,v3.3.2,Thanks for fixing this. Merged to master and branch-3.4.,"1,-3    ",1,-3,-2
1431565156,9616802,2023/2/16,v3.3.2,cc @zhenlineo we discussed this today,"1,-1    ",1,-1,0
1431708473,4190164,2023/2/16,v3.3.2,"Thanks for looking into this. We should definitely fix the leak. I would probably do the same:
When we close the session, we go through all buffers and close them. The Result and cleaner also holds some resources.","2,-1    ",2,-1,1
1432389031,9616802,2023/2/16,v3.3.2,We can also just clean-up the results. Part of the problem is that the E2E test does not close results. I am not sure if there are similar issues on the server side.,"2,-1    ",2,-1,1
1432389301,9616802,2023/2/16,v3.3.2,Will submit a PR shortly.,"1,-1    ",1,-1,0
1432401244,4190164,2023/2/16,v3.3.2,"@hvanhovell What about this? https://github.com/apache/spark/compare/master...zhenlineo:spark:allocator-memleak?expand=1 when a session is closed, we force to close all buffers.","2,-1    ",2,-1,1
1432416730,1475305,2023/2/16,v3.3.2,"Let me close this one, thanks @hvanhovell @zhenlineo ","1,-1    ",1,-1,0
1432401420,5399861,2023/2/16,v3.3.2,Merged to master.,"1,-2    ",1,-2,-1
1432415758,1475305,2023/2/16,v3.3.2,Thanks @wangyum @dongjoon-hyun ,"1,-2    ",1,-2,-1
1445410662,24260474,2023/2/16,v3.3.2,"@srielau could you take a look, pls?","1,-1    ",1,-1,0
1445466115,24260474,2023/2/16,v3.3.2,"Also, I'd like to share some performance measurements from my local machine, using JMH:
code example:
```java
...
    @Benchmark
    public void convert_branch_master(Blackhole bh) {
        // Convert to unsigned
        for (int i = -10_000; i < 10_000; i++) {
            UTF8String convert = NumberConverter
                    .convert(UTF8String.fromString(String.valueOf(i)).getBytes(), 10, 16);
            bh.consume(convert);
        }

        // Convert to signed
        for (int i = -10_000; i < 10_000; i++) {
            UTF8String convert = NumberConverter
                    .convert(UTF8String.fromString(String.valueOf(i)).getBytes(), 10, -16);
            bh.consume(convert);
        }
    }
...

// the same code for SPARK-42399 branch
```

With Java 8, current PR even speeds up the performance:
```java
# JMH version: 1.36
# VM version: JDK 1.8.0_362, OpenJDK 64-Bit Server VM, 25.362-b00
# VM invoker: /usr/local/Cellar/openjdk@8/1.8.0+362/libexec/openjdk.jdk/Contents/Home/jre/bin/java
# Blackhole mode: full + dont-inline hint (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)
# Warmup: 5 iterations, 10 s each
# Measurement: 5 iterations, 10 s each
# Timeout: 10 min per iteration
# Threads: 1 thread, will synchronize iterations
# Benchmark mode: Average time, time/op



Benchmark                                           Mode  Cnt   Score   Error  Units
NumberConverterBenchmark.convert_branch_master      avgt   10  30.458 Â± 1.638  ms/op
NumberConverterBenchmark.convert_branch_spark42399  avgt   10  22.857 Â± 0.421  ms/op

```

With Java 11, implementation from master branch works more then 2 times faster than the same implementation with Java 8!
But there is not a big gap in performance difference between master branch implementation and implementation from current branch. 
```java
# JMH version: 1.36
# VM version: JDK 11.0.16.1, OpenJDK 64-Bit Server VM, 11.0.16.1+0
# VM invoker: /usr/local/Cellar/openjdk@11/11.0.16.1/libexec/openjdk.jdk/Contents/Home/bin/java
# Blackhole mode: full + dont-inline hint (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)
# Warmup: 5 iterations, 10 s each
# Measurement: 5 iterations, 10 s each
# Timeout: 10 min per iteration
# Threads: 1 thread, will synchronize iterations
# Benchmark mode: Average time, time/op

Benchmark                                           Mode  Cnt   Score   Error  Units
NumberConverterBenchmark.convert_branch_master      avgt   10  14.453 Â± 1.082  ms/op
NumberConverterBenchmark.convert_branch_spark42399  avgt   10  17.956 Â± 0.489  ms/op

```

With Java 17, implementation from master branch works more then 3 times faster than the same implementation with Java 8 and ~ 2 times faster then the same implementation with Java 11!
And here is a significant difference between master branch implementation and current branch (master branch is ~2x times faster...). 
```java
# JMH version: 1.36
# VM version: JDK 17.0.4.1, OpenJDK 64-Bit Server VM, 17.0.4.1+1
# VM invoker: /usr/local/Cellar/openjdk@17/17.0.4.1_1/libexec/openjdk.jdk/Contents/Home/bin/java
# Blackhole mode: compiler (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)
# Warmup: 5 iterations, 10 s each
# Measurement: 5 iterations, 10 s each
# Timeout: 10 min per iteration
# Threads: 1 thread, will synchronize iterations
# Benchmark mode: Average time, time/op

Benchmark                                           Mode  Cnt   Score   Error  Units
NumberConverterBenchmark.convert_branch_master      avgt   10   8.410 Â± 0.161  ms/op
NumberConverterBenchmark.convert_branch_spark42399  avgt   10  18.162 Â± 0.036  ms/op
```","1,-1    ",1,-1,0
1431832387,1938382,2023/2/16,v3.3.2,@hvanhovell ,"1,-1    ",1,-1,0
1432343731,3182036,2023/2/16,v3.3.2,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1432046172,1097932,2023/2/16,v3.3.2,cc @sadikovi @HyukjinKwon ,"1,-1    ",1,-1,0
1432284703,6477701,2023/2/16,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1432057981,1097932,2023/2/16,v3.3.2,cc @sadikovi @HyukjinKwon ,"1,-1    ",1,-1,0
1432104742,7788766,2023/2/16,v3.3.2,We discussed before and decided to keep prefersDate similar to a decimal config cc @HyukjinKwon ,"1,-1    ",1,-1,0
1432116814,1097932,2023/2/16,v3.3.2,"Hmm, I take a quick check, do you mean the json option `prefersDecimal`?  It is inconsistent with other JSON data source options either https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala#L240 ... Users can make mistakes since they are used to data source options without `s` on the verb.

Shall we support `preferDecimal` and `prefersDecimal` in JSON, and use `prefer` in the new options of Spark 3.4?","1,-1    ",1,-1,0
1432284408,6477701,2023/2/16,v3.3.2,im fine w/ that,"1,-1    ",1,-1,0
1432287936,7788766,2023/2/16,v3.3.2,I am curious why did we not do it from the beginning?,"1,-1    ",1,-1,0
1432443063,1097932,2023/2/16,v3.3.2,"> I am curious why did we not do it from the beginning?

I didn't join the discussion, but let's make the naming consistent before it is too late. There is only one exception `prefersDecimal` now.

","1,-1    ",1,-1,0
1433963496,1097932,2023/2/16,v3.3.2,"Thanks, merging to master/3.4","1,-1    ",1,-1,0
1432127548,112507318,2023/2/16,v3.3.2,CC @HyukjinKwon @zhengruifeng ,"1,-1    ",1,-1,0
1432199865,112507318,2023/2/16,v3.3.2,"Attaching screenshots of the updated Migration Guides page and the new version upgrade sub-page.

<img width=""1340"" alt=""pyspark_migration_guides_doc_page"" src=""https://user-images.githubusercontent.com/112507318/219209282-27bea6e8-beb5-4336-a6c1-7e91a011a225.png"">

<img width=""1340"" alt=""pyspark_migration_guide_upgrade_doc_page"" src=""https://user-images.githubusercontent.com/112507318/219209350-bdbf18e4-e77b-414e-a19d-8e431fb2a60a.png"">
","1,-1    ",1,-1,0
1432336441,6477701,2023/2/16,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1432662837,6477701,2023/2/17,v3.3.2,"Yeah, let's close. I don't think this is an issue.","1,-1    ",1,-1,0
1432696569,9700541,2023/2/17,v3.3.2,"Thank you, @HyukjinKwon and @rithwik-db .","1,-1    ",1,-1,0
1432388816,6477701,2023/2/17,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1432420701,6477701,2023/2/17,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1432411649,9616802,2023/2/17,v3.3.2,cc @zhenlineo @LuciferYang ,"3,-1    ",3,-1,2
1432449028,1475305,2023/2/17,v3.3.2,"https://github.com/apache/spark/blob/7ee8a32077b09cb847b6ac41cdc5067cf7bd83e9/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala#L155-L159

should we remove this `try catch`? This may cover up some problems, but this may be another pr

","1,-1    ",1,-1,0
1432606038,9700541,2023/2/17,v3.3.2,"According to your comment, is it okay for me to merge, @LuciferYang ?","1,-1    ",1,-1,0
1432606445,1475305,2023/2/17,v3.3.2,"> According to your comment, is it okay for me to merge, @LuciferYang ?

ok to merge ~","1,-1    ",1,-1,0
1432607346,9700541,2023/2/17,v3.3.2,"Thanks, @LuciferYang . Merged to master/3.4.
Thank you, @hvanhovell , @HyukjinKwon , @LuciferYang .","1,-1    ",1,-1,0
1436542110,3182036,2023/2/17,v3.3.2,"thanks for the review, merging to master/3.4!","1,-1    ",1,-1,0
1437534098,9700541,2023/2/17,v3.3.2,"Interestingly, it passed locally while GitHub Action jobs keep failing.
```
$ build/sbt ""sql/testOnly *.OrcSourceV1Suite -- -z SPARK-11412""
...
[info] All tests passed.
[success] Total time: 23 s, completed Feb 20, 2023, 12:54:28 PM
```","1,-1    ",1,-1,0
1437632660,9700541,2023/2/17,v3.3.2,"Since this happens in GitHub Action currently, I made a WIP PR for further investigation. If it's valid, I'll convert it to the official PR separately from this PR.
- https://github.com/apache/spark/pull/40095","1,-1    ",1,-1,0
1437705253,9700541,2023/2/17,v3.3.2,I closed my PR because the failure seems to happen more earlier than this commit.,"1,-1    ",1,-1,0
1432438668,9616802,2023/2/17,v3.3.2,Note for the reviewer. I still want to add a couple of tests for duplicate functions.,"1,-1    ",1,-1,0
1433076065,9616802,2023/2/17,v3.3.2,@dongjoon-hyun I will fix those today. I do think we should have a discussion about this. Currently we have both maven and scala-2.13 that are not tested during CI. That seems wrong if both are apparently supported. The mental overhead of testing these manually is very high.,"1,-1    ",1,-1,0
1433077927,9616802,2023/2/17,v3.3.2,"As for blocking other community members severely, the same applies to the lack of testing of scala-2.13.","1,-1    ",1,-1,0
1433709843,9616802,2023/2/17,v3.3.2,Fix for 2.13 has merged. I am going to hold this off until https://github.com/apache/spark/pull/40056 is merged.,"1,-1    ",1,-1,0
1434562822,9616802,2023/2/17,v3.3.2,merging,"2,-1    ",2,-1,1
1434564036,9616802,2023/2/17,v3.3.2,thanks all,"1,-1    ",1,-1,0
1432511333,68855,2023/2/17,v3.3.2,Some info I searched about it: https://docs.docker.com/build/attestations/slsa-provenance/.,"2,-1    ",2,-1,1
1432511424,9700541,2023/2/17,v3.3.2,Thank you. The affected versions are the future releases (3.4.0/3.3.3/3.2.4) instead of the released version.,"4,-1    ",4,-1,3
1432561510,9700541,2023/2/17,v3.3.2,K8s Integration Tests passed. Merged to master/3.4/3.3/3.2.,"1,-1    ",1,-1,0
1432646662,1475305,2023/2/17,v3.3.2,cc @wangyum @srowen FYI,"1,-1    ",1,-1,0
1432653093,5399861,2023/2/17,v3.3.2,Could you test against hadoop-2?,"1,-1    ",1,-1,0
1432702958,1475305,2023/2/17,v3.3.2,"> Could you test against hadoop-2?

Do same check with -Phadoop-2

**Maven** 

```
build/mvn clean
build/mvn clean install -DskipTestes -pl resource-managers/yarn -am -Pyarn -Phadoop-2
build/mvn -Dtest=none -DwildcardSuites=org.apache.spark.deploy.yarn.YarnClusterSuite -pl resource-managers/yarn test -Pyarn -Phadoop-2
build/mvn test -pl resource-managers/yarn -Pyarn -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest  -Phadoop-2
```

**SBT**

```
build/sbt clean yarn/test -Pyarn -Dtest.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest -Phadoop-2
```


All tests passed.

","1,-1    ",1,-1,0
1433264221,822522,2023/2/17,v3.3.2,Merged to master,"1,-1    ",1,-1,0
1435483886,13592258,2023/2/17,v3.3.2,Merged to master. Thanks @LuciferYang ,"2,-2    ",2,-2,0
1435487106,9700541,2023/2/17,v3.3.2,"Thank you, @LuciferYang and @huaxingao !","1,-2    ",1,-2,-1
1435520624,1475305,2023/2/17,v3.3.2,Thanks @huaxingao @dongjoon-hyun ,"1,-1    ",1,-1,0
1433329522,9616802,2023/2/17,v3.3.2,"@nija-at can you file a ticket in the JIRA (or ask me to do it), and add that to the title?","2,-1    ",2,-1,1
1433330261,9616802,2023/2/18,v3.3.2,Can you also do this for the scala client?,"1,-1    ",1,-1,0
1433779252,1938382,2023/2/18,v3.3.2,Actually. Please open a JIRA and add JIRA id to the PR title along with module names (example: `[SPARK-xxxx][CONNECT][PYTHON]) accept user_agent in spark connect's connection string`),"2,-1    ",2,-1,1
1434552098,16217941,2023/2/18,v3.3.2,"@hvanhovell 

> can you file a ticket in the JIRA (or ask me to do it), and add that to the title?

Done.

> Can you also do this for the scala client?

I'll do that as a separate change
","3,-1    ",3,-1,2
1437070103,6477701,2023/2/18,v3.3.2,Merged to master and branch-3.4.,"3,-1    ",3,-1,2
1433174367,9616802,2023/2/18,v3.3.2,@dongjoon-hyun this time I have tested 2.13.,"2,-1    ",2,-1,1
1433559240,9616802,2023/2/18,v3.3.2,Merging.,"1,-1    ",1,-1,0
1433780489,9700541,2023/2/18,v3.3.2,"Thank you so much, @hvanhovell and @cloud-fan !","2,-1    ",2,-1,1
1433708810,9616802,2023/2/18,v3.3.2,"For the reviews, I have manually tested this with 2.13.","2,-1    ",2,-1,1
1433808631,9616802,2023/2/18,v3.3.2,Alright merging this.,"1,-1    ",1,-1,0
1433830795,9700541,2023/2/18,v3.3.2,"> For the reviews, I have manually tested this with 2.13.

Thank you so much, @hvanhovell !","2,-1    ",2,-1,1
1433744359,1938382,2023/2/18,v3.3.2,@hvanhovell ,"1,-1    ",1,-1,0
1434094774,9700541,2023/2/18,v3.3.2,"Merged to master/3.4 for Apache Spark 3.4.0. Thank you, @huaxingao and @viirya .

cc @MaxGekk since he filed SPARK-39859 originally.","1,-2    ",1,-2,-1
1434094977,13592258,2023/2/18,v3.3.2,Thanks @dongjoon-hyun @viirya ,"1,-1    ",1,-1,0
1433828508,7788766,2023/2/18,v3.3.2,cc @dongjoon-hyun @HyukjinKwon ,"1,-2    ",1,-2,-1
1434093141,9700541,2023/2/18,v3.3.2,"Merged to master for Apache Spark 3.5. Thank you, @sadikovi and @HyukjinKwon .","1,-1    ",1,-1,0
1433966961,7322292,2023/2/19,v3.3.2,merged into master/3.4,"1,-2    ",1,-2,-1
1434288211,9700541,2023/2/19,v3.3.2,cc @viirya ,"2,-1    ",2,-1,1
1434334097,9700541,2023/2/19,v3.3.2,"Thank you, @viirya !","1,-1    ",1,-1,0
1434348426,9700541,2023/2/19,v3.3.2,"The tests passed in the GitHub Action. Merged to master
![Screenshot 2023-02-17 at 1 08 13 AM](https://user-images.githubusercontent.com/9700541/219601614-e84bf609-e8fd-4a0b-80b8-0afa5f41b457.png)

Merged to master/3.4.","2,-1    ",2,-1,1
1434491422,1475305,2023/2/19,v3.3.2,late LGTM,"1,-1    ",1,-1,0
1434578211,7322292,2023/2/19,v3.3.2,"It seems more complicated than I thought, I think we can simplify it in this way

In client:
```
    def sql(self, sqlQuery: str, args: Optional[Dict[str, str]] = None) -> ""DataFrame"":
        df = DataFrame.withPlan(SQL(sqlQuery, args), self)
        print(df.schema)   <- eagerly analyze the plan
        return df
```

In connect planner:
```
  private def transformSql(sql: proto.SQL): LogicalPlan = {
    // scalastyle:off println
    println(s""invoke transformSql $sql"")
    session
      .sql(sql.getQuery, sql.getArgsMap.asScala.toMap)    <- directly invoke the spark session api
      .logicalPlan
  }
```


bin/pyspark --remote ""local[*]""
```
>>> spark.sql(""set spark.sql.adaptive.enabled=false"")
invoke transformSql query: ""set spark.sql.adaptive.enabled=false""

StructType([StructField('key', StringType(), False), StructField('value', StringType(), False)])
DataFrame[key: string, value: string]
```","2,-1    ",2,-1,1
1434549249,51110188,2023/2/19,v3.3.2,@cloud-fan @boneanxs could you please take a look if you find a time?,"1,-1    ",1,-1,0
1437838332,3182036,2023/2/19,v3.3.2,"@Yikf can you provide a test case, or at least the error stacktrace you hit in your environment?","1,-1    ",1,-1,0
1437856558,51110188,2023/2/19,v3.3.2,"@cloud-fan This case is the error that Apache kyuubi encountered when upgrading from spark 3.3.1 to 3.3.2, can see this [link](https://github.com/apache/kyuubi/actions/runs/4192366930/jobs/7268919556#step:6:2611) to find the error stacktrace.","1,-1    ",1,-1,0
1437928464,51110188,2023/2/19,v3.3.2,"updated, verified w/ kyuubi on spark 3.3.2 and all tests passed
```
build/mvn clean test -pl :kyuubi-spark-connector-hive_2.12 -am -Pspark-3.3 -Dspark.version=3.3.2
```","1,-1    ",1,-1,0
1441134396,51110188,2023/2/20,v3.3.2,"kindly ping @cloud-fan , @boneanxs Any suggestions?","1,-2    ",1,-2,-1
1441168996,1591700,2023/2/20,v3.3.2,I have not followed the changes in this part of the code too much in a while - but this specific PR will result in a different `jobId` each time the class is deserialized - I would expect that to cause issues with output formats ?,"1,-1    ",1,-1,0
1441179787,51110188,2023/2/20,v3.3.2,"@mridulm Thanks your review, this is a nice question for me, `JobId` maybe is different when each time the class is deserialized.

How about this idea that `SparkHadoopWriterUtils.createJobTrackerID` to generate an ID for a job tracker, and the job tracker is unique, JobId is constructed using a unique tackerid when the class is deserialized.","1,-1    ",1,-1,0
1441190260,1591700,2023/2/20,v3.3.2,"@Yikf Agree - we only specify two parts for the `JobID` - the `String jtIdentifier` and `int id`.
We can persist those in the class - and make jobId a `transient lazy val` which recreates it each time.

Something like this instead:

```

  private[this] val jobIdParts: (String, Int) = {
    val id = SparkHadoopWriterUtils.createJobID(new Date, 0)
    (id.getJtIdentifier, id.getId)
  }

  @transient private lazy val jobId = new JobID(jobIdParts._1, jobIdParts._2)
```

Thoughts ?
","1,-1    ",1,-1,0
1441378610,51110188,2023/2/20,v3.3.2,"@mridulm Nice suggestion, and we can simplify to as follow since `int id` is unique.

```scala
  private[this] val jobTrackerID = SparkHadoopWriterUtils.createJobTrackerID(new Date)
  @transient private lazy val jobId = new JobID(jobTrackerID, 0)
```","1,-1    ",1,-1,0
1445935152,3182036,2023/2/20,v3.3.2,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1454080985,9700541,2023/2/20,v3.3.2,"Hi, @cloud-fan . SPARK-41448 landed to master/3.3/3.2 and this is merge this to master/3.4 only. I'm wondering if we are planning backporting to branch-3.3 and 3.2.
- https://github.com/apache/spark/pull/38980#issuecomment-1346229161
","1,-1    ",1,-1,0
1454081754,9700541,2023/2/20,v3.3.2,"Also, cc @sunchao ","1,-1    ",1,-1,0
1455335925,3182036,2023/2/20,v3.3.2,@Yikf can you help to open a backport PR for 3.2/3.3? Thanks!,"1,-1    ",1,-1,0
1455364691,51110188,2023/2/20,v3.3.2,"> @Yikf can you help to open a backport PR for 3.2/3.3? Thanks!

Sure","1,-1    ",1,-1,0
1434828500,1475305,2023/2/20,v3.3.2,cc @dongjoon-hyun ,"1,-1    ",1,-1,0
1434952516,1475305,2023/2/20,v3.3.2,"I make another one build with maven 3.8.7 + cyclonedx-maven-plugin  2.7.4 https://github.com/LuciferYang/spark/actions/runs/4205904014/jobs/7298678641

<img width=""1074"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/219719321-dc1e6aa3-1a21-4e93-92ce-60cee921493b.png"">
","1,-1    ",1,-1,0
1435007566,9700541,2023/2/20,v3.3.2,"I mean in our GitHub Action repo. We are using CycloneDX 2.7.3, aren't we?

> I make another one build with maven 3.8.7 + cyclonedx-maven-plugin 2.7.4 https://github.com/LuciferYang/spark/actions/runs/4205904014/jobs/7298678641","1,-1    ",1,-1,0
1435045931,1475305,2023/2/20,v3.3.2,"Yes, we use CycloneDX 2.7.3. So I should not explain that 2.7.4 has such issue in the pr description, because it does not affect Spark now, am I right?","1,-1    ",1,-1,0
1435050161,1475305,2023/2/20,v3.3.2,"Please let me explain my intention more:

1. First of all, I want to update maven to 3.9.0(keep use CycloneDX 2.7.3), then I found the following error:

```
[ERROR] An error occurred attempting to read POM
org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
```

I think We should see similar errors here: https://github.com/LuciferYang/spark/actions/runs/4206035140/jobs/7299042843 later

2. then I want to test maven 3.9.0 + CycloneDX 2.7.4 couple of days ago, but there an error same as  `maven 3.8.7 + cyclonedx-maven-plugin 2.7.4`,  I think we should see it here: https://github.com/LuciferYang/spark/runs/11424487074 later

3. then I test maven 3.9.0 + CycloneDX 2.7.5 today, there is no above issues(we can check https://github.com/LuciferYang/spark/runs/11424568023 later).

So If I want to upgrade Spark to use maven 3.9.0, I must upgrade cyclonedx-maven-plugin to 2.7.5, I should upgrade them in one or two pr? 
","1,-1    ",1,-1,0
1435116865,9700541,2023/2/20,v3.3.2,"I'm trying to assess the issue. So, those combination issue is not the AS-IS Apache Spark issue in both master/branch-3.4, right?

FYI, Cyclone plugin 2.7.4 issue is a known one. When I started SBOM works, 2.7.4 was the lastest but was unusable across multiple ASF projects. That was the main reason I chose 2.7.3 instead of the latest at that time. I'm not quite sure if 2.7.5 is stable enough.

Anyway, we can apply this PR on `master` branch for Apache Spark 3.5.0 only separately from the Maven issue. Maven is also another big issues always.","1,-1    ",1,-1,0
1435129166,1475305,2023/2/20,v3.3.2,"Yeah, Spark 3.4.0 does not need this pr.

","1,-1    ",1,-1,0
1435279944,9700541,2023/2/20,v3.3.2,"If you don't mind, please allow me one or two days. I'll check this during weekend~ Thank you for your patience always.","1,-1    ",1,-1,0
1435477929,1475305,2023/2/20,v3.3.2,"@dongjoon-hyun found a new issue related to 2.7.5: https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/284


","1,-1    ",1,-1,0
1435482433,9700541,2023/2/20,v3.3.2,Got it. Thank you for informing.,"1,-1    ",1,-1,0
1435520529,1475305,2023/2/20,v3.3.2,"I think we should wait for 2.7.6 or higher to test usability, then we can reuse this jira. I will close this pr first, thanks @dongjoon-hyun ","1,-1    ",1,-1,0
1435785032,9700541,2023/2/20,v3.3.2,"+1 for your decision, @LuciferYang . Thank you for letting me know before I started my work~ :) ","1,-1    ",1,-1,0
1441571006,162090,2023/2/20,v3.3.2,I'm hitting this when trying to build hadoop having updated maven via homebrew so as to get spark to work.  joy. ,"1,-1    ",1,-1,0
1502216909,9700541,2023/2/20,v3.3.2,This PR is superseded by https://github.com/apache/spark/pull/40726 .,"1,-1    ",1,-1,0
1436149452,6477701,2023/2/20,v3.3.2,I think we should better file a JIRA.,"1,-1    ",1,-1,0
1437189282,16217941,2023/2/20,v3.3.2,Discussed this with @grundprinzip. Apparently the retries are kept so long intentionally. The fix here needs to occur differently. Closing this PR for now.,"1,-2    ",1,-2,-1
1436282392,44108233,2023/2/20,v3.3.2,cc @HyukjinKwon mind taking a look when you find some time?,"1,-1    ",1,-1,0
1436379916,44108233,2023/2/20,v3.3.2,"Thanks, @HyukjinKwon !","1,-1    ",1,-1,0
1437102805,6477701,2023/2/20,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1438322214,7322292,2023/2/20,v3.3.2,Late LGTM,"1,-1    ",1,-1,0
1434974229,1475305,2023/2/20,v3.3.2,wait https://github.com/apache/spark/pull/40065,"1,-1    ",1,-1,0
1435520994,1475305,2023/2/20,v3.3.2,"Need to wait for a stable CycloneDX version, close this first
","1,-1    ",1,-1,0
1439487785,65108011,2023/2/20,v3.3.2,"Addressed comments. @LuciferYang 
And gentle ping @wangyum @sunchao: could you also take a look? ","1,-1    ",1,-1,0
1442242138,65108011,2023/2/20,v3.3.2,Add a conf `spark.sql.hive.dropPartitionByName.enabled` and two tests. cc: @sunchao ,"1,-1    ",1,-1,0
1461080621,506679,2023/2/20,v3.3.2,"Merged to master, thanks!","1,-1    ",1,-1,0
1461082510,9700541,2023/2/20,v3.3.2,"Thank youso much, @wecharyu, @sunchao and all!","1,-1    ",1,-1,0
1435241324,1938382,2023/2/20,v3.3.2,@hvanhovell ,"1,-1    ",1,-1,0
1435257024,1938382,2023/2/20,v3.3.2,I need to update golden files in this PR.,"1,-1    ",1,-1,0
1435428773,9616802,2023/2/20,v3.3.2,Merging.,"1,-1    ",1,-1,0
1436139937,6477701,2023/2/20,v3.3.2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1435486887,9700541,2023/2/20,v3.3.2,"When you have some time, could you review this, @viirya ? I want to merge this to proceed the further investigations.","2,-1    ",2,-1,1
1435501963,9700541,2023/2/20,v3.3.2,"Thank you so much always for your help, @viirya !
Merged to master for Apache Spark 3.5.","1,-1    ",1,-1,0
1438998141,10248890,2023/2/20,v3.3.2,"Some tests always fail with 
```
112 did not equal 104 Invalid stopIndex of a query context. Actual:SQLQueryContext(Some(1),Some(15),Some(15),Some(112),Some(select id from hive.`/home/runner/work/oss-spark/oss-spark/target/tmp/spark-9ff647b3-c1b5-449d-ae54-19a7f3baff9d`),None,None)
```
Length of string ""select id from hive.`/home/runner/work/oss-spark/oss-spark/target/tmp/spark-9ff647b3-c1b5-449d-ae54-19a7f3baff9d`"" is 113, so the last index is 112.

I guess that's because my folder name is `oss-spark`. The two `oss-` contains 8 chars, that's equal to 112 - 104.

https://github.com/WweiL/oss-spark/actions/runs/4235566927/jobs/7359363624

So I changed the [hard-coded length here](https://github.com/apache/spark/blob/b36d1484c1a090a33d9add056730128b9ba5729f/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala#L1410) to a variable-length one.

@MaxGekk @srielau @itholic I found that this is related to the PR (https://github.com/apache/spark/pull/39977) you pushed / reviewed. Can you guys also take a look? Thanks!","1,-1    ",1,-1,0
1439306082,44108233,2023/2/20,v3.3.2,"Thanks for catching out, @WweiL !","1,-1    ",1,-1,0
1439485656,10248890,2023/2/20,v3.3.2,@cloud-fan Can you merge this to master when you get a chance? Thank you!,"1,-1    ",1,-1,0
1441255439,3182036,2023/2/20,v3.3.2,"thanks, merging to master!","1,-1    ",1,-1,0
1445877228,1317309,2023/2/20,v3.3.2,"Was this merged only to Spark 3.5 (master branch)? The JIRA ticket is not properly marked for fix version as well as status, and we need to make it clear to determine the version range to apply SPARK-42572.","1,-2    ",1,-2,-1
1445885564,10248890,2023/2/20,v3.3.2,"> Was this merged only to Spark 3.5 (master branch)? The JIRA ticket is not properly marked for fix version as well as status, and we need to make it clear to determine the version range to apply [SPARK-42572](https://issues.apache.org/jira/browse/SPARK-42572).

Yes this was only merged to master. I've updated the version in SPARK-42572. BTW is there a way to quickly decide what's the version of the current master branch? I thought it was 3.4...","1,-1    ",1,-1,0
1445895867,1317309,2023/2/20,v3.3.2,"Sorry I seem to look at different JIRA ticket. SPARK-42484 contains the fixed version, 3.5.0. That said, SPARK-42572 only needs to be applied to master branch.","1,-1    ",1,-1,0
1435511800,1580697,2023/2/20,v3.3.2,"The kub test is not related to this PR, I believe:
```
[info] org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite *** ABORTED *** (27 minutes, 35 seconds)
[info]   io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/api/v1/namespaces. Message: object is being deleted: namespaces ""spark-e7de0ffd81044f09afb2693a0e227a43"" already exists. Received status: 
```","1,-1    ",1,-1,0
1435512116,1580697,2023/2/20,v3.3.2,"+1, LGTM. Merging to master/3.4.
Thank you, @gengliangwang.","1,-1    ",1,-1,0
1437225017,1475305,2023/2/20,v3.3.2,"Move `SaveMode` to catalyst module is a break change, need add `ProblemFilters` to `MimaExcludes`","1,-1    ",1,-1,0
1437771626,1938382,2023/2/20,v3.3.2,"> Move `SaveMode` to catalyst module is a break change, need add `ProblemFilters` to `MimaExcludes`

I think we shouldn't make such breaking change?","1,-1    ",1,-1,0
1437772288,1475305,2023/2/21,v3.4.0-rc1,"> > Move `SaveMode` to catalyst module is a break change, need add `ProblemFilters` to `MimaExcludes`
> 
> I think we shouldn't make such breaking change?

best to avoid","1,-1    ",1,-1,0
1439321033,4190164,2023/2/21,v3.4.0-rc1,Depends on https://github.com/apache/spark/pull/40109 to get the correct error message for `create`.,"1,-1    ",1,-1,0
1440779753,9616802,2023/2/21,v3.4.0-rc1,merging.,"1,-1    ",1,-1,0
1436140411,6477701,2023/2/21,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1436327185,7322292,2023/2/21,v3.4.0-rc1,Late LGTM,"1,-1    ",1,-1,0
1437884897,47337188,2023/2/21,v3.4.0-rc1,Sorry late LGTM,"1,-1    ",1,-1,0
1436041768,9616802,2023/2/21,v3.4.0-rc1,"Before we try to use multiple task schedulers, why not fix the parallelism issues in the existing one. Moving from a locking everywhere approach, to an event loop should yield quite a throughput improvement.","2,-1    ",2,-1,1
1438945634,125713420,2023/2/21,v3.4.0-rc1,"@hvanhovell it won't be enough, we still need to scale communication with executors","1,-1    ",1,-1,0
1435730649,47577197,2023/2/21,v3.4.0-rc1,@yaooqinn you did the last upgrade on this one in https://github.com/apache/spark/pull/38733 will you have a look at this one? ,"1,-1    ",1,-1,0
1435979942,47577197,2023/2/21,v3.4.0-rc1,"@dongjoon-hyun yes, that's right.
I think we can have this small update now to master and branch 3.4. When 3.4 is released, we can try to update to a newer version. ","1,-1    ",1,-1,0
1436104835,9700541,2023/2/21,v3.4.0-rc1,Got it. Thank you for the clarification. Sounds safe.,"1,-1    ",1,-1,0
1436165602,9700541,2023/2/21,v3.4.0-rc1,"Oh, I missed that you wrote `branch-3.4` explicitly. Are you sure, @bjornjorgensen ?
> I think we can have this small update now to master and branch 3.4. When 3.4 is released, we can try to update to a newer version.

If you want to claim this PR as a blocker, you need to do that properly. In general, you had better do two things at least.
- Ping, @xinrong-meng , in order to inform her the existence of the blocker.
- Update SPARK-42486 properly. Currently, the JIRA Affected Version is quite opposite from your comment (or intention) here.

![Screenshot 2023-02-19 at 5 15 05 PM](https://user-images.githubusercontent.com/9700541/219988088-7bc0c346-4ef3-41ce-91cf-0f83b57868f5.png)
","1,-1    ",1,-1,0
1436609776,47577197,2023/2/21,v3.4.0-rc1,"I did upgrade the JIRA ticket now to include 3.4.0. 
This is not a blocker. 
I think this is more a nice too have upgrade for 3.4.0. ","1,-2    ",1,-2,-1
1437668926,9700541,2023/2/21,v3.4.0-rc1,"I'm not sure new 3.6.4 Zookeepr is urgent or safe in branch-3.4. According to the Maven Central, 3.6.4 is the least adopted (unverified) release among the recent versions. Do we have a test case to support that we need this?

![Screenshot 2023-02-20 at 3 30 59 PM](https://user-images.githubusercontent.com/9700541/220212850-0ae350c0-bf7d-4b9d-a844-7780bd6362e6.png)


","1,-1    ",1,-1,0
1437697727,6477701,2023/2/21,v3.4.0-rc1,Let's probably don't add it to branch-3.4 ...,"1,-1    ",1,-1,0
1437840387,47337188,2023/2/21,v3.4.0-rc1,Since branch-3.4 CI is under fix so I may have to re-create the tag later. Please let me know if we shall wait for this PR or not.,"1,-1    ",1,-1,0
1437840964,9700541,2023/2/21,v3.4.0-rc1,"IMO, we don't need to wait for this PR, @xinrong-meng .","1,-1    ",1,-1,0
1437841242,47337188,2023/2/21,v3.4.0-rc1,Thanks @dongjoon-hyun!,"1,-1    ",1,-1,0
1438155936,47577197,2023/2/21,v3.4.0-rc1,"Ok, thank you, @dongjoon-hyun ","1,-1    ",1,-1,0
1435872911,502522,2023/2/21,v3.4.0-rc1,"cc: @gengliangwang, @HeartSaVioR, @SandishKumarHN ","1,-1    ",1,-1,0
1435873432,3078999,2023/2/21,v3.4.0-rc1,"@rangadi with this change, we can conclude that spark-protobuf supports proto2 and proto3 versions right? ","1,-1    ",1,-1,0
1436361799,502522,2023/2/21,v3.4.0-rc1,"@dongjoon-hyun, done. Remove 3.4 in the summary. 

> @rangadi with this change, we can conclude that spark-protobuf supports proto2 and proto3 versions right?

Yes. And we always supported Proto2. 
","1,-1    ",1,-1,0
1436363361,9700541,2023/2/21,v3.4.0-rc1,"Thank you for updates, @rangadi .

cc @hvanhovell ","1,-1    ",1,-1,0
1438905661,502522,2023/2/21,v3.4.0-rc1,"@gengliangwang could you take a quick look and merge in to master and 3.4.
Currently, it looks like you are the only one who can merge Protobuf patches. Please suggest someone else also for next time. 
","1,-1    ",1,-1,0
1438952601,1097932,2023/2/21,v3.4.0-rc1,@rangadi can you update the PR description to follow the PR template?,"2,-1    ",2,-1,1
1439031344,502522,2023/2/21,v3.4.0-rc1,"> @rangadi can you update the PR description to follow the PR template?

@gengliangwang update the summary to follow the template. Sorry about that. 
Also added a new test. PTAL.","1,-1    ",1,-1,0
1439312954,1097932,2023/2/21,v3.4.0-rc1,"Thanks, merging to master/3.4","1,-1    ",1,-1,0
1435964877,1475305,2023/2/21,v3.4.0-rc1,cc @srowen ,"1,-1    ",1,-1,0
1436186506,1475305,2023/2/21,v3.4.0-rc1,Thanks @srowen @dongjoon-hyun ,"1,-1    ",1,-1,0
1435965880,1475305,2023/2/21,v3.4.0-rc1,test first,"1,-2    ",1,-2,-1
1436333698,1475305,2023/2/21,v3.4.0-rc1,Thanks @dongjoon-hyun @srowen ,"1,-1    ",1,-1,0
1436336072,9700541,2023/2/21,v3.4.0-rc1,"BTW, I have one question, @LuciferYang . We explicitly exclude `jna` from `commons-crypto`. Do you know what are we losing from the enumerated items in the `commons-crypto` releasenotes?

https://github.com/apache/spark/blob/5fc44dabe5084fb784f064afe691951a3c270793/pom.xml#L2679-L2689","2,-1    ",2,-1,1
1436378739,1475305,2023/2/21,v3.4.0-rc1,"From the release notes, I think nothing is losing.

`OpenSsl20XNativeJna`,  which is not mentioned in release notes, is a new support in version 1.2.0(Because jna is excluded, it cannot be used in Spark).

","3,-1    ",3,-1,2
1443129166,9700541,2023/2/21,v3.4.0-rc1,"Oh, this seems to break Apple Silicon environment, @LuciferYang . I hit the failure on Macbook environment.
```
$ build/sbt ""sql/testOnly *.RowQueueSuite""
...
[info] RowQueueSuite:
[info] - in-memory queue (33 milliseconds)
[info] - disk queue (encryption = off) (28 milliseconds)
[info] org.apache.spark.sql.execution.python.RowQueueSuite *** ABORTED *** (125 milliseconds)
[info]   java.lang.ExceptionInInitializerError:
[info]   at java.base/java.lang.Class.forName0(Native Method)
[info]   at java.base/java.lang.Class.forName(Class.java:398)
[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)
[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)
[info]   at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)
[info]   at org.apache.spark.security.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)
[info]   at org.apache.spark.security.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)
[info]   at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)
[info]   at scala.Option.map(Option.scala:230)
[info]   at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)
[info]   at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)
[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)
[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)
[info]   at org.apache.spark.security.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[info]   at java.base/java.lang.Thread.run(Thread.java:829)
[info]   Cause: java.lang.IllegalStateException: java.security.GeneralSecurityException: Native library is not loaded
[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:67)
[info]   at java.base/java.lang.Class.forName0(Native Method)
[info]   at java.base/java.lang.Class.forName(Class.java:398)
[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)
[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)
[info]   at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)
[info]   at org.apache.spark.security.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)
[info]   at org.apache.spark.security.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)
[info]   at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)
[info]   at scala.Option.map(Option.scala:230)
[info]   at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)
[info]   at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)
[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)
[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)
[info]   at org.apache.spark.security.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[info]   at java.base/java.lang.Thread.run(Thread.java:829)
[info]   Cause: java.security.GeneralSecurityException: Native library is not loaded
[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.checkNative(OpenSslCryptoRandom.java:79)
[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:65)
[info]   at java.base/java.lang.Class.forName0(Native Method)
[info]   at java.base/java.lang.Class.forName(Class.java:398)
[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)
[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)
[info]   at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)
[info]   at org.apache.spark.security.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)
[info]   at org.apache.spark.security.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)
[info]   at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)
[info]   at scala.Option.map(Option.scala:230)
[info]   at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)
[info]   at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)
[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)
[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)
[info]   at org.apache.spark.security.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[info]   at java.base/java.lang.Thread.run(Thread.java:829)
[error] Uncaught exception when running org.apache.spark.sql.execution.python.RowQueueSuite: java.lang.ExceptionInInitializerError
[error] sbt.ForkMain$ForkError: java.lang.ExceptionInInitializerError: null
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)
[error] 	at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)
[error] 	at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)
[error] 	at org.apache.spark.security.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)
[error] 	at org.apache.spark.security.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)
[error] 	at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)
[error] 	at scala.Option.map(Option.scala:230)
[error] 	at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)
[error] 	at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)
[error] 	at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)
[error] 	at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)
[error] 	at org.apache.spark.security.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)
[error] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[error] 	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[error] 	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[error] 	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[error] 	at org.scalatest.Transformer.apply(Transformer.scala:22)
[error] 	at org.scalatest.Transformer.apply(Transformer.scala:20)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[error] 	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[error] 	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[error] 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[error] 	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[error] 	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227
[error] 	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[error] 	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413
[error] 	at scala.collection.immutable.List.foreach(List.scala:431)
[error] 	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[error] 	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[error] 	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[error] 	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[error] 	at org.scalatest.Suite.run(Suite.scala:1114)
[error] 	at org.scalatest.Suite.run$(Suite.scala:1096)
[error] 	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[error] 	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[error] 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[error] 	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[error] 	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[error] 	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[error] 	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[error] 	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[error] 	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[error] 	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[error] 	at java.base/java.lang.Thread.run(Thread.java:829)
[error] Caused by: sbt.ForkMain$ForkError: java.lang.IllegalStateException: java.security.GeneralSecurityException: Native library is not loaded
[error] 	at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:67)
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)
[error] 	at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)
[error] 	at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)
[error] 	at org.apache.spark.security.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)
[error] 	at org.apache.spark.security.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)
[error] 	at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)
[error] 	at scala.Option.map(Option.scala:230)
[error] 	at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)
[error] 	at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)
[error] 	at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)
[error] 	at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)
[error] 	at org.apache.spark.security.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)
[error] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[error] 	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[error] 	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[error] 	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[error] 	at org.scalatest.Transformer.apply(Transformer.scala:22)
[error] 	at org.scalatest.Transformer.apply(Transformer.scala:20)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[error] 	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[error] 	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[error] 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[error] 	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[error] 	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227
[error] 	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[error] 	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413
[error] 	at scala.collection.immutable.List.foreach(List.scala:431)
[error] 	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[error] 	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[error] 	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[error] 	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[error] 	at org.scalatest.Suite.run(Suite.scala:1114)
[error] 	at org.scalatest.Suite.run$(Suite.scala:1096)
[error] 	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[error] 	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[error] 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[error] 	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[error] 	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[error] 	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[error] 	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[error] 	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[error] 	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[error] 	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[error] 	at java.base/java.lang.Thread.run(Thread.java:829)
[error] Caused by: sbt.ForkMain$ForkError: java.security.GeneralSecurityException: Native library is not loaded
[error] 	at org.apache.commons.crypto.random.OpenSslCryptoRandom.checkNative(OpenSslCryptoRandom.java:79)
[error] 	at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:65)
[error] 	at java.base/java.lang.Class.forName0(Native Method)
[error] 	at java.base/java.lang.Class.forName(Class.java:398)
[error] 	at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)
[error] 	at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)
[error] 	at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)
[error] 	at org.apache.spark.security.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)
[error] 	at org.apache.spark.security.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)
[error] 	at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)
[error] 	at scala.Option.map(Option.scala:230)
[error] 	at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)
[error] 	at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)
[error] 	at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)
[error] 	at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)
[error] 	at org.apache.spark.security.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)
[error] 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[error] 	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[error] 	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[error] 	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[error] 	at org.scalatest.Transformer.apply(Transformer.scala:22)
[error] 	at org.scalatest.Transformer.apply(Transformer.scala:20)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[error] 	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[error] 	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[error] 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)
[error] 	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[error] 	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227
[error] 	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[error] 	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413
[error] 	at scala.collection.immutable.List.foreach(List.scala:431)
[error] 	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[error] 	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[error] 	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[error] 	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[error] 	at org.scalatest.Suite.run(Suite.scala:1114)
[error] 	at org.scalatest.Suite.run$(Suite.scala:1096)
[error] 	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[error] 	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[error] 	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[error] 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)
[error] 	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[error] 	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[error] 	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[error] 	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)
[error] 	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[error] 	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[error] 	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[error] 	at java.base/java.lang.Thread.run(Thread.java:829)
[info] Run completed in 1 second, 175 milliseconds.
[info] Total number of tests run: 2
[info] Suites: completed 0, aborted 1
[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0
[info] *** 1 SUITE ABORTED ***
[error] Error during tests:
[error] 	org.apache.spark.sql.execution.python.RowQueueSuite
[error] (sql / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 176 s (02:56), completed Feb 24, 2023, 12:30:57 AM
```","1,-1    ",1,-1,0
1443132354,9700541,2023/2/21,v3.4.0-rc1,"In short, it's `Native library is not loaded` error. Could you verify from your side, @LuciferYang ?
```
java.lang.IllegalStateException: java.security.GeneralSecurityException: Native library is not loaded
[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:67)
```","1,-1    ",1,-1,0
1443153442,1475305,2023/2/21,v3.4.0-rc1,"Thanks @dongjoon-hyun, it's my bad, I hit same issue, let me revert this one, I will remember check this part next time.

","1,-1    ",1,-1,0
1443155202,9700541,2023/2/21,v3.4.0-rc1,"Well, let me revert directly~","1,-1    ",1,-1,0
1443155790,9700541,2023/2/21,v3.4.0-rc1,"Since it's on master only, it's easier. ","1,-1    ",1,-1,0
1443157552,1475305,2023/2/21,v3.4.0-rc1,"ok, thanks @dongjoon-hyun ","1,-1    ",1,-1,0
1443158499,9700541,2023/2/21,v3.4.0-rc1,It's reverted via https://github.com/apache/spark/commit/ca22c41eb30c6ff4588cf4e91252fe93ae946236 .,"1,-1    ",1,-1,0
1443184863,9700541,2023/2/21,v3.4.0-rc1,"FYI, I also confimed that the revert commit recovers the following streaming module test, too.
```
org.apache.spark.streaming.ReceivedBlockHandlerWithEncryptionSuite
```","1,-1    ",1,-1,0
1435967063,1475305,2023/2/21,v3.4.0-rc1,test first,"1,-1    ",1,-1,0
1437092078,822522,2023/2/21,v3.4.0-rc1,Merged to master,"1,-1    ",1,-1,0
1436005281,822522,2023/2/21,v3.4.0-rc1,"On these, can you give any info about why the upgrade is useful or important?","1,-1    ",1,-1,0
1436279676,1475305,2023/2/21,v3.4.0-rc1,"Updated pr description, listing bug fix and possibly useful improvements","1,-1    ",1,-1,0
1437094016,822522,2023/2/21,v3.4.0-rc1,Merged to master,"1,-1    ",1,-1,0
1436148025,6477701,2023/2/21,v3.4.0-rc1,Merged to master.,"1,-1    ",1,-1,0
1436158404,6477701,2023/2/22,v3.4.0-rc1,+1 to raise a discussion thread in mailing list,"1,-1    ",1,-1,0
1436161575,112507318,2023/2/22,v3.4.0-rc1,Absolutely! Will start a thread @dongjoon-hyun.,"1,-1    ",1,-1,0
1436161884,9700541,2023/2/22,v3.4.0-rc1,"Thank you, @allanf-db and @HyukjinKwon .","1,-1    ",1,-1,0
1436162671,9700541,2023/2/22,v3.4.0-rc1,"BTW, I changed the affected version of this JIRA from 3.4.0 to 3.5.0.
I don't think Apache Spark 3.4 needs this in any cases. To simply put, it's too late.","1,-1    ",1,-1,0
1436200153,112507318,2023/2/22,v3.4.0-rc1,"Attaching screenshot illustrating the change.
Current page on the left - Scala code example on the first tab.
New page on the right - Python code example on the first tab.

<img width=""1706"" alt=""making_python_first_tab"" src=""https://user-images.githubusercontent.com/112507318/219992905-c4c079d8-ae3f-4462-b41e-2de38d83e6d8.png"">
","1,-1    ",1,-1,0
1438896328,228859,2023/2/22,v3.4.0-rc1,"We should discuss this on the list, but I'm +1 on making Python first (in fact I suggested to Allan to do this). There are way more users of Spark in Python than Scala today, and people who want to see another language can just click that other tab. (In fact, if we want to make this better, we could even remember the language preference in a cookie and flip all tabs at the same time when you click one). Even our homepage at https://spark.apache.org has used Python for its example for a long time.","1,-1    ",1,-1,0
1438916416,822522,2023/2/22,v3.4.0-rc1,(FWIW I'm OK with showing Python first too),"1,-1    ",1,-1,0
1438919638,9700541,2023/2/22,v3.4.0-rc1,"Of course, I'm also open for discussion. IIRC, there are several issues like feature parity because Scala is still our first development language.","1,-1    ",1,-1,0
1447444815,6477701,2023/2/22,v3.4.0-rc1,"Per the discussion in https://lists.apache.org/thread/1p8s09ysrh4jqsfd47qdtrl7rm4rrs05, I will merge this in with few days if there's no objection.","1,-1    ",1,-1,0
1451083487,6477701,2023/2/22,v3.4.0-rc1,Merged to master.,"1,-1    ",1,-1,0
1451089176,9700541,2023/2/22,v3.4.0-rc1,Thank you all for initiating the official discussion. I also agree with the decision to merge this for Apache Spark 3.5.0.,"1,-1    ",1,-1,0
1436181584,6477701,2023/2/22,v3.4.0-rc1,cc @gengliangwang ,"1,-1    ",1,-1,0
1436312194,6477701,2023/2/22,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1436780504,9616802,2023/2/22,v3.4.0-rc1,"I forgot to update it. TBH since these are verbatim copies of the existing API, I think there is a case to be made to use the original version instead the version of when the client was built.","1,-1    ",1,-1,0
1436847091,6477701,2023/2/22,v3.4.0-rc1,"Or we can remove all `@since *` and just add one `@since 3.4.0` at the top level of the connect. And then, let the new (master branch only) APIs to have `@since 3.5.0` in the next release.","1,-1    ",1,-1,0
1436849553,6477701,2023/2/22,v3.4.0-rc1,Or just grip and replace `@since *` to `@since 3.4.0` ...,"1,-1    ",1,-1,0
1437356607,9700541,2023/2/22,v3.4.0-rc1,"When we add new languages like Python/R, did you use the same versions of Scala API, @hvanhovell ? They were also a wrappers on top of Scala API.

> I forgot to update it. TBH since these are verbatim copies of the existing API, I think there is a case to be made to use the original version instead the version of when the client was built.

","2,-1    ",2,-1,1
1437370469,1475305,2023/2/22,v3.4.0-rc1,"Personally, I think it is a little strange to change `@since `. For example,  the following function is changed to `@since 3.4.0`, but `deprecated` since 2.1.0 ....

https://github.com/apache/spark/blob/1688a8768fb34060548f8790e77f645027f65db2/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/functions.scala#L221-L226","2,-1    ",2,-1,1
1437378292,9700541,2023/2/22,v3.4.0-rc1,"That's a good question. Maybe, what about avoiding this propagation of deprecated methods, @hvanhovell , @HyukjinKwon , @LuciferYang ? Since we already deprecated these, it sounds more consistent with what Apache Spark has been claiming (not to use this APIs in new use cases).","1,-3    ",1,-3,-2
1437382771,1475305,2023/2/22,v3.4.0-rc1,"+1, Agree `avoiding this propagation of deprecated methods`","1,-1    ",1,-1,0
1437726276,9616802,2023/2/22,v3.4.0-rc1,"I have updated the since tags.

A couple of things I would like to highlight. The scala clients tries to be as compatible as possible (both source and binary) with the exiting API. That is why this is different that API that is built on top of the scala API, that was new, this however is not, it should be or at least aspires to be the same API. Hence my argument that you could also use the original since tags. Anyway I am fine with updating them, like I did in other PRs, I just missed it in this case.

As for the removal of deprecated methods. Again we want to be as compatible as possible, so I want to given users access to them. We aspire that in the best case a user can migrate their code to connect without any issues (preferably no recompilation). I don't think we should ever remove these methods, they are poorly named that is all. They do not inhibit us in any way to keep evolving Spark since they are just aliases. I have kept out a few deprecated methods in the current connect Dataset, but I do intent to bring them back once we can support them.

","1,-1    ",1,-1,0
1436809590,5399861,2023/2/22,v3.4.0-rc1,"Merged to master, branch-3.4 and branch-3.3.","1,-1    ",1,-1,0
1436374948,26535726,2023/2/22,v3.4.0-rc1,"Based on https://github.com/apache/parquet-mr/pull/982#issuecomment-1376750703, I guess that the Parquet community may think it's not a critical issue, but in my case, it's critical.","1,-1    ",1,-1,0
1436389833,1475305,2023/2/22,v3.4.0-rc1,"banch-3.3/3.2 use parquet 1.12.2, if this fix is accepted, would you mind submitting pr for these two branches? @pan3793 

","3,-1    ",3,-1,2
1436411798,26535726,2023/2/22,v3.4.0-rc1,"> banch-3.3/3.2 use parquet 1.12.2, if this fix is accepted, would you mind submitting pr for these two branches? @pan3793

sure.","1,-1    ",1,-1,0
1436414874,26535726,2023/2/22,v3.4.0-rc1,I verified this patch by scanning a large parquet/zstd table and updated the PR description.,"1,-1    ",1,-1,0
1437353508,9700541,2023/2/22,v3.4.0-rc1,"Thank you all. And, feel free to merge to land this to the release branches, @sunchao .

cc @cloud-fan , @HyukjinKwon , @mridulm , @tgravescs ","1,-1    ",1,-1,0
1437363350,506679,2023/2/22,v3.4.0-rc1,Thanks! merged to master/branch-3.3/branch-3.2,"1,-3    ",1,-3,-2
1437364337,1475305,2023/2/22,v3.4.0-rc1,@sunchao also need merge to branch-3.4 ...,"1,-1    ",1,-1,0
1437366108,506679,2023/2/22,v3.4.0-rc1,"Yes, it's in branch-3.4 as well ","2,-2    ",2,-2,0
1437098718,6477701,2023/2/22,v3.4.0-rc1,"This seems duplicating a lot of the existing quickstart.
Should we maybe just mention in the original quickstart, and have a separate quickstart for connect?
We should demonstrate the idea of being separate client and server Ideally.","1,-2    ",1,-2,-1
1437101747,6477701,2023/2/22,v3.4.0-rc1,"note for myself, I checked this PR by https://mybinder.org/v2/gh/itholic/spark.git/SPARK-42475?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_connect.ipynb","1,-1    ",1,-1,0
1441807965,47577197,2023/2/22,v3.4.0-rc1,"Can we replace  ""SparkSession.builder.master(""local[*]"").getOrCreate().stop()"" 
with just spark.stop() ?


","1,-1    ",1,-1,0
1442676018,6477701,2023/2/22,v3.4.0-rc1,"> Can we replace ""SparkSession.builder.master(""local[*]"").getOrCreate().stop()""
> with just spark.stop() ?

This was actually my suggestion to explicitly show the diff between SparkSession.builder.master vs SparkSession.builder.remote ;-).","2,-1    ",2,-1,1
1442708777,44108233,2023/2/22,v3.4.0-rc1,"> This was actually my suggestion to explicitly show the diff between SparkSession.builder.master vs SparkSession.builder.remote ;-).

Yes, and additionally I considered the case where the user does not use Spark through the `pyspark` shell command or does not give the name `spark` to the Spark session.

Thanks for the suggestion!","1,-1    ",1,-1,0
1442711503,6477701,2023/2/22,v3.4.0-rc1,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1437782372,5399861,2023/2/22,v3.4.0-rc1,cc @cloud-fan ,"1,-1    ",1,-1,0
1445252943,5399861,2023/2/22,v3.4.0-rc1,"TiDB also supports these optimizations:
<img width=""857"" alt=""image"" src=""https://user-images.githubusercontent.com/5399861/221442994-5b350e5d-76b0-4cbe-a0b8-fb3c39bb2cd3.png"">

","1,-1    ",1,-1,0
1437738671,7322292,2023/2/22,v3.4.0-rc1,"I think we should ignore the `plan_id` when comparing the plans, since it depends on the order of invocation and is not stable.","1,-2    ",1,-2,-1
1437703777,9700541,2023/2/22,v3.4.0-rc1,"Unfortunately, this seems to be unable to mitigate it.","1,-2    ",1,-2,-1
1437846634,47337188,2023/2/22,v3.4.0-rc1,Sorry I may be out of context. Do we happen to have an overview of the situation of 3.4 CI? I removed the RC tag just now. Let me know if there is something I could help with.,"1,-1    ",1,-1,0
1437963238,9700541,2023/2/22,v3.4.0-rc1,"Hi, @xinrong-meng . The branch-3.4 CI is still broken and I made the second PR to mitigate it.
- https://github.com/apache/spark/pull/40101","1,-1    ",1,-1,0
1439518530,19235986,2023/2/22,v3.4.0-rc1,"@zhengruifeng 

Would you update the PR description to attach the list of classes that are moved to mllib-common ? ","1,-1    ",1,-1,0
1439528700,19235986,2023/2/22,v3.4.0-rc1,"@zhengruifeng If the PR is ready for review, would you remove the WIP and draft mark on PR ?","1,-1    ",1,-1,0
1441155203,19235986,2023/2/22,v3.4.0-rc1,"> This PR also copies following testsuites to spark-mllib-common:
> 1, org.apache.spark.ml.attribute.*
> 2, org.apache.spark.ml.linalg.* except:
> 
> test(""JavaTypeInference with VectorUDT"") in VectorUDTSuite due to cyclical dependency;
> 3, org.apache.spark.ml.param.* except:
> 
> test(""Filtering ParamMap"") in ParamsSuite due to cyclical dependency;

This is bad for code maintenance. Can we move these tests to a ""ml-common-test"" jar, and in ml-client / ml-server tests they both run tests in ""ml-common-test"" jar ?","1,-1    ",1,-1,0
1441536275,7322292,2023/2/22,v3.4.0-rc1,"> > This PR also copies following testsuites to spark-mllib-common:
> > 1, org.apache.spark.ml.attribute.*
> > 2, org.apache.spark.ml.linalg.* except:
> > test(""JavaTypeInference with VectorUDT"") in VectorUDTSuite due to cyclical dependency;
> > 3, org.apache.spark.ml.param.* except:
> > test(""Filtering ParamMap"") in ParamsSuite due to cyclical dependency;
> 
> This is bad for code maintenance. Can we move these tests to a ""ml-common-test"" jar, and in ml-client / ml-server tests they both run tests in ""ml-common-test"" jar ?

let me take a look","1,-1    ",1,-1,0
1442685177,7322292,2023/2/22,v3.4.0-rc1,convert to draft since we will focus on `pyspark.ml` first,"1,-1    ",1,-1,0
1456200587,19235986,2023/2/22,v3.4.0-rc1,@zhengruifeng Is it ready for another pass review ?,"1,-1    ",1,-1,0
1457733081,7322292,2023/2/22,v3.4.0-rc1,@WeichenXu123  I think it is ready for review,"1,-1    ",1,-1,0
1467213725,19235986,2023/2/22,v3.4.0-rc1,"For the 2 exceptions:
> 2, org.apache.spark.ml.linalg.* except VectorUDTSuite due to cyclical dependency; (it copies the VectorUDTSuite except test(""JavaTypeInference with VectorUDT""))
> 3, org.apache.spark.ml.param.* except ParamsSuite due to cyclical dependency; (it copies the ParamsSuite except test(""Filtering ParamMap""))


You can move ""JavaTypeInference with VectorUDT"" out of VectorUDTSuite, and move ""Filtering ParamMap"" out of ParamsSuite, and then can move remaining test code without duplication.","1,-2    ",1,-2,-1
1467297939,7322292,2023/2/22,v3.4.0-rc1,"> For the 2 exceptions:
> 
> > 2, org.apache.spark.ml.linalg.* except VectorUDTSuite due to cyclical dependency; (it copies the VectorUDTSuite except test(""JavaTypeInference with VectorUDT""))
> > 3, org.apache.spark.ml.param.* except ParamsSuite due to cyclical dependency; (it copies the ParamsSuite except test(""Filtering ParamMap""))
> 
> You can move ""JavaTypeInference with VectorUDT"" out of VectorUDTSuite, and move ""Filtering ParamMap"" out of ParamsSuite, and then can move remaining test code without duplication.

good point, done","1,-1    ",1,-1,0
1467715623,7322292,2023/2/23,v3.4.0-rc1,cc @hvanhovell @grundprinzip @srowen @HyukjinKwon would you mind taking a look?,"1,-1    ",1,-1,0
1469004142,6477701,2023/2/23,v3.4.0-rc1,Merged to master.,"1,-1    ",1,-1,0
1439610519,12025282,2023/2/23,v3.4.0-rc1,"cc @viirya @cloud-fan , thank you","1,-1    ",1,-1,0
1437930816,1475305,2023/2/23,v3.4.0-rc1,"Adding test cases

","1,-1    ",1,-1,0
1437946552,5399861,2023/2/23,v3.4.0-rc1,cc @zhengruifeng ,"1,-2    ",1,-2,-1
1437975144,7322292,2023/2/23,v3.4.0-rc1,"I'm not sure whether we can use `child.maxRowsPerPartition` in a global sort, for example, the partition sizes maybe [5, 5, 5] in child, but after global sort the distribution maybe [3, 3, 3, 3, 3]

cc @cloud-fan ","1,-1    ",1,-1,0
1438028048,5399861,2023/2/23,v3.4.0-rc1,"Thank you @zhengruifeng, makes sense.","1,-1    ",1,-1,0
1437962223,9700541,2023/2/23,v3.4.0-rc1,"Hi, @xinrong-meng , @HyukjinKwon , @cloud-fan 
I decided to simplify the test case to be more robust in terms of the order of merged schema.
This will unblock Apache Spark 3.4.0 RC1.","1,-1    ",1,-1,0
1438035954,1475305,2023/2/23,v3.4.0-rc1,"or catch `TestFailedException` then check another set of parameters ?

","1,-1    ",1,-1,0
1438168519,47337188,2023/2/23,v3.4.0-rc1,LGTM. Thanks @dongjoon-hyun !,"1,-1    ",1,-1,0
1438172180,47337188,2023/2/23,v3.4.0-rc1,"Thanks all, merged to master and branch-3.4","1,-1    ",1,-1,0
1438687259,9700541,2023/2/23,v3.4.0-rc1,"Thank you, @xinrong-meng , @HyukjinKwon , @LuciferYang .","1,-1    ",1,-1,0
1438371092,6477701,2023/2/23,v3.4.0-rc1,"Merged to master, branch-3.4, branch-3.3 and branch-3.2.","2,-1    ",2,-1,1
1439566773,1580697,2023/2/23,v3.4.0-rc1,"@cloud-fan Could you review this PR, please.","2,-1    ",2,-1,1
1439824905,1580697,2023/2/23,v3.4.0-rc1,"Merging to master/3.4/3.3. Thank you, @cloud-fan for review.","2,-1    ",2,-1,1
1442754625,47337188,2023/2/23,v3.4.0-rc1,"May I get a review, please? @zhengruifeng @HyukjinKwon 

Also cc @grundprinzip @ueshin ","1,-1    ",1,-1,0
1444765441,47337188,2023/2/23,v3.4.0-rc1,"Merged to master and branch-3.4, thanks all!","1,-1    ",1,-1,0
1438769481,1475305,2023/2/23,v3.4.0-rc1,cc @HyukjinKwon @hvanhovell @dongjoon-hyun FYI,"2,-1    ",2,-1,1
1438880126,9616802,2023/2/23,v3.4.0-rc1,"@LuciferYang what else are you working on? I am preparing a PR to add most of the remaining functions, it would be a pity if we duplicate our efforts.","1,-1    ",1,-1,0
1438898130,1475305,2023/2/23,v3.4.0-rc1,"@hvanhovell The original plan is `Collection functions`. However, I have encountered some problems in `LambdaFunction` that I haven't solved it yet.  So you can submit new prs freely. If possible, please let me help to review, thanks ~


","1,-1    ",1,-1,0
1439018843,9616802,2023/2/23,v3.4.0-rc1,@LuciferYang let me take a look at the collections functions.,"1,-1    ",1,-1,0
1439281948,9616802,2023/2/23,v3.4.0-rc1,Merged to master/3.4. Thanks for doing this!,"1,-1    ",1,-1,0
1438859255,1938382,2023/2/23,v3.4.0-rc1,@cloud-fan @techaddict,"1,-1    ",1,-1,0
1439231653,6477701,2023/2/23,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1439031825,1938382,2023/2/23,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1439541201,3182036,2023/2/23,v3.4.0-rc1,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1439554576,1475305,2023/2/23,v3.4.0-rc1,"Good work, late LGTM","1,-1    ",1,-1,0
1439378554,506656,2023/2/23,v3.4.0-rc1,"The tests fail with the error:

```
pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.UNAVAILABLE
	details = ""failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:44883: Failed to connect to remote host: Connection refused""
	debug_error_string = ""UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:44883: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:""2023-02-22T00:04:47.405869565+00:00""}""
>
```

They pass in my local env, though.

cc @HyukjinKwon @zhengruifeng ","1,-1    ",1,-1,0
1439537460,6477701,2023/2/23,v3.4.0-rc1,let me take a quick look,"1,-1    ",1,-1,0
1439859513,6477701,2023/2/23,v3.4.0-rc1,okay .. finally found out why .. let me create a PR ..,"1,-1    ",1,-1,0
1440215857,9616802,2023/2/23,v3.4.0-rc1,retriggered tests,"1,-1    ",1,-1,0
1440679008,1938382,2023/2/23,v3.4.0-rc1,LGTM,"2,-1    ",2,-1,1
1440781686,9616802,2023/2/23,v3.4.0-rc1,merging this,"1,-1    ",1,-1,0
1439350799,6477701,2023/2/23,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1439356239,6477701,2023/2/23,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1439508403,6477701,2023/2/23,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1443578223,8486025,2023/2/23,v3.4.0-rc1,https://github.com/apache/spark/pull/40159 will replace this one.,"3,-1    ",3,-1,2
1439465658,5399861,2023/2/23,v3.4.0-rc1,cc @cloud-fan ,"1,-1    ",1,-1,0
1464940304,5399861,2023/2/23,v3.4.0-rc1,"This is a real case from production: `SELECT ... LIMIT 5000`
Before this PR | After this PR
-- | --
<img src=""https://user-images.githubusercontent.com/5399861/224493524-a0aac742-025f-4e1b-9551-82d07f32bce0.jpg"" width=""400"" height=""750""> | <img src=""https://user-images.githubusercontent.com/5399861/224493280-09d487db-be57-4a80-8051-1c1ab8bf3109.jpg"" width=""400"" height=""750"">
","2,-1    ",2,-1,1
1500110950,5399861,2023/2/23,v3.4.0-rc1,"Date | No. of queries optimized by this patch | No. of total queries
-- | -- | --
2023/4/5 | 62 | 167608
2023/4/4 | 139 | 203393
2023/4/3 | 62 | 191147
2023/4/2 | 14 | 133519
2023/4/1 | 10 | 175728

","1,-1    ",1,-1,0
1439646748,5275075,2023/2/23,v3.4.0-rc1,cc @cloud-fan @wangyum ,"1,-1    ",1,-1,0
1441144948,5399861,2023/2/23,v3.4.0-rc1,@zml1206 Could you update the PR title to `[SPARK-42525][SQL] Collapse ...`?,"2,-1    ",2,-1,1
1445259053,5399861,2023/2/23,v3.4.0-rc1,Merged to master.,"1,-1    ",1,-1,0
1445778387,3182036,2023/2/23,v3.4.0-rc1,the change LGTM but the PR title is a bit confusing. How is it related to subquery?,"2,-1    ",2,-1,1
1445889648,5275075,2023/2/23,v3.4.0-rc1,"> the change LGTM but the PR title is a bit confusing. How is it related to subquery?

subquery is one of the cases where the qualifiers are differentï¼it's really confusing, is there anything else I can do to modify it","1,-1    ",1,-1,0
1445925729,3182036,2023/2/23,v3.4.0-rc1,`... with semantically-same partition/order`,"1,-1    ",1,-1,0
1441434811,7322292,2023/2/23,v3.4.0-rc1,"I guess you may need to `Go to âActionsï¿½?tab on your forked repository and enable âBuild and testï¿½?and âReport test resultsï¿½?workflows`

https://spark.apache.org/contributing.html","2,-1    ",2,-1,1
1442010487,13139216,2023/2/23,v3.4.0-rc1,> Did that,"1,-1    ",1,-1,0
1445132726,822522,2023/2/23,v3.4.0-rc1,"Eh, this does not explain the issue at all. Please do so.","1,-1    ",1,-1,0
1445232031,13139216,2023/2/23,v3.4.0-rc1,I have enabled the workflows on the branch. Is there something else that I need to do?,"1,-1    ",1,-1,0
1445233793,13139216,2023/2/23,v3.4.0-rc1,Sean not sure which issue you were referring to. I updated the why the changes are needed section of the pull request to mirror what Zheng had already put in his pull request.  ,"1,-1    ",1,-1,0
1445261438,822522,2023/2/23,v3.4.0-rc1,"This is about SPARK-41391? it also doesn't contain a simple description of what you're reporting, just code snippets. I can work it out, but this could be explained in just a few sentences","1,-1    ",1,-1,0
1445261458,822522,2023/2/23,v3.4.0-rc1,Please fix the PR description too https://spark.apache.org/contributing.html,"1,-2    ",1,-2,-1
1445296889,13139216,2023/2/23,v3.4.0-rc1,Sean  I tried to correct the two things pointed  out by you. Let me know if that works,"1,-1    ",1,-1,0
1445388789,822522,2023/2/23,v3.4.0-rc1,Looks better. Title should start with `[SPARK-41391]` to link it. Please include the description in the title; there is nothing there now,"1,-1    ",1,-1,0
1446869584,13139216,2023/2/23,v3.4.0-rc1,Not sure how my checkins are causing javadoc genration error,"1,-1    ",1,-1,0
1446881913,822522,2023/2/23,v3.4.0-rc1,It's the `[[Star]]` in the scaladoc you added. Just don't make it a reference,"1,-1    ",1,-1,0
1449133178,13139216,2023/2/23,v3.4.0-rc1,Is there anything else that I need to do for the fix to be accepted?,"1,-1    ",1,-1,0
1449140455,822522,2023/2/23,v3.4.0-rc1,@cloud-fan or @HyukjinKwon do you have an opinion?,"1,-1    ",1,-1,0
1451285915,13139216,2023/2/23,v3.4.0-rc1,"Not sure why the suggested changes made the build fail in the 
catalyst,hive-thriftserver module  and
sql-other test module.
2023-03-01T22:23:36.6700903Z Error instrumenting class:org.apache.spark.sql.execution.streaming.state.SchemaHelper$SchemaV2Reader2023-03-01T22:23:36.8662344Z Error instrumenting class:org.apache.spark.sql.v2.avro.AvroScan
2023-03-01T22:23:36.8712474Z Error instrumenting class:org.apache.spark.api.python.DoubleArrayWritable","1,-1    ",1,-1,0
1453013580,13139216,2023/2/23,v3.4.0-rc1,"@cloud-fan  always using unresolvedAlias seems to be causing the sql-other module to fail. Will be reverting to the original fix of creating unresolvedAlias only for ""*"" or distinct.","1,-1    ",1,-1,0
1453976515,13139216,2023/2/24,v3.4.0-rc1,Any comments. Apparently having all expr as unresolvedAlias is not working. ,"1,-1    ",1,-1,0
1459854807,3182036,2023/2/24,v3.4.0-rc1,"> Apparently having all expr as unresolvedAlias is not working.

Can you share the test failures? Maybe we just need to update the tests with the different alias name.","1,-1    ",1,-1,0
1460579037,13139216,2023/2/24,v3.4.0-rc1,"[7_Run  Build modules sql - other tests.txt](https://github.com/apache/spark/files/10923393/7_Run.Build.modules.sql.-.other.tests.txt)
","1,-1    ",1,-1,0
1463337176,3182036,2023/2/24,v3.4.0-rc1,"I think the test is easy to fix. It wants to test the aggregate function result, but not the generated alias, so we just change the testing query to add alias explicitly.
```
val avgDF = intervalData.select(
      avg($""year-month"").as(""a1""),
      avg($""year"").as(""a2""),
      ...
```","1,-2    ",1,-2,-1
1464842147,13139216,2023/2/24,v3.4.0-rc1,"> I think the test is easy to fix. It wants to test the aggregate function result, but not the generated alias, so we just change the testing query to add alias explicitly.
> 
> ```
> val avgDF = intervalData.select(
>       avg($""year-month"").as(""a1""),
>       avg($""year"").as(""a2""),
>       ...
> ```

Couple of questions

1.  Is it required and documented that we should add  alias with the aggregate functions? If that is not a requirement then fixing the test case is potentially  covering an issue.
2. The Thread leaks reported in the sql-other tests in not just from DataFrameAggregateSuite, but from multiple other suites

023-03-03T04:05:16.9822203Z 04:05:16.978 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 393.0 failed 1 times; aborting job
2023-03-03T04:05:16.9866693Z [0m[[0m[0minfo[0m] [0m[0m[32m- SPARK-30668: use legacy timestamp parser in to_timestamp (154 milliseconds)[0m[0m
2023-03-03T04:05:17.0464670Z [0m[[0m[0minfo[0m] [0m[0m[32m- SPARK-30752: convert time zones on a daylight saving day (62 milliseconds)[0m[0m
2023-03-03T04:05:17.1930942Z [0m[[0m[0minfo[0m] [0m[0m[32m- SPARK-30766: date_trunc of old timestamps to hours and days (142 milliseconds)[0m[0m
2023-03-03T04:05:17.3358608Z [0m[[0m[0minfo[0m] [0m[0m[32m- SPARK-30793: truncate timestamps before the epoch to seconds and minutes (146 milliseconds)[0m[0m
2023-03-03T04:05:17.3824844Z 04:05:17.382 WARN org.apache.spark.sql.DateFunctionsSuite: 
2023-03-03T04:05:17.3845065Z 
2023-03-03T04:05:17.3846873Z ===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.DateFunctionsSuite,  ","1,-1    ",1,-1,0
1465459756,3182036,2023/2/24,v3.4.0-rc1,"The auto-generated alias name is fragile and we are trying to improve it at https://github.com/apache/spark/pull/40126

Can you give some examples of how the new update changes the alias name? If it's not reasonable, we should keep the previous code.","1,-1    ",1,-1,0
1468635397,13139216,2023/2/24,v3.4.0-rc1,"> The auto-generated alias name is fragile and we are trying to improve it at #40126
> 
> Can you give some examples of how the new update changes the alias name? If it's not reasonable, we should keep the previous code.

I am attaching a file showing some failures when all the aggregate expressions were made UnresolvedAlias. My latest checkin where I only make those aggregate expressions that have ""*"" as UnresolvedAlias works. The build went through.So it is essentially the unresolvedstar() that is being produced by the toPrettySQL  for the agg expr with star that the Analyzer is not able to resolve. 
[sqlOtherTests.txt](https://github.com/apache/spark/files/10972570/sqlOtherTests.txt)
","1,-1    ",1,-1,0
1470374550,13139216,2023/2/24,v3.4.0-rc1,"Can anyone tell me how I am getting this single quote in count expression. Attaching the picture. This can potentially cause problems down the lane where tree nodes are compared in the transformDownWithPruning where the two nodes are not same because of this single quote
<img width=""1495"" alt=""Screen Shot 2023-03-15 at 9 34 51 AM"" src=""https://user-images.githubusercontent.com/13139216/225378952-74ba895b-2c36-407a-ab1c-7ad46b469ae7.png"">
","1,-1    ",1,-1,0
1471235372,3182036,2023/2/24,v3.4.0-rc1,"The single quote indicates that the expression is unresolved, I think it doesn't matter here.","1,-1    ",1,-1,0
1478932824,13139216,2023/2/24,v3.4.0-rc1,"Perhaps the following would be better solution. Instead of looking for star any  UnresolvedFunction should have UnresolvedAlias.  Any comments?

  private[this] def alias(expr: Expression): NamedExpression = expr match {
    case expr: NamedExpression => expr
    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))
    case expr: Expression =>
       if (expr.isInstanceOf[UnresolvedFunction]) {
        UnresolvedAlias(expr, None)
      } else {
         Alias(expr, toPrettySQL(expr))()
      }
  }","1,-1    ",1,-1,0
1482331121,3182036,2023/2/24,v3.4.0-rc1,"> any UnresolvedFunction should have UnresolvedAlias.

SGTM. Or more aggressively, any expression should have `UnresolvedAlias`, and update failed tests.","1,-1    ",1,-1,0
1483606678,13139216,2023/2/24,v3.4.0-rc1,Right. This is simple 1 file fix with addition of test case versus the other one which may involve number of files.,"1,-1    ",1,-1,0
1487331617,13139216,2023/2/24,v3.4.0-rc1,Please see if this fix can be pulled.,"1,-1    ",1,-1,0
1491261819,3182036,2023/2/24,v3.4.0-rc1,"thanks, merging to master!","1,-1    ",1,-1,0
1439507275,6477701,2023/2/24,v3.4.0-rc1,"cc @gengliangwang, sorry I missed two more tests.","1,-1    ",1,-1,0
1439875276,6477701,2023/2/24,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1477559688,52876270,2023/2/24,v3.4.0-rc1,"Hello @dongjoon-hyun @holdenk , please help review this PR, given that there is a lot of user feedback on JIRA about this issue","1,-1    ",1,-1,0
1487539939,59893,2023/2/24,v3.4.0-rc1,"Overall I like it, but I would want to see a unit test (does not need to go all the way to exit but something to show that the code would match).","1,-1    ",1,-1,0
1487916674,52876270,2023/2/24,v3.4.0-rc1,"> Overall I like it, but I would want to see a unit test (does not need to go all the way to exit but something to show that the code would match).

Thanks for your review.

Added unit test for `LoggingPodStatusWatcher` get driver exit code after receive event.","2,-1    ",2,-1,1
1439607365,7322292,2023/2/24,v3.4.0-rc1,cc @WeichenXu123 ,"2,-1    ",2,-1,1
1439822577,19235986,2023/2/24,v3.4.0-rc1,Reminder: backport to 3.4 branch.,"1,-1    ",1,-1,0
1439825458,7322292,2023/2/24,v3.4.0-rc1,"sure, merged into master/branch-3.4","2,-1    ",2,-1,1
1440180933,9616802,2023/2/24,v3.4.0-rc1,Merging this to master/3.4. Thanks for doing this!,"1,-2    ",1,-2,-1
1441131900,1475305,2023/2/24,v3.4.0-rc1,Thanks @hvanhovell ,"1,-2    ",1,-2,-1
1440741016,1591700,2023/2/24,v3.4.0-rc1,"Can we minimize diff's to this file ? A large fraction is whitespace changes and due to the renames ... will take a look at the changes as well.

Also given this is an optimization change - include benchmark to quantify the impact ?","1,-1    ",1,-1,0
1441264762,265981,2023/2/24,v3.4.0-rc1,"> Also given this is an optimization change - include benchmark to quantify the impact ?

I did benchmarking live in a cluster. Profiles before show ~1% of scheduler time in PercentileHeap operations. Profiles after do not have PercentileHeap operations at all.","1,-1    ",1,-1,0
1441267376,265981,2023/2/24,v3.4.0-rc1,"> Can we minimize diff's to this file ? A large fraction is whitespace changes and due to the renames ... will take a look at the changes as well.

Can you treat is a new implementation? There is only 15 lines (L55-L70) that matter on the new implementation - the code inside `insert`. Outside of `insert` there is nothing meaty or interesting.","1,-1    ",1,-1,0
1441443226,1591700,2023/2/24,v3.4.0-rc1,"> I did benchmarking live in a cluster. Profiles before show ~1% of scheduler time in PercentileHeap operations. Profiles after do not have PercentileHeap operations at all.


Can you add a benchmark in the PR ? With results for best and after in description or as comment ? Thanks !
For example, take a look at `core/src/test/scala/org/apache/spark/MapStatusesSerDeserBenchmark.scala`","1,-1    ",1,-1,0
1441455987,265981,2023/2/24,v3.4.0-rc1,"I ran this benchmark offline:

```
  test(""benchmark"") {
    val input: Seq[Int] = 0 until 1000
    val numRuns = 1000

    def kernel(): Long = {
      val shuffled = Random.shuffle(input).toArray
      val start = System.nanoTime()
      val h = new PercentileHeap(0.95)
      shuffled.foreach { x =>
        h.insert(x)
        for (_ <- 0 until input.length) h.percentile
      }
      System.nanoTime() - start
    }
    for (_ <- 0 until numRuns) kernel()  // warmup

    var elapsed: Long = 0
    for (_ <- 0 until numRuns) elapsed += kernel()
    val perOp = elapsed / (numRuns * input.length)
    println(s""$perOp ns per op on heaps of size ${input.length}"")
  }
```

Results:
```
    BEFORE 3886 ns per op on heaps of size 1000
    AFTER  1703 ns per op on heaps of size 1000 (with scala PriorityQueue) 
    AFTER    36 ns per op on heaps of size 1000 (with java PriorityQueue)
```

(yes 100x improvement is not a typo)

I left this test in the PR instead of a full blown benchmark.","2,-1    ",2,-1,1
1441535493,265981,2023/2/24,v3.4.0-rc1,"I updated the implementation and the description. TLDR I use a comparator-less java `PriorityQueue` now for a total of 100x speedup over the original implementation.

@mridulm good call on the benchmark, in my internal tests I had a handrolled heap implementation that was even faster than the java one. If not for the benchmark I wouldn't have noticed that Scala's priority queue is so bad vs Java's.","1,-1    ",1,-1,0
1445779613,3182036,2023/2/24,v3.4.0-rc1,"The failed HealthTrackerIntegrationSuite is definitely unrelated, I'm merging it to master, thanks!","2,-1    ",2,-1,1
1442753105,6477701,2023/2/24,v3.4.0-rc1,"qq,

> this is a breaking change to this experimental API.

What's breaking? would be good to keep the PR desc template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)","1,-1    ",1,-1,0
1445208101,3813695,2023/2/24,v3.4.0-rc1,"> qq,
> 
> > this is a breaking change to this experimental API.
> 
> What's breaking? would be good to keep the PR desc template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)

@HyukjinKwon I have changed the description to include the breaking change and some more details.","1,-1    ",1,-1,0
1466761262,3813695,2023/2/24,v3.4.0-rc1,@HyukjinKwon may I request for a review on this PR?,"1,-1    ",1,-1,0
1468164542,44700269,2023/2/24,v3.4.0-rc1,CC @cloud-fan @sunchao @zhengruifeng ,"1,-2    ",1,-2,-1
1439873394,6477701,2023/2/24,v3.4.0-rc1,cc @ueshin ,"1,-1    ",1,-1,0
1440540510,1938382,2023/2/24,v3.4.0-rc1,"LGTM Nice catch!

","2,-1    ",2,-1,1
1448303753,1825975,2023/2/24,v3.4.0-rc1,@HyukjinKwon can you review this please / find a reviewer for me?,"1,-1    ",1,-1,0
1493874156,1825975,2023/2/24,v3.4.0-rc1,@cloud-fan can this be merged?,"1,-1    ",1,-1,0
1496815727,3182036,2023/2/24,v3.4.0-rc1,"thanks, merging to master!","1,-1    ",1,-1,0
1440535383,1938382,2023/2/24,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1440709794,9616802,2023/2/24,v3.4.0-rc1,@amaliujia what existing PR cover this?,"1,-1    ",1,-1,0
1440713723,1938382,2023/2/24,v3.4.0-rc1,@hvanhovell this one https://github.com/apache/spark/pull/40057.  A few agg API that are built on top of that are not in Dataset.,"1,-1    ",1,-1,0
1440778768,9616802,2023/2/24,v3.4.0-rc1,@amaliujia those tests are not exercising the new code paths. Since the change is small I am fine with merging it though.,"1,-1    ",1,-1,0
1440783521,9616802,2023/2/24,v3.4.0-rc1,Merging this.,"1,-1    ",1,-1,0
1452182391,1580697,2023/2/24,v3.4.0-rc1,"@srielau Could you take a look at the PR one more time, please.","1,-2    ",1,-2,-1
1460079053,1580697,2023/2/24,v3.4.0-rc1,"@cloud-fan @srielau Could you take a look at the PR, please.","1,-1    ",1,-1,0
1460318660,3514644,2023/2/24,v3.4.0-rc1,"I have lost track of the externally described rules. Do you have an updated summary?
Especially since we recently discussed about what to do with whitespace
Also, do we plan to leave this OFF by default? If we don't make this default it will never get adoption because most folk just won't care. ","1,-1    ",1,-1,0
1460335905,1580697,2023/2/24,v3.4.0-rc1,"> Do you have an updated summary?

Yes, it is very easy to check - just look at the last commit.

> Especially since we recently discussed about what to do with whitespace

? @srielau Please, look at the PR changes first of all.

> Also, do we plan to leave this OFF by default?

Yes, Serge could you look at the PR's description and changes before asking the questions.","1,-1    ",1,-1,0
1460908751,3514644,2023/2/24,v3.4.0-rc1,"> > Do you have an updated summary?
> 
> Yes, it is very easy to check - just look at the last commit.
> 
> > Especially since we recently discussed about what to do with whitespace
> 
> ? @srielau Please, look at the PR changes first of all.
> 
> > Also, do we plan to leave this OFF by default?
> 
> Yes, Serge could you look at the PR's description and changes before asking the questions.
You sum up the root cause for a lot of our QA troubles right there....
Please assure that the description of the PR is accurate and up to date (or have a pointer to a google-doc that does the same).
We cannot reciew code against itself. We have to review it against a common understanding on what it is meant to do.


","1,-1    ",1,-1,0
1477333100,1580697,2023/2/24,v3.4.0-rc1,"Merging to master. Thank you, @srielau and @cloud-fan for review.","2,-1    ",2,-1,1
1440681650,9700541,2023/2/24,v3.4.0-rc1,"cc @xinrong-meng , @HyukjinKwon ","1,-1    ",1,-1,0
1441097534,9700541,2023/2/24,v3.4.0-rc1,"Thank you, @HyukjinKwon . Merged to master/3.4.","1,-1    ",1,-1,0
1449406133,109815907,2023/2/24,v3.4.0-rc1,@dongjoon-hyun @holdenk Can you please review this PR?,"1,-1    ",1,-1,0
1453938288,109815907,2023/2/24,v3.4.0-rc1,Gentle ping @dongjoon-hyun @holdenk @srowen ,"1,-2    ",1,-2,-1
1463227858,109815907,2023/2/24,v3.4.0-rc1,Gentle ping @holdenk,"1,-1    ",1,-1,0
1465517975,109815907,2023/2/24,v3.4.0-rc1,Gentle ping @holdenk ,"2,-1    ",2,-1,1
1472394127,109815907,2023/2/24,v3.4.0-rc1,Gentle ping @holdenk,"1,-1    ",1,-1,0
1476234703,109815907,2023/2/24,v3.4.0-rc1,Gentle ping @holdenk,"1,-1    ",1,-1,0
1480215990,109815907,2023/2/24,v3.4.0-rc1,"Hi @dongjoon-hyun 
The change to clean up the upload directory is not specific to HDFS. The reason we should do cleanup is because if the spark job is creating new directories/files, it should clean them up too just like it's being done in YARN and also for other files like shuffle spill. 
Also, can you please explain why the approach seems to be incomplete? How is unable to prevent leftover from upload directory?","1,-1    ",1,-1,0
1480402673,9700541,2023/2/24,v3.4.0-rc1,"@shrprasa . 

1. It seems that you have an assumption that Shutdown hook is magically reliable. However, shutdown hook has a well-known limitation where JVM can be destroyed abruptly and K8s Pod can be deleted also without giving the inside processes enough time to handle business logic.

2. As I mentioned in the above, public cloud storage systems have a better and complete TTL-based solution for that issue. In that context, this PR is only trying to mitigate HDFS issue, https://issues.apache.org/jira/browse/HDFS-6382, partially.

> The change to clean up the upload directory is not specific to HDFS. The reason we should do cleanup is because if the spark job is creating new directories/files, it should clean them up too just like it's being done in YARN and also for other files like shuffle spill.
Also, can you please explain why the approach seems to be incomplete? How is unable to prevent leftover from upload directory?","1,-1    ",1,-1,0
1480495550,109815907,2023/2/25,v3.4.0-rc1,"@dongjoon-hyun  Thanks for the clarification. But the unreliability for shutdown hook is common for all other shutdown tasks also. This doesn't mean we haven't impletened them. So, why this should stop us from adding the clean up logic. 
The  TTL-based solution is specific to the filesystem. It's nice to have but I don't understand why we should rely on external solution to perform cleanup on behalf of spark job and why not the job itself does the cleanup? What's the downside of doing this?","3,-1    ",3,-1,2
1482363384,109815907,2023/2/25,v3.4.0-rc1,Gentle Ping @dongjoon-hyun @holdenk ,"3,-1    ",3,-1,2
1486231326,109815907,2023/2/25,v3.4.0-rc1,Gentle Ping @dongjoon-hyun @holdenk,"1,-1    ",1,-1,0
1487537833,59893,2023/2/25,v3.4.0-rc1,"So as I think about it, what about if we did a cleanup as a script for the driver pod on terminate to address some of @dongjoon-hyun's concern?

The TTL based approach feels fundamentally ""better"" though. Certainly it is filesystem specific, but I think (besides) HDFS all of the major filesystems we would use support TTL yes? Perhaps we should just document this?","1,-1    ",1,-1,0
1492818493,109815907,2023/2/25,v3.4.0-rc1,"@holdenk @dongjoon-hyun TTL based clearnce has limitations as pointed out in this comment
https://github.com/apache/spark/pull/40363#issuecomment-1467439787","1,-1    ",1,-1,0
1517271816,109815907,2023/2/25,v3.4.0-rc1,@dongjoon-hyun @holdenk Can we have some conclusion on this issue?,"1,-1    ",1,-1,0
1517285071,59893,2023/2/25,v3.4.0-rc1,"I'm a soft +1, I think it's a useful incremental feature (even if imperfect) but if @dongjoon-hyun would rather not then I defer to his judgement here (I mostly run with object storage where TTLs are fine).","1,-1    ",1,-1,0
1532589815,109815907,2023/2/25,v3.4.0-rc1,@dongjoon-hyun can you please comment on whether we should be doing this change or not.,"1,-1    ",1,-1,0
1440844134,1938382,2023/2/25,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1441187112,9616802,2023/2/25,v3.4.0-rc1,merging,"1,-1    ",1,-1,0
1440776529,9616802,2023/2/25,v3.4.0-rc1,For the reviewer I have manually tested this with scala-2.13.,"1,-1    ",1,-1,0
1440776736,9616802,2023/2/25,v3.4.0-rc1,cc @LuciferYang ,"1,-1    ",1,-1,0
1441434091,1475305,2023/2/25,v3.4.0-rc1,"Checked the new test with Scala 2.13, all passed","1,-1    ",1,-1,0
1440862457,9700541,2023/2/25,v3.4.0-rc1,"Could you review this, @viirya ?","1,-1    ",1,-1,0
1440881422,9700541,2023/2/25,v3.4.0-rc1,Thank you so much! Merged to master/3.4!,"1,-1    ",1,-1,0
1440918027,9700541,2023/2/25,v3.4.0-rc1,"Could you review this too, @viirya ? I fixed some K8s documentation issues while reviewing and testing 3.4.0 RC1 and updated YuniKorn documentation with the latest information.","1,-2    ",1,-2,-1
1440934308,68855,2023/2/25,v3.4.0-rc1,Looks good to me.,"1,-1    ",1,-1,0
1440969626,9700541,2023/2/25,v3.4.0-rc1,"Thank you so much, @viirya . Merged to master/3.4.","1,-1    ",1,-1,0
1441018465,4190164,2023/2/25,v3.4.0-rc1,cc @grundprinzip ,"1,-2    ",1,-2,-1
1443634139,9616802,2023/2/25,v3.4.0-rc1,Merging to master/3.4,"1,-3    ",1,-3,-2
1441145746,7788766,2023/2/25,v3.4.0-rc1,cc @dongjoon-hyun @cloud-fan ,"1,-1    ",1,-1,0
1441208481,7788766,2023/2/25,v3.4.0-rc1,"@dongjoon-hyun I have addressed the comment, could you review again please? Thank you.

Also, do you know whom I can ping on this PR to approve DB2 SQL semantics?","1,-1    ",1,-1,0
1442450477,7788766,2023/2/25,v3.4.0-rc1,@dongjoon-hyun Is there anything else left on this PR to merge?,"1,-1    ",1,-1,0
1442703376,3182036,2023/2/25,v3.4.0-rc1,"thanks, merging to master/3.4 (bug fix)!","1,-1    ",1,-1,0
1442704002,3182036,2023/2/25,v3.4.0-rc1,"it has conflicts with 3.4, @sadikovi can you open a backport PR? thanks!","1,-1    ",1,-1,0
1442783758,7788766,2023/2/25,v3.4.0-rc1,"Yes, I will open a separate PR.","1,-1    ",1,-1,0
1442797402,7788766,2023/2/25,v3.4.0-rc1,PR for 3.4: https://github.com/apache/spark/pull/40155.,"1,-1    ",1,-1,0
1441159415,7322292,2023/2/25,v3.4.0-rc1,cc @HyukjinKwon @xinrong-meng ,"1,-1    ",1,-1,0
1441240940,47337188,2023/2/26,v3.4.0-rc1,"Shall we add an example to **Does this PR introduce any user-facing change?** in the PR description? Like

```py
>>> df3.show()
+---+----+------+----+
|age|name|height|name|
+---+----+------+----+
| 16| Bob|    85| Bob|
| 14| Tom|    80| Tom|
+---+----+------+----+

### BEFORE
>>> df3.drop(""name"", ""age"").columns
Traceback (most recent call last):
...
pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].

### AFTER
>>> df3.drop(""name"", ""age"").columns
['height']
```","1,-1    ",1,-1,0
1441301397,3182036,2023/2/26,v3.4.0-rc1,which commit caused the regression?,"1,-1    ",1,-1,0
1441368042,7322292,2023/2/26,v3.4.0-rc1,@cloud-fan it was introduced in https://github.com/apache/spark/commit/69f402ad8085bc50837128645b61168e7d5244b9,"1,-1    ",1,-1,0
1441371565,7322292,2023/2/26,v3.4.0-rc1,"In the JVM side, the logics of `drop(column: Column)` and `drop(columnName: String)` are different, we can not simply always convert a column name to column via `col()` method.

If there are multi columns with same name, you can use `drop(column_name)` to drop all of them;  or use `drop(df1.name)` to drop column from specific dataframe; But if you invoke `drop(col(name))`, it will fail due to ambiguous issue","1,-1    ",1,-1,0
1442227648,1938382,2023/2/26,v3.4.0-rc1,LGTM,"1,-1    ",1,-1,0
1442524626,3813695,2023/2/26,v3.4.0-rc1,Good catch @zhengruifeng ð,"1,-1    ",1,-1,0
1442597752,7322292,2023/2/26,v3.4.0-rc1,thank you all for the reivews! merged to master/branch-3.4,"1,-2    ",1,-2,-1
1441185226,1475305,2023/2/26,v3.4.0-rc1,cc @hvanhovell ,"1,-2    ",1,-2,-1
1443787996,1475305,2023/2/26,v3.4.0-rc1,"Is the fix feasible?

","1,-1    ",1,-1,0
1447459839,1475305,2023/2/26,v3.4.0-rc1,Thanks @zhenlineo friendly ping @HyukjinKwon @hvanhovell ,"1,-1    ",1,-1,0
1447460558,1475305,2023/2/26,v3.4.0-rc1,"A Ga task failed, let me re-trigger it","1,-2    ",1,-2,-1
1447537692,6477701,2023/2/26,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1447542446,1475305,2023/2/26,v3.4.0-rc1,Thanks @HyukjinKwon @hvanhovell @zhenlineo ,"1,-1    ",1,-1,0
1441186992,3182036,2023/2/26,v3.4.0-rc1,cc @peter-toth ,"1,-1    ",1,-1,0
1442126791,68855,2023/2/26,v3.4.0-rc1,"Hmm, the failed test seems a related one.","1,-2    ",1,-2,-1
1442143437,9700541,2023/2/26,v3.4.0-rc1,Oh is it still failing?,"1,-2    ",1,-2,-1
1442165227,68855,2023/2/26,v3.4.0-rc1,I think it was failing due to latest commit.,"1,-1    ",1,-1,0
1442699291,3182036,2023/2/26,v3.4.0-rc1,The failed test checks invalid ordering and I've updated it.,"1,-1    ",1,-1,0
1442846535,9700541,2023/2/26,v3.4.0-rc1,"Merged to master/3.4.
Thank you, @cloud-fan , @peter-toth , @viirya .","1,-1    ",1,-1,0
1441283164,12025282,2023/2/26,v3.4.0-rc1,cc @cloud-fan @tgravescs ,"1,-2    ",1,-2,-1
1441351627,3182036,2023/2/26,v3.4.0-rc1,"@ulysses-you to help other reviewers understand it, can you add more explaination in the PR description about how `boundExpr` is used in the window operator? To convince people that it's safe to skip overflow check for it.","1,-1    ",1,-1,0
1441402559,12025282,2023/2/27,v3.4.0-rc1,"@cloud-fan addressed, hope it is helpful","1,-1    ",1,-1,0
1441697059,3182036,2023/2/27,v3.4.0-rc1,"thanks, merging to master/3.4!","1,-2    ",1,-2,-1
1446948990,13592258,2023/2/27,v3.4.0-rc1,cc @cloud-fan Could you please take a look when you have a moment? Thanks!,"1,-2    ",1,-2,-1
1447430855,3182036,2023/2/27,v3.4.0-rc1,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1447436491,9700541,2023/2/27,v3.4.0-rc1,"Thank you, @huaxingao and @cloud-fan .","1,-1    ",1,-1,0
1447442268,13592258,2023/2/27,v3.4.0-rc1,Thanks @cloud-fan @dongjoon-hyun ,"1,-1    ",1,-1,0
1441269735,5399861,2023/2/27,v3.4.0-rc1,cc @gengliangwang ,"1,-1    ",1,-1,0
1442053265,21131848,2023/2/27,v3.4.0-rc1,"Looks like this also fixes SPARK-41991 (a bug that exists also in 3.3.x, but takes a little more work to reproduce than in 3.4.0).

Should the title should be `[SPARK-42286][SPARK-41991][SQL] ...etc...`?","1,-1    ",1,-1,0
1442647126,5399861,2023/2/27,v3.4.0-rc1,@RunyaoChen Could you add `[3.3]` to PR title.,"1,-1    ",1,-1,0
1442766193,6477701,2023/2/27,v3.4.0-rc1,cc @rednaxelafx FYI,"1,-1    ",1,-1,0
1444261708,1097932,2023/2/27,v3.4.0-rc1,"Thanks, merging to 3.3","1,-1    ",1,-1,0
1444383771,9700541,2023/2/27,v3.4.0-rc1,cc @huaxingao too.,"1,-1    ",1,-1,0
1441455325,502522,2023/2/27,v3.4.0-rc1,"@SandishKumarHN, @gengliangwang : Please review when you get a chance. ","1,-1    ",1,-1,0
1447567391,1097932,2023/2/27,v3.4.0-rc1,"+1, the new schema is more reasonable.","1,-1    ",1,-1,0
1447567460,1097932,2023/2/27,v3.4.0-rc1,"Thanks, merging to master/3.4","1,-1    ",1,-1,0
1442784894,8486025,2023/2/27,v3.4.0-rc1,ping @cloud-fan cc @wangyum ,"1,-2    ",1,-2,-1
1447623059,3182036,2023/2/27,v3.4.0-rc1,"thanks, merging to master!","1,-1    ",1,-1,0
1447849095,8486025,2023/2/27,v3.4.0-rc1,@cloud-fan @dongjoon-hyun Thank you very much!,"1,-2    ",1,-2,-1
1441888619,1475305,2023/2/27,v3.4.0-rc1,"Some other things to do, will continue tomorrow","1,-1    ",1,-1,0
1442065680,9616802,2023/2/27,v3.4.0-rc1,@LuciferYang thanks for the PR! Which datatypes are we still missing? I think we still some collection support?,"1,-1    ",1,-1,0
1442072479,1475305,2023/2/27,v3.4.0-rc1,"I refer 

https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala#L64-L102

the missing is `case a: Array[_]`","1,-1    ",1,-1,0
1442098437,9616802,2023/2/27,v3.4.0-rc1,"A couple of things:
- For Array I guess we can just make a nested call to lit for each element. No need to get CatalystTypeConverters involved.
- Bonus points if you check if all elements are the same.
- We do have to check what `CatalystTypeConverters` currently supports though. I think Map/Seq/Product support is also in there.
- You may want to put this in a separate file.","1,-1    ",1,-1,0
1442101584,9616802,2023/2/27,v3.4.0-rc1,"Oh and if it becomes too large I am fine with merging this first, and doing array in a follow-up.
","1,-1    ",1,-1,0
1442123998,1475305,2023/2/27,v3.4.0-rc1,"hmm... If I understand correctly, the current Literal does not support any collection type? Do we need to add some message types to support them?

https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/connector/connect/common/src/main/protobuf/spark/connect/expressions.proto#L149-L175","1,-1    ",1,-1,0
1442127677,9616802,2023/2/27,v3.4.0-rc1,"lol, no it does not. Let's just implement what we support, and do the rest in a different PR.","1,-1    ",1,-1,0
1442128569,1475305,2023/2/27,v3.4.0-rc1,OK,"1,-2    ",1,-2,-1
1442133796,1475305,2023/2/27,v3.4.0-rc1,"> Oh and if it becomes too large I am fine with merging this first, and doing array in a follow-up.

I hope we can merge this pr first if no other need to change. In addition, I need to go to bed as soon as possible. It's 1:00 in my time zone :)

","1,-1    ",1,-1,0
1442135134,9616802,2023/2/27,v3.4.0-rc1,go to sleep!,"1,-1    ",1,-1,0
1442968963,1475305,2023/2/27,v3.4.0-rc1,@hvanhovell Is there anything else can help Scala Client? @panbingkun told me that he also wanted to take some work related to connect.,"1,-1    ",1,-1,0
1443585778,9616802,2023/2/27,v3.4.0-rc1,"@LuciferYang @panbingkun that would be great! I will create an epic, with a bunch of todo's.","1,-1    ",1,-1,0
1443786173,1475305,2023/2/27,v3.4.0-rc1,Is there anything else need change this pr?,"1,-1    ",1,-1,0
1444170840,9616802,2023/2/27,v3.4.0-rc1,@LuciferYang can you update your PR?,"2,-1    ",2,-1,1
1444249755,9616802,2023/2/27,v3.4.0-rc1,@LuciferYang @panbingkun I created an epic with a bunch of things you can pick up: https://issues.apache.org/jira/browse/SPARK-42554,"1,-1    ",1,-1,0
1444271928,1938382,2023/2/27,v3.4.0-rc1,LGTM but please rebase this PR to solve conflict.,"1,-1    ",1,-1,0
1445172271,9616802,2023/2/27,v3.4.0-rc1,Merging to master/3.4. Thanks!,"1,-1    ",1,-1,0
1442227955,6570401,2023/2/27,v3.4.0-rc1,"cc @sunchao @AngersZhuuuu since you've worked on somewhat-related changes in #34690, #32887, etc.
cc @srowen @squito since you were involved in #24057 for the Java 9+ changes
cc @HyukjinKwon @dongjoon-hyun for any general interest","1,-1    ",1,-1,0
1442251188,9700541,2023/2/27,v3.4.0-rc1,"Also, cc @mridulm , @cloud-fan , @rednaxelafx, @zsxwing, @kiszk , @maropu .","2,-1    ",2,-1,1
1442714986,3182036,2023/2/27,v3.4.0-rc1,"It makes sense to use the builtin classloader when using builtin Hive. To clarify: we still have the class loading issue if people specifies a certain hive version (not builtin), right?","2,-1    ",2,-1,1
1444004552,6570401,2023/2/27,v3.4.0-rc1,"Great question @cloud-fan , and actually no, we don't. For all of the other values of `spark.sql.hive.metastore.jars` besides 'builtin', the user JARs are not included at all ([refer to this section of HiveUtils](https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L451-L513)). In all of those cases, the JAR list is constructed purely from the dependencies for the specified Hive version. Whether that behavior is correct is another question -- @shardulm94 informed me that user JARs are required to support custom serdes inside of the Hive client -- but in any case, 'builtin' is the only mode that is susceptible to this issue.","2,-1    ",2,-1,1
1445933911,3182036,2023/2/27,v3.4.0-rc1,lgtm if all tests pass,"3,-1    ",3,-1,2
1447241820,506679,2023/2/27,v3.4.0-rc1,Merged to master/branch-3.4. Thanks @xkrogen !,"1,-1    ",1,-1,0
1447269849,6570401,2023/2/27,v3.4.0-rc1,Thanks @sunchao and @cloud-fan !,"1,-1    ",1,-1,0
1447543451,6477701,2023/2/27,v3.4.0-rc1,"Seems like the tests didn't pass .. I am reverting this as it causes a lot of test failures. e.g.)
- https://github.com/apache/spark/runs/11644563592
- https://github.com/apache/spark/actions/runs/4288729207/jobs/7471072831
","1,-1    ",1,-1,0
1447565379,9700541,2023/2/27,v3.4.0-rc1,"Thank you for recovering `master` branch by reverting, @HyukjinKwon ! The reverting unblocks other PRs.","1,-1    ",1,-1,0
1447566249,3182036,2023/2/27,v3.4.0-rc1,Shall we revert it from 3.4 as well?,"2,-1    ",2,-1,1
1447568429,9700541,2023/2/27,v3.4.0-rc1,"Yes, it was reverted here, 26009d47c1.","1,-1    ",1,-1,0
1447576391,506679,2023/2/27,v3.4.0-rc1,Hmm interesting. Somehow the tests were shown all passing for me when I merged this. Sorry for the trouble. ,"1,-1    ",1,-1,0
1448420938,6570401,2023/2/27,v3.4.0-rc1,"I will take a look at the test failures, thanks @HyukjinKwon for addressing the revert!","1,-1    ",1,-1,0
1448614830,9700541,2023/2/27,v3.4.0-rc1,"If you folks don't mind, shall we consider this for Apache Spark 3.5 only? `ClassLoader` issue has been tricky always in the community. We need enough time to stabilize in `master` branch to give a chance to be verified in several cases by different organizations.","1,-2    ",1,-2,-1
1448644334,506679,2023/2/27,v3.4.0-rc1,+1. I agree with @dongjoon-hyun .,"1,-1    ",1,-1,0
1448966321,6570401,2023/2/27,v3.4.0-rc1,I agree as well. Posted a new PR at #40224.,"1,-1    ",1,-1,0
1442422396,1938382,2023/2/27,v3.4.0-rc1,@hvanhovell ,"2,-1    ",2,-1,1
1444946241,9616802,2023/2/27,v3.4.0-rc1,Merging,"1,-1    ",1,-1,0
1442805577,66282705,2023/2/27,v3.4.0-rc1,The test failure seems unrelated. @HyukjinKwon can we include this PR in spark 3.4 as well?,"3,-1    ",3,-1,2
1445719261,66282705,2023/2/27,v3.4.0-rc1,Combined in https://github.com/apache/spark/pull/40151,"2,-1    ",2,-1,1
1442630247,9616802,2023/2/28,v3.4.0-rc1,"@xkrogen yes, Spark Connect decouples client and server. That we synchronize jars and REPL generated classes does not mean we are re-introducing the coupling between client and server. All it means is that we will be using the server (driver) to orchistrate the execution of user defined code. This code could be executed within the same engine, however we can also use separate processes or VMs to execute this code. I do feel that adding this kind of functionality to connect keeps thing simple from the client's POV; it does not make a lot sense to me to do the synchronisation through a different service, because we need to go through the driver anyway.

I do want to call out that this mechanism is just not here for JARs and REPL generated classed, but that there are also quite a few other use cases that we need this mechanism for, e.g.: reading client local files, synchronizing other kinds of dependencies (for example python wheels), uploading models, ...

Finally it is early days for connect. We currently want to make it easy to folks to try and to move to connect. We need a REPL experience that is similar to the current experience for that. We want to do the simple thing first, and that is reuse the current UDF execution code (with some classpath isolation). I am happy to discuss the steps after that, if you are up to that please reach out to me (herman@databricks.com).","1,-1    ",1,-1,0
1444053186,6570401,2023/2/28,v3.4.0-rc1,"> This code could be executed within the same engine, however we can also use separate processes or VMs to execute this code.

Good point; nothing about the protocol itself precludes us from using separate UDF processes in the future.

> Finally it is early days for connect. We currently want to make it easy to folks to try and to move to connect ...

Generally agreed, but APIs and contracts can be challenging to change once adopted, so it's important to get it right on the first try, at least to the extent that it doesn't force us into bad decisions or tech debt later. That's why I'm interested in discussing a longer-term roadmap about this kind of support.

But I think you have convinced me that this protocol is generally useful!

cc @mridulm in case you're interested in taking a look","1,-1    ",1,-1,0
1448558937,21010250,2023/2/28,v3.4.0-rc1,@xkrogen Thank you for all of the feedback! Do you mind doing a review of the current state? We're looking to merge and were wondering if there are any other points to address. ,"1,-1    ",1,-1,0
1449127907,9616802,2023/2/28,v3.4.0-rc1,I am merging this. Let's address further concerns in a follow-up.,"1,-1    ",1,-1,0
1451013462,6570401,2023/2/28,v3.4.0-rc1,thanks for incorporating the feedback!,"1,-1    ",1,-1,0
1442502183,1938382,2023/2/28,v3.4.0-rc1,@hvanhovell ,"2,-1    ",2,-1,1
1445718970,66282705,2023/2/28,v3.4.0-rc1,Merged in https://github.com/apache/spark/pull/40151,"1,-1    ",1,-1,0
1444877430,6477701,2023/2/28,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-3    ",1,-3,-2
1447344107,3182036,2023/2/28,v3.4.0-rc1,"thanks, merging to master/3.4 (it's weird if some generate functions can be used as TVF and some can not)","1,-1    ",1,-1,0
1447353229,1938382,2023/2/28,v3.4.0-rc1,late LGTM!,"1,-1    ",1,-1,0
1442679131,9700541,2023/2/28,v3.4.0-rc1,"Do you have any concerns about this, @Yikun ?","1,-1    ",1,-1,0
1442685587,2069152,2023/2/28,v3.4.0-rc1,/LGTM,"1,-1    ",1,-1,0
1442690768,9700541,2023/2/28,v3.4.0-rc1,"Thank you, @Yikun , @HyukjinKwon , @william-wang .
Merged to master/3.4.","1,-1    ",1,-1,0
1442744400,6477701,2023/2/28,v3.4.0-rc1,cc @rithwik-db @xinrong-meng ,"1,-1    ",1,-1,0
1442745218,6477701,2023/2/28,v3.4.0-rc1,cc @WeichenXu123 I think we should test other python versions too before the actual release.,"1,-1    ",1,-1,0
1442753794,1938382,2023/2/28,v3.4.0-rc1,"LGTM


From https://spark.apache.org/docs/latest/ it says `Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+ and R 3.5+` so it makes sense to test against Python 3.7","1,-1    ",1,-1,0
1442755668,6477701,2023/2/28,v3.4.0-rc1,"yeah, I think we should set a scheduled job ... cc @dongjoon-hyun @Yikun too FYI","1,-1    ",1,-1,0
1442766810,9700541,2023/2/28,v3.4.0-rc1,"According to the Affected Version field, only Apache Spark 3.4 is affected?","1,-1    ",1,-1,0
1442769339,6477701,2023/2/28,v3.4.0-rc1,"Yup, this is actually rather a followup of https://github.com/apache/spark/commit/d5b08f8d99b14efa14ad706f9118762a2e570b34","1,-1    ",1,-1,0
1442822704,9700541,2023/2/28,v3.4.0-rc1,Merged to master/3.4.,"3,-1    ",3,-1,2
1444803655,6477701,2023/2/28,v3.4.0-rc1,"Although I had to manually skip a couple of tests (due to my env issue), I tested Python 3.7, 3.8, 3.9 (by CI), 3.10 and the tests pass.","2,-1    ",2,-1,1
1444872170,9700541,2023/2/28,v3.4.0-rc1,"Thank you so much, @HyukjinKwon !

May I ask which `requirements.txt` file is used there?","3,-1    ",3,-1,2
1444876363,6477701,2023/2/28,v3.4.0-rc1,I used `pip install -r dev/requirements.txt`.,"3,-1    ",3,-1,2
1447719980,12025282,2023/2/28,v3.4.0-rc1,@cloud-fan can we have this in branch-3.4 ? since it prevent potential bug and it's friendly for developers.,"1,-1    ",1,-1,0
1447725911,3182036,2023/2/28,v3.4.0-rc1,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1442797191,7788766,2023/2/28,v3.4.0-rc1,@cloud-fan I opened the backport for 3.4 but there is no JDBC query builder commit in 3.4 so one of the tests was removed.,"1,-1    ",1,-1,0
1443633926,3182036,2023/2/28,v3.4.0-rc1,"thanks, merging to 3.4!","1,-1    ",1,-1,0
1442811869,9616802,2023/2/28,v3.4.0-rc1,cc @grundprinzip ,"1,-1    ",1,-1,0
1442812096,9616802,2023/2/28,v3.4.0-rc1,Large change because all the golden files were updated.,"1,-1    ",1,-1,0
1444039024,9616802,2023/2/28,v3.4.0-rc1,Merging.,"1,-1    ",1,-1,0
1445583743,3626747,2023/2/28,v3.4.0-rc1,cc @cloud-fan  Could you help to review this PR? Thanks,"1,-1    ",1,-1,0
1447456449,3182036,2023/2/28,v3.4.0-rc1,how do other operators support CSE in whole-stage-codegen? also cc @viirya ,"1,-3    ",1,-3,-2
1449275235,3626747,2023/2/28,v3.4.0-rc1,"Hi, @cloud-fan 
I have updated the PR description and code to support more CSE cases following the three steps.","1,-1    ",1,-1,0
1453516931,3626747,2023/2/28,v3.4.0-rc1,"Hi, @cloud-fan @viirya 
I updated the PR and support subexpression elimination in FilterExec and JoinExec.
We can support some other operator subexpression elimination in two steps.
Do you have any good advice?","1,-1    ",1,-1,0
1508789020,3626747,2023/2/28,v3.4.0-rc1,"Use https://github.com/apache/spark/pull/40713 instead of this PR, close it.","2,-1    ",2,-1,1
1444830537,9616802,2023/2/28,v3.4.0-rc1,Merging. Thanks!,"2,-2    ",2,-2,0
1449252377,7322292,2023/2/28,v3.4.0-rc1,do we still need `message SQL` in relations.proto?,"2,-2    ",2,-2,0
1449328002,3421,2023/2/28,v3.4.0-rc1,"Yes, during eager execution the server generates the SQL relation if it's not a command. ","1,-1    ",1,-1,0
1449610069,7322292,2023/2/28,v3.4.0-rc1,merged into master/branch-3.4,"1,-1    ",1,-1,0
1451296217,9700541,2023/2/28,v3.4.0-rc1,"I made a PR.
- https://github.com/apache/spark/pull/40246","2,-2    ",2,-2,0
1444617521,1317309,2023/2/28,v3.4.0-rc1,Thanks! Merged to master.,"1,-1    ",1,-1,0
1445041550,1317309,2023/2/28,v3.4.0-rc1,Thanks! Merging to master.,"1,-1    ",1,-1,0
1444530972,100322362,2023/2/28,v3.4.0-rc1,@HeartSaVioR - please take a look. Thx,"1,-1    ",1,-1,0
1444903424,100322362,2023/2/28,v3.4.0-rc1,@HeartSaVioR - looks like the tests finished fine. Not sure why the Actions result is not updated here,"2,-2    ",2,-2,0
1445041909,1317309,2023/2/28,v3.4.0-rc1,Thanks! Merging to master.,"1,-2    ",1,-2,-1
1444587648,1938382,2023/2/28,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1445171880,9616802,2023/2/28,v3.4.0-rc1,"I am merging this one. Can you do persist in a follow-up?
","1,-1    ",1,-1,0
1444884892,6477701,2023/2/28,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1444807986,1938382,2023/2/28,v3.4.0-rc1,"LGTM

What is the default source BTW? ","1,-2    ",1,-2,-1
1444823976,506656,2023/2/28,v3.4.0-rc1,"> What is the default source BTW?

If format is not set, the value from SQL conf 'spark.sql.sources.default' will be used.","2,-1    ",2,-1,1
1445173286,9616802,2023/2/28,v3.4.0-rc1,Merging.,"1,-1    ",1,-1,0
1444806239,1938382,2023/2/28,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1445171335,9616802,2023/2/28,v3.4.0-rc1,Merging. Thanks!,"1,-1    ",1,-1,0
1445170286,9616802,2023/2/28,v3.4.0-rc1,"Merging, thanks for doing this!","1,-1    ",1,-1,0
1444905157,4190164,2023/2/28,v3.4.0-rc1,cc @hvanhovell @amaliujia @LuciferYang I am a bit tired of losing `// scalastyle:ignore funsuite` when my imports get auto formatted. Let's move to this `ConnectFunSuite`,"2,-1    ",2,-1,1
1444937486,4190164,2023/2/28,v3.4.0-rc1,cc @vicennial too!,"1,-2    ",1,-2,-1
1445171047,9616802,2023/2/28,v3.4.0-rc1,Merging. FYI - we are planning a refactor of Catalyst soon (post 3.4) and then we will integrate this with Spark's exception and error framework.,"1,-1    ",1,-1,0
1445189613,9700541,2023/2/28,v3.4.0-rc1,"> Merging. FYI - we are planning a refactor of Catalyst soon (post 3.4) and then we will integrate this with Spark's exception and error framework.

Thank you for sharing the future plan, @hvanhovell !","1,-1    ",1,-1,0
1445516304,6477701,2023/2/28,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1445528865,7322292,2023/2/28,v3.4.0-rc1,"late LGTM, thanks!","1,-2    ",1,-2,-1
1445041525,47717334,2023/2/28,v3.4.0-rc1,Can someone please help in creating issue for this pull request ? I do not have a ASF Jira account.,"1,-1    ",1,-1,0
1445168808,47577197,2023/2/28,v3.4.0-rc1,[private@spark.apache.org](mailto:private@spark.apache.org),"1,-1    ",1,-1,0
1448124856,47717334,2023/2/28,v3.4.0-rc1,@maropu @HyukjinKwon  ,"1,-1    ",1,-1,0
1471571501,47717334,2023/2/28,v3.4.0-rc1,retest this please,"1,-1    ",1,-1,0
1471587034,47717334,2023/2/28,v3.4.0-rc1,Gentle ping!! @gengliangwang  @cloud-fan @viirya @Ngone51 @maropu @HyukjinKwon. ,"1,-1    ",1,-1,0
1471587177,47577197,2023/2/28,v3.4.0-rc1,you must enable github action on yours repo https://github.com/apache/spark/pull/40171/checks?check_run_id=11594767505 ,"1,-2    ",1,-2,-1
1445228075,1938382,2023/2/28,v3.4.0-rc1,@hvanhovell ,"2,-1    ",2,-1,1
1445518268,6477701,2023/2/28,v3.4.0-rc1,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1445232248,1938382,2023/2/28,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1445252735,9616802,2023/2/28,v3.4.0-rc1,Merging.,"1,-1    ",1,-1,0
1445245652,1938382,2023/2/28,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1445246195,1938382,2023/2/28,v3.4.0-rc1,Preferring https://github.com/apache/spark/pull/40173 over this PR.,"1,-1    ",1,-1,0
1445462573,9616802,2023/2/28,v3.4.0-rc1,@cloud-fan can you take a look?,"1,-1    ",1,-1,0
1446306643,9616802,2023/2/28,v3.4.0-rc1,Merging,"1,-1    ",1,-1,0
1447458438,3182036,2023/2/28,v3.4.0-rc1,late LGTM,"2,-1    ",2,-1,1
1445521025,9616802,2023/2/28,v3.4.0-rc1,Merging,"1,-1    ",1,-1,0
1445521369,9616802,2023/2/28,v3.4.0-rc1,Thanks for doing this!,"3,-1    ",3,-1,2
1445586703,9700541,2023/2/28,v3.4.0-rc1,Thank you for doing this too!,"2,-1    ",2,-1,1
1445494765,5399861,2023/2/28,v3.4.0-rc1,cc @cloud-fan @ulysses-you,"1,-1    ",1,-1,0
1445418994,47577197,2023/2/28,v3.4.0-rc1,@srowen @dongjoon-hyun @viirya,"2,-1    ",2,-1,1
1445419415,47577197,2023/2/28,v3.4.0-rc1,And CC @xinrong-meng This is for updating documentation for spark 3.4 release.  ,"1,-1    ",1,-1,0
1445501146,9700541,2023/2/28,v3.4.0-rc1,"To be clear, the code change itself looks okay, @bjornjorgensen .","1,-1    ",1,-1,0
1445501633,9700541,2023/2/28,v3.4.0-rc1,"Lastly, are you claiming a followup across `spark-website` and `spark` repositories? To me, `[FOLLOWUP]` doesn't make sense at all in that case, @bjornjorgensen .","2,-1    ",2,-1,1
1445501723,9700541,2023/2/28,v3.4.0-rc1,"Also, cc @HyukjinKwon .","1,-1    ",1,-1,0
1445591135,9616802,2023/2/28,v3.4.0-rc1,Merging.,"2,-1    ",2,-1,1
1445559302,9700541,2023/2/28,v3.4.0-rc1,"Could you review this editorial patch, @HyukjinKwon and @viirya ?","1,-1    ",1,-1,0
1445587270,9700541,2023/2/28,v3.4.0-rc1,Thank you! Merged to master/3.4.,"1,-1    ",1,-1,0
1445615890,9700541,2023/2/28,v3.4.0-rc1,"Hi, @viirya . Could you review this PR too?","1,-1    ",1,-1,0
1445625931,9700541,2023/2/28,v3.4.0-rc1,Let me close this and fix the branch first.,"1,-1    ",1,-1,0
1445894470,5275075,2023/2/28,v3.4.0-rc1,@wangyum ,"1,-1    ",1,-1,0
1487904058,5275075,2023/3/1,v3.4.0-rc1,"cc @cloud-fan , thanks","2,-1    ",2,-1,1
1445628852,9700541,2023/3/1,v3.4.0-rc1,cc @viirya ,"1,-1    ",1,-1,0
1445636922,9700541,2023/3/1,v3.4.0-rc1,"Now, it passed. 
![Screenshot 2023-02-26 at 7 25 33 PM](https://user-images.githubusercontent.com/9700541/221465853-833cb047-d751-43f1-a341-7a6b01f5ce21.png)
","1,-1    ",1,-1,0
1445651565,9700541,2023/3/1,v3.4.0-rc1,"Thank you so much, @hvanhovell . Sorry for the troubles.","1,-1    ",1,-1,0
1445652117,9700541,2023/3/1,v3.4.0-rc1,Merged to master/3.4.,"1,-1    ",1,-1,0
1445724214,68855,2023/3/1,v3.4.0-rc1,Looks good.,"1,-2    ",1,-2,-1
1445745535,9700541,2023/3/1,v3.4.0-rc1,"Thank you, @viirya . Sorry for missing at the first PR.","1,-1    ",1,-1,0
1445656241,1938382,2023/3/1,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1445682796,9616802,2023/3/1,v3.4.0-rc1,@amaliujia can you please update the compatibility test for these?,"1,-1    ",1,-1,0
1446861766,1938382,2023/3/1,v3.4.0-rc1,@hvanhovell Compatibility tests updated,"2,-1    ",2,-1,1
1448653567,1938382,2023/3/1,v3.4.0-rc1,@hvanhovell I should make the Compatibility list correct now.,"1,-1    ",1,-1,0
1449124523,9616802,2023/3/1,v3.4.0-rc1,"Merging
","2,-1    ",2,-1,1
1446848451,1938382,2023/3/1,v3.4.0-rc1,Late LGTM!,"1,-3    ",1,-3,-2
1446301464,9616802,2023/3/1,v3.4.0-rc1,Merging.,"1,-1    ",1,-1,0
1446853871,1938382,2023/3/1,v3.4.0-rc1,Late LGTM!,"1,-1    ",1,-1,0
1447507928,1317309,2023/3/1,v3.4.0-rc1,Thanks! Merging to master.,"1,-3    ",1,-3,-2
1445889649,1317309,2023/3/1,v3.4.0-rc1,"If possible, this change should be ideally incorporated to the doc of Spark 3.4.0. It'd be nice if we can review and merge before 3.4.0 RC passed.","1,-1    ",1,-1,0
1445891074,1317309,2023/3/1,v3.4.0-rc1,cc. @zsxwing @viirya @HyukjinKwon,"2,-1    ",2,-1,1
1445948821,6477701,2023/3/1,v3.4.0-rc1,cc @allanf-db FYI as we would put this Python example first too in your PR ,"1,-1    ",1,-1,0
1447285732,1317309,2023/3/1,v3.4.0-rc1,Thanks! Merging to master/3.4.,"1,-1    ",1,-1,0
1446348023,5399861,2023/3/1,v3.4.0-rc1,cc @sunchao  @cloud-fan @huaxingao ,"1,-3    ",1,-3,-2
1467139918,5399861,2023/3/1,v3.4.0-rc1,Merged to master.,"1,-1    ",1,-1,0
1446278547,6477701,2023/3/1,v3.4.0-rc1,cc @Yikun FYI,"1,-1    ",1,-1,0
1447477225,1475305,2023/3/1,v3.4.0-rc1,"This pr often conflicts....

","1,-1    ",1,-1,0
1447532241,6477701,2023/3/1,v3.4.0-rc1,cc @hvanhovell too FYI,"1,-1    ",1,-1,0
1450433667,9616802,2023/3/1,v3.4.0-rc1,@LuciferYang can we close this one in favor of https://github.com/apache/spark/pull/40213?,"1,-1    ",1,-1,0
1451080327,1475305,2023/3/1,v3.4.0-rc1,"> @LuciferYang can we close this one in favor of #40213?

OK, let me close this one and focus on https://github.com/apache/spark/pull/40213

","1,-1    ",1,-1,0
1447448235,3182036,2023/3/1,v3.4.0-rc1,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1446443930,3182036,2023/3/1,v3.4.0-rc1,I agree with this change. This should be ok for tracking duration of speculative tasks. cc @mridulm ,"1,-1    ",1,-1,0
1446641305,822522,2023/3/1,v3.4.0-rc1,"This wouldn't change any user-facing results right? this is just internal? seems OK if so. If it changed, for example, the result of PERCENTILE somewhere, that could be an issue","1,-1    ",1,-1,0
1447649892,1591700,2023/3/1,v3.4.0-rc1,"My main concern is for tasksets with very low number of tasks, where the change can result in nontrivial difference.
Given the performance impact is minimal overall, I am not convinced whether we want to make this change.

Does this still show up in the flamegraphs based on the earlier experiments you did @alkis ?","1,-1    ",1,-1,0
1447719888,3182036,2023/3/1,v3.4.0-rc1,"The pyspark failure is totally unrelated. I'm merging it to master, thanks!","1,-1    ",1,-1,0
1447351035,44179472,2023/3/1,v3.4.0-rc1,"@cloud-fan, @jiangxb1987, could you review this? ","1,-1    ",1,-1,0
1451370644,1580697,2023/3/1,v3.4.0-rc1,"+1, LGTM. Merging to master/3.4.
Thank you, @jiang13021.","1,-1    ",1,-1,0
1451372140,1580697,2023/3/1,v3.4.0-rc1,@jiang13021 Congratulations with the first contribution to Apache Spark!,"2,-1    ",2,-1,1
1451372811,1580697,2023/3/1,v3.4.0-rc1,@jiang13021 The changes causes some conflicts in branch-3.3. Could you open a separate PR with a backport to Spark 3.3.,"2,-1    ",2,-1,1
1451819657,45600652,2023/3/1,v3.4.0-rc1,"> @jiang13021 The changes causes some conflicts in branch-3.3. Could you open a separate PR with a backport to Spark 3.3.
Thanks for your review. Do you mean launch a new PR to branch-3.3? Here it is: https://github.com/apache/spark/pull/40253","1,-1    ",1,-1,0
1451827970,1580697,2023/3/1,v3.4.0-rc1,"> Do you mean launch a new PR to branch-3.3? Here it is: https://github.com/apache/spark/pull/40253

Yep. Thank you.","1,-1    ",1,-1,0
1447471031,35296098,2023/3/1,v3.4.0-rc1,"> > 
> 
> 




> You are suggesting a breaking change here. The existing behavior is correct and consistent with old Spark versions. I guess we need to revise the test comment instead, @huangxiaopingRD .

Sorry, I don't understand what you say we should do next. I want to make this change just because I think the default format of Spark's data source is parquet, and I think the default fileformat of ""`create table`"" should be consistent with the default value (parquet) of ""spark.sql.sources.default"".","1,-1    ",1,-1,0
1449375189,13965087,2023/3/1,v3.4.0-rc1,"In my test, the hive table format depends on the conf `hive.default.fileformat` when set `spark.sql.legacy.createHiveTableByDefault` is true . And if `spark.sql.legacy.createHiveTableByDefault` is false, it hive table format depends on the conf `spark.sql.sources.default` .

I think it's reasonable .

So maybe we should only fix the describtion of the https://github.com/apache/spark/blob/master/docs/sql-ref-syntax-ddl-create-table-datasource.md?plain=1#L121 ?","1,-1    ",1,-1,0
1450064629,35296098,2023/3/1,v3.4.0-rc1,"> In my test, the hive table format depends on the conf `hive.default.fileformat` when set `spark.sql.legacy.createHiveTableByDefault` is true . And if `spark.sql.legacy.createHiveTableByDefault` is false, it hive table format depends on the conf `spark.sql.sources.default` .
> 
> I think it's reasonable .
> 
> So maybe we should only fix the describtion of the https://github.com/apache/spark/blob/master/docs/sql-ref-syntax-ddl-create-table-datasource.md?plain=1#L121 ?

I agree with you. I will launch another PR to modify the description of the document. Thanks @zzzzming95 

what do you think about this? @dongjoon-hyun ","1,-1    ",1,-1,0
1446908271,9616802,2023/3/1,v3.4.0-rc1,Merging.,"1,-1    ",1,-1,0
1449015094,66282705,2023/3/1,v3.4.0-rc1,cc @srielau ,"2,-1    ",2,-1,1
1449343284,1580697,2023/3/1,v3.4.0-rc1,"+1, LGTM. Merging to master.
Thank you, @allisonwang-db.","1,-1    ",1,-1,0
1447225712,9700541,2023/3/1,v3.4.0-rc1,"cc @WeichenXu123 , @HyukjinKwon ","1,-1    ",1,-1,0
1447340266,6477701,2023/3/1,v3.4.0-rc1,@jzhuge let's fix the PR description. This technically does not revert https://github.com/apache/spark/pull/38699 but fixes a mistake that removed the code (that author and reviewers thought it's a duplicate).,"1,-1    ",1,-1,0
1447455996,6477701,2023/3/1,v3.4.0-rc1,nit but mind fixing up the PR description? want to merge it now :-),"1,-1    ",1,-1,0
1447461080,1883812,2023/3/1,v3.4.0-rc1,Updated the PR description,"1,-1    ",1,-1,0
1447467586,6477701,2023/3/1,v3.4.0-rc1,"Merged to master, branch-3.4, branch-3.3, and branch-3.2.","1,-1    ",1,-1,0
1446959797,1938382,2023/3/1,v3.4.0-rc1,@hvanhovell ,"1,-1    ",1,-1,0
1447051899,1938382,2023/3/1,v3.4.0-rc1,@hvanhovell done.,"1,-1    ",1,-1,0
1447325044,6477701,2023/3/1,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1447485217,9700541,2023/3/1,v3.4.0-rc1,"Could you check the failures or re-trigger the CI for this PR, @aokolnychyi ?","1,-1    ",1,-1,0
1447551597,6235869,2023/3/1,v3.4.0-rc1,Test failures don't seem to be related. I see similar failures on other PRs. I think the root cause has been recently reverted in master.,"1,-1    ",1,-1,0
1447564604,9700541,2023/3/1,v3.4.0-rc1,"You need to re-trigger CI, @aokolnychyi , because we cannot re-trigger for you.","1,-1    ",1,-1,0
1447808792,9700541,2023/3/1,v3.4.0-rc1,"Merged to master/3.4 for Apache Spark 3.4.0.
Thank you, @aokolnychyi and all!","1,-1    ",1,-1,0
1448505299,6235869,2023/3/1,v3.4.0-rc1,"Thank you, @dongjoon-hyun @huaxingao @cloud-fan!","1,-1    ",1,-1,0
1447540397,6477701,2023/3/1,v3.4.0-rc1,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1534265498,8486025,2023/3/1,v3.4.0-rc1,ping @cloud-fan ,"1,-1    ",1,-1,0
1447470558,6235869,2023/3/1,v3.4.0-rc1,cc @huaxingao @cloud-fan @dongjoon-hyun @sunchao @viirya ,"1,-1    ",1,-1,0
1447477469,6235869,2023/3/1,v3.4.0-rc1,"@dongjoon-hyun, thanks, I overlooked. I corrected the JIRA description and added 3.3.3 as well but feel free to adjust the list of versions as needed.","1,-1    ",1,-1,0
1447483660,9700541,2023/3/1,v3.4.0-rc1,Thank you!,"1,-1    ",1,-1,0
1447570541,9700541,2023/3/1,v3.4.0-rc1,Could you rebase to the `master` branch?,"1,-1    ",1,-1,0
1447605704,3182036,2023/3/1,v3.4.0-rc1,"This is a good catch! I think the fix can be simpler: we encode the raw type information in the root `StructField`, so we don't need to decode it recursively. We can restore the raw type at the very beginning:
```
def resolveOutputColumns(
    ...,
    expected: Seq[Attribute],
    ...) {
    val actualExpectedCols = expected.map { attr =>
        attr.withDataType(restore(attr.metadata).getOrElse(attr.dataType))
    }
}
``` ","1,-1    ",1,-1,0
1447651920,6235869,2023/3/1,v3.4.0-rc1,"@cloud-fan, are you thinking of passing raw types all the way? I actually considered that but my worry was we will have to modify `checkField` casting logic as it seems we can't use the raw type there.

```
  private def checkField(
      tableAttr: Attribute,
      queryExpr: NamedExpression,
      byName: Boolean,
      conf: SQLConf,
      addError: String => Unit,
      colPath: Seq[String]): Option[NamedExpression] = {

    val storeAssignmentPolicy = conf.storeAssignmentPolicy
    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&
      tableAttr.name == queryExpr.name &&
      tableAttr.metadata == queryExpr.metadata) {
      Some(queryExpr)
    } else {
      val casted = storeAssignmentPolicy match {
        case StoreAssignmentPolicy.ANSI =>
          val cast = Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone),
            ansiEnabled = true)
          cast.setTagValue(Cast.BY_TABLE_INSERTION, ())
          checkCastOverflowInTableInsert(cast, colPath.quoted)
        case StoreAssignmentPolicy.LEGACY =>
          Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone),
            ansiEnabled = false)
        case _ =>
          Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))
      }
      val exprWithStrLenCheck = if (conf.charVarcharAsString) {
        casted
      } else {
        CharVarcharUtils.stringLengthCheck(casted, tableAttr)
      }
      // Renaming is needed for handling the following cases like
      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2
      // 2) Target tables have column metadata
      Some(Alias(exprWithStrLenCheck, tableAttr.name)(explicitMetadata = Some(tableAttr.metadata)))
    }

    storeAssignmentPolicy match {
      case StoreAssignmentPolicy.LEGACY =>
        outputField

      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>
        // run the type check first to ensure type errors are present
        val canWrite = DataType.canWrite(
          queryExpr.dataType, tableAttr.dataType, byName, conf.resolver, colPath.quoted,
          storeAssignmentPolicy, addError)
        if (queryExpr.nullable && !tableAttr.nullable) {
          addError(s""Cannot write nullable values to non-null column '${colPath.quoted}'"")
          None

        } else if (!canWrite) {
          None

        } else {
          outputField
        }
    }
  }
```","1,-1    ",1,-1,0
1447654974,6235869,2023/3/1,v3.4.0-rc1,"@cloud-fan, passing raw types from the start would be simper and more efficient but we will have to modify `checkField` to work with raw types. Let me know if I got you correctly.","1,-1    ",1,-1,0
1447718309,3182036,2023/3/1,v3.4.0-rc1,"Yea, we will have to modify `checkField`. We can replace char/varchar type with string type in `checkField` so that most code can be unchanged, and only use the raw type for length check related logic.","1,-1    ",1,-1,0
1448503358,6235869,2023/3/1,v3.4.0-rc1,"Sounds good, I'll make the change today.","1,-1    ",1,-1,0
1449501496,3182036,2023/3/1,v3.4.0-rc1,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1449503428,9700541,2023/3/1,v3.4.0-rc1,"Thank you, @aokolnychyi , @cloud-fan , @huaxingao , @viirya !","1,-1    ",1,-1,0
1450710472,6235869,2023/3/1,v3.4.0-rc1,"Thanks for reviewing, @dongjoon-hyun @cloud-fan @huaxingao @viirya!","1,-1    ",1,-1,0
1447604135,9700541,2023/3/2,v3.4.0-rc2,It seems that `lint-scala` complain again. Could you apply `./dev/lint-scala`?,"1,-1    ",1,-1,0
1448645023,9700541,2023/3/2,v3.4.0-rc2,"Scala/Java linter passed. Merged to master/3.4. Thank you, @hvanhovell and all!","1,-1    ",1,-1,0
1447506796,1317309,2023/3/2,v3.4.0-rc2,cc. @HyukjinKwon @viirya Sorry to bug you but can I get another quick review for this? My bad.,"1,-1    ",1,-1,0
1447534680,1317309,2023/3/2,v3.4.0-rc2,Thanks! Merging to master/3.4.,"1,-1    ",1,-1,0
1447564499,68855,2023/3/2,v3.4.0-rc2,Looks good to me.,"1,-1    ",1,-1,0
1447553959,6477701,2023/3/2,v3.4.0-rc2,cc @gengliangwang sorry for many followups ..  I don't know why I missed this ... this is the last followup.,"1,-1    ",1,-1,0
1447601513,1097932,2023/3/2,v3.4.0-rc2,@HyukjinKwon I also verified MathFunctionsSuite can pass with ANSI Enabled ,"1,-1    ",1,-1,0
1447757856,6477701,2023/3/2,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1447610873,7322292,2023/3/2,v3.4.0-rc2,@HyukjinKwon @grundprinzip @amaliujia @hvanhovell @ueshin ,"1,-1    ",1,-1,0
1448497999,9616802,2023/3/2,v3.4.0-rc2,Merging.,"1,-1    ",1,-1,0
1449143151,7322292,2023/3/2,v3.4.0-rc2,thanks for the reviews!,"1,-1    ",1,-1,0
1449193517,8326978,2023/3/2,v3.4.0-rc2,"thanks, merged to master and 3.4","1,-2    ",1,-2,-1
1447620208,7322292,2023/3/2,v3.4.0-rc2,cc @WeichenXu123 ,"1,-1    ",1,-1,0
1447653122,1883812,2023/3/2,v3.4.0-rc2,"@HyukjinKwon The change is pretty simple. Did I miss anything?

Should we add this to release notes? Since this PR changes the behavior described by SPARK-28843.","1,-1    ",1,-1,0
1448549056,1883812,2023/3/2,v3.4.0-rc2,"> Did you check these envs are also set correctly ?

@WeichenXu123 It is ok to set default OMP_NUM_THREADS to driver cores. ""Task"" only applies to executors.","1,-1    ",1,-1,0
1449175374,1883812,2023/3/2,v3.4.0-rc2,Test `KafkaMicroBatchV1SourceWithAdminSuite` failed. Not sure it is related.,"1,-1    ",1,-1,0
1449218552,6477701,2023/3/2,v3.4.0-rc2,Will merge this one in few days if there aren't any objection.,"1,-1    ",1,-1,0
1451082899,6477701,2023/3/2,v3.4.0-rc2,Merged to master,"2,-1    ",2,-1,1
1448906338,4190164,2023/3/2,v3.4.0-rc2,LGTM,"1,-1    ",1,-1,0
1450431615,9616802,2023/3/2,v3.4.0-rc2,@LuciferYang thanks for the hard work! Can we merge this one instead of https://github.com/apache/spark/pull/40191.,"1,-1    ",1,-1,0
1450887406,1938382,2023/3/2,v3.4.0-rc2,Looks awesome overall!,"1,-1    ",1,-1,0
1450890886,1938382,2023/3/2,v3.4.0-rc2,How would we deal with `CompatibilitySuite`? ,"1,-1    ",1,-1,0
1450904403,1938382,2023/3/2,v3.4.0-rc2,Ah I see now that we still need to update the excluding rules.,"1,-1    ",1,-1,0
1451099529,1475305,2023/3/2,v3.4.0-rc2,"> How would we deal with `CompatibilitySuite`
> Ah I see now that we still need to update the excluding rules.

Yes, just check in a different way :)

","1,-1    ",1,-1,0
1451253791,1475305,2023/3/2,v3.4.0-rc2,"<img width=""948"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/222325279-7ef9ec94-3e79-44c3-864c-19b2b0737c4b.png"">

The new change of `org.apache.spark.sql.Dataset#plan` in SPARK-42631 is checked as incompatible. ","2,-1    ",2,-1,1
1452355034,9616802,2023/3/2,v3.4.0-rc2,Alirght merging.,"2,-1    ",2,-1,1
1452840888,1475305,2023/3/2,v3.4.0-rc2,Thanks @hvanhovell @amaliujia @zhenlineo ,"1,-1    ",1,-1,0
1448331298,822522,2023/3/2,v3.4.0-rc2,Looks fine for master,"1,-1    ",1,-1,0
1449245816,1475305,2023/3/2,v3.4.0-rc2,GA passed,"1,-1    ",1,-1,0
1449248073,822522,2023/3/2,v3.4.0-rc2,Merged to master,"1,-1    ",1,-1,0
1449249164,1475305,2023/3/2,v3.4.0-rc2,Thanks @srowen ,"1,-1    ",1,-1,0
1447730252,1317309,2023/3/2,v3.4.0-rc2,This should be merged after #39931.,"1,-1    ",1,-1,0
1457584928,1317309,2023/3/2,v3.4.0-rc2,"cc. @zsxwing @rangadi @jerrypeng @anishshri-db @chaoqin-li1123 
cc-ing folks who reviewed the code change PR. This PR is a doc change to show up what is being unblocked, like we did in https://github.com/apache/spark/pull/40188 for fixing broken late record filtering.","2,-1    ",2,-1,1
1457585553,1317309,2023/3/2,v3.4.0-rc2,cc. @viirya as well who may be interested with new feature in SS.,"1,-1    ",1,-1,0
1458815185,1317309,2023/3/2,v3.4.0-rc2,Thanks for reviewing! Merging to master.,"1,-1    ",1,-1,0
1449688758,6477701,2023/3/2,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1467332483,11567269,2023/3/2,v3.4.0-rc2,Let us mention all the breaking changes and deprecation in both release notes and migration guides ,"1,-1    ",1,-1,0
1518739172,47577197,2023/3/2,v3.4.0-rc2,"hmm.. 
`F05.info()`

```
TypeError                                 Traceback (most recent call last)
Cell In[12], line 1
----> 1 F05.info()

File /opt/spark/python/pyspark/pandas/frame.py:12167, in DataFrame.info(self, verbose, buf, max_cols, null_counts)
  12163     count_func = self.count
  12164     self.count = (  # type: ignore[assignment]
  12165         lambda: count_func()._to_pandas()  # type: ignore[assignment, misc, union-attr]
  12166     )
> 12167     return pd.DataFrame.info(
  12168         self,  # type: ignore[arg-type]
  12169         verbose=verbose,
  12170         buf=buf,
  12171         max_cols=max_cols,
  12172         memory_usage=False,
  12173         null_counts=null_counts,
  12174     )
  12175 finally:
  12176     del self._data

TypeError: DataFrame.info() got an unexpected keyword argument 'null_counts'

```
Hope we can get a better description as error messages here.","1,-1    ",1,-1,0
1518773295,47577197,2023/3/2,v3.4.0-rc2,https://github.com/apache/spark/pull/40913 can be a fix for this. ,"1,-1    ",1,-1,0
1455348717,9616802,2023/3/2,v3.4.0-rc2,@panbingkun can you update your PR?,"1,-1    ",1,-1,0
1455351159,9616802,2023/3/2,v3.4.0-rc2,@panbingkun can you update the CompatibilitySuite?,"1,-1    ",1,-1,0
1455743204,15246973,2023/3/2,v3.4.0-rc2,"> @panbingkun can you update the CompatibilitySuite?

Done","1,-1    ",1,-1,0
1456804274,9616802,2023/3/2,v3.4.0-rc2,LGTM,"1,-1    ",1,-1,0
1456804495,9616802,2023/3/2,v3.4.0-rc2,Merging,"1,-1    ",1,-1,0
1451322878,1475305,2023/3/2,v3.4.0-rc2,cc @hvanhovell @HyukjinKwon ,"1,-1    ",1,-1,0
1454426268,1475305,2023/3/2,v3.4.0-rc2,"Some other things to do, will update later","1,-1    ",1,-1,0
1457206111,9616802,2023/3/2,v3.4.0-rc2,Merging.,"1,-1    ",1,-1,0
1457373651,1475305,2023/3/2,v3.4.0-rc2,Thanks @hvanhovell ,"1,-1    ",1,-1,0
1448306334,70568553,2023/3/2,v3.4.0-rc2,@jelmerk could you please raise a bug [here](https://issues.apache.org/jira/projects/SPARK/summary) and use this bug id in the PR for e.g. you can refer the PR https://github.com/apache/spark/pull/40202,"1,-1    ",1,-1,0
1448329979,822522,2023/3/2,v3.4.0-rc2,When would the field value have a substitution string in it - does this actually happen in practice? ,"1,-1    ",1,-1,0
1448364029,133639,2023/3/2,v3.4.0-rc2,"> When would the field value have a substitution string in it - does this actually happen in practice?

I mention one such scenario in my description. Starting from the image on

Try and run this in a spark 3.4.x spark-shell or 12.1 databricks runtime

```
import org.apache.spark.sql.types._

val schema = new StructType().add(""properties"", new StructType())

val df = Seq(""""""{""properties"":""${foo}""}"""""").toDF(""value"").as[String]

spark.read.schema(schema).json(df).count()
```","1,-1    ",1,-1,0
1448381649,822522,2023/3/2,v3.4.0-rc2,"Sorry I guess I mean I am not clear how this class, which reads error messages, is coming into play here. This is my ignorance. Seems like it wouldn't be related to just parsing user JSON, but the change fixes it?","1,-1    ",1,-1,0
1448394326,133639,2023/3/2,v3.4.0-rc2,"> Sorry I guess I mean I am not clear how this class, which reads error messages, is coming into play here. This is my ignorance. Seems like it wouldn't be related to just parsing user JSON, but the change fixes it?

The example defines `properties` in the schema as a struct but in the json it is a string with the value `""${foo}""`
An exception is raised during parsing and the message is resolved by that class 
","1,-1    ",1,-1,0
1448413797,822522,2023/3/2,v3.4.0-rc2,"Ah, I get it now. I agree with this change for sure.","1,-1    ",1,-1,0
1448414336,822522,2023/3/2,v3.4.0-rc2,(Can you just file a JIRA and link it per https://spark.apache.org/contributing.html ?),"1,-1    ",1,-1,0
1448415053,133639,2023/3/2,v3.4.0-rc2,"> (Can you just file a JIRA and link it per https://spark.apache.org/contributing.html ?)

Already did that a few mins ago and added it on top of the PR description","1,-1    ",1,-1,0
1448419107,822522,2023/3/2,v3.4.0-rc2,Edit the title to link it. See other PRs. Minor thing but helps ,"1,-1    ",1,-1,0
1448422622,133639,2023/3/2,v3.4.0-rc2,"> Edit the title to link it. See other PRs. Minor thing but helps

done","1,-1    ",1,-1,0
1449190687,822522,2023/3/2,v3.4.0-rc2,@dongjoon-hyun I'd like a second opinion - should we merge to 3.4 or 3.3? Reasonable fix I think and I'm a little concerned there could be some weird way this would expand some info it shouldn't ,"1,-1    ",1,-1,0
1449193109,9700541,2023/3/2,v3.4.0-rc2,"For branch-3.4, I'm +1 for backporting.
Does this affect branch-3.3 too, @srowen ? ","1,-1    ",1,-1,0
1449194251,9700541,2023/3/2,v3.4.0-rc2,"Since [SPARK-40530](https://issues.apache.org/jira/browse/SPARK-40530) added this, `branch-3.3` seems to be not affected.","1,-1    ",1,-1,0
1449194829,9700541,2023/3/2,v3.4.0-rc2,"Also, cc @cloud-fan , @MaxGekk , @viirya from SPARK-40530.","1,-1    ",1,-1,0
1449217153,822522,2023/3/2,v3.4.0-rc2,"Oh yeah, won't affect 3.3. A quick test like your proof of concept would be great indeed.","1,-1    ",1,-1,0
1449485716,133639,2023/3/2,v3.4.0-rc2,"> Oh yeah, won't affect 3.3. A quick test like your proof of concept would be great indeed.

The class itself is not present on 3.3. It is not affected.  In-fact thats how we mitigated this problem, that occurred in a production setting for our pipeline. by downgrading","1,-1    ",1,-1,0
1449487177,9700541,2023/3/2,v3.4.0-rc2,"Wow. Thank you for sharing the details, @jelmerk .","1,-1    ",1,-1,0
1449507636,3182036,2023/3/3,v3.4.0-rc2,good catch!,"1,-1    ",1,-1,0
1449539893,68855,2023/3/3,v3.4.0-rc2,"The fix looks okay, but the PR description looks confusing as `What changes were proposed in this pull request?` describes an alternative approach to current `getErrorMessage`?","1,-1    ",1,-1,0
1449559149,133639,2023/3/3,v3.4.0-rc2,"> Please make sure updating the description before merging this.

I removed the alternative solution. Imho It's a much better solution, but if experience has taught me anything, it is that getting anything but the smallest of changes merged is an uphill battle so I chose the 1 line fix","1,-1    ",1,-1,0
1450161777,822522,2023/3/3,v3.4.0-rc2,Simple is good. Indeed the smallest of changes can even break things. This looks good pending CI/CD tests,"1,-1    ",1,-1,0
1450916393,133639,2023/3/3,v3.4.0-rc2,"> Simple is good. Indeed the smallest of changes can even break things. This looks good pending CI/CD tests

I tried triggering it twice but those tests seem unstable","1,-1    ",1,-1,0
1450918182,68855,2023/3/3,v3.4.0-rc2,Can you re-trigger it again? Although the CI failure looks unrelated.,"1,-1    ",1,-1,0
1450932083,822522,2023/3/3,v3.4.0-rc2,"Hm, no looks like something else is wrong in even setting up the environment. Github issue?
`Error: Error response from daemon: Get ""https://ghcr.io/v2/"": received unexpected HTTP status: 503 Service Unavailable`","1,-1    ",1,-1,0
1451982884,822522,2023/3/3,v3.4.0-rc2,Merged to master/3.4,"1,-2    ",1,-2,-1
1449141477,6477701,2023/3/3,v3.4.0-rc2,Sounds good. Mind filing a JIRA please? ,"1,-1    ",1,-1,0
1449733892,2598924,2023/3/3,v3.4.0-rc2,"Good morning @HyukjinKwon,

I requested for a JIRA account yesterday, still waiting for confirmation.
I have a few questions:

- Shall we fix the data types also in tests?
- For the moment, I have looked only for np.bool, np.object ans np.object0 but not the rest. Shall i try to find if there is something else?
- I have a workflow error,  Workflow run detection failed, i enabled github actions on my forked repo, how can i re run the tests?

","1,-1    ",1,-1,0
1449837100,6477701,2023/3/3,v3.4.0-rc2,"
> - Shall we fix the data types also in tests?

Yeah, if the test failures look related, let's fix them.

> - For the moment, I have looked only for np.bool, np.object ans np.object0 but not the rest. Shall i try to find if there is something else?

If there are not a lot, it would be great to fix them together. If the occurrences are a lot, feel free to create a separate PR.

> - I have a workflow error,  Workflow run detection failed, i enabled github actions on my forked repo, how can i re run the tests?

If you rebase or merge the upstream, the test will be triggered, and the GitHub check status will be updated.
","1,-1    ",1,-1,0
1450063606,2598924,2023/3/3,v3.4.0-rc2,"@HyukjinKwon: I grepped for all the deprecated types and I list my findings below, please let me know if you see something that should not be changed.

For the deprecations introduced by numpy 1.24.0 and greping master branch as cloned yesterday:

```
spark % git grep np.object0
python/pyspark/sql/pandas/conversion.py:                                np.object0 if pandas_type is None else pandas_type
spark % git grep np.str0
spark % git grep np.bytes0
spark % git grep np.void0
spark % git grep np.int0
spark % git grep np.uint0
spark % git grep np.bool8
```

As we see we have only one np.object0 so we are pretty safe with these numpy changes.

For the deprecations introduced by numpy 1.20.0 that resulted in removals in 1.24.0 and greping master branch as cloned yesterday:

```
spark % git grep np.float | grep -v np.float_ | grep -v np.float64 | grep -v np.float32 | grep -v np.float8 | grep -v np.float16
mllib/src/test/scala/org/apache/spark/ml/feature/RobustScalerSuite.scala:      X = np.array([[0, 0], [1, -1], [2, -2], [3, -3], [4, -4]], dtype=np.float)
python/docs/source/user_guide/pandas_on_spark/types.rst:np.float      DoubleType
python/pyspark/pandas/tests/indexes/test_base.py:        self.assert_eq(psidx.astype(np.float), pidx.astype(np.float))
python/pyspark/pandas/tests/test_series.py:        self.assert_eq(psser.astype(np.float), pser.astype(np.float))
python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ps.DataFrame[np.float, str]:
python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ps.DataFrame[np.float]:
python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> 'ps.DataFrame[np.float, str]':
python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> 'ps.DataFrame[np.float]':
python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ps.DataFrame['a': np.float, 'b': int]:
python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ""ps.DataFrame['a': np.float, 'b': int]"":
spark % git grep np.str | grep -v np.str_ | grep -v np.string_
python/docs/source/user_guide/pandas_on_spark/types.rst:np.string\_   BinaryType
python/docs/source/user_guide/pandas_on_spark/types.rst:np.str        StringType
python/pyspark/pandas/tests/test_typedef.py:            np.str: (np.unicode_, StringType()),
spark % git grep np.object | grep -v np.object_ 
python/pyspark/sql/pandas/conversion.py:                                np.object0 if pandas_type is None else pandas_type
python/pyspark/sql/pandas/conversion.py:                corrected_dtypes[index] = np.object  # type: ignore[attr-defined]
python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[1], np.object)
python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[4], np.object)  # datetime.date
python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[1], np.object)
python/pyspark/sql/tests/test_dataframe.py:                self.assertEqual(types[6], np.object)
python/pyspark/sql/tests/test_dataframe.py:                self.assertEqual(types[7], np.object)
spark % git grep np.complex | grep -v np.complex_ 
spark % git grep np.long  
spark % git grep np.unicode | grep -v np.unicode_ 
python/docs/source/user_guide/pandas_on_spark/types.rst:np.unicode\_  StringType
spark % git grep np.bool | grep -v np.bool_ 
python/docs/source/user_guide/pandas_on_spark/types.rst:np.bool       BooleanType
python/pyspark/pandas/tests/test_typedef.py:            np.bool: (np.bool, BooleanType()),
python/pyspark/pandas/tests/test_typedef.py:            bool: (np.bool, BooleanType()),
python/pyspark/sql/tests/test_dataframe.py:        self.assertEqual(types[2], np.bool)
spark % git grep np.int | grep -v np.int_ | grep -v np.int64 | grep -v np.int32 | grep -v np.int8 | grep -v np.int16
connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/consumer/KafkaDataConsumer.scala:      // sadly we can't pinpoint specific data and invalidate cause we don't have unique id
core/src/main/resources/org/apache/spark/ui/static/vis-timeline-graph2d.min.js:
python/docs/source/user_guide/pandas_on_spark/types.rst:np.int        LongType
python/pyspark/mllib/regression.py:        return np.interp(x, self.boundaries, self.predictions)  # type: ignore[arg-type]
python/pyspark/pandas/groupby.py:        >>> def plus_max(x) -> ps.Series[np.int]:
python/pyspark/pandas/groupby.py:        >>> def plus_length(x) -> np.int:
python/pyspark/pandas/groupby.py:        >>> def calculation(x, y, z) -> np.int:
python/pyspark/pandas/groupby.py:        >>> def plus_max(x) -> ps.Series[np.int]:
python/pyspark/pandas/groupby.py:        >>> def calculation(x, y, z) -> ps.Series[np.int]:
python/pyspark/pandas/tests/indexes/test_base.py:        self.assert_eq(psidx.astype(np.int), pidx.astype(np.int))
python/pyspark/pandas/tests/test_series.py:        self.assert_eq(psser.astype(np.int), pser.astype(np.int))
```

As you can see the 2 most difficult types are the int and the float where I find even scala files and 1 js file. i will go through thoroughly the lines and let you know.","1,-1    ",1,-1,0
1450247224,2598924,2023/3/3,v3.4.0-rc2,"@HyukjinKwon,

I fixed all the changes I could find. 
The tests are running but the base image is not working for me, consequently some of them are incomplete.
```
Run docker/login-action@v2
Logging into ghcr.io...
Error: Error response from daemon: Get ""https://ghcr.io/v2/"": received unexpected HTTP status: 503 Service Unavailable
```

I suggest we do the review, then I squash and remove the wip tag. What do you say?","1,-1    ",1,-1,0
1450253526,822522,2023/3/3,v3.4.0-rc2,"We need to file a JIRA too.
BTW we merged a similar change for np.bool back to Spark 3.3.x; maybe we should do the same here.","2,-1    ",2,-1,1
1450254324,822522,2023/3/3,v3.4.0-rc2,"Actually I don't know, this change might theoretically be breaking? I wasn't clear","1,-1    ",1,-1,0
1450344357,2598924,2023/3/3,v3.4.0-rc2,"@srowen: I will create the JIRA still waiting on an answer from the mailing list.
Why do you think the change is a breaking one?","1,-1    ",1,-1,0
1450370402,822522,2023/3/3,v3.4.0-rc2,"I dont' know, maybe it doesn't. For example if I have something like `def func() -> ps.DataFrame[np.float, str]` in my code, does it still work?","1,-1    ",1,-1,0
1450588436,2598924,2023/3/3,v3.4.0-rc2,@srowen: While that code does not exist in pyspark it is only used as an example in the infer_return_type() function I can tell you that for your example using numpy>=1.24.0 will result in an attribute error.,"1,-1    ",1,-1,0
1450590783,822522,2023/3/3,v3.4.0-rc2,"OK. After this change, would this still work with numpy 1.20.0, for example? I think that's the question.","1,-1    ",1,-1,0
1450611648,2598924,2023/3/3,v3.4.0-rc2,"> OK. After this change, would this still work with numpy 1.20.0, for example? I think that's the question.

Actually to make it even more clear, 

The only changes that are functional are related with the conversion.py file. The rest of the changes are inside tests in the user_guide or in some docstrings describing specific functions. Since I am not an expert in these tests I wait for the reviewer and some people with more experience in the pyspark code.

But for your question, these types are aliases for classic python types so yes they should work with all the numpy versions [1](https://numpy.org/devdocs/release/1.20.0-notes.html), [2](https://stackoverflow.com/questions/74844262/how-can-i-solve-error-module-numpy-has-no-attribute-float-in-python). The error or warning comes from the call to the numpy.

I attached 2 links which explain the use-case.","1,-1    ",1,-1,0
1451085954,6477701,2023/3/3,v3.4.0-rc2,"Maybe let's create a JIRA .. 

> I will create the JIRA still waiting on an answer from the mailing list.

BTW, what's the title of your email?","1,-1    ",1,-1,0
1451394298,2598924,2023/3/3,v3.4.0-rc2,"> Maybe let's create a JIRA ..
> 
> > I will create the JIRA still waiting on an answer from the mailing list.
> 
> BTW, what's the title of your email?

Email never arrived although i sent it to private[at]spark.apache.org like it says in the contribution guide.
I self registered with the other process described in the guide.

Consequently, I just created the JIRA.","2,-1    ",2,-1,1
1451407265,6477701,2023/3/3,v3.4.0-rc2,Seems like linter fails (https://github.com/aimtsou/spark/actions/runs/4304579333/jobs/7506798202).,"2,-1    ",2,-1,1
1451412360,2598924,2023/3/3,v3.4.0-rc2,Yes but this is the original [code](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/conversion.py#L235). Shall I remove the comment and how did it pass the linter to be in master?,"1,-1    ",1,-1,0
1451854672,822522,2023/3/3,v3.4.0-rc2,"The line changed, and now the 'ignore' is no longer relevant - yes remove it to pass the linter","1,-1    ",1,-1,0
1452145653,2598924,2023/3/3,v3.4.0-rc2,"Tests are completed, shall I squash and remove wip tag from the pull request?","1,-1    ",1,-1,0
1452232660,822522,2023/3/3,v3.4.0-rc2,"Yes remove WIP just for completeness. No need to squash, the script does that","1,-1    ",1,-1,0
1452779722,822522,2023/3/3,v3.4.0-rc2,"Merged to master/3.4/3.3, for consistency with https://issues.apache.org/jira/browse/SPARK-40376","1,-1    ",1,-1,0
1448581948,1938382,2023/3/3,v3.4.0-rc2,@hvanhovell ,"1,-1    ",1,-1,0
1448968612,9700541,2023/3/3,v3.4.0-rc2,"Merged to master/3.4. Thank you, @amaliujia and @hvanhovell .","2,-1    ",2,-1,1
1449136238,7322292,2023/3/3,v3.4.0-rc2,"Late LGTM, in the previous PR I am not very sure about the `session.version` so didn't touch it","1,-1    ",1,-1,0
1449140514,6477701,2023/3/3,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1448966981,6570401,2023/3/3,v3.4.0-rc2,"cc @sunchao @cloud-fan @dongjoon-hyun 

Note that [this commit](https://github.com/apache/spark/pull/40224/commits/a31cd9bf95cd1282ea6fe6880693ad52332fc145) represents the delta from #40144 ","1,-1    ",1,-1,0
1450890699,506679,2023/3/3,v3.4.0-rc2,"Merged to master, thanks @xkrogen !","1,-1    ",1,-1,0
1451071963,9700541,2023/3/3,v3.4.0-rc2,"Thank you, @xkrogen and @sunchao !","1,-1    ",1,-1,0
1449170950,9700541,2023/3/3,v3.4.0-rc2,"Thank you, @HyukjinKwon . Merged to master for Apache Spark 3.5.","1,-1    ",1,-1,0
1477003627,2249648,2023/3/3,v3.4.0-rc2,Hey @dongjoon-hyun - Can you please point me to some documentation about how this benchmarking is done? I'd like to run the same benchmark locally.,"1,-1    ",1,-1,0
1477024392,9700541,2023/3/3,v3.4.0-rc2,"Here is the official document about how to run the benchmark in your GitHub Action. Please see `Running benchmarks in your forked repository` Section.
- https://spark.apache.org/developer-tools.html

In addition, you can check GitHub Action benchmark job to see the logic.
- https://github.com/apache/spark/blob/master/.github/workflows/benchmark.yml

Lastly, each benchmark file has a command-line direction in their header file, @JohnTortugo .","1,-1    ",1,-1,0
1477026864,2249648,2023/3/3,v3.4.0-rc2,Thanks a lot!,"1,-1    ",1,-1,0
1449190050,7322292,2023/3/3,v3.4.0-rc2,"thanks, merged to master/3.4","1,-1    ",1,-1,0
1449618600,7322292,2023/3/3,v3.4.0-rc2,merged into master/branch-3.4,"1,-1    ",1,-1,0
1449148370,1938382,2023/3/3,v3.4.0-rc2,@hvanhovell,"1,-1    ",1,-1,0
1454255864,9616802,2023/3/3,v3.4.0-rc2,"@amaliujia if you have time, let's also get this one over the line.","1,-1    ",1,-1,0
1454313336,1938382,2023/3/4,v3.4.0-rc2,@hvanhovell I just addressed actionable comments,"1,-1    ",1,-1,0
1455352755,9616802,2023/3/4,v3.4.0-rc2,@amaliujia can you update the PR?,"1,-1    ",1,-1,0
1455359011,1938382,2023/3/4,v3.4.0-rc2,@hvanhovell waiting for CI,"1,-1    ",1,-1,0
1455466444,7322292,2023/3/4,v3.4.0-rc2,merged into master/branch-3.4,"1,-1    ",1,-1,0
1449173060,99207096,2023/3/4,v3.4.0-rc2,Hi @gengliangwang ð ,"1,-1    ",1,-1,0
1449391578,1097932,2023/3/4,v3.4.0-rc2,LGTM except one comment and the pending test failures.,"1,-1    ",1,-1,0
1451285970,1097932,2023/3/4,v3.4.0-rc2,"Thanks, merging to master/3.4
This makes the column default feature simpler and more reasonable.
cc @xinrong-meng ","1,-1    ",1,-1,0
1450159427,822522,2023/3/4,v3.4.0-rc2,Merged to master,"1,-1    ",1,-1,0
1449621060,9700541,2023/3/4,v3.4.0-rc2,"Could you review this PR, @HyukjinKwon ?","1,-1    ",1,-1,0
1449683073,9700541,2023/3/4,v3.4.0-rc2,"Thank you, @HyukjinKwon ! Merged to master/3.4.","1,-1    ",1,-1,0
1459503928,7322292,2023/3/4,v3.4.0-rc2,close in favor of https://github.com/apache/spark/pull/40260,"1,-1    ",1,-1,0
1450620126,46442880,2023/3/4,v3.4.0-rc2,@dongjoon-hyun It is! Not sure how that happened... Should be fixed now!,"1,-1    ",1,-1,0
1451097485,9616802,2023/3/4,v3.4.0-rc2,@dongjoon-hyun are you ok with this PR now?,"1,-1    ",1,-1,0
1451098428,9700541,2023/3/4,v3.4.0-rc2,"Sure! Thank you for checking again, @hvanhovell .","1,-1    ",1,-1,0
1451100966,9616802,2023/3/4,v3.4.0-rc2,"Alright, merging this one.","1,-1    ",1,-1,0
1450588332,9616802,2023/3/4,v3.4.0-rc2,cc @zhenlineo @LuciferYang ,"1,-1    ",1,-1,0
1451078247,6477701,2023/3/4,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1451377823,21204523,2023/3/4,v3.4.0-rc2,"The build is failing due to formatting issues, but due to older code: should I patch it all in this PR or create a separate one?","1,-1    ",1,-1,0
1453042684,21204523,2023/3/4,v3.4.0-rc2,@MaxGekk Can you take a look at this please?,"1,-1    ",1,-1,0
1454045008,44108233,2023/3/4,v3.4.0-rc2,"Just FYI and really not a big deal, we typically use upper-cased ""TESTS"" or ""TEST"" for PR title when the change only includes the tests.","1,-1    ",1,-1,0
1462210586,1580697,2023/3/4,v3.4.0-rc2,@the8thC Do you have an account of OSS JIRA (https://issues.apache.org/jira/browse/SPARK-38735)?,"1,-1    ",1,-1,0
1462470101,21204523,2023/3/4,v3.4.0-rc2,"@MaxGekk No, I don't, but I've just requested one using ""selfserve"". Is that the right way?","1,-1    ",1,-1,0
1462661054,1580697,2023/3/4,v3.4.0-rc2,"> @MaxGekk No, I don't, but I've just requested one using ""selfserve"". Is that the right way?

Yep, thank you.
","1,-1    ",1,-1,0
1462661888,1580697,2023/3/5,v3.4.0-rc2,"+1, LGTM. Merging to master.
Thank you, @the8thC and @itholic for review.","1,-1    ",1,-1,0
1462664558,1580697,2023/3/5,v3.4.0-rc2,@the8thC Congratulations with your first contribution to Apache Spark!,"1,-1    ",1,-1,0
1450713052,122326661,2023/3/5,v3.4.0-rc2,@MaxGekk could you take a look at it? Thank you very much!,"1,-1    ",1,-1,0
1453041214,122326661,2023/3/5,v3.4.0-rc2,"@MaxGekk Thanks for reviewing! I guess I need to ask you to merge this PR right? Also, it should be ported into older branches with `TimestampAdd` (3.3.0, 3.3.1, 3.3.2).","1,-2    ",1,-2,-1
1453050502,1580697,2023/3/5,v3.4.0-rc2,"+1, LGTM. Merging to master/3.4.
Thank you, @chenhao-db.","1,-1    ",1,-1,0
1453051043,1580697,2023/3/5,v3.4.0-rc2,"@chenhao-db Your changes cause some conflicts in `branch-3.3`. Please, open a separate PR with a backport to Spark 3.3.","1,-1    ",1,-1,0
1453059745,1580697,2023/3/5,v3.4.0-rc2,@chenhao-db Congratulations with your first contribution to Apache Spark!,"1,-1    ",1,-1,0
1450758457,1938382,2023/3/5,v3.4.0-rc2,Need add a test for the new added field?,"1,-1    ",1,-1,0
1453013336,6477701,2023/3/5,v3.4.0-rc2,test: https://github.com/hvanhovell/spark/actions/runs/4318951397,"1,-1    ",1,-1,0
1451077152,6477701,2023/3/5,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1451076639,6477701,2023/3/5,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1451104705,7322292,2023/3/5,v3.4.0-rc2,"good catch, we may leverage the new proto `DDLParse` later","1,-1    ",1,-1,0
1451141281,506656,2023/3/5,v3.4.0-rc2,"> we may leverage the new proto `DDLParse` later

Sounds good. Are you working on it? Please let me know once it's done. I'll address it.","1,-1    ",1,-1,0
1451089774,1938382,2023/3/5,v3.4.0-rc2,@hvanhovell @LuciferYang ,"1,-1    ",1,-1,0
1451090043,1938382,2023/3/5,v3.4.0-rc2,We may have https://github.com/apache/spark/pull/40213 merged first since that looks pretty good already.,"1,-1    ",1,-1,0
1452528841,9616802,2023/3/5,v3.4.0-rc2,Merging.,"1,-1    ",1,-1,0
1451399400,9700541,2023/3/6,v3.4.0-rc2,"Thank you, @hvanhovell and @HyukjinKwon . Merged to master/3.4.","2,-1    ",2,-1,1
1451586124,7322292,2023/3/6,v3.4.0-rc2,merged into master/3.4,"2,-1    ",2,-1,1
1455922911,47337188,2023/3/6,v3.4.0-rc2,CC @zhengruifeng @HyukjinKwon @ueshin @hvanhovell ,"1,-1    ",1,-1,0
1456074152,47337188,2023/3/6,v3.4.0-rc2,"After double thoughts, I add `spark.udf.registerJavaUDAF`'s implementation into this PR, since both APIs rely on the same proto message JavaUDF.","1,-1    ",1,-1,0
1457397715,6477701,2023/3/6,v3.4.0-rc2,WDYT @hvanhovell ?,"1,-1    ",1,-1,0
1459608526,47337188,2023/3/6,v3.4.0-rc2,"Merged to master and branch-3.4, thanks all!","1,-1    ",1,-1,0
1451293292,6477701,2023/3/6,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1451297921,9700541,2023/3/6,v3.4.0-rc2,"cc @grundprinzip , @hvanhovell , @zhengruifeng, @HyukjinKwon , @xinrong-meng ","1,-1    ",1,-1,0
1451307674,9700541,2023/3/6,v3.4.0-rc2,Thank you! I missed the doctest. Let me fix the doctest too.,"2,-1    ",2,-1,1
1451339775,1475305,2023/3/6,v3.4.0-rc2,"Hi ~ @dongjoon-hyun , could you please help check the results of local execution of 

```
build/sbt clean ""connect-client-jvm/test"" -Dspark.debug.sc.jvm.client=true
```
?
I found the following errors:
```
[info] ClientE2ETestSuite:
Starting the Spark Connect Server...
Using jar: /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/server/target/scala-2.12/spark-connect-assembly-3.5.0-SNAPSHOT.jar
Ready for client connections.
java.lang.RuntimeException: Failed to start the test server on port 15971.
  | => cat org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll(RemoteSparkSession.scala:129)
	at org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll$(RemoteSparkSession.scala:120)
	at org.apache.spark.sql.ClientE2ETestSuite.beforeAll(ClientE2ETestSuite.scala:36)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:36)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
....

Suppressed: io.grpc.StatusRuntimeException: INTERNAL: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
		at io.grpc.Status.asRuntimeException(Status.java:535)
		at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
		at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
		at scala.collection.Iterator.find(Iterator.scala:993)
		at scala.collection.Iterator.find$(Iterator.scala:992)
		at scala.collection.AbstractIterator.find(Iterator.scala:1431)
		at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:133)
		at org.apache.spark.sql.SparkSession.$anonfun$sql$1$adapted(SparkSession.scala:125)
		at org.apache.spark.sql.SparkSession.newDataset(SparkSession.scala:258)
		at org.apache.spark.sql.SparkSession.newDataFrame(SparkSession.scala:252)
		at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:125)
		at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:109)
		at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:147)
		at org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll(RemoteSparkSession.scala:135)
		... 13 more
```

Seems also related to [SPARK-41725](https://issues.apache.org/jira/browse/SPARK-41725)? Thanks ~","1,-1    ",1,-1,0
1451340820,9700541,2023/3/6,v3.4.0-rc2,"Yes, you need `-Phive` after SPARK-41725.  I already checked that. I made this PR because of that, @LuciferYang .","1,-1    ",1,-1,0
1451341835,1475305,2023/3/6,v3.4.0-rc2,"> Yes, you need `-Phive` after [SPARK-41725](https://issues.apache.org/jira/browse/SPARK-41725). I already checked that. I made this PR because of that, @LuciferYang .

Thanks ~","1,-1    ",1,-1,0
1451377939,9700541,2023/3/6,v3.4.0-rc2,"Thank you, @HyukjinKwon and @LuciferYang . Merged to master/3.4.","1,-1    ",1,-1,0
1451380308,7322292,2023/3/6,v3.4.0-rc2,"Late LGTM, thank you @dongjoon-hyun !","1,-1    ",1,-1,0
1451319467,15246973,2023/3/6,v3.4.0-rc2,cc @dongjoon-hyun ,"1,-1    ",1,-1,0
1451319998,1475305,2023/3/6,v3.4.0-rc2,https://github.com/apache/spark/pull/40065#issuecomment-1435520529,"1,-1    ",1,-1,0
1451320048,9700541,2023/3/6,v3.4.0-rc2,"We already verified this.
- https://github.com/apache/spark/pull/40065","1,-1    ",1,-1,0
1451320180,9700541,2023/3/6,v3.4.0-rc2,"Ya, @LuciferYang did it. Let me close this first.","1,-1    ",1,-1,0
1451320363,1475305,2023/3/6,v3.4.0-rc2,Thanks @dongjoon-hyun ,"1,-1    ",1,-1,0
1453588768,822522,2023/3/6,v3.4.0-rc2,Merged to master,"1,-1    ",1,-1,0
1451470960,9700541,2023/3/6,v3.4.0-rc2,cc @srowen and @HyukjinKwon ,"1,-1    ",1,-1,0
1451524437,9700541,2023/3/6,v3.4.0-rc2,"Thank you, @HyukjinKwon and @LuciferYang .

The license test passed. Merged to master/3.4/3.3/3.2.
![Screenshot 2023-03-02 at 1 02 02 AM](https://user-images.githubusercontent.com/9700541/222381571-afcd08bf-0503-4250-8085-c2c824f570a5.png)
","1,-1    ",1,-1,0
1451560563,6477701,2023/3/6,v3.4.0-rc2,Merged to master.,"1,-1    ",1,-1,0
1451727734,6477701,2023/3/6,v3.4.0-rc2,"All related tests passed.

Merged to master and branch-3.4.","1,-1    ",1,-1,0
1451774998,8486025,2023/3/6,v3.4.0-rc2,"The jdbc API seems hard to test, do we need a test case? @hvanhovell @HyukjinKwon @zhengruifeng @dongjoon-hyun ","1,-1    ",1,-1,0
1451778555,8486025,2023/3/6,v3.4.0-rc2,"There is another kind jdbc API, see: https://github.com/apache/spark/blob/79da1ab400f25dbceec45e107e5366d084138fa8/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L316
I will create another PR to add proto msg and implement it.","1,-1    ",1,-1,0
1451803222,7322292,2023/3/6,v3.4.0-rc2,I guess you can refer to `JDBCSuite` and `ClientE2ETestSuite` ?,"1,-1    ",1,-1,0
1452353820,9616802,2023/3/6,v3.4.0-rc2,"@beliefer you can create a test in `PlanGenerationTestSuite`. That will at least validate the proto message we are generating, and it will validate that plan you are producing yields a valid plan in `ProtoToPlanTestSuite`.","1,-1    ",1,-1,0
1452763000,8486025,2023/3/6,v3.4.0-rc2,"> I guess you can refer to `JDBCSuite` and `ClientE2ETestSuite` ?

The built-in H2 running in server side and we can't start H2 database at connect API.","1,-1    ",1,-1,0
1453086009,8486025,2023/3/6,v3.4.0-rc2,"> @beliefer you can create a test in `PlanGenerationTestSuite`. That will at least validate the proto message we are generating, and it will validate that plan you are producing yields a valid plan in `ProtoToPlanTestSuite`.

OK. I added the test cases. But throws
```
org.h2.jdbc.JdbcSQLSyntaxErrorException: Schema ""TEST"" not found; SQL statement:
SELECT * FROM TEST.TIMETYPES WHERE 1=0 [90079-214]
```
 when ProtoToParsedPlanTestSuite validating the golden file. So I create some schema and table in H2 database.","1,-1    ",1,-1,0
1454320335,8486025,2023/3/6,v3.4.0-rc2,@dongjoon-hyun @hvanhovell It seems the build scala 2.13 failed is unrelated to this PR.,"1,-1    ",1,-1,0
1454350262,9616802,2023/3/6,v3.4.0-rc2,merging to master/3.4,"1,-1    ",1,-1,0
1454351727,8486025,2023/3/6,v3.4.0-rc2,@hvanhovell @dongjoon-hyun @LuciferYang Thank you!,"1,-1    ",1,-1,0
1451830956,1580697,2023/3/6,v3.4.0-rc2,"@jiang13021 Thank you for the backport. Could add the following, please:
1. The tag `[3.3]` to PR's title.
2. `This is a backport of https://github.com/apache/spark/pull/40195` to PR's description.","1,-1    ",1,-1,0
1451860426,45600652,2023/3/6,v3.4.0-rc2,"> @jiang13021 Thank you for the backport. Could add the following, please:
> 
> 1. The tag `[3.3]` to PR's title.
> 2. `This is a backport of https://github.com/apache/spark/pull/40195` to PR's description.

Done","1,-1    ",1,-1,0
1452051052,1580697,2023/3/6,v3.4.0-rc2,"+1, LGTM. All GAs passed. Merging to 3.3.
Thank you, @jiang13021.","1,-1    ",1,-1,0
1453831820,1475305,2023/3/6,v3.4.0-rc2,"friendly ping @dongjoon-hyun , I found the following error message in Java11&17 maven build log

```
Error: [ERROR] An error occurred attempting to read POM
org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:568)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
```

- https://github.com/apache/spark/actions/runs/4324211751/jobs/7548768567
- https://github.com/apache/spark/actions/runs/4323972537/jobs/7548220619
- https://github.com/LuciferYang/spark/actions/runs/4315955520/jobs/7542233374

Will this cause errors in the build of bom?

","1,-1    ",1,-1,0
1454336997,1475305,2023/3/6,v3.4.0-rc2,"Recently, I often encounter Maven build failed  of Java 11&17 GA build task due to timeout ... a little strange","1,-1    ",1,-1,0
1454360432,1475305,2023/3/6,v3.4.0-rc2,"> friendly ping @dongjoon-hyun , I found the following error message in Java11&17 maven build log
> 
> ```
> Error: [ERROR] An error occurred attempting to read POM
> org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
>     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
>     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
>     at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
>     at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
>     at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
>     at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
>     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
>     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
>     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
>     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
>     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
>     at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
>     at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
>     at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
>     at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
>     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
>     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
>     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
>     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
>     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
>     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
>     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
>     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
>     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
>     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
>     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
>     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
>     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
>     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
>     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
>     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
>     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
>     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
>     at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
>     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
>     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
>     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
>     at java.lang.reflect.Method.invoke (Method.java:568)
>     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
>     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
>     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
>     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
> ```
> 
> * https://github.com/apache/spark/actions/runs/4324211751/jobs/7548768567
> * https://github.com/apache/spark/actions/runs/4323972537/jobs/7548220619
> * https://github.com/LuciferYang/spark/actions/runs/4315955520/jobs/7542233374
> 
> Will this cause errors in the build of bom?

I know, GA already use maven 3.9.0 to build, this is a well know issue

<img width=""961"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/222873867-bdcd885f-96bf-4abc-8106-6859ed97d25d.png"">
","1,-1    ",1,-1,0
1454978104,1475305,2023/3/6,v3.4.0-rc2,"@srowen  this one ready, all test passed","1,-1    ",1,-1,0
1454978494,1475305,2023/3/6,v3.4.0-rc2,"> > friendly ping @dongjoon-hyun , I found the following error message in Java11&17 maven build log
> > ```
> > Error: [ERROR] An error occurred attempting to read POM
> > org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) 
> >     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)
> >     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)
> >     at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)
> >     at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)
> >     at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)
> >     at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)
> >     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)
> >     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)
> >     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)
> >     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)
> >     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)
> >     at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)
> >     at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)
> >     at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)
> >     at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)
> >     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)
> >     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)
> >     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)
> >     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)
> >     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)
> >     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)
> >     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)
> >     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)
> >     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)
> >     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)
> >     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)
> >     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)
> >     at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)
> >     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
> >     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)
> >     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
> >     at java.lang.reflect.Method.invoke (Method.java:568)
> >     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
> >     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
> >     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
> >     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > 
> > * https://github.com/apache/spark/actions/runs/4324211751/jobs/7548768567
> > * https://github.com/apache/spark/actions/runs/4323972537/jobs/7548220619
> > * https://github.com/LuciferYang/spark/actions/runs/4315955520/jobs/7542233374
> > 
> > Will this cause errors in the build of bom?
> 
> I know, GA already use maven 3.9.0 to build, this is a well know issue
> 
> <img alt=""image"" width=""961"" src=""https://user-images.githubusercontent.com/1475305/222873867-bdcd885f-96bf-4abc-8106-6859ed97d25d.png"">

This is not related to current pr. Let me see how to solve this problem later



","1,-1    ",1,-1,0
1455328473,1475305,2023/3/6,v3.4.0-rc2,friendly ping @srowen ,"1,-1    ",1,-1,0
1455341403,822522,2023/3/6,v3.4.0-rc2,Merged to master,"1,-1    ",1,-1,0
1455349598,1475305,2023/3/6,v3.4.0-rc2,Thanks @srowen ,"1,-1    ",1,-1,0
1453850392,9616802,2023/3/6,v3.4.0-rc2,@LuciferYang can you update the binary compatibility tests?,"1,-1    ",1,-1,0
1453872602,1475305,2023/3/6,v3.4.0-rc2,"> @LuciferYang can you update the binary compatibility tests?

done","1,-1    ",1,-1,0
1454330973,1475305,2023/3/6,v3.4.0-rc2,Now all paased,"1,-1    ",1,-1,0
1454994633,1475305,2023/3/6,v3.4.0-rc2,GA passed,"1,-1    ",1,-1,0
1455323028,9616802,2023/3/6,v3.4.0-rc2,Merging.,"1,-2    ",1,-2,-1
1455324716,1475305,2023/3/6,v3.4.0-rc2,Thanks @hvanhovell @HyukjinKwon @zhengruifeng @amaliujia ,"1,-1    ",1,-1,0
1452899534,1938382,2023/3/6,v3.4.0-rc2,Overall looks good. Thank you! ,"1,-1    ",1,-1,0
1452827799,9616802,2023/3/6,v3.4.0-rc2,Merging this.,"1,-1    ",1,-1,0
1452877870,6477701,2023/3/6,v3.4.0-rc2,"Seems like tests did not pass, and it fails in the master branch (https://github.com/apache/spark/actions/runs/4319488463/jobs/7538760733).

Let me quickly revert this for now.","1,-1    ",1,-1,0
1453013522,6477701,2023/3/6,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1453913043,109815907,2023/3/6,v3.4.0-rc2,"@srowen  Please ignore that change. It was work in progress to check few things. 
The reason why we get ambiguous error in below scenario and why it's not correct is the result of attribute resolution returns  
two values but both values are same. Thus, it should not throw ambiguous error.

val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_mixed_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""ID"")
val df3 = df1.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)
df3.select(""id"").show()
org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id.

df3.explain()
== Physical Plan ==
*(1) Project [_1#6 AS id#17, _2#7 AS col2#18, _3#8 AS col3#19, _4#9 AS col4#20, _5#10 AS col5#21, _1#6 AS ID#17]

Before the fix, attributes matched were:
attributes: Vector(id#17, id#17)
Thus, it throws ambiguous reference error. But if we consider only unique matches, it will return correct result.
unique attributes: Vector(id#17)
","2,-2    ",2,-2,0
1454348175,109815907,2023/3/6,v3.4.0-rc2,@srowen @dongjoon-hyun Can you please review this PR?,"1,-1    ",1,-1,0
1457585690,109815907,2023/3/6,v3.4.0-rc2,Gentle Ping @srowen  @dongjoon-hyun @mridulm @HyukjinKwon ,"1,-1    ",1,-1,0
1458138622,822522,2023/3/6,v3.4.0-rc2,"I'm not sure about the change, not sure I'm qualified to review it. I think at best the error message should change; I am not clear that the result is 'wrong'","1,-1    ",1,-1,0
1458159825,109815907,2023/3/6,v3.4.0-rc2,"> I'm not sure about the change, not sure I'm qualified to review it. I think at best the error message should change; I am not clear that the result is 'wrong'

Thanks for replying. Can you please tag someone who should be right person to review this change?","1,-1    ",1,-1,0
1459535564,109815907,2023/3/6,v3.4.0-rc2,Gentle ping @dongjoon-hyun @mridulm @HyukjinKwon @yaooqinn Can you please review this PR?,"1,-1    ",1,-1,0
1459648316,8326978,2023/3/6,v3.4.0-rc2,Can you try `set spark.sql.caseSensitive=true`?,"1,-1    ",1,-1,0
1459652942,109815907,2023/3/6,v3.4.0-rc2,"> Can you try `set spark.sql.caseSensitive=true`?

Yes, I have tried it. With caseSensitive set to true, it will work as then id and ID will be treated as separate columns.
Issue is when columns names are supposed to considered  as case insensitive.","2,-1    ",2,-1,1
1459654900,8326978,2023/3/6,v3.4.0-rc2,"You first defined a case-sensitive data set, then queried in a case-insensitive way, I guess the error is expected.","2,-1    ",2,-1,1
1459668923,109815907,2023/3/6,v3.4.0-rc2,"> You first defined a case-sensitive data set, then queried in a case-insensitive way, I guess the error is expected.

In the physical plan, both id and ID columns are projected to the same column in the dataframe: _1#6
_1#6 AS id#17,  _1#6 AS ID#17
So, there is no ambiguity,

Also, in the matched attributes, results are same: attributes: Vector(id#17, id#17)
Just because, we have duplicates in the matched result, it's being considered as ambiguous.

If the matched attribute result was Vector(id#17, ID#17) , then it would have been valid error.

And even if the dataset has columns in different cases, Spark being case insensitive by default, should consider both columns as same.
","1,-1    ",1,-1,0
1460147096,822522,2023/3/6,v3.4.0-rc2,"I don't get it, it is due to case sensitivity; that's why it becomes ambiguous and that's what you see. The issue is that the error isn't super helpful because it shows the lower-cased column right? that's what I was saying. Or: does your change still result in an error without case sensitivity? it should","1,-1    ",1,-1,0
1461262849,109815907,2023/3/6,v3.4.0-rc2,"> I don't get it, it is due to case sensitivity; that's why it becomes ambiguous and that's what you see. The issue is that the error isn't super helpful because it shows the lower-cased column right? that's what I was saying. Or: does your change still result in an error without case sensitivity? it should

The issue is not with the error message. Problem is that in this case error should not be thrown. Select query should return result.  After this change, ambiguous error will not be thrown as we are fixing the duplicate attribute match. ","1,-1    ",1,-1,0
1461279768,822522,2023/3/6,v3.4.0-rc2,"Hm, how is it not ambiguous? When case insensitive, 'id' could mean one of two different columns ","1,-1    ",1,-1,0
1461288036,109815907,2023/3/6,v3.4.0-rc2,"> Hm, how is it not ambiguous? When case insensitive, 'id' could mean one of two different columns

It's not ambiguous because the  when we are selecting using list of column names, both id and ID are getting value from same column 'id' in the source dataframe. 
val **df1** = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""**id""**,""col2"",""col3"",""col4"", ""col5"")
val op_cols_mixed_case = List(**""id""**,""col2"",""col3"",""col4"", ""col5"", **""ID""**)
val df3 = **df1**.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)
df3.select(""id"").show()

df3.explain()
== Physical Plan ==
*(1) Project [**_1#6 AS id#17**, _2#7 AS col2#18, _3#8 AS col3#19, _4#9 AS col4#20, _5#10 AS col5#21, **_1#6 AS ID#17**]","1,-1    ",1,-1,0
1461291351,822522,2023/3/6,v3.4.0-rc2,"That isn't relevant. You are selecting from a DataFrame with cols id and ID. Imagine for instance they do not come from the same source, it's clearly ambiguous. It wouldn't make sense if it were different in this case. ","1,-1    ",1,-1,0
1461303721,109815907,2023/3/6,v3.4.0-rc2,"It's very much relevant as this is the only case which requires the fix. If they do not come from same source, the plan  will reflect that and it will throw the ambiguous error even after this fix.","2,-1    ",2,-1,1
1461314871,822522,2023/3/7,v3.4.0-rc2,"Hm, I just don't see the logic in that. It isn't how SQL works either, as far as I understand. Here's maybe another example, imagine a DataFrame defined by `SELECT 3 as id, 3 as ID`. Would you also say selecting ""id"" is unambiguous? and it makes sense to you if I change a 3 to a 4 that this query is no longer semantically valid?","1,-1    ",1,-1,0
1463230011,109815907,2023/3/7,v3.4.0-rc2,"> Hm, I just don't see the logic in that. It isn't how SQL works either, as far as I understand. Here's maybe another example, imagine a DataFrame defined by `SELECT 3 as id, 3 as ID`. Would you also say selecting ""id"" is unambiguous? and it makes sense to you if I change a 3 to a 4 that this query is no longer semantically valid?

If it's valid as per the plan then yes. ","1,-1    ",1,-1,0
1465517104,109815907,2023/3/7,v3.4.0-rc2,Gentle ping @dongjoon-hyun @mridulm @HyukjinKwon @yaooqinn Can you please review this PR or direct it to someone who can review this PR.,"1,-1    ",1,-1,0
1466139560,822522,2023/3/7,v3.4.0-rc2,"That's a ""no"" from me, per the logic above","1,-1    ",1,-1,0
1466394445,109815907,2023/3/7,v3.4.0-rc2,"> That's a ""no"" from me, per the logic above

Thanks @srowen But seems I am not able to explain the change to you. So it's better to get review from someone who is qualified to review the change and aware of this code.","1,-1    ",1,-1,0
1476233946,109815907,2023/3/7,v3.4.0-rc2,Gentle ping @dongjoon-hyun @mridulm @HyukjinKwon @yaooqinn Can you please review this PR or direct it to someone who can review this PR.,"1,-1    ",1,-1,0
1480217186,109815907,2023/3/7,v3.4.0-rc2,Gentle ping @dongjoon-hyun @mridulm @HyukjinKwon @yaooqinn Can you please review this PR or direct it to someone who can review this PR.,"1,-1    ",1,-1,0
1480469070,8326978,2023/3/7,v3.4.0-rc2,I second @srowen âs view. cc @cloud-fan,"1,-1    ",1,-1,0
1480532873,109815907,2023/3/7,v3.4.0-rc2,"> I second @srowen âs view. cc @cloud-fan

Thanks @yaooqinn for replying. Can you please explain why you think it's not the right fix? 
The fix only proposes to remove duplicates from the resolved columns. As it's incorrect to consider the only one column match as ambiguous just because it occurs more than once in the resolved column list.","1,-1    ",1,-1,0
1480540953,3182036,2023/3/7,v3.4.0-rc2,"I think column resolution should only look at one level, to make the behavior simple and predictable. I tried it on pgsql and it fails as well:
```
create table t(i int);
select id from (select i as id, i as ID from t) sub;
ERROR: column reference ""id"" is ambiguous Position: 8
```","1,-1    ",1,-1,0
1480574606,109815907,2023/3/7,v3.4.0-rc2,"> df3.select(""id"").show()

@cloud-fan The example you have shared will behave the same even after this fix. It will give ambiguous error. 
The use case which the fix is trying to solve is different. Can you please try these two cases:
Case 1: which works fine
val df1 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_same_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""id"")
val df3 = df1.select(op_cols_same_case.head, op_cols_same_case.tail: _*)
df3.select(""id"").show()  

Case 2: which doesn't work fine and the fix is to solve this issue
val df2 = sc.parallelize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(""id"",""col2"",""col3"",""col4"", ""col5"")
val op_cols_mixed_case = List(""id"",""col2"",""col3"",""col4"", ""col5"", ""ID"")
val df4 = df2.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)
df4.select(""id"").show()","1,-1    ",1,-1,0
1480574647,8326978,2023/3/7,v3.4.0-rc2,"@shrprasa 
At the dataset definition phase, especially for intermediate datasets, Spark is lenient/lazy with case sensitivity. This is because the checks happen in SQL Analyzing, which is not required for defining a Dataset. This gives the user more freedom but also cognitive disorders. On the other hand, in the read phase, SQL Analyzing is a mandatory step, and checks will be performed, so the configuration provided by Spark at this stage is sufficient to resolve all ambiguities.","1,-1    ",1,-1,0
1480589254,3182036,2023/3/7,v3.4.0-rc2,@shrprasa do you know how the case 1 works?,"1,-1    ",1,-1,0
1480595916,109815907,2023/3/7,v3.4.0-rc2,"> @shrprasa do you know how the case 1 works?

yes. It works because the resolved column has just one match 
attributes: Vector(id#17)

but for second case, the match result is
attributes: Vector(id#17, id#17)
Since, there are more than one value although both are exactly same, it fails. This fix proposes to fix this by taking distinct values of match result.


","1,-1    ",1,-1,0
1452625458,1938382,2023/3/7,v3.4.0-rc2,@hvanhovell ,"1,-1    ",1,-1,0
1453086077,7322292,2023/3/7,v3.4.0-rc2,merged into master/3.4,"1,-1    ",1,-1,0
1461828508,6477701,2023/3/7,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1452864911,7322292,2023/3/7,v3.4.0-rc2,"thank you, merged into master/3.4","1,-1    ",1,-1,0
1453092378,7322292,2023/3/7,v3.4.0-rc2,"just a idea, in this case what about:
1,  Convert Global SortExec to Local SortExec;
2, Make `SortExec` requires `SinglePartition`;

in order to let one executor instead of driver to do the computation.","1,-1    ",1,-1,0
1453105982,12025282,2023/3/7,v3.4.0-rc2,"@zhengruifeng thank you for your thought.
 
The original idea of driver sort is to avoid one shuffle. Requires SinglePartition seems does not help since it still requires a shuffle.

Besides, finally, the result would go to driver, i.e. `df.sort.collect` (It's the reason I match `ReturnAnswer`), so it should be fine to do at driver. Plus, do sort at driver is not the first code place except driver sort. e.g., the merge function of rdd.takeOrdered. It should be safe if the size of plan is small enough.

The idea of Convert Global SortExec to Local SortExec and make SortExec requires SinglePartition looks fine that It can avoid the `sample` of range partitioner. But it seems orthogonal with this pr.","1,-1    ",1,-1,0
1455303198,12025282,2023/3/7,v3.4.0-rc2,cc @cloud-fan @viirya thank you,"1,-1    ",1,-1,0
1467021744,1591700,2023/3/7,v3.4.0-rc2,"Meta comment: moving this to the driver has potential for destabilizing the application.
In SPARK-36419, we added option to move the final `treeAggregate` to executor given the scale challenges seen when it is run at driver.

One option would be to do this in an executor as @zhengruifeng suggested, and that should be much more scalable as a solution.","1,-1    ",1,-1,0
1465662299,7322292,2023/3/7,v3.4.0-rc2,"I did a quick test with dataset `T10I4D100K` in http://fimi.uantwerpen.be/data/ 

fit:
```
scala> val df = sc.textFile(""/Users/ruifeng.zheng/.dev/data/T10I4D100K.dat"").map(_.split("" "")).toDF(""items"")
df: org.apache.spark.sql.DataFrame = [items: array<string>]

scala> df.count
res16: Long = 100000

scala> val model = new FPGrowth().setMinSupport(0.01).setMinConfidence(0.01).fit(df)
model: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fpgrowth_92901252345a, numTrainingRecords=100000

scala> model.freqItemsets.count
res17: Long = 385                                                               

scala> model.associationRules.count
res18: Long = 21                                                                

scala> model.save(""/tmp/fpm.model"")
```


transformation:
```
import org.apache.spark.ml.fpm._
val df = sc.textFile(""/Users/ruifeng.zheng/.dev/data/T10I4D100K.dat"").map(_.split("" "")).toDF(""items"")
df.cache()
df.count()

val model = FPGrowthModel.load(""/tmp/fpm.model"")
model.transform(df).explain(""extended"")
Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up
val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start
val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start
```

master:
```
scala> val model = FPGrowthModel.load(""/tmp/fpm.model"")
model: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fpgrowth_92901252345a, numTrainingRecords=100000

scala> model.transform(df).explain(""extended"")
== Parsed Logical Plan ==
'Project [items#5, UDF('items) AS prediction#70]
+- Project [value#2 AS items#5]
   +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
      +- ExternalRDD [obj#1]

== Analyzed Logical Plan ==
items: array<string>, prediction: array<string>
Project [items#5, UDF(items#5) AS prediction#70]
+- Project [value#2 AS items#5]
   +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
      +- ExternalRDD [obj#1]

== Optimized Logical Plan ==
Project [items#5, UDF(items#5) AS prediction#70]
+- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)
      +- *(1) Project [value#2 AS items#5]
         +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
            +- Scan[obj#1]

== Physical Plan ==
*(1) Project [items#5, UDF(items#5) AS prediction#70]
+- InMemoryTableScan [items#5]
      +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)
            +- *(1) Project [value#2 AS items#5]
               +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
                  +- Scan[obj#1]


scala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up

scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start
start: Long = 1678692855532
end: Long = 1678692860098
res4: Long = 4566

scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start
start: Long = 1678692860277
end: Long = 1678692862372
res5: Long = 2095
```

this PR:
```
scala> model.transform(df).explain(""extended"")
== Parsed Logical Plan ==
'Project [items#5, CASE WHEN NOT isnull('items) THEN aggregate('prediction, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda 'y_1[antecedent], lambdafunction(array_contains('items, lambda 'x_2), lambda 'x_2, false)) THEN array_union(lambda 'x_0, array_except(lambda 'y_1[consequent], 'items)) ELSE lambda 'x_0 END, lambda 'x_0, lambda 'y_1, false), lambdafunction(lambda 'x_3, lambda 'x_3, false)) ELSE cast(array() as array<string>) END AS prediction#72]
+- Join Cross
   :- Project [value#2 AS items#5]
   :  +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
   :     +- ExternalRDD [obj#1]
   +- ResolvedHint (strategy=broadcast)
      +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]
         +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))
            +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false

== Analyzed Logical Plan ==
items: array<string>, prediction: array<string>
Project [items#5, CASE WHEN NOT isnull(items#5) THEN aggregate(prediction#68, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE cast(array() as array<string>) END AS prediction#72]
+- Join Cross
   :- Project [value#2 AS items#5]
   :  +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
   :     +- ExternalRDD [obj#1]
   +- ResolvedHint (strategy=broadcast)
      +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]
         +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))
            +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false

== Optimized Logical Plan ==
Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(prediction#68, [], lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE [] END AS prediction#72]
+- Join Cross, rightHint=(strategy=broadcast)
   :- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)
   :     +- *(1) Project [value#2 AS items#5]
   :        +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
   :           +- Scan[obj#1]
   +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]
      +- Project [antecedent#57, consequent#58]
         +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(prediction#68, [], lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE [] END AS prediction#72]
   +- BroadcastNestedLoopJoin BuildRight, Cross
      :- InMemoryTableScan [items#5]
      :     +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)
      :           +- *(1) Project [value#2 AS items#5]
      :              +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
      :                 +- Scan[obj#1]
      +- BroadcastExchange IdentityBroadcastMode, [plan_id=117]
         +- ObjectHashAggregate(keys=[], functions=[collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[prediction#68])
            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=114]
               +- ObjectHashAggregate(keys=[], functions=[partial_collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[buf#95])
                  +- Project [antecedent#57, consequent#58]
                     +- Scan ExistingRDD[antecedent#57,consequent#58,confidence#59,lift#60,support#61]


scala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up

scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start
start: Long = 1678693708534
end: Long = 1678693713436
res6: Long = 4902

scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start
start: Long = 1678693713596
end: Long = 1678693713807
res7: Long = 211
```


the transformation is a bit slower 4566 -> 4902, but when we need to analyze the dataframe it will be much faster 2095 -> 211 since the `collect` execution is delayed.","1,-1    ",1,-1,0
1466280043,822522,2023/3/7,v3.4.0-rc2,"So this seems slower on a medium-sized data set. I don't know if delaying the collect() matters much; the overall execution time matters. I'm worried that this gets much slower on 1M or 10M records. Does this buy us other benefits, like, is this necessary to support ""Safe Spark"" or something?","1,-1    ",1,-1,0
1467821846,7322292,2023/3/7,v3.4.0-rc2,"yes, the `BroadcastNestedLoopJoin` is slower.

Then I have another try with subquery, and it's faster in both execution and analysis, but I have to create temp view and write the sql query then, see https://github.com/apache/spark/pull/40263/commits/63595ba03d9f18fe0b43bfb09f974ea50cb2c651

`model.transform(df).count()`: 4566 -> 3046
`model.transform(df).schema`: 2095 -> 298

So I'm trying to add a new method `Dataset.withScalarSubquery` in https://github.com/apache/spark/pull/40263/commits/c41ac094eb40520948d95108a78431694a33772d 

not sure whether it is the correct way to support `ScalarSubquery` in DataFrame APIs, but it is actually a pain point to me.

```
scala> model.transform(df).explain(""extended"")
== Parsed Logical Plan ==
'Project [items#5, CASE WHEN NOT isnull('items) THEN aggregate('prediction, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda 'y_1[antecedent], lambdafunction(array_contains('items, lambda 'x_2), lambda 'x_2, false)) THEN array_union(lambda 'x_0, array_except(lambda 'y_1[consequent], 'items)) ELSE lambda 'x_0 END, lambda 'x_0, lambda 'y_1, false), lambdafunction(lambda 'x_3, lambda 'x_3, false)) ELSE cast(array() as array<string>) END AS prediction#74]
+- Project [items#5, scalar-subquery#70 [] AS prediction#71]
   :  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]
   :     +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))
   :        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false
   +- Project [value#2 AS items#5]
      +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
         +- ExternalRDD [obj#1]

== Analyzed Logical Plan ==
items: array<string>, prediction: array<string>
Project [items#5, CASE WHEN NOT isnull(items#5) THEN aggregate(prediction#71, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE cast(array() as array<string>) END AS prediction#74]
+- Project [items#5, scalar-subquery#70 [] AS prediction#71]
   :  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]
   :     +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))
   :        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false
   +- Project [value#2 AS items#5]
      +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
         +- ExternalRDD [obj#1]

== Optimized Logical Plan ==
Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(scalar-subquery#70 [], [], lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE [] END AS prediction#74]
:  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]
:     +- Project [antecedent#57, consequent#58]
:        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false
+- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)
      +- *(1) Project [value#2 AS items#5]
         +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
            +- Scan[obj#1]

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(Subquery subquery#70, [id=#105], [], lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE [] END AS prediction#74]
   :  +- Subquery subquery#70, [id=#105]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[collect_list(struct(antecedent, consequent))#68])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=103]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[buf#97])
   :                 +- Project [antecedent#57, consequent#58]
   :                    +- Scan ExistingRDD[antecedent#57,consequent#58,confidence#59,lift#60,support#61]
   +- InMemoryTableScan [items#5]
         +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)
               +- *(1) Project [value#2 AS items#5]
                  +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]
                     +- Scan[obj#1]


scala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up

scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start
start: Long = 1678788928604
end: Long = 1678788931650
res4: Long = 3046

scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start
start: Long = 1678788931785
end: Long = 1678788932083
res5: Long = 298
```","1,-1    ",1,-1,0
1473064231,7322292,2023/3/7,v3.4.0-rc2,"@srowen if the latest performance test seems fine, then I'd ask the SQL guys whether we can have a subquery method in DataFrame APIs.","1,-1    ",1,-1,0
1473071461,822522,2023/3/7,v3.4.0-rc2,"If it's faster and gives the right answers, sure","1,-1    ",1,-1,0
1477193966,7322292,2023/3/7,v3.4.0-rc2,"TL;DR  I want to apply scalar subquery to optimize `FPGrowthModel.transform`, there are two options:

1, create temp views and use `spark.sql`, see https://github.com/apache/spark/commit/63595ba03d9f18fe0b43bfb09f974ea50cb2c651;

2, add `private[spark] def withScalarSubquery(colName: String, subquery: Dataset[_]): DataFrame`, it seems much more convenient but not sure whether it is a proper way.

cc @cloud-fan @HyukjinKwon ","1,-1    ",1,-1,0
1477246959,822522,2023/3/7,v3.4.0-rc2,"I don't know enough to say whether it's worth a new method. Can we start with the change that needs no new API, is it a big enough win?","1,-1    ",1,-1,0
1478764552,7322292,2023/3/7,v3.4.0-rc2,@srowen sounds reasonable,"1,-1    ",1,-1,0
1453062944,122326661,2023/3/7,v3.4.0-rc2,"@MaxGekk Please take a look, thanks for reviewing!","1,-1    ",1,-1,0
1453222106,1580697,2023/3/7,v3.4.0-rc2,"@chenhao-db Could you fix the build errors:
```
[error] /home/runner/work/apache-spark/apache-spark/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala:2047:7: not found: value checkErrorInExpression
[error]       checkErrorInExpression[SparkArithmeticException](TimestampAdd(""DAY"",
[error]       ^
[error] /home/runner/work/apache-spark/apache-
```","1,-1    ",1,-1,0
1453847644,122326661,2023/3/7,v3.4.0-rc2,"@MaxGekk It seems that `checkErrorInExpression` doesn't exist in 3.3, so I still have to use the old `checkExceptionInExpression`. Is that okay?","1,-1    ",1,-1,0
1454657486,1580697,2023/3/7,v3.4.0-rc2,"Seems like the test failure is related to the changes:
```
[info] - SPARK-42635: timestampadd unit conversion overflow *** FAILED *** (12 milliseconds)
[info]   (non-codegen mode) Expected error message is `[DATETIME_OVERFLOW] Datetime operation overflow`, but `Datetime operation overflow: add 106751992 DAY to TIMESTAMP '1970-01-01 00:00:00'.` found (ExpressionEvalHelper.scala:176)
```","1,-1    ",1,-1,0
1454660710,122326661,2023/3/7,v3.4.0-rc2,@MaxGekk I see. In old versions Spark doesn't include the error class in the error message: https://github.com/apache/spark/blob/branch-3.3/core/src/main/scala/org/apache/spark/ErrorInfo.scala#L74. I just removed the error class prefix in the expected error message.,"1,-1    ",1,-1,0
1454846526,1580697,2023/3/7,v3.4.0-rc2,"+1, LGTM. All GAs passed. Merging to 3.3.
Thank you, @chenhao-db.","1,-1    ",1,-1,0
1453443430,9616802,2023/3/7,v3.4.0-rc2,@beliefer can you add a test to SparkPlannerSuite or ClientE2ESuite to make sure this properly works?,"1,-1    ",1,-1,0
1453443869,9616802,2023/3/7,v3.4.0-rc2,cc @zhengruifeng can you also take a look?,"1,-1    ",1,-1,0
1454489494,7322292,2023/3/7,v3.4.0-rc2,"we'd better always add e2e tests, since it was added in `ClientE2ESuite`, I think don't need to add one in `test_connect_basic`","1,-1    ",1,-1,0
1454493588,7322292,2023/3/7,v3.4.0-rc2,merged into master/branch-3.4,"1,-2    ",1,-2,-1
1454533003,8486025,2023/3/7,v3.4.0-rc2,@hvanhovell @zhengruifeng Thank you!,"1,-1    ",1,-1,0
1453097749,106726387,2023/3/7,v3.4.0-rc2,"TPCH q21 plan change
|Before|After|
|------|----|
|![Web capture_3-3-2023_125513_ms web azuresynapse net_crop](https://user-images.githubusercontent.com/106726387/222658668-830d9ddb-2cc2-4013-8dbc-29967d957954.jpeg)|![Web capture_3-3-2023_12324_ms web azuresynapse net_crop](https://user-images.githubusercontent.com/106726387/222658801-f917a4cd-f825-4567-aca8-cb596dd64e51.jpeg)|","1,-1    ",1,-1,0
1453149163,106726387,2023/3/7,v3.4.0-rc2,cc @cloud-fan ,"1,-1    ",1,-1,0
1461267270,106726387,2023/3/7,v3.4.0-rc2,cc: @wangyum @peter-toth ,"1,-1    ",1,-1,0
1462138771,7253827,2023/3/7,v3.4.0-rc2,"This change makes sense to me and new plans look ok to me.
However, seemingly `InferFiltersFromConstraints` has a dedicated place in the optimizer and so there are 2 special batches `Operator Optimization before Inferring Filters` and `Operator Optimization after Inferring Filters` before and after the rule to make sure the inferred filtes are optimized. It also seems like the `RewriteSubquery` batch slowly becomes larger and larger with rules from those batches (see SPARK-39511, SPARK-22662, SPARK-36280). And now you want to add `InferFiltersFromConstraints` too. So I wonder if `RewritePredicateSubquery` is at the right place or what else would make sense to be executed after `RewritePredicateSubquery`? Maybe rerunning a full `operatorOptimizationBatch` would make sense despite it comes with a cost?","1,-1    ",1,-1,0
1463196395,5399861,2023/3/7,v3.4.0-rc2,I had a change like this before: https://github.com/apache/spark/pull/22778.,"2,-2    ",2,-2,0
1463418560,7253827,2023/3/7,v3.4.0-rc2,"> I had a change like this before: https://github.com/apache/spark/pull/22778.

Ah ok, thanks @wangyum! It looks like the very same discussuion has come up before: https://github.com/apache/spark/pull/22778#discussion_r229178084","1,-1    ",1,-1,0
1469424781,106726387,2023/3/7,v3.4.0-rc2,"@wangyum @peter-toth Thanks for pointing on previous attempts.

It does seem moving `RewritePredicateSubquery` rule is right way so that in future we don't add anymore rule to that batch (`RewriteSubquery`).

In this pr https://github.com/apache/spark/pull/17520, they tried to put RewritePredicateSubquery right after `Subquery` batch (of `OptimizeSubqueries`). `operatorOptimizationBatch` will run after this.  They also added one rule to push LeftSemi/LeftAnti through join, but that has been added in 3.0 by [SPARK-19712](https://issues.apache.org/jira/browse/SPARK-19712). So now we only need to change rule position.

If this seems right to you guys, I can update this PR to move `RewritePredicateSubquery` after `Subqury` batch?","1,-1    ",1,-1,0
1476205668,7253827,2023/3/7,v3.4.0-rc2,"Looks like there are a few failures after moving the rule (https://github.com/apache/spark/pull/40266/commits/22e7886ff1059b98d1525380b2cb22718fd5dd09). @mskapilks, do you think you can look into those failures?","1,-1    ",1,-1,0
1477245713,106726387,2023/3/7,v3.4.0-rc2,"> Looks like there are a few failures after moving the rule ([22e7886](https://github.com/apache/spark/commit/22e7886ff1059b98d1525380b2cb22718fd5dd09)). @mskapilks, do you think you can look into those failures?

Yup I am working on them. I had wrong SPARK_HOME setup so missed the plan changes","1,-1    ",1,-1,0
1480845859,106726387,2023/3/7,v3.4.0-rc2,More failures. Seems this might take real effort to make it work like other rules modifications.,"1,-1    ",1,-1,0
1481013209,7253827,2023/3/7,v3.4.0-rc2,"> More failures. Seems this might take real effort to make it work like other rules modifications.

Why was the latest commit (https://github.com/apache/spark/pull/40266/commits/b1ed7bee8b91faf8286963c7867eeed9f781b487) needed?","1,-1    ",1,-1,0
1517796434,7253827,2023/3/7,v3.4.0-rc2,"@mskapilks, do you have any update on this? I can take over this PR and investigate the idea further if you don't have time for it.","1,-1    ",1,-1,0
1453464858,6477701,2023/3/7,v3.4.0-rc2,cc @hvanhovell ,"2,-1    ",2,-1,1
1453467412,9616802,2023/3/7,v3.4.0-rc2,"merge it?
","2,-1    ",2,-1,1
1453468505,6477701,2023/3/7,v3.4.0-rc2,Thanks! Let me merge after https://github.com/HyukjinKwon/spark/actions/runs/4323411173/jobs/7547000388. Should take less then 20 mins.,"1,-1    ",1,-1,0
1453547362,6477701,2023/3/7,v3.4.0-rc2,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1454668297,7253827,2023/3/7,v3.4.0-rc2,"Tests look ok, removed WIP flag.

cc @wangyum, @cloud-fan ","1,-1    ",1,-1,0
1464116439,7253827,2023/3/7,v3.4.0-rc2,"@wangyum, @cloud-fan, do you think this PR is good enough or shall we go further with improvements?
The following features from https://github.com/apache/spark/pull/24553 are not in this PR currently to keep the change simple, but we can add them if needed:
- Extend propagation from attribute => constant mapping to deterministic expression => constant mapping:
  e.g. `abs(a) = 5 AND b = abs(a) + 3` => `abs(a) = 5 AND b = 8`
- Allow substitution in other than `BinaryComparison`s:
  e.g. `a = 5 AND abs(a) = 1` => `a = 5 AND abs(5) = 1`
- Allow propagation of constant non-nullable expressions in `Project` and other nodes:
  e.g. `SELECT a = 5 AND b = a + 3` => `SELECT a = 5 AND b = 8`
  (Currently only `Filter` is supported.)
- Allows deep constant propagation:
  e.g. `WHERE IF(..., a = 5 AND b = a + 3, ...)` => `WHERE IF(..., a = 5 AND b = 8, ...)`
  (Currently only top level `And`/`Or`/`Not` are supported.)

@cloud-fan, I know you had concerns: https://github.com/apache/spark/pull/40093#discussion_r1121416560 and actually I'm not sure either how much performance improvement we could expect from the above 4 in real-life usecases...","1,-1    ",1,-1,0
1478292434,7253827,2023/3/7,v3.4.0-rc2,"@cloud-fan, @wangyum please let me know if this PR needs further improvements.","1,-1    ",1,-1,0
1454352840,1097932,2023/3/7,v3.4.0-rc2,"@grundprinzip Thanks for the work. +1 for the approach.
Could you point out where is exactly the same as the PR https://github.com/apache/spark-website/pull/359 so that we can review this PR easier?

Also, I notice the tabs need adjustment:
this PR: 
<img width=""927"" alt=""image"" src=""https://user-images.githubusercontent.com/1097932/222872786-faa168df-ffcb-428e-9f05-2d4dc8912d1e.png"">

current:
<img width=""963"" alt=""image"" src=""https://user-images.githubusercontent.com/1097932/222872764-37bd48fd-75c1-48a8-b781-f46acfbdb273.png"">


","1,-1    ",1,-1,0
1454353833,1097932,2023/3/7,v3.4.0-rc2,"There is also differences on the top bar and left menu when scrolling down the page:
Take https://spark.apache.org/docs/3.3.2/sql-ref-ansi-compliance.html as an example, of this PR:
<img width=""1139"" alt=""image"" src=""https://user-images.githubusercontent.com/1097932/222872915-e246493b-4dd6-45ce-b2e7-91f3aaa22944.png"">

Current:
<img width=""787"" alt=""image"" src=""https://user-images.githubusercontent.com/1097932/222872929-fef162d6-f033-4939-9f1b-043f16226f8e.png"">
","2,-1    ",2,-1,1
1464882869,3421,2023/3/8,v3.4.0-rc2,"@gengliangwang I fixed the issues you mentioned, I left an explanation in the PR desc to explain what exactly I did, but I'm not sure what you mean with:

> Could you point out where is exactly the same as the PR https://github.com/apache/spark-website/pull/359 so that we can review this PR easier?","1,-1    ",1,-1,0
1474463839,1097932,2023/3/8,v3.4.0-rc2,@grundprinzip This one LGTM overall. Could you create a Spark jira for it and update the PR title?,"1,-1    ",1,-1,0
1475355783,1097932,2023/3/8,v3.4.0-rc2,"Thanks, merging to master","1,-1    ",1,-1,0
1477238760,44108233,2023/3/8,v3.4.0-rc2,"As the #40456 has been completed, will resume this one. Let me close this PR for convenience and open a new one.","1,-1    ",1,-1,0
1477485243,44108233,2023/3/8,v3.4.0-rc2,Revisit PR: https://github.com/apache/spark/pull/40507,"1,-1    ",1,-1,0
1454247387,1938382,2023/3/8,v3.4.0-rc2,@hvanhovell ,"1,-1    ",1,-1,0
1454301804,100322362,2023/3/8,v3.4.0-rc2,@HeartSaVioR - please take a look. Thx,"1,-1    ",1,-1,0
1455022742,1317309,2023/3/8,v3.4.0-rc2,"Mind retriggering the build, please? Probably simplest way to do is pushing an empty commit. You can retrigger the build in your fork but it won't be reflected here.","1,-1    ",1,-1,0
1455371384,100322362,2023/3/8,v3.4.0-rc2,"> Mind retriggering the build, please? Probably simplest way to do is pushing an empty commit. You can retrigger the build in your fork but it won't be reflected here.

Sure done","1,-1    ",1,-1,0
1455674013,1317309,2023/3/8,v3.4.0-rc2,"It's taking lot much longer than usual. I've just pushed in my repo as well. Let's see the result.
https://github.com/HeartSaVioR/spark/actions/runs/4341254564/jobs/7580632902
","1,-1    ",1,-1,0
1455951938,1317309,2023/3/8,v3.4.0-rc2,"https://github.com/anishshri-db/spark/actions/runs/4339563807/jobs/7577302766

Here it failed ""only"" with [Run / Build modules: sql - other tests](https://github.com/anishshri-db/spark/actions/runs/4339563807/jobs/7577290837#logs)

https://github.com/HeartSaVioR/spark/actions/runs/4341254564/jobs/7580668669

Here [Run / Build modules: sql - other tests](https://github.com/HeartSaVioR/spark/actions/runs/4341254564/jobs/7580648546) succeeded. Although there's other failure in different module, it seems like flaky one, rather than consistent one.","1,-1    ",1,-1,0
1455953566,1317309,2023/3/8,v3.4.0-rc2,Thanks! Merging to master.,"1,-1    ",1,-1,0
1454313007,4190164,2023/3/8,v3.4.0-rc2,"@hvanhovell 
cc @LuciferYang","1,-1    ",1,-1,0
1454314742,4190164,2023/3/8,v3.4.0-rc2,"The full error (even with the clean master branch):
```
build/mvn clean
build/mvn -Pscala-2.13 compile -pl connector/connect/client/jvm -am -DskipTests
build/mvn -Pscala-2.13 test -pl connector/connect/client/jvm -> errored here

[ERROR] [Error] /Users/zhen.li/code/spark-sbt/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/util/ConvertToArrow.scala:28: object arrow is not a member of package org.apache.spark.sql.execution
[ERROR] [Error] /Users/zhen.li/code/spark-sbt/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/util/ConvertToArrow.scala:46: not found: type ArrowWriter
[ERROR] [Error] /Users/zhen.li/code/spark-sbt/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/util/ConvertToArrow.scala:46: not found: value ArrowWriter
```

","1,-1    ",1,-1,0
1454336041,1475305,2023/3/8,v3.4.0-rc2,"Thanks for your work @zhenlineo 
If you don't mind, please give me more time to think about this pr ï¼ï¼

","1,-1    ",1,-1,0
1455012036,1475305,2023/3/8,v3.4.0-rc2,"In the pr description, `build/mvn compile -pl connector/connect/client/jvm` should be `build/mvn compile -pl connector/connect/client/jvm -am` ?

","1,-1    ",1,-1,0
1455094955,1475305,2023/3/8,v3.4.0-rc2,"On the whole, it is good for me. There is only one question. Spark still uses maven for version release and deploy. But after this pr, the E2E test change to use sbt assembly server jar instead of maven shaded server jar for testing, which may weaken the maven test. We may need other ways to ensure the correctness of maven shaded server jar.

In the future, we may use sbt to completely replace maven(should not be in Spark 3.4.0), including version release, deploy and other help tools, which will no longer be a problem at that time.


","1,-1    ",1,-1,0
1455105130,1475305,2023/3/8,v3.4.0-rc2,"There is another problem that needs to be confirmed, which may not related to current pr: if other Suites inherit `RemoteSparkSession`, they will share the same connect server, right? (`SparkConnectServerUtils` is an object, so `SparkConnect` will only submit once)

","1,-1    ",1,-1,0
1456456654,4190164,2023/3/8,v3.4.0-rc2,@LuciferYang Thanks for your review. This PR was trying to simplify the test running steps. But as you said it make the maven commands to call sbt implicitly. I will split the changes into smaller PRs to allow this PR only deal with the IT command change. Then we can votes if we like this change or not :),"1,-1    ",1,-1,0
1456552298,4190164,2023/3/8,v3.4.0-rc2,https://github.com/apache/spark/pull/40304 https://github.com/apache/spark/pull/40303,"1,-1    ",1,-1,0
1461210301,1475305,2023/3/8,v3.4.0-rc2,"seems `SimpleSparkConnectService` startup failed, the error message is 

```
Error: Missing application resource.

Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]
Usage: spark-submit run-example [options] example-class [example args]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,
                              k8s://https://host:port, or local (Default: local[*]).
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (""client"") or
                              on one of the worker machines inside the cluster (""cluster"")
                              (Default: client).
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of jars to include on the driver
...
```","1,-1    ",1,-1,0
1463235387,4190164,2023/3/8,v3.4.0-rc2,"> seems `SimpleSparkConnectService` startup failed, the error message is
> 
> ```
> Error: Missing application resource.
> 
> Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
> Usage: spark-submit --kill [submission ID] --master [spark://...]
> Usage: spark-submit --status [submission ID] --master [spark://...]
> Usage: spark-submit run-example [options] example-class [example args]
> 
> Options:
>   --master MASTER_URL         spark://host:port, mesos://host:port, yarn,
>                               k8s://https://host:port, or local (Default: local[*]).
>   --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (""client"") or
>                               on one of the worker machines inside the cluster (""cluster"")
>                               (Default: client).
>   --class CLASS_NAME          Your application's main class (for Java / Scala apps).
>   --name NAME                 A name of your application.
>   --jars JARS                 Comma-separated list of jars to include on the driver
> ...
> ```

Yeah, this was caused by the bug we had in the scripts.
","1,-1    ",1,-1,0
1463236560,4190164,2023/3/8,v3.4.0-rc2,@hvanhovell Want to keep this or shall we skip? It helps a bit when not knowing `build/sbt -Pconnect -Phive package` before running the IT. ,"1,-1    ",1,-1,0
1455071084,8486025,2023/3/8,v3.4.0-rc2,@LuciferYang I want support the similar `withSQLConf`.,"1,-1    ",1,-1,0
1455071764,8486025,2023/3/8,v3.4.0-rc2,@LuciferYang Thank you.,"1,-1    ",1,-1,0
1455280706,8486025,2023/3/8,v3.4.0-rc2,ping @HyukjinKwon @zhengruifeng @dongjoon-hyun ,"1,-1    ",1,-1,0
1455321694,9616802,2023/3/8,v3.4.0-rc2,Merging.,"1,-1    ",1,-1,0
1455357573,8486025,2023/3/8,v3.4.0-rc2,@hvanhovell @LuciferYang Thank you.,"1,-1    ",1,-1,0
1454583496,7322292,2023/3/8,v3.4.0-rc2,"I don't know the internal of Parser well, but I guess if we want to reach 100% compatibility, we may need to reuse the `.g4` files and implement a subset of  `AstBuilder` to support `singleDataType` and `singleTableSchema`.

For example, the DDL also support `NOT NULL` and `COMMENT`:
```
>>> df = spark.createDataFrame([], """"""a string  COMMENT 'this is just a simple string' """""")
>>> df.schema
StructType([StructField('a', StringType(), True)])
>>> df = spark.createDataFrame([], """"""a string  NOT NULL COMMENT 'this is just a simple string' """""")
>>> df.schema
StructType([StructField('a', StringType(), False)])
```

cc @hvanhovell @cloud-fan @HyukjinKwon  WDYT?","1,-1    ",1,-1,0
1458245647,3182036,2023/3/8,v3.4.0-rc2,"does it mean every spark connect client must implement a data type parser in its language? This seems a bit overkill. Can we revisit all the places that need to parse data type at client side, and see if we can delay it to the server side?","1,-1    ",1,-1,0
1458274759,9616802,2023/3/8,v3.4.0-rc2,At the end of the day it is an optimization. However I do think it is a sound one to have. ,"1,-1    ",1,-1,0
1463286761,506656,2023/3/8,v3.4.0-rc2,Close this in favor of #40260.,"1,-1    ",1,-1,0
1455280396,8486025,2023/3/8,v3.4.0-rc2,ping @hvanhovell @HyukjinKwon @dongjoon-hyun  cc @LuciferYang ,"1,-1    ",1,-1,0
1460020463,8486025,2023/3/8,v3.4.0-rc2,@hvanhovell Do you have any other advice? cc @HyukjinKwon @zhengruifeng @dongjoon-hyun ,"1,-1    ",1,-1,0
1461105352,8486025,2023/3/8,v3.4.0-rc2,@hvanhovell @zhengruifeng Thank you.,"1,-1    ",1,-1,0
1464915794,822522,2023/3/8,v3.4.0-rc2,Merged to master,"1,-1    ",1,-1,0
1454964971,15246973,2023/3/8,v3.4.0-rc2,"### Full stack:

INTERNAL: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '<': extra input '<'.(line 1, pos 6)

== SQL ==
struct<c1:struct<c1-1:string,c1-2:string>>
------^^^

io.grpc.StatusRuntimeException: INTERNAL: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '<': extra input '<'.(line 1, pos 6)

== SQL ==
struct<c1:struct<c1-1:string,c1-2:string>>
------^^^

	at io.grpc.Status.asRuntimeException(Status.java:535)
	at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
	at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:61)
	at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:106)
	at org.apache.spark.sql.Dataset.$anonfun$show$2(Dataset.scala:529)
	at org.apache.spark.sql.Dataset.$anonfun$show$2$adapted(Dataset.scala:528)
	at org.apache.spark.sql.Dataset.withResult(Dataset.scala:2752)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:528)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:444)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:399)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:408)
	at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$85(ClientE2ETestSuite.scala:608)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
	at org.scalatest.Suite.run(Suite.scala:1114)
	at org.scalatest.Suite.run$(Suite.scala:1096)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
	at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:34)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:34)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:47)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1321)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1315)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1315)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:992)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:970)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1481)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:970)
	at org.scalatest.tools.Runner$.run(Runner.scala:798)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:43)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:26)
","1,-1    ",1,-1,0
1454967088,15246973,2023/3/8,v3.4.0-rc2,cc @hvanhovell ,"1,-1    ",1,-1,0
1455567210,44108233,2023/3/8,v3.4.0-rc2,"Thanks @panbingkun for the nice fix!
Btw, think I found another `createDataFrame` bug which is not working properly with non-nullable schema as below:
```python
>>> from pyspark.sql.types import *
>>> schema_false = StructType([StructField(""id"", IntegerType(), False)])
>>> spark.createDataFrame([[1]], schema=schema_false)
Traceback (most recent call last):
...
pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.
```
whereas working find with nullable schema as below:
```python
>>> schema_true = StructType([StructField(""id"", IntegerType(), True)])
>>> spark.createDataFrame([[1]], schema=schema_true)
DataFrame[id: int]
```

Do you have any idea what might be causing this? Could you take a look at it if you're interested in? I have filed an issue at SPARK-42679.

Also cc @hvanhovell as an original author for `createDataFrame`.","1,-1    ",1,-1,0
1457349284,15246973,2023/3/8,v3.4.0-rc2,"> Thanks @panbingkun for the nice fix! Btw, think I found another `createDataFrame` bug which is not working properly with non-nullable schema as below:
> 
> ```python
> >>> from pyspark.sql.types import *
> >>> schema_false = StructType([StructField(""id"", IntegerType(), False)])
> >>> spark.createDataFrame([[1]], schema=schema_false)
> Traceback (most recent call last):
> ...
> pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.
> ```
> 
> whereas working find with nullable schema as below:
> 
> ```python
> >>> schema_true = StructType([StructField(""id"", IntegerType(), True)])
> >>> spark.createDataFrame([[1]], schema=schema_true)
> DataFrame[id: int]
> ```
> 
> Do you have any idea what might be causing this? Could you take a look at it if you're interested in? I have filed an issue at [SPARK-42679](https://issues.apache.org/jira/browse/SPARK-42679).
> 
> Also cc @hvanhovell as an original author for `createDataFrame`.

Let me try to investigate it.","1,-2    ",1,-2,-1
1457558427,44108233,2023/3/8,v3.4.0-rc2,"Thanks, @panbingkun !
By the way, I think this issue has a pretty high priority since the default nullability of a schema is `False`.

```python
>>> sdf = spark.range(10).schema
self._schema: StructType([StructField('id', LongType(), False)])
```

For example, even intuitive and simple code like creating a DataFrame from a pandas DataFrame fails as follows:
```python
>>> sdf = spark.range(10)
>>> pdf = sdf.toPandas()
>>> spark.createDataFrame(pdf, sdf.schema)
Traceback (most recent call last):
...
pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.
```

Please feel free to ping me anytime if you need any help!
Thanks again for your time on investigating this :-)","1,-1    ",1,-1,0
1458081353,15246973,2023/3/8,v3.4.0-rc2,"> Thanks, @panbingkun ! By the way, I think this issue has a pretty high priority since the default nullability of a schema is `False`.
> 
> ```python
> >>> sdf = spark.range(10).schema
> self._schema: StructType([StructField('id', LongType(), False)])
> ```
> 
> For example, even intuitive and simple code like creating a DataFrame from a pandas DataFrame fails as follows:
> 
> ```python
> >>> sdf = spark.range(10)
> >>> pdf = sdf.toPandas()
> >>> spark.createDataFrame(pdf, sdf.schema)
> Traceback (most recent call last):
> ...
> pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.
> ```
> 
> Please feel free to ping me anytime if you need any help! Thanks again for your time on investigating this :-)


I have found root cause. Let me think about how to fix it.
Temporary solutions: https://github.com/apache/spark/pull/40316","1,-1    ",1,-1,0
1458111230,44108233,2023/3/8,v3.4.0-rc2,"Awesome!! Let me take a look at your PR when it's ready.
Thanks!","1,-1    ",1,-1,0
1455001035,15122230,2023/3/8,v3.4.0-rc2,cc @mridulm @Ngone51 @ulysses-you ,"1,-1    ",1,-1,0
1456411134,15122230,2023/3/8,v3.4.0-rc2,Thanks @HyukjinKwon @LuciferYang ,"1,-1    ",1,-1,0
1455270795,6477701,2023/3/8,v3.4.0-rc2,cc @MaxGekk and @srielau ,"1,-1    ",1,-1,0
1459096241,44108233,2023/3/8,v3.4.0-rc2,Just created ticket for SQL side: SPARK-42706 FYI.,"1,-1    ",1,-1,0
1463083039,44108233,2023/3/8,v3.4.0-rc2,"Documentation for SQL side is get merged from https://github.com/apache/spark/pull/40336.

Note that Python side are simpler compared to SQL side because we do not have SQLSTATE, and there is currently no main error class with sub-error classes. Also, the overall volume of errors is not as high as in SQL documents.","1,-1    ",1,-1,0
1467350642,44108233,2023/3/8,v3.4.0-rc2,Reminder for @HyukjinKwon @srielau @MaxGekk for error class document for PySpark.,"3,-3    ",3,-3,0
1498422658,6477701,2023/3/8,v3.4.0-rc2,Merged to master.,"1,-1    ",1,-1,0
1455027425,1475305,2023/3/8,v3.4.0-rc2,"for example:

- https://github.com/apache/spark/actions/runs/4329598884/jobs/7560252913
- https://github.com/apache/spark/actions/runs/4329598884/jobs/7560252970
- https://github.com/apache/spark/actions/runs/4329004998/jobs/7559232726
- https://github.com/apache/spark/actions/runs/4329004998/jobs/7559232794

<img width=""1241"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/222950677-95c48561-924d-45c7-a59b-23f66a997af3.png"">
","1,-2    ",1,-2,-1
1455028122,1475305,2023/3/8,v3.4.0-rc2,"For check, I add a `./build/mvn -version` before in `java-11-17` GA task without this pr:

<img width=""1682"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/222950915-3dcbf4f1-6f00-4e34-a003-936197cdef57.png"">

And it print as follows:

https://github.com/LuciferYang/spark/actions/runs/4328951282/jobs/7559134224


<img width=""1196"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/222950899-11ca9796-11b4-400f-9a89-5f22b9367b2b.png"">
","1,-1    ",1,-1,0
1455041804,1475305,2023/3/8,v3.4.0-rc2,"no error message with this pr 

- https://github.com/LuciferYang/spark/actions/runs/4335123778/jobs/7569484003
- https://github.com/LuciferYang/spark/actions/runs/4335123778/jobs/7569484044","1,-3    ",1,-3,-2
1455043698,1475305,2023/3/8,v3.4.0-rc2,cc @dongjoon-hyun ,"1,-2    ",1,-2,-1
1455497586,1475305,2023/3/8,v3.4.0-rc2,also cc @HyukjinKwon ,"1,-1    ",1,-1,0
1455633233,237462,2023/3/8,v3.4.0-rc2,"there is a known issue in Maven 3.9.0 (related to plexus-utils XML stricter reading https://github.com/codehaus-plexus/plexus-utils/issues/238 ) that is fixed in 3.9.1-SNAPSHOT: https://issues.apache.org/jira/browse/MNG-7697

3.9.1 will be released soon: can you eventually check with 3.9.1-SNAPSHOT if you're in a different case of this ""too strict"" XML parsing?","1,-2    ",1,-2,-1
1455637895,237462,2023/3/8,v3.4.0-rc2,"[@cstamas ](https://github.com/cstamas) do you know if the lax parsing covers that `org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) `?","1,-1    ",1,-1,0
1455648868,1475305,2023/3/8,v3.4.0-rc2,"> https://issues.apache.org/jira/browse/MNG-7697

OK, let me test 3.9.1-SNAPSHOT later. @pan3793 Do you have any other issues besides those in GA task?

","2,-1    ",2,-1,1
1455681437,84022,2023/3/8,v3.4.0-rc2,"> [@cstamas ](https://github.com/cstamas) do you know if the lax parsing covers that `org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=""1.0"" encoding=""ISO-8859-1""... @1:42) `?

It does not.  This is a separate issue which is not actually a problem in maven or plexus-utils.  See a similar fix for it in https://github.com/takari/polyglot-maven/commit/14514b672a07ec0ba582574efdcf913e308c9e91
It has to be fixed in the `org.cyclonedx.maven.BaseCycloneDxMojo.readPom` method.","1,-1    ",1,-1,0
1455747662,237462,2023/3/8,v3.4.0-rc2,"oh, I did not see that Spark was still using cyclonedx-maven-plugin old 2.7.3, thank you @gnodet: @LuciferYang @pan3793 you should upgrade to 2.7.5, which has completely changed the implementation and should not have the issue","1,-1    ",1,-1,0
1455757524,1475305,2023/3/8,v3.4.0-rc2,"> oh, I did not see that Spark was still using cyclonedx-maven-plugin old 2.7.3, thank you @gnodet: @LuciferYang @pan3793 you should upgrade to 2.7.5, which has completely changed the implementation and should not have the issue

Thanks for you suggestion, in https://github.com/apache/spark/pull/40065 I discussed with @dongjoon-hyun , I know 2.7.5 fixed these issues, but 2.7.5 has other issues like https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/284 and https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/289,   so we hope wait until at least 2.7.6 to upgrade `cyclonedx-maven-plugin`","1,-2    ",1,-2,-1
1457450172,1475305,2023/3/8,v3.4.0-rc2,"Thanks @dongjoon-hyun @pan3793 ~
Also thanks @gnodet @hboutemy ","1,-1    ",1,-1,0
1455270404,6477701,2023/3/8,v3.4.0-rc2,Merged to master.,"1,-1    ",1,-1,0
1455258629,5399861,2023/3/9,v3.4.0-rc3,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1455325164,1475305,2023/3/9,v3.4.0-rc3,Thanks @wangyum ,"1,-1    ",1,-1,0
1464878883,15122230,2023/3/9,v3.4.0-rc3,"Hi @mridulm comments addressed in the latest iteration, please take a look. Thanks.","2,-1    ",2,-1,1
1465226363,15122230,2023/3/9,v3.4.0-rc3,"> Just a doc change I had missed last time around. Rest looks good to me - can you check the proposed change, and reformulate it to something similar ? I will merge it once done

Hi @mridulm thanks a lot for the suggestion. Updated.","1,-2    ",1,-2,-1
1465327881,1591700,2023/3/9,v3.4.0-rc3,"Can you update to latest @ivoson ?
The style failure is not related to your change, but blocks build","1,-2    ",1,-2,-1
1465620182,15122230,2023/3/9,v3.4.0-rc3,"> Can you update to latest @ivoson ? The style failure is not related to your change, but blocks build

Hi @mridulm , branch rebased. Please take a look. Thanks.","1,-1    ",1,-1,0
1465633950,1591700,2023/3/9,v3.4.0-rc3,"Merged to master.
Thanks for fixing this @ivoson !","1,-1    ",1,-1,0
1465712015,15122230,2023/3/9,v3.4.0-rc3,Thanks for the review. @mridulm ,"1,-1    ",1,-1,0
1455366786,9616802,2023/3/9,v3.4.0-rc3,@HyukjinKwon @zhengruifeng the rationale for this change is that analyzer takes care of making lambda variables unique.,"1,-1    ",1,-1,0
1455384317,8486025,2023/3/9,v3.4.0-rc3,"@hvanhovell After my test, `python/run-tests --testnames 'pyspark.sql.tests.test_functions'` will not passed.
","1,-1    ",1,-1,0
1455388960,7322292,2023/3/9,v3.4.0-rc3,"I guess we will need to rewrite the lamda function in spark connect planner.

cc @ueshin as well, since existing implementation follows the fix in https://github.com/apache/spark/pull/32523","2,-1    ",2,-1,1
1455390728,8486025,2023/3/9,v3.4.0-rc3,"![image](https://user-images.githubusercontent.com/8486025/223014232-bf9b26ee-d0e8-4de4-a8fe-2d252813ac4d.png)
","2,-2    ",2,-2,0
1456026258,8486025,2023/3/9,v3.4.0-rc3,It seems pyspark supports the nested lambda variables and two PR fix the issue.,"1,-1    ",1,-1,0
1456054467,9616802,2023/3/9,v3.4.0-rc3,"@beliefer scala does support nested lambda variables as well, and they actually work. So either (my) testing on the scala side is incomplete (which might well be the case), or something weird is going on here.","1,-1    ",1,-1,0
1457418420,8486025,2023/3/9,v3.4.0-rc3,"@hvanhovell Scala also uses `UnresolvedNamedLambdaVariable.freshVarName(""x"")` to get the unique names. see: 
https://github.com/apache/spark/blob/201e08c03a31c763e3120540ac1b1ca8ef252e6b/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L4096","1,-1    ",1,-1,0
1457433571,9616802,2023/3/9,v3.4.0-rc3,"@beliefer here is the thing. When this was designed it was mainly aimed at sql, and there we definitely do not generate unique names in lambda functions either. This is all done in the analyzer. We should be able to follow the same path.

Do you happen to know if test failing for python also fail for scala?","2,-1    ",2,-1,1
1458109080,8486025,2023/3/9,v3.4.0-rc3,"> @beliefer here is the thing. When this was designed it was mainly aimed at sql, and there we definitely do not generate unique names in lambda functions either. This is all done in the analyzer. We should be able to follow the same path.
>

It seems only the lambda functions in SQL will be transformed with analyzer. But the scala, pyspark API will not through analyzer.
","1,-1    ",1,-1,0
1458116337,9616802,2023/3/9,v3.4.0-rc3,Ehhhh... SQL/scala/Python all use the analyzer; they are all just frontends to the same thing.,"1,-1    ",1,-1,0
1458193681,8486025,2023/3/9,v3.4.0-rc3,"> Ehhhh... SQL/scala/Python all use the analyzer; they are all just frontends to the same thing.

I found the reason. Although the scala API use analyzer too. `object ResolveLambdaVariables extends Rule[LogicalPlan]` can't fix the issue.

If I removed the` UnresolvedNamedLambdaVariable.freshVarName(...)`

![image](https://user-images.githubusercontent.com/8486025/223439632-cd7dcd93-c60d-4e7c-844b-fddb89d00bec.png)

and test the case, see at: https://github.com/apache/spark/blob/2e7207f96e1ff848def135de63f63bcda7402517/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala#L5250
![image](https://user-images.githubusercontent.com/8486025/223439443-efa6346d-a829-4dd2-a430-df5f24fbd819.png)

The related PR.
https://github.com/apache/spark/pull/32424","1,-1    ",1,-1,0
1459329942,8486025,2023/3/9,v3.4.0-rc3,@hvanhovell Do we still need this change ?,"1,-1    ",1,-1,0
1459594072,7322292,2023/3/9,v3.4.0-rc3,"If the nested lambda issue also exists in the Scala Client, do we need to fix it in the same way?","1,-1    ",1,-1,0
1455348864,44108233,2023/3/9,v3.4.0-rc3,cc @tgravescs since this is a Spark Connect introduction including a note about built in authentication you [mentioned in JIRA ticket](https://issues.apache.org/jira/browse/SPARK-42374?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=17687978) before.,"1,-1    ",1,-1,0
1457564181,44108233,2023/3/9,v3.4.0-rc3,cc @allanf-db addressed the comments we discussed in offline,"1,-1    ",1,-1,0
1457691357,44108233,2023/3/9,v3.4.0-rc3,Also cc @HyukjinKwon ,"1,-1    ",1,-1,0
1459047783,44108233,2023/3/9,v3.4.0-rc3,"Let me close this for now, since the contents in this PR will be included in the future Spark Connect documents soon.
cc @allanf-db  FYI","1,-1    ",1,-1,0
1455379959,51110188,2023/3/9,v3.4.0-rc3,cc @cloud-fan @dongjoon-hyun ,"1,-1    ",1,-1,0
1455380079,51110188,2023/3/9,v3.4.0-rc3,cc @cloud-fan @dongjoon-hyun ,"1,-1    ",1,-1,0
1457080979,9700541,2023/3/9,v3.4.0-rc3,"Merged to branch-3.3. Thank you, @Yikf and @cloud-fan .","2,-1    ",2,-1,1
1455384866,8486025,2023/3/9,v3.4.0-rc3,@hvanhovell It seems that add test cases no way.,"1,-2    ",1,-2,-1
1455425240,9616802,2023/3/9,v3.4.0-rc3,hmmm - let me think about it.,"1,-1    ",1,-1,0
1456060688,9616802,2023/3/9,v3.4.0-rc3,@beliefer we should be able to create an in-memory table and append a couple of rows to that right?,"2,-1    ",2,-1,1
1487890609,9616802,2023/3/9,v3.4.0-rc3,@beliefer are you abandoning this one?,"1,-1    ",1,-1,0
1487935419,8486025,2023/3/9,v3.4.0-rc3,"> @beliefer are you abandoning this one?

Because other PR implement this function.","1,-1    ",1,-1,0
1489025120,9616802,2023/3/9,v3.4.0-rc3,Is that https://github.com/apache/spark/pull/40415?,"1,-1    ",1,-1,0
1489556667,8486025,2023/3/9,v3.4.0-rc3,"> Is that #40415?

It is https://github.com/apache/spark/pull/40358","1,-1    ",1,-1,0
1455397903,100322362,2023/3/9,v3.4.0-rc3,@HeartSaVioR - please take a look. Thx,"1,-1    ",1,-1,0
1455549225,1317309,2023/3/9,v3.4.0-rc3,Thanks! Merging to master.,"2,-1    ",2,-1,1
1455683396,7322292,2023/3/9,v3.4.0-rc3,"LGTM, merged into master/3.4","1,-1    ",1,-1,0
1455720452,8486025,2023/3/9,v3.4.0-rc3,@hvanhovell @zhengruifeng Thank you!,"1,-1    ",1,-1,0
1455701195,5399861,2023/3/9,v3.4.0-rc3,@cloud-fan @sunchao ,"1,-1    ",1,-1,0
1489605259,5399861,2023/3/9,v3.4.0-rc3,"Close it, because this change may have potential data issue. Users can `set spark.sql.legacy.typeCoercion.datetimeToString.enabled` to `true` to restore the old behavior.
","1,-1    ",1,-1,0
1457383616,104398294,2023/3/9,v3.4.0-rc3,build timed out but succeeded on rerun: https://github.com/vitaliili-db/spark/actions/runs/4346311324/jobs/7598960402,"1,-1    ",1,-1,0
1457384015,104398294,2023/3/9,v3.4.0-rc3,@gengliangwang can you review this please?,"1,-1    ",1,-1,0
1458779733,1097932,2023/3/9,v3.4.0-rc3,"@vitaliili-db Thanks for the work! 
LGTM overall. I got only one comment: the wording ""column options"" sounds weird. Shall we call it ""column descriptor""?
<img width=""763"" alt=""image"" src=""https://user-images.githubusercontent.com/1097932/223539786-da8c29d9-de24-495d-add5-3575687d8a2e.png"">
","1,-1    ",1,-1,0
1458880038,104398294,2023/3/9,v3.4.0-rc3,"@gengliangwang great catch, yes, we should follow standard. Renamed.","2,-2    ",2,-2,0
1459387912,1097932,2023/3/9,v3.4.0-rc3,"Thanks, merging to master","1,-2    ",1,-2,-1
1456016175,8486025,2023/3/9,v3.4.0-rc3,ping @hvanhovell @zhengruifeng @HyukjinKwon ,"1,-1    ",1,-1,0
1457271633,7322292,2023/3/9,v3.4.0-rc3,@beliefer I think it's not a `new features` mentioned in the PR description,"1,-1    ",1,-1,0
1457393807,6477701,2023/3/9,v3.4.0-rc3,Merged to master and branch-3.4.,"3,-2    ",3,-2,1
1457422020,8486025,2023/3/9,v3.4.0-rc3,@HyukjinKwon @zhengruifeng Thank you.,"1,-1    ",1,-1,0
1459426907,3182036,2023/3/9,v3.4.0-rc3,cc @gengliangwang @dtenedor ,"2,-2    ",2,-2,0
1460056826,3182036,2023/3/9,v3.4.0-rc3,"thanks for review, merging to master/3.4!","1,-1    ",1,-1,0
1457500143,3182036,2023/3/9,v3.4.0-rc3,"It's a good idea to provide an API that allows people to unambiguously reference metadata columns, and I like the new `Dataset.metadataColumn` function. However, I think the prepending underscore approach is a bit hacky. It's too implicit and I'd prefer a more explicit syntax like `SELECT metadata(_metadata) FROM t`. We can discuss this more and invite more SQL experts. Shall we exclude it from this PR for now?","1,-1    ",1,-1,0
1463005688,79601771,2023/3/9,v3.4.0-rc3,"> It's a good idea to provide an API that allows people to unambiguously reference metadata columns, and I like the new `Dataset.metadataColumn` function. However, I think the prepending underscore approach is a bit hacky. It's too implicit and I'd prefer a more explicit syntax like `SELECT metadata(_metadata) FROM t`. We can discuss this more and invite more SQL experts. Shall we exclude it from this PR for now?

@cloud-fan The prepended underscore is _NOT_ primarily intended as a user surface. Rather, it's a reliale way to get a unique column name that's still at least somewhat readable if you look at the query plan (unlike e.g. a uuid). The new `Dataset.metadataColumn` method does not even _look_ at a renamed attribute's name, for example.

I updated the PR description to not mention the specific renaming mechanism, and to indicate that a SQL user surface is out of scope.

At this point, the only remaining reference to prepended underscores is the two unit tests (""metadata name conflict resolved with leading underscores""), which validate that the renaming reliably produces unique names as intended. If you don't think the test coverage is important, we could remove even that?","1,-1    ",1,-1,0
1485216976,3182036,2023/3/9,v3.4.0-rc3,"about https://github.com/apache/spark/pull/40300/files#r1129818813 , I think if `SubqueryAlias` can't propagate metadata columns, then `df.metadataColumn` should not be able to get the column, what do you think? @ryan-johnson-databricks ","1,-2    ",1,-2,-1
1487610774,79601771,2023/3/9,v3.4.0-rc3,"> about https://github.com/apache/spark/pull/40300/files#r1129818813 , I think if `SubqueryAlias` can't propagate metadata columns, then `df.metadataColumn` should not be able to get the column, what do you think? @ryan-johnson-databricks

IMO changing the behavior of `SubqueryAlias` (and other specific plan node types) is out of scope for this PR -- this PR does not change that existing situation.","1,-1    ",1,-1,0
1489959075,3182036,2023/3/9,v3.4.0-rc3,"thanks, merging to master!","1,-1    ",1,-1,0
1464132393,822522,2023/3/9,v3.4.0-rc3,Merged to master,"1,-1    ",1,-1,0
1457930310,6477701,2023/3/9,v3.4.0-rc3,Mind retriggering https://github.com/alkis/spark/runs/11797022157?,"1,-1    ",1,-1,0
1457984961,265981,2023/3/10,v3.4.0-rc4,"> Mind retriggering https://github.com/alkis/spark/runs/11797022157?

Done.","1,-1    ",1,-1,0
1460901311,26535726,2023/3/10,v3.4.0-rc4,"Would the following code achieve the same improvements?
```
if (logger.isDebugEnabled()) {
  logger.debug(""string {}"", <expensive_function>);
}
```","1,-1    ",1,-1,0
1461170777,1591700,2023/3/10,v3.4.0-rc4,@pan3793's suggestion is simpler and less change.,"1,-1    ",1,-1,0
1461760841,265981,2023/3/10,v3.4.0-rc4,Good idea! Done.,"1,-1    ",1,-1,0
1463061643,6477701,2023/3/10,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1456893990,1938382,2023/3/10,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1457165581,4190164,2023/3/10,v3.4.0-rc4,Or even better? -> https://github.com/apache/spark/pull/40305,"3,-1    ",3,-1,2
1457382781,9616802,2023/3/10,v3.4.0-rc4,Merging,"1,-1    ",1,-1,0
1457679758,1475305,2023/3/10,v3.4.0-rc4,"cc @HyukjinKwon , can we merge this first before new RC? otherwise, the maven test will still fail

","1,-1    ",1,-1,0
1457690785,6477701,2023/3/10,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1457691351,1475305,2023/3/10,v3.4.0-rc4,Thanks @HyukjinKwon :),"1,-1    ",1,-1,0
1457166376,4190164,2023/3/10,v3.4.0-rc4,If this PR accepted then no need to merge https://github.com/apache/spark/pull/40303 as this PR override the changes needed there.,"1,-1    ",1,-1,0
1458096673,9616802,2023/3/10,v3.4.0-rc4,Merging.,"1,-1    ",1,-1,0
1457192756,1317309,2023/3/10,v3.4.0-rc4,"Thanks, merging to master!","1,-1    ",1,-1,0
1456844136,1591700,2023/3/10,v3.4.0-rc4,"This is still WIP, but want to get early feedback.
+CC @Ngone51, @otterc, @waitinfuture","1,-1    ",1,-1,0
1457000054,1591700,2023/3/10,v3.4.0-rc4,We are evaluating it currently @dongjoon-hyun :-),"1,-1    ",1,-1,0
1457022823,9700541,2023/3/10,v3.4.0-rc4,"If you don't mind, please share some results later~ :) ","1,-1    ",1,-1,0
1457315803,1591700,2023/3/10,v3.4.0-rc4,"The test failure is unrelated, so existing tests work fine - will work on specifically checking for the changes in this PR later today.","1,-1    ",1,-1,0
1457606879,8159038,2023/3/10,v3.4.0-rc4,"Hi @mridulm , thanks for your great work! Apache Uniffle is similar project to Apache Celeborn.  We also patched to the Apache Spark like https://github.com/apache/incubator-uniffle/blob/master/spark-patches/spark-3.2.1_dynamic_allocation_support.patch.  Considering that the shuffle data is stored in in distributed filesystem or in a disaggregated shuffle cluster, maybe  we should modify the method `ShuffledRowRDD#getPreferredLocations`, too.","1,-1    ",1,-1,0
1457619866,26535726,2023/3/10,v3.4.0-rc4,"@jerqi locality may still have benefits when RSS works in hybrid deployments, besides, there is a dedicated configuration for that `spark.shuffle.reduceLocality.enabled`","1,-1    ",1,-1,0
1457625037,8159038,2023/3/10,v3.4.0-rc4,"> spark.shuffle.reduceLocality.enabled

Thanks, I got it. But if we want to use the locality when RSS works in hybrid deployments, should we expose an interface for ShuffleDriverComponent to provide `getPreferredLocationsForShuffle`.","1,-1    ",1,-1,0
1457723960,8159038,2023/3/10,v3.4.0-rc4,"> @jerqi locality may still have benefits when RSS works in hybrid deployments, besides, there is a dedicated configuration for that `spark.shuffle.reduceLocality.enabled`

`ShuffledRdd#getPreferredLocations` will call the method `MapOutputTracker#getPreferredLocationsForShuffle` and `MapOutputTracker#getMapLocation`.
For `MapOutpuTracker#getMapLocation`, `spark.shuffle.reduceLocality.enabled` is useless. ","1,-1    ",1,-1,0
1458107057,948245,2023/3/10,v3.4.0-rc4,"> This is still WIP, but want to get early feedback. +CC @Ngone51, @otterc, @waitinfuture

Hi @mridulm , thanks for the work and it really simplifies the usage of Apache Celeborn (and other similar systems)! This patch looks good to me.

I totally agree with @pan3793 that in hybrid deployed environment, it'll benefit if locality can be exploited, and it's among future works of Apache Celeborn. Maybe we can add locality-related interface in ShuffleManager, because that is the base class that remote shuffle services overwrite.","1,-1    ",1,-1,0
1458674205,1591700,2023/3/10,v3.4.0-rc4,"@jerqi Agree that we should have a way to specify locality preference for disaggregated shuffle implementations to spark scheduler - so that shuffle tasks are closer to the data.

@otterc is relooking at SPARK-25299, given the platform changes that have gone in since it was proposed - and propose changes in the upcoming months. We can add this to that discussion/jira.

Hope that sounds fine.
For now, as @pan3793 suggested, turning off reduce locality for Uniffle should remove the need for the code patch (once this PR is merged) - if there are gaps, we can address those specifically to enforce the behavior of `spark.shuffle.reduceLocality.enabled`.","1,-1    ",1,-1,0
1459156913,8159038,2023/3/10,v3.4.0-rc4,"> > @jerqi locality may still have benefits when RSS works in hybrid deployments, besides, there is a dedicated configuration for that `spark.shuffle.reduceLocality.enabled`
> 
> `ShuffledRdd#getPreferredLocations` will call the method `MapOutputTracker#getPreferredLocationsForShuffle` and `MapOutputTracker#getMapLocation`. For `MapOutpuTracker#getMapLocation`, `spark.shuffle.reduceLocality.enabled` is useless.

My mistake. I means  `ShuffledRowRdd#getPreferredLocations` instead of `ShuffledRdd#getPreferredLocations`","1,-1    ",1,-1,0
1459167441,8159038,2023/3/10,v3.4.0-rc4,"> @jerqi Agree that we should have a way to specify locality preference for disaggregated shuffle implementations to spark scheduler - so that shuffle tasks are closer to the data.
> 
> @otterc is relooking at [SPARK-25299](https://issues.apache.org/jira/browse/SPARK-25299), given the platform changes that have gone in since it was proposed - and propose changes in the upcoming months. We can add this to that discussion/jira.
> 
> Hope that sounds fine. For now, as @pan3793 suggested, turning off reduce locality for Uniffle should remove the need for the code patch (once this PR is merged) - if there are gaps, we can address those specifically to enforce the behavior of `spark.shuffle.reduceLocality.enabled`.

There is some gaps. If we store data in the DFS or in disaggregated shuffle cluster. We should return `Nil` in the  `ShuffledRowRDD#getPreferredLocations`. For `CoalescedPartitionSpec`,  `spark.shuffle.reduceLocality.enabled` will help solve this problem. For `PartialReducerPartitionSpec`, `PartialMapperPartitionSpec` and `CoalescedMapperPartitionSpec `,  `spark.shuffle.reduceLocality.enabled`  can't help us. Because we call the method `MapOutpuTracker#getMapLocation`.
Could we modify this method in this pr like?
```
  def getMapLocation(
      dep: ShuffleDependency[_, _, _],
      startMapIndex: Int,
      endMapIndex: Int): Seq[String] =
  {
    val shuffleStatus = shuffleStatuses.get(dep.shuffleId).orNull
    if (shuffleStatus != null || shuffleLocalityEnabled) {
      shuffleStatus.withMapStatuses { statuses =>
        if (startMapIndex < endMapIndex &&
          (startMapIndex >= 0 && endMapIndex <= statuses.length)) {
          val statusesPicked = statuses.slice(startMapIndex, endMapIndex).filter(_ != null)
          statusesPicked.map(_.location.host).toSeq
        } else {
          Nil
        }
      }
    } else {
      Nil
    }
  }
```
Or we should raise an another pr to fix this issue?","1,-1    ",1,-1,0
1459618430,1591700,2023/3/10,v3.4.0-rc4,"@jerqi the basic issue here is, `getPreferredLocations` in `ShuffledRowRDD` should return `Nil` at the very beginning in case `spark.shuffle.reduceLocality.enabled = false` (conceptually).

This logic is pushed into MapOutputTracker though - and `getPreferredLocationsForShuffle` honors `spark.shuffle.reduceLocality.enabled` - but `getMapLocation` does not.

So the fix would be to fix `getMapLocation` to honor the parameter.

We should fix this in a different PR though.","3,-1    ",3,-1,2
1459630906,8159038,2023/3/10,v3.4.0-rc4,"> @jerqi the basic issue here is, `getPreferredLocations` in `ShuffledRowRDD` should return `Nil` at the very beginning in case `spark.shuffle.reduceLocality.enabled = false` (conceptually).
> 
> This logic is pushed into MapOutputTracker though - and `getPreferredLocationsForShuffle` honors `spark.shuffle.reduceLocality.enabled` - but `getMapLocation` does not.
> 
> So the fix would be to fix `getMapLocation` to honor the parameter.
> 
> We should fix this in a different PR though.

> 

Could I raise another pr to fix this issue?","1,-1    ",1,-1,0
1459652457,1591700,2023/3/10,v3.4.0-rc4,Sure ! Please go ahead :-),"1,-1    ",1,-1,0
1459802404,1591700,2023/3/10,v3.4.0-rc4,"Added tests, will wait for CI to complete.

+CC @otterc, @Ngone51 ","2,-1    ",2,-1,1
1461162897,1591700,2023/3/10,v3.4.0-rc4,Thanks for the review @dongjoon-hyun ! Really helpful,"1,-1    ",1,-1,0
1461445933,1591700,2023/3/10,v3.4.0-rc4,"All tests passed, merging to master.","1,-1    ",1,-1,0
1461449991,1591700,2023/3/10,v3.4.0-rc4,"Thanks for all the reviews @dongjoon-hyun, @otterc, @jerqi, @pan3793 and @waitinfuture :-)","1,-1    ",1,-1,0
1457537193,6235869,2023/3/10,v3.4.0-rc4,cc @huaxingao @cloud-fan @dongjoon-hyun @sunchao @viirya @gengliangwang ,"1,-1    ",1,-1,0
1500441091,6235869,2023/3/10,v3.4.0-rc4,Failures don't seem to be related.,"1,-1    ",1,-1,0
1512248162,6235869,2023/3/10,v3.4.0-rc4,Failures in streaming tests don't seem related.,"1,-2    ",1,-2,-1
1512630753,3182036,2023/3/10,v3.4.0-rc4,"thanks, merging to master!","1,-2    ",1,-2,-1
1513265266,6235869,2023/3/10,v3.4.0-rc4,"Thanks for reviewing, @cloud-fan @huaxingao @dongjoon-hyun @viirya @johanl-db!","1,-1    ",1,-1,0
1457104885,1938382,2023/3/10,v3.4.0-rc4,cc @zhengruifeng @HyukjinKwon @grundprinzip @cloud-fan @hvanhovell ,"1,-1    ",1,-1,0
1457390771,9616802,2023/3/10,v3.4.0-rc4,Merging.,"1,-1    ",1,-1,0
1457579306,1938382,2023/3/10,v3.4.0-rc4,LGTM!,"1,-1    ",1,-1,0
1459084648,6477701,2023/3/10,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1457682741,1938382,2023/3/10,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1457730273,6477701,2023/3/10,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1473032777,5399861,2023/3/10,v3.4.0-rc4,cc @cloud-fan ,"1,-1    ",1,-1,0
1519064385,8609142,2023/3/10,v3.4.0-rc4,"Itâs a good improvement, especially for RSS ","1,-1    ",1,-1,0
1560404852,26535726,2023/3/10,v3.4.0-rc4,cc @ulysses-you ,"1,-1    ",1,-1,0
1562365414,1591700,2023/3/10,v3.4.0-rc4,+CC @shardulm94 ,"1,-1    ",1,-1,0
1461171648,8326978,2023/3/10,v3.4.0-rc4,"cc @HyukjinKwon @srowen @dongjoon-hyun ,thanks","1,-1    ",1,-1,0
1461316501,8326978,2023/3/10,v3.4.0-rc4,"thanks @mridulm @srowen, merged to master/3.4/3.3/3.2","1,-1    ",1,-1,0
1457967882,46485123,2023/3/10,v3.4.0-rc4,ping @cloud-fan @dongjoon-hyun @HyukjinKwon ,"1,-1    ",1,-1,0
1459223471,46485123,2023/3/10,v3.4.0-rc4,"> Hi, @AngersZhuuuu .
> This PR seems to have insufficient information. Could you provide more details about how to validate this in what environment?

We run a client mode SparkSubmit job and throw below exception
```
23/03/07 18:34:50 INFO YarnClientSchedulerBackend: Shutting down all executors
23/03/07 18:34:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
23/03/07 18:34:50 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
23/03/07 18:34:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/03/07 18:34:50 INFO BlockManager: BlockManager stopped
23/03/07 18:34:50 INFO BlockManagerMaster: BlockManagerMaster stopped
23/03/07 18:34:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/03/07 18:34:50 INFO SparkContext: Successfully stopped SparkContext
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Table or view not found: xxx.xxx; line 1 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [xxx, xxx], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:115)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:95)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:184)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:183)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:183)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:183)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:95)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:178)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:175)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:621)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:616)
	at org.apache.spark.sql.auth.QueryAuthChecker$.main(QueryAuthChecker.scala:33)
	at org.apache.spark.sql.auth.QueryAuthChecker.main(QueryAuthChecker.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
23/03/07 18:34:50 INFO ShutdownHookManager: Shutdown hook called
23/03/07 18:34:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-8ce833e1-3cd4-4a9f-960d-695be85b12f4
23/03/07 18:34:50 INFO ShutdownHookManager: Deleting directory /hadoop/spark/sparklocaldir/spark-58bbd530-6144-4ad6-b62b-a690baac9f96
23/03/07 18:34:50 INFO SparkExecutionPlanProcessor: Lineage thread pool prepares to shut down
23/03/07 18:34:50 INFO SparkExecutionPlanProcessor: Lineage thread pool finishes to await termination and shuts down

```


This job failed, but with call `sparkContext.stop()`, client side failed but in AM it shows SUCCESS
In spark-3.1.2 the code like this
```
    try {
      app.start(childArgs.toArray, sparkConf)
    } catch {
      case t: Throwable =>
        throw findCause(t)
    } finally {
      if (!isShell(args.primaryResource) && !isSqlShell(args.mainClass) &&
        !isThriftServer(args.mainClass)) {
        try {
          SparkContext.getActive.foreach(_.stop())
        } catch {
          case e: Throwable => logError(s""Failed to close SparkContext: $e"")
        }
      }
    }
```

So here for normal job, I think we should pass the exit code to SchedulerBackend, right?


Then after your mention, I see that https://github.com/apache/spark/pull/33403 change the behavior that only k8s call `sc.stop()`, then I think for k8s and yarn mode we booth need to pass the exit code the backend.

After this pr, we also need to check if k8s backend exit code is same as client side in client mode too.
","1,-2    ",1,-2,-1
1459834474,3182036,2023/3/11,v3.4.0-rc4,Does YARN still have this issue with Spark 3.4?,"1,-1    ",1,-1,0
1460653478,9700541,2023/3/11,v3.4.0-rc4,"cc @mridulm and @tgravescs , too","1,-1    ",1,-1,0
1461250566,46485123,2023/3/11,v3.4.0-rc4,"> Does YARN still have this issue with Spark 3.4?

Didn't see such fix in current code.","1,-1    ",1,-1,0
1463331316,3182036,2023/3/11,v3.4.0-rc4,This seems to be a revert of https://github.com/apache/spark/pull/33403 as now we stop SparkContext in YARN environment as well. We should justify it in the PR description. This is not simply passing the exitCode. Please update the PR title as well.,"1,-1    ",1,-1,0
1463340552,46485123,2023/3/11,v3.4.0-rc4,"> This seems to be a revert of #33403 as now we stop SparkContext in YARN environment as well. We should justify it in the PR description. This is not simply passing the exitCode. Please update the PR title as well.

DOne","1,-1    ",1,-1,0
1463378857,3182036,2023/3/11,v3.4.0-rc4,@dongjoon-hyun do you have more context about https://github.com/apache/spark/pull/33403? Why do we limit the stopping spark context behavior to k8s only?,"1,-1    ",1,-1,0
1463389076,46485123,2023/3/11,v3.4.0-rc4,Failed UT should not related to this pr.,"1,-1    ",1,-1,0
1463391491,46485123,2023/3/11,v3.4.0-rc4,"@cloud-fan Seems this code https://github.com/apache/spark/pull/32283 first want to fix issue in k8s, then @dongjoon-hyun make it limit in k8s env. But this also can work for yarn env....","1,-2    ",1,-2,-1
1464153355,9700541,2023/3/11,v3.4.0-rc4,"To @cloud-fan and all. Here is the full context.

- #32081 was the initial commit.
- The initial commit was reverted via ed3f103ee8 due to Hive Thrift Server failure, https://github.com/apache/spark/pull/32081#issuecomment-816399871 .
- The second commit of SPARK-34674 excludes all `Shell` and `STS` environments to avoid the UT failures.

Three months later after merging the second commit, there was a post-commit review.
- https://github.com/apache/spark/pull/32283#discussion_r670881700

    ![Screenshot 2023-03-10 at 9 42 04 AM](https://user-images.githubusercontent.com/9700541/224386302-ba1780ab-8be3-461f-aa7d-01a58d964df8.png)

Since SPARK-34674 was released already to Spark 3.1.2, according to the post-commit comment, I made a new JIRA, SPARK-36193 (#33403) which was released as 3.1.3.
- [SPARK-36193][CORE] Recover SparkSubmit.runMain not to stop SparkContext in non-K8s env","1,-1    ",1,-1,0
1465541487,3182036,2023/3/11,v3.4.0-rc4,@AngersZhuuuu can you comment on the original discussion thread and convince related people to add back `sc.stop`? https://github.com/apache/spark/pull/32081#discussion_r663434289,"1,-1    ",1,-1,0
1465695376,46485123,2023/3/11,v3.4.0-rc4,"> @AngersZhuuuu can you comment on the original discussion thread and convince related people to add back `sc.stop`? [#32081 (comment)](https://github.com/apache/spark/pull/32081#discussion_r663434289)

TBH, you can think it as a new feature, since they first just want to only support k8s, and this pr support yarn too","1,-1    ",1,-1,0
1465701863,3182036,2023/3/11,v3.4.0-rc4,I believe people in that discussion thread have the most context (some of them are committers) and I'm not comfortable merging it without them taking a look.,"1,-1    ",1,-1,0
1500073974,46485123,2023/3/11,v3.4.0-rc4,ping @dongjoon-hyun @HyukjinKwon @attilapiros @srowen ,"1,-1    ",1,-1,0
1457984092,46485123,2023/3/11,v3.4.0-rc4,ping @HyukjinKwon ,"1,-1    ",1,-1,0
1461590509,46485123,2023/3/11,v3.4.0-rc4,ping @HyukjinKwon ,"1,-1    ",1,-1,0
1500104571,46485123,2023/3/11,v3.4.0-rc4,@amaliujia Like current? also ping @HyukjinKwon ,"1,-1    ",1,-1,0
1500531162,1938382,2023/3/11,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1458241313,15246973,2023/3/11,v3.4.0-rc4,"<img width=""797"" alt=""image"" src=""https://user-images.githubusercontent.com/15246973/223446693-3c296b56-f9aa-4b70-9eb3-5bc9059ba631.png"">
","1,-1    ",1,-1,0
1459182974,15246973,2023/3/11,v3.4.0-rc4,cc @itholic ,"1,-1    ",1,-1,0
1459721237,15246973,2023/3/11,v3.4.0-rc4,"## Root cause
The data type nullable inferred from the data is true by default
However, on the connector server side, `to(schema: StructType)` is called, and then `Project.matchSchema (logicalPlan, schema, sparkSession. sessionState. conf)` is executed","1,-1    ",1,-1,0
1464790036,506656,2023/3/11,v3.4.0-rc4,"Hi @panbingkun, Good catch and thanks for working on this!
But I'm afraid I don't think this is a good direction. It won't fix the other cases.
Let me take a look at this issue and submit a PR later.","1,-1    ",1,-1,0
1464812653,15246973,2023/3/11,v3.4.0-rc4,"> Hi @panbingkun, Good catch and thanks for working on this! But I'm afraid I don't think this is a good direction. It won't fix the other cases. Let me take a look at this issue and submit a PR later.

Okay.","1,-1    ",1,-1,0
1465005658,506656,2023/3/11,v3.4.0-rc4,I submitted the PR #40382.,"1,-1    ",1,-1,0
1458123804,1475305,2023/3/11,v3.4.0-rc4,"cc @HyukjinKwon fix a maven test failed of connect server module due to dependency loss

","1,-1    ",1,-1,0
1458151326,9616802,2023/3/11,v3.4.0-rc4,"@HyukjinKwon @gatorsmile @cloud-fan @dongjoon-hyun this is the 101st we have broken the maven build in the last month alone. We don't test with it, but we do feel comfortable to release with it. Are we sure the dual build setup is still a thing we want to pursue? Isn't it time to start thinking about greener pastures...","1,-1    ",1,-1,0
1458156395,1475305,2023/3/11,v3.4.0-rc4,"> this is the 101st we have broken the maven build in the last month alone. We don't test with it, but we do feel comfortable to release with it. Are we sure the dual build setup is still a thing we want to pursue? Isn't it time to start thinking about greener pastures...

Personally, I think we should consider migrating the build to sbt only in Spark 3.5.0, also cc @pan3793 

","1,-1    ",1,-1,0
1458723866,9700541,2023/3/11,v3.4.0-rc4,"For the following @hvanhovell 's question, I'd ask @srowen 's advice.
> this is the 101st we have broken the maven build in the last month alone. We don't test with it, but we do feel comfortable to release with it. Are we sure the dual build setup is still a thing we want to pursue? Isn't it time to start thinking about greener pastures...

","2,-1    ",2,-1,1
1458757289,822522,2023/3/11,v3.4.0-rc4,"While I'm a Maven person myself, I above all have also long preferred one build over two, and an SBT build is fine with me. I actually can't recall if there were strong reasons to keep one or the other in the past; seems like there were Reasons we needed both. We do use mvn in a few places in scripts to parse out stuff about the build, like to determine when new dependencies have been added. mvn and sbt dependency resolution aren't the same, but all the more reason to just pick one, even if we have to migrate some scripts","1,-1    ",1,-1,0
1459157408,1475305,2023/3/12,v3.4.0-rc4,re-triggered GA,"1,-1    ",1,-1,0
1459375825,8486025,2023/3/12,v3.4.0-rc4,@LuciferYang Thank you for the job. https://github.com/apache/spark/pull/40291 need this one.,"2,-1    ",2,-1,1
1459462492,6477701,2023/3/12,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1459646333,1475305,2023/3/12,v3.4.0-rc4,Thanks @HyukjinKwon @hvanhovell @dongjoon-hyun @srowen @beliefer ,"1,-1    ",1,-1,0
1458469794,1938382,2023/3/12,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1459765490,1475305,2023/3/12,v3.4.0-rc4,Thanks @hvanhovell @amaliujia ,"1,-1    ",1,-1,0
1458652487,1134574,2023/3/12,v3.4.0-rc4,"@justaparth Thanks for the contribution. Please create a JIRA ticket on https://issues.apache.org/jira/projects/SPARK/summary and change the title to 

`[SPARK-?????][DOC] Update code example formatting for protobuf parsing readme` 

Thanks.","1,-2    ",1,-2,-1
1459045889,79601771,2023/3/12,v3.4.0-rc4,"Something went wrong with [Run spark on kubernetes integration test](https://github.com/ryan-johnson-databricks/spark/actions/runs/4358877500/jobs/7620022040):
```
[info] *** Test still running after 2 minutes, 57 seconds: suite name: KubernetesSuite, test name: Test decommissioning with dynamic allocation & shuffle cleanups. 
[info] - Test decommissioning with dynamic allocation & shuffle cleanups (3 minutes, 3 seconds)
[info] - Test decommissioning timeouts (1 minute)
[info] - SPARK-37576: Rolling decommissioning (1 minute, 11 seconds)
[info] org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite *** ABORTED *** (25 minutes, 32 seconds)
[info]   io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/api/v1/namespaces. Message: object is being deleted: namespaces ""spark-6bff7607e9884740a4bac53b1fb655ae"" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=namespaces, name=spark-6bff7607e9884740a4bac53b1fb655ae, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: namespaces ""spark-6bff7607e9884740a4bac53b1fb655ae"" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).
[info]   at io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)
[info]   at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:538)
   ...
[info]   at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesTestComponents.createNamespace(KubernetesTestComponents.scala:51)
[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.setUpTest(KubernetesSuite.scala:202)
   ...
[info]   at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.runTest(KubernetesSuite.scala:45)
   ...
[info]   Cause: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/api/v1/namespaces. Message: object is being deleted: namespaces ""spark-6bff7607e9884740a4bac53b1fb655ae"" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=namespaces, name=spark-6bff7607e9884740a4bac53b1fb655ae, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: namespaces ""spark-6bff7607e9884740a4bac53b1fb655ae"" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).
```
(not sure how that could be related to this PR?)","1,-1    ",1,-1,0
1459625070,7322292,2023/3/12,v3.4.0-rc4,merged into master/branch-3.4,"1,-1    ",1,-1,0
1459158853,1475305,2023/3/12,v3.4.0-rc4,"Is there a similar case on Scala connect client ï¿½?

","1,-1    ",1,-1,0
1459184767,506656,2023/3/12,v3.4.0-rc4,"> Is there a similar case on Scala connect client ï¿½?

I haven't tried Scala client, but yes, it would happen, and this will fix both.","1,-1    ",1,-1,0
1459249689,1475305,2023/3/12,v3.4.0-rc4,"Is there a chance to add a similar case in `ClientE2ETestSuite`?

","1,-1    ",1,-1,0
1459251014,7322292,2023/3/12,v3.4.0-rc4,"@LuciferYang This PR fix it in the connect planner, so should also works for the Scala Client.","1,-1    ",1,-1,0
1459252795,1475305,2023/3/12,v3.4.0-rc4,"> @LuciferYang This PR fix it in the connect planner, so should also works for the Scala Client.

OK, got it","1,-1    ",1,-1,0
1459254286,7322292,2023/3/12,v3.4.0-rc4,"thank you all, merged into master/branch-3.4","1,-1    ",1,-1,0
1467450414,6477701,2023/3/12,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1459464317,7322292,2023/3/12,v3.4.0-rc4,merged into master/3.4,"3,-1    ",3,-1,2
1501325172,32387433,2023/3/12,v3.4.0-rc4,"It seems that this PR change is not necessary, I will close it, thank @LuciferYang @HyukjinKwon  for your review","1,-1    ",1,-1,0
1459455421,6477701,2023/3/12,v3.4.0-rc4,cc @zhengruifeng @ueshin @grundprinzip FYI,"1,-1    ",1,-1,0
1459962799,6477701,2023/3/12,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1460605486,9700541,2023/3/12,v3.4.0-rc4,Merged to master/3.4.,"2,-1    ",2,-1,1
1459636445,47337188,2023/3/12,v3.4.0-rc4,CC @HyukjinKwon @hvanhovell ,"2,-1    ",2,-1,1
1461095654,6477701,2023/3/12,v3.4.0-rc4,@xinrong-meng mind rebasing this? Otherwise should be good to go,"2,-1    ",2,-1,1
1461587410,7322292,2023/3/12,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1461592928,47337188,2023/3/12,v3.4.0-rc4,Thanks @zhengruifeng !,"1,-1    ",1,-1,0
1459635902,47337188,2023/3/12,v3.4.0-rc4,CC @HyukjinKwon @hvanhovell ,"3,-1    ",3,-1,2
1459961430,6477701,2023/3/12,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1460004396,6477701,2023/3/12,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1459952731,7322292,2023/3/12,v3.4.0-rc4,why not using `from_json` and `from_csv` to do this?,"2,-1    ",2,-1,1
1459996898,1475305,2023/3/12,v3.4.0-rc4,"> why not using `from_json` and `from_csv` to do this?

How to get the schema?

","1,-1    ",1,-1,0
1460106740,7322292,2023/3/12,v3.4.0-rc4,"not sure whether Iâm missing something, but isnât the schema already provided by users?","1,-1    ",1,-1,0
1461175796,7322292,2023/3/12,v3.4.0-rc4,"probably not related to this PR: 

https://github.com/apache/spark/blob/39a55121888d2543a6056be65e0c74126a9d3bdf/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L63-L76

```
  def schema(schemaString: String): DataFrameReader = {
    schema(StructType.fromDDL(schemaString))
  }
```

when the user provide a DDL string, it invoke the parser. Here I think we should keep both StructType and DDL string, and pass them to the server side.","1,-1    ",1,-1,0
1461183365,1475305,2023/3/12,v3.4.0-rc4,"> probably not related to this PR:
> 
> https://github.com/apache/spark/blob/39a55121888d2543a6056be65e0c74126a9d3bdf/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L63-L76
> 
> ```
>   def schema(schemaString: String): DataFrameReader = {
>     schema(StructType.fromDDL(schemaString))
>   }
> ```
> 
> when the user provide a DDL string, it invoke the parser. Here I think we should keep both StructType and DDL string, and pass them to the server side.

message `Read` seems also need to consider thisï¼I think we can further discuss this problem in a separate pr?
","1,-1    ",1,-1,0
1461430958,7322292,2023/3/13,v3.4.0-rc4,"@LuciferYang thank you for woking on this.

merged into master/branch-3.4","3,-1    ",3,-1,2
1461435665,1475305,2023/3/13,v3.4.0-rc4,Thanks @zhengruifeng @hvanhovell @HyukjinKwon  ~,"1,-1    ",1,-1,0
1463075399,3182036,2023/3/13,v3.4.0-rc4,"The failed `BasicSchedulerIntegrationSuite` is not related to this PR, I'm merging it to master/3.4, thanks for the review!","1,-1    ",1,-1,0
1463076663,3182036,2023/3/13,v3.4.0-rc4,"This is a bug fix of a new feature in 3.4, so I won't call it a release blocker. I've set the fixed version to 3.4.0, if rc3 passes, I'll change it to 3.4.1.","1,-1    ",1,-1,0
1463085558,6477701,2023/3/13,v3.4.0-rc4,Seems like the compliation didn't pass. Let me just quickly revert this and reopen.,"2,-1    ",2,-1,1
1463145544,3182036,2023/3/13,v3.4.0-rc4,"maybe there is a conflict right after my last commit, let me rebase","1,-1    ",1,-1,0
1463321220,3182036,2023/3/13,v3.4.0-rc4,"GA passes, let me merge it back.","1,-3    ",1,-3,-2
1477439623,44700269,2023/3/13,v3.4.0-rc4,"@cloud-fan is reporting a clustered distribution still supported? Data sources should be able to report that partitions are partitioned by some columns, without reporting the actual partitioning mechanism (like hash). That fact can be reused by `groupby` or windows functions with partitioning.","1,-1    ",1,-1,0
1461318716,8326978,2023/3/13,v3.4.0-rc4,"thanks, merged to master","1,-1    ",1,-1,0
1460067796,44108233,2023/3/13,v3.4.0-rc4,cc @srielau @MaxGekk @cloud-fan This is user-facing documentation for SQL error classes.,"1,-2    ",1,-2,-1
1460234386,1580697,2023/3/13,v3.4.0-rc4,"@itholic Did you write those files manually, or generated by a script?","1,-1    ",1,-1,0
1461084364,44108233,2023/3/13,v3.4.0-rc4,"> @itholic Did you write those files manually, or generated by a script?

I did both. I used the script, and for the parts that didn't print correctly, I fixed them manually by checking the page one by one. Since the script is not very polished for now, let me submit it as a follow-up task if we aim for automation in the future.","1,-3    ",1,-3,-2
1462195522,1580697,2023/3/13,v3.4.0-rc4,"+1, LGTM. Merging to master.
Thank you, @itholic and @srielau for review.","1,-3    ",1,-3,-2
1467329403,11567269,2023/3/13,v3.4.0-rc4,@MaxGekk  should we merge it to 3.4?,"1,-1    ",1,-1,0
1467595434,1580697,2023/3/13,v3.4.0-rc4,"@gatorsmile I stoped merging PRs related to error classes after RC0. Need to re-check carefully which error classes are in branch-3.4 already. @itholic Please, backport the changes to branch-3.4.","1,-1    ",1,-1,0
1469189625,3182036,2023/3/13,v3.4.0-rc4,It's probably ok to mention non-existing errors in the doc in 3.4,"1,-1    ",1,-1,0
1469347578,44108233,2023/3/13,v3.4.0-rc4,Just created PR for 3.4: https://github.com/apache/spark/pull/40433,"1,-1    ",1,-1,0
1468158739,904824,2023/3/13,v3.4.0-rc4,"> Looks pretty good. Mind taking a look at https://github.com/apache/spark/pull/40338/checks?check_run_id=11851128079?

@HyukjinKwon I've rebased against the latest master, however it seems that workflows won't start for new first-time contributors not inside the organization. If I read the documentation [1][2] correctly, a maintainer will need to activate the workflow?

[1] https://docs.github.com/en/actions/managing-workflow-runs/approving-workflow-runs-from-public-forks
[2] https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/enabling-features-for-your-repository/managing-github-actions-settings-for-a-repository#controlling-changes-from-forks-to-workflows-in-public-repositories","1,-1    ",1,-1,0
1469012020,6477701,2023/3/13,v3.4.0-rc4,"Apache Spark has a custom implementation that leverages the build (and resources) from forked repository. Mind checking if the workload is enabled in your fork (https://github.com/MaicoTimmerman/spark/actions/workflows/build_and_test.yml), and rebase this?","1,-1    ",1,-1,0
1469845702,6477701,2023/3/13,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1469847407,6477701,2023/3/13,v3.4.0-rc4,cc @zero323 in case you have some feedback on this.,"1,-1    ",1,-1,0
1469912441,1554276,2023/3/13,v3.4.0-rc4,"> cc @zero323 in case you have some feedback on this.

@HyukjinKwon I am OK with that, though there is a bigger issue here. We have `TypeVars` in `py` modules in quite a few places, which are a side effect of migrating to inline hints before dropping Python 3.8 support (looking at you, `Protocol`...). 

Ideally, we'd handle all of that consistently and use consistent naming convention, but that's not something will be able to do any time soon.
","1,-1    ",1,-1,0
1460624804,1591700,2023/3/13,v3.4.0-rc4,"Is this still in draft @jerqi ?
Also, +CC @cloud-fan who reviewed this earlier.","1,-1    ",1,-1,0
1461148705,8159038,2023/3/13,v3.4.0-rc4,"> Is this still in draft @jerqi ? Also, +CC @cloud-fan who reviewed this earlier.

Thanks @mridulm It's ready for review.","1,-1    ",1,-1,0
1461226978,1591700,2023/3/13,v3.4.0-rc4,Will wait for the CI to succeed. Thanks for fixing this @jerqi !,"1,-1    ",1,-1,0
1461236436,8159038,2023/3/13,v3.4.0-rc4,thanks @mridulm ,"1,-1    ",1,-1,0
1461353825,8159038,2023/3/13,v3.4.0-rc4,"The test failure is unrelated, I retrigger the test.","1,-1    ",1,-1,0
1462498084,1591700,2023/3/13,v3.4.0-rc4,"Merged to master.
Thanks for working on this @jerqi !
Thanks for the reviews @cloud-fan, @LuciferYang, @advancedxy :-)","1,-1    ",1,-1,0
1461349007,1580697,2023/3/13,v3.4.0-rc4,"Merging to master. Thank you, @srielau and @HyukjinKwon for review.","1,-1    ",1,-1,0
1461439265,1475305,2023/3/13,v3.4.0-rc4,Need run `./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm` to format the code in connect modules,"1,-1    ",1,-1,0
1461455759,502522,2023/3/13,v3.4.0-rc4,@LuciferYang thanks. Fixed.,"1,-1    ",1,-1,0
1463998900,1475305,2023/3/13,v3.4.0-rc4,"fine to me, cc @zhenlineo @amaliujia @hvanhovell FYI","1,-1    ",1,-1,0
1464810053,9700541,2023/3/13,v3.4.0-rc4,"Hi, @rangadi and @hvanhovell . There is a `scalafmt` error. Here is a follow-up to recover CI
- https://github.com/apache/spark/pull/40374","1,-1    ",1,-1,0
1464952775,502522,2023/3/13,v3.4.0-rc4,@dongjoon-hyun thank you! I should have checked. missed it.,"1,-1    ",1,-1,0
1460821308,1938382,2023/3/13,v3.4.0-rc4,@HyukjinKwon @hvanhovell ,"1,-1    ",1,-1,0
1461088027,6477701,2023/3/13,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1460916956,26535726,2023/3/13,v3.4.0-rc4,"> Manually tested

@zhenlineo would you mind supplying more details about your test steps and results? Actually, I have been trying these scripts since you added them the first time, finally, it success w/ a workaround(by adding an extra parameter `spark-internal`, I suppose this PR will fix it) after your last update. It would be good if you can leave some commands in your PR description, which makes it easier for reviewers or learners to verify your changes.","1,-1    ",1,-1,0
1461128696,6477701,2023/3/13,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1461214457,1097932,2023/3/13,v3.4.0-rc4,"@HyukjinKwon Thanks for the review.
Merging to master/3.4","1,-1    ",1,-1,0
1461053761,1938382,2023/3/13,v3.4.0-rc4,@hvanhovell ,"1,-1    ",1,-1,0
1461163592,1938382,2023/3/13,v3.4.0-rc4,"@hvanhovell do you know how to fix this

```
Error: ] /home/runner/work/spark/spark/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala:395: inferred existential type io.grpc.ManagedChannelBuilder[?0]( forSome { type ?0 <: io.grpc.ManagedChannelBuilder[?0] }), which cannot be expressed by wildcards,  should be enabled
by making the implicit value scala.language.existentials visible.
```","1,-1    ",1,-1,0
1461241503,1938382,2023/3/13,v3.4.0-rc4,"In fact, the complete error message says how to fix it....","1,-1    ",1,-1,0
1462842661,1938382,2023/3/13,v3.4.0-rc4,@hvanhovell can you take another look?,"1,-1    ",1,-1,0
1464644878,9616802,2023/3/13,v3.4.0-rc4,Merging.,"1,-1    ",1,-1,0
1461324040,8326978,2023/3/13,v3.4.0-rc4,"This script is just a fork from the sbt repo, and the hint info for current java seems correct to me. FYI, https://github.com/sbt/sbt/blob/1.9.x/sbt#L576","1,-1    ",1,-1,0
1461364371,35164941,2023/3/13,v3.4.0-rc4,"Thanks for comment, @yaooqinn .  The build/sbt script seems to be a simplified sbt script. I think the hint info for current java is not correct.to be honest, I think it is a copy-paste error from the hint info for sbt_version argument. I can create a PR in sbt/sbt project to see if it can be updated.","1,-1    ",1,-1,0
1466284139,822522,2023/3/13,v3.4.0-rc4,"OK, so this is just changing some stuff for shellcheck. It seems OK. This is just a copy of the sbt script. Normally I'd say, does this make it harder to reason about changes if we pull a new copy from upstream? but probably not. This also probably doesn't fix any actual problem. I'm neutral on bothering to change these.","1,-2    ",1,-2,-1
1467133558,35164941,2023/3/13,v3.4.0-rc4,"Yes, this PR is kind of useless. I close it now.","1,-2    ",1,-2,-1
1461495703,7322292,2023/3/13,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1461580489,7322292,2023/3/13,v3.4.0-rc4,cc @WeichenXu123 @HyukjinKwon ,"1,-3    ",1,-3,-2
1463120914,7322292,2023/3/13,v3.4.0-rc4,merged into master/branch-3.4,"1,-2    ",1,-2,-1
1463058011,6477701,2023/3/14,v3.4.0-rc4,"The test failure seems unrelated.

Merged to master and branch-3.4.","1,-1    ",1,-1,0
1463193506,47337188,2023/3/14,v3.4.0-rc4,Thanks @HyukjinKwon !,"1,-1    ",1,-1,0
1500146157,1475305,2023/3/14,v3.4.0-rc4,"In the last commit, make `BloomFilterAggregate` explicitly supported `IntegerType/ShortType/ByteType` and added corresponding updaters, then removed pass `dataType`  and `adding cast nodes` 

","1,-1    ",1,-1,0
1500147099,1475305,2023/3/14,v3.4.0-rc4,"GA failure is not related to the current PR
","1,-1    ",1,-1,0
1461919214,19235986,2023/3/14,v3.4.0-rc4,CC @HyukjinKwon @zhengruifeng ,"2,-1    ",2,-1,1
1462597849,9616802,2023/3/14,v3.4.0-rc4,@WeichenXu123 in what case won't Spark Connect ML have access to the session?,"1,-1    ",1,-1,0
1463060514,19235986,2023/3/14,v3.4.0-rc4,"> @WeichenXu123 in what case won't Spark Connect ML have access to the session?

For some APIs, like `estimator.fit(dataset)`, `model.transform(dataset)`, we can get session from the input spark dataframe, in other cases e.g. get a model attribute, we have no input dataframe, so we need `getActiveSession` to get the session and then send getting attribute requests to server side.","1,-1    ",1,-1,0
1464800450,19235986,2023/3/14,v3.4.0-rc4,"CC @HyukjinKwon @grundprinzip Thoughts ? ML use cases requires the global session and the session must be the same during the whole ML program execution.

And we cannot use thread-local session, it must be a global session, because some ML algorithm like ""CrossValidator"" will run estimator training in background threads.","1,-1    ",1,-1,0
1467834824,19235986,2023/3/14,v3.4.0-rc4,merged to master,"2,-1    ",2,-1,1
1467900779,6477701,2023/3/14,v3.4.0-rc4,@WeichenXu123 mind fixing the PR title? This PR doesn't add the getActiveSession method,"1,-1    ",1,-1,0
1469765761,19235986,2023/3/14,v3.4.0-rc4,"> @WeichenXu123 mind fixing the PR title? This PR doesn't add the getActiveSession method

Updated.","2,-1    ",2,-1,1
1462380542,1475305,2023/3/14,v3.4.0-rc4,"Thanks for your work. I'll take a closer look tomorrow

","1,-2    ",1,-2,-1
1464821519,8486025,2023/3/14,v3.4.0-rc4,ping @hvanhovell @HyukjinKwon @zhengruifeng @amaliujia ,"1,-1    ",1,-1,0
1467866018,8486025,2023/3/14,v3.4.0-rc4,ping @hvanhovell @zhengruifeng @LuciferYang ,"1,-1    ",1,-1,0
1469539349,8486025,2023/3/14,v3.4.0-rc4,@hvanhovell Could you have time to take a look ? Thanks!,"1,-1    ",1,-1,0
1477836670,8486025,2023/3/14,v3.4.0-rc4,ping @hvanhovell @HyukjinKwon Could you have time to take a look?,"1,-3    ",1,-3,-2
1480570911,7322292,2023/3/14,v3.4.0-rc4,ping @hvanhovell @zhenlineo ,"1,-1    ",1,-1,0
1489622921,8486025,2023/3/14,v3.4.0-rc4,@hvanhovell Could you take a review ?,"1,-1    ",1,-1,0
1518995122,8486025,2023/3/14,v3.4.0-rc4,cc @ueshin ,"1,-1    ",1,-1,0
1539453237,1475305,2023/3/14,v3.4.0-rc4,friendly ping @HyukjinKwon @hvanhovell @zhengruifeng ,"1,-1    ",1,-1,0
1540082810,6477701,2023/3/14,v3.4.0-rc4,Merged to master.,"1,-3    ",1,-3,-2
1541163674,8486025,2023/3/14,v3.4.0-rc4,@HyukjinKwon @LuciferYang @zhengruifeng @amaliujia @zhenlineo Thank you !,"1,-1    ",1,-1,0
1463045799,7322292,2023/3/14,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1463187698,47337188,2023/3/14,v3.4.0-rc4,"Merged to master and branch-3.4, thanks!","1,-1    ",1,-1,0
1463260567,9700541,2023/3/14,v3.4.0-rc4,"Thank you, @xinrong-meng and @HyukjinKwon .","1,-3    ",1,-3,-2
1467729639,6477701,2023/3/14,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1463340665,8486025,2023/3/14,v3.4.0-rc4,ping @cloud-fan cc @sadikovi ,"1,-1    ",1,-1,0
1465448190,3182036,2023/3/14,v3.4.0-rc4,"thanks, merged to master! can you open a backport PR for 3.4?","1,-1    ",1,-1,0
1465460479,8486025,2023/3/14,v3.4.0-rc4,"> thanks, merged to master! can you open a backport PR for 3.4?

Thank you! I will create it.","1,-1    ",1,-1,0
1464840038,9700541,2023/3/14,v3.4.0-rc4,Thank you for updating.,"1,-1    ",1,-1,0
1467294557,5399861,2023/3/14,v3.4.0-rc4,cc @cloud-fan  ,"1,-1    ",1,-1,0
1468021595,5399861,2023/3/14,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1467426007,5370302,2023/3/14,v3.4.0-rc4,"In our product environment, the k8s cluster is managed by other system and the port of apiserver is different between clusters. In this case, the spark.kubernetes.driver.master  on spark client side can't set a port for all clusters.","1,-1    ",1,-1,0
1463674599,1580697,2023/3/14,v3.4.0-rc4,"+1, LGTM. Merging to master.
Thank you, @gengliangwang and @cloud-fan for review.","1,-1    ",1,-1,0
1464149478,1097932,2023/3/14,v3.4.0-rc4,@MaxGekk @cloud-fan FYI I cherry-pick this one to branch 3.4 as well.,"1,-1    ",1,-1,0
1467439787,5370302,2023/3/14,v3.4.0-rc4,"> BTW, you can prevent the leak very easily by using TTL like S3/MinIO lifecycle rules.



We are using HDFS as the storage. 
And the ttl is not enough to all apps, such as streaming and AI training job which runs for several days.
If TTL is too short, it will affect the failover for spark apps. If TTL is very long, it cases storage waste.","1,-1    ",1,-1,0
1490831779,109815907,2023/3/14,v3.4.0-rc4,"@thousandhu @dongjoon-hyun  @holdenk 
The approach in this PR only handles the cleanup on driver side. It won't clean up the files if files were uploaded during job submission but then submission fails due to some reason. As in this driver won't be launched and clean up will not happen. 

This Jira SPARK-42744 is duplicate of SPARK-42466. I had already created PR for this issue which handles cleanup on both driver as well client side in case of app submission failure. 
Other than this it also optimises the file upload by creating just one upload sub directory rather than created several sub - directories for each file getting uploaded. 

https://github.com/apache/spark/pull/40128
","1,-1    ",1,-1,0
1463558877,7253827,2023/3/14,v3.4.0-rc4,"cc @cloud-fan, @ulysses-you

@xinrong-meng, this seems to be a regression from 3.3 to 3.4 so could you please wait with 3.4.0-RC4 if possible?","1,-1    ",1,-1,0
1463591644,12025282,2023/3/14,v3.4.0-rc4,"it seems to me that:
1. we should skip add `PlanExpression` into `aliasMap`
2. make `aliasMap` as immutable, and use a local mutable map, then merge them after done multiTransform","1,-1    ",1,-1,0
1463602749,7253827,2023/3/14,v3.4.0-rc4,"> it seems to me that:
> 
> 1. we should skip add `PlanExpression` into `aliasMap`
> 2. make `aliasMap` as immutable, and use a local mutable map, then merge them after done multiTransform

Oh ok, then we see the right fix a bit differently then.
But I'm not entirelly sure I get 2. We can make `aliasMap` immutable, by just adding a `.toMap` to here: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/AliasAwareOutputExpression.scala#L61. But what do you mean by

> and use a local mutable map, then merge them after done multiTransform

?","1,-1    ",1,-1,0
1463734692,47337188,2023/3/14,v3.4.0-rc4,Unfortunately v3.4.0-rc4 has been cut. Let's ensure it to be in RC5.,"1,-1    ",1,-1,0
1463754605,12025282,2023/3/14,v3.4.0-rc4,"@peter-toth nvm, skip adding PlanExpression and make it as immutable are kinds of `improvement`, the fix in this pr looks good","1,-1    ",1,-1,0
1463768672,3182036,2023/3/14,v3.4.0-rc4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1463885619,7253827,2023/3/14,v3.4.0-rc4,Thanks for the review!,"1,-1    ",1,-1,0
1463716561,67422435,2023/3/14,v3.4.0-rc4,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1463748784,5399861,2023/3/14,v3.4.0-rc4,@kenny-ddd Could you add `[SQL]` to title?,"1,-1    ",1,-1,0
1463787563,67422435,2023/3/14,v3.4.0-rc4,cc @wangyum ,"1,-1    ",1,-1,0
1465210746,5399861,2023/3/14,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1465290797,9700541,2023/3/14,v3.4.0-rc4,"Oh, it seems to break Scala style, @wangyum .
```
[error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala:33: File line length exceeds 100 characters
```","1,-3    ",1,-3,-2
1465294691,9700541,2023/3/14,v3.4.0-rc4,"To @wangyum , sometimes, the contributor's GitHub Action doesn't work properly. I also made similar mistakes before by forgetting the check. It's our responsibility, the committers, are responsible to check it manually and ask them to enable it properly. :)

![Screenshot 2023-03-12 at 1 39 08 PM](https://user-images.githubusercontent.com/9700541/224572230-7ba55e19-ff83-48dc-923f-1fd72c5389e9.png)
","1,-1    ",1,-1,0
1465332554,5399861,2023/3/14,v3.4.0-rc4,@dongjoon-hyun Sorry. I will pay attention to check next time.,"1,-1    ",1,-1,0
1465332828,9700541,2023/3/14,v3.4.0-rc4,"Oh, no problem at all. I just wanted to share my old mistakes.","1,-1    ",1,-1,0
1464866400,7322292,2023/3/14,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1464895976,8486025,2023/3/14,v3.4.0-rc4,@hvanhovell @zhengruifeng Thank you!,"1,-1    ",1,-1,0
1463813204,7322292,2023/3/14,v3.4.0-rc4,cc @WeichenXu123 @srowen @huaxingao ,"1,-1    ",1,-1,0
1464926814,822522,2023/3/14,v3.4.0-rc4,Merged to master/3.4/3.3/3.2,"1,-2    ",1,-2,-1
1465353652,7322292,2023/3/14,v3.4.0-rc4,@srowen Thank you for the reviews!,"2,-2    ",2,-2,0
1478027975,21010250,2023/3/14,v3.4.0-rc4,PR is mergeable @hvanhovell ,"1,-1    ",1,-1,0
1478874581,9616802,2023/3/14,v3.4.0-rc4,@vicennial can you update?,"1,-1    ",1,-1,0
1479506184,21010250,2023/3/14,v3.4.0-rc4,@hvanhovell updated,"2,-2    ",2,-2,0
1480408251,9616802,2023/3/14,v3.4.0-rc4,Merging this one.,"1,-1    ",1,-1,0
1463954820,3182036,2023/3/14,v3.4.0-rc4,cc @gengliangwang ,"1,-1    ",1,-1,0
1464466087,9700541,2023/3/14,v3.4.0-rc4,"cc @xinrong-meng , too","1,-1    ",1,-1,0
1464879868,20475650,2023/3/14,v3.4.0-rc4,Tests will not pass until Pandas 2.0.0 is released.,"2,-2    ",2,-2,0
1467721718,6477701,2023/3/14,v3.4.0-rc4,cc @itholic ,"1,-1    ",1,-1,0
1523825165,6477701,2023/3/14,v3.4.0-rc4,cc @itholic @zhengruifeng @xinrong-meng if you find some time to review.,"1,-1    ",1,-1,0
1524450947,44108233,2023/3/14,v3.4.0-rc4,"Change itself looks pretty good to me, once the CI is passed after the [initial pandas 2.0 support](https://github.com/apache/spark/pull/40658) completing.","1,-2    ",1,-2,-1
1464034053,3182036,2023/3/14,v3.4.0-rc4,cc @fred-db @bart-samwel ,"1,-1    ",1,-1,0
1464034554,3182036,2023/3/14,v3.4.0-rc4,"also cc @xinrong-meng , this should be included in 3.4.0","1,-1    ",1,-1,0
1464219331,9700541,2023/3/14,v3.4.0-rc4,Merged to master/3.4.,"1,-1    ",1,-1,0
1465459996,7322292,2023/3/14,v3.4.0-rc4,cc @itholic @HyukjinKwon ,"1,-1    ",1,-1,0
1468108487,822522,2023/3/14,v3.4.0-rc4,Merged to master,"1,-1    ",1,-1,0
1474472321,3187938,2023/3/14,v3.4.0-rc4,Thanks for reviews and merging.,"1,-2    ",1,-2,-1
1464526547,502522,2023/3/14,v3.4.0-rc4,cc: @HeartSaVioR ,"1,-1    ",1,-1,0
1464810221,9700541,2023/3/14,v3.4.0-rc4,"cc @hvanhovell , @xinrong-meng , @HyukjinKwon ","1,-1    ",1,-1,0
1464812929,1938382,2023/3/15,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1464814181,9700541,2023/3/15,v3.4.0-rc4,"Thank you, @amaliujia .","1,-1    ",1,-1,0
1464814364,9700541,2023/3/15,v3.4.0-rc4,"Scala linter passed. I'll merge this to recover the branches.
![Screenshot 2023-03-10 at 7 42 06 PM](https://user-images.githubusercontent.com/9700541/224463100-a6ee094f-615d-4b8d-9924-5ae7820fb76b.png)
","1,-1    ",1,-1,0
1464952660,502522,2023/3/15,v3.4.0-rc4,Thanks @dongjoon-hyun . Sorry about that. I missed that. ,"1,-1    ",1,-1,0
1464867349,7322292,2023/3/15,v3.4.0-rc4,cc @hvanhovell @LuciferYang @HyukjinKwon @WeichenXu123 ,"1,-1    ",1,-1,0
1464898823,1475305,2023/3/15,v3.4.0-rc4,"Also cc @beliefer , who is adding new function for `LiteralValueProtoConverter` to support `typedLit`

","1,-1    ",1,-1,0
1465584351,7322292,2023/3/15,v3.4.0-rc4,"thanks you all, merged to master/branch-3.4","1,-1    ",1,-1,0
1464865482,7322292,2023/3/15,v3.4.0-rc4,cc @WeichenXu123 @HyukjinKwon ,"1,-1    ",1,-1,0
1465422914,47337188,2023/3/15,v3.4.0-rc4,Do we intentionally ignore `YearMonthIntervalType`?,"1,-1    ",1,-1,0
1465436796,7322292,2023/3/15,v3.4.0-rc4,"> Do we intentionally ignore `YearMonthIntervalType`?

it seems that we have not support `YearMonthIntervalType` in vanilla PySpark, and the Python Client is reusing PySpark's types.
So it seems that we should add `YearMonthIntervalType` in PySpark first","1,-1    ",1,-1,0
1465585755,7322292,2023/3/15,v3.4.0-rc4,"thank you all, merged to master/branch-3.4","1,-3    ",1,-3,-2
1464895876,15246973,2023/3/15,v3.4.0-rc4,cc @hvanhovell ,"1,-1    ",1,-1,0
1467694021,9616802,2023/3/15,v3.4.0-rc4,Thanks for doing this. Small comment on the implementation.,"2,-1    ",2,-1,1
1469010474,6477701,2023/3/15,v3.4.0-rc4,Merged to master and branch-3.4.,"3,-1    ",3,-1,2
1465058951,822522,2023/3/15,v3.4.0-rc4,Merged to master,"1,-2    ",1,-2,-1
1464961368,1475305,2023/3/15,v3.4.0-rc4,install_app already check target,"1,-1    ",1,-1,0
1465301926,822522,2023/3/15,v3.4.0-rc4,"Wrong JIRA is linked, please adjust","2,-1    ",2,-1,1
1464999138,47577197,2023/3/15,v3.4.0-rc4,@dongjoon-hyun FYI ,"2,-2    ",2,-2,0
1465062311,9700541,2023/3/15,v3.4.0-rc4,"If you agree with the above assessment, please remove the misleading CVE information from the PR description, @bjornjorgensen .","2,-2    ",2,-2,0
1465141874,47577197,2023/3/15,v3.4.0-rc4,"Well, the comment that you are refereeing to, have a link but I cant get in 
![image](https://user-images.githubusercontent.com/47577197/224536606-58b733ab-cfb9-47e6-bf19-485fae5e3f2c.png)

3 weeks later they merged a PR https://github.com/fabric8io/kubernetes-client/commit/43b04f6cc2cde0b8cebb76c842c09de30c236780 that fix this issue. 

And yesterday SNYK open a PR to my repo for this issue. https://github.com/bjornjorgensen/spark/pull/102 
I can always change the text for this PR, but I haven't seen anything that makes me believe that kubernets-client is not affected by this CVE.

    ","1,-1    ",1,-1,0
1465158915,47577197,2023/3/15,v3.4.0-rc4,"**The maintainers of the library contend that the application's trust would already have had to be compromised or established and therefore dispute the risk associated with this issue on the basis that there is a high bar for exploitation. Thus, no fix is expected.**

https://security.snyk.io/vuln/SNYK-JAVA-ORGYAML-3152153


This is part of snakeyaml release notes

2.0 (2023-02-26)

Fix #570: SafeConstructor ignores LoaderOptions setCodePointLimit() (thanks to Robert Patrick)

Update #565: (Backwards-incompatible) Do not allow global tags by default to fix CVE-2022-1471 (thanks to Jonathan Leitschuh)


1.32 (2022-09-12)

Fix #547: Set the limit for incoming data to prevent a CVE report in NIST. By default it is 3MB

https://bitbucket.org/snakeyaml/snakeyaml/wiki/Changes","2,-1    ",2,-1,1
1465293060,9700541,2023/3/15,v3.4.0-rc4,"@bjornjorgensen . Do you mean you can not trust `winniegy`, the fabric8io community member's comment? He close the issue after that comment.
![Screenshot 2023-03-12 at 1 26 33 PM](https://user-images.githubusercontent.com/9700541/224571715-ee1bd3d2-07c6-4097-9f6f-09e08d9c920f.png)

Hence, the migration happens independently from the CVE. It's just for the future release.

In short, the following claim sounds wrong to me according to the context.
> 3 weeks later they merged a PR https://github.com/fabric8io/kubernetes-client/commit/43b04f6cc2cde0b8cebb76c842c09de30c236780 that fix this issue.","1,-1    ",1,-1,0
1465293982,9700541,2023/3/15,v3.4.0-rc4,"BTW, I have two additional questions for the following PR you referred.


1. Do you mean it's the evidence of the previous assessment?

> SafeConstructor ignores LoaderOptions setCodePointLimit() (thanks to Robert Patrick)

2. Do you think the following major version change is safe to us?
```
- <snakeyaml.version>1.33</snakeyaml.version>
+ <snakeyaml.version>2.5</snakeyaml.version>
```","1,-2    ",1,-2,-1
1465294879,47577197,2023/3/15,v3.4.0-rc4,"ok, I didn't know who this user was. I have updated the PR text now.","1,-1    ",1,-1,0
1465295815,47577197,2023/3/15,v3.4.0-rc4,Now that it turns out that the information that I have been given is not correct. This change the whole picture with why we should include this PR. I think we can wait until we are done with 3.4 so that we are on the safe side.,"1,-1    ",1,-1,0
1516409365,6799918,2023/3/15,v3.4.0-rc4,"Hey @dongjoon-hyun, 

Will this fix will be backported to other maintained branches (specifically the one I care about is the 3.3 branch)? 

Thanks! 

","1,-1    ",1,-1,0
1516498054,47577197,2023/3/15,v3.4.0-rc4,https://github.com/apache/spark/blob/cd166243ae4e3c8aafd1062994ce9daa94f58253/pom.xml#L213 upgrade from 5.12.2 to 6.5.0 thats alot of work.,"3,-1    ",3,-1,2
1526136105,47577197,2023/3/15,v3.4.0-rc4,"Are there any reasons why I need to get this error messages like this one? 
![image](https://user-images.githubusercontent.com/47577197/234956928-09d7f2c0-5488-47da-b6bc-b4ecca16f4cc.png)
","2,-1    ",2,-1,1
1526592183,9700541,2023/3/15,v3.4.0-rc4,"To @fryz . As @bjornjorgensen mentioned, the answer is no.
> Will this fix will be backported to other maintained branches (specifically the one I care about is the 3.3 branch)?","1,-1    ",1,-1,0
1465447635,7322292,2023/3/15,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1465064951,59893,2023/3/15,v3.4.0-rc4,"So we only keep it in the window between an executor requesting it's ID and it disconnecting to restabmudh a proper connection (generally <1s). Given it's a few bytes of information per outstanding executor without an ID assigned yet I'm not super worried about the memory usage.

We could also close the connection from the driver side instead I suppose but that might cause a race condition with the executor (would have to check how we handle disconnects and if we process outstanding messages).","1,-1    ",1,-1,0
1465065222,59893,2023/3/15,v3.4.0-rc4,"it would be used by in any situation where the executors don't have IDs pre-assigned to them, that might include other allocation techniques now that its pluggable but the only built in one is statefulsets.","1,-1    ",1,-1,0
1465067025,9700541,2023/3/15,v3.4.0-rc4,Thank you for the answer. Let me follow the code patch again.,"1,-1    ",1,-1,0
1465068495,9700541,2023/3/15,v3.4.0-rc4,Let me close this in favor of the previous PR.,"1,-1    ",1,-1,0
1465068554,15246973,2023/3/15,v3.4.0-rc4,"> Let me close this in favor of the previous PR.

Ok","1,-1    ",1,-1,0
1465068599,9700541,2023/3/15,v3.4.0-rc4,"To @panbingkun , you can search the JIRA or PR before making a PR in order to avoid unintentional duplication.","1,-1    ",1,-1,0
1467245909,3182036,2023/3/15,v3.4.0-rc4,"Yea AQE may remove materialized query stages due to optimizations like empty relation propagation, but I think it's fine as the shuffle files are still there (we don't unregister the shuffle), so the reused shuffle operator can still read these shuffle files using the shuffle id. The problem with EXPLAIN is it only looks for the referenced exchange in the query plan tree, I think we can also look up from the AQE stage cache map?","2,-1    ",2,-1,1
1467287615,83618776,2023/3/15,v3.4.0-rc4,"@cloud-fan Yes this is purely UI and EXPLAIN issue. It does not affect query execution. 

I'm not sure how AQE context stageCache map would help. The issue in EXPLAIN is that the ReusedExchange.child references a Exchange node that is not referenced anywhere else in the plan tree so we need to generate IDs on the subtree rooted at ReusedExchange.child and print them out. To do this, we need a way to check whether the ReusedExchange.child is referenced anywhere else - if they are not referenced anywhere else, we need to recursively generate IDs for subtree. I keep a HashSet of nodes with IDs already generated and check ReusedExchange.child against it to see if we need to recursively generate IDs on the subtree. ","1,-1    ",1,-1,0
1468104875,3182036,2023/3/15,v3.4.0-rc4,"@StevenChenDatabricks Thanks for the explanation, now I understand it.

I still have some high-level questions:
1. Does `ReusedSubquery` have the same issue?
2. what if there are more than one `ReusedExchange`s that references non-existing exchange?

I have a new idea that we still display `ReusedExchange` as it is, but we add a new section to display non-existing `Exchanges`:
```
==== Adaptively Optimized Out Exchanges ====
(132) Exchange
Input [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L]
Arguments: hashpartitioning(sr_store_sk#221, 200), ENSURE_REQUIREMENTS, [plan_id=1791]
```

Then there is no duplication even if more than one `ReusedExchange` reference it.","1,-1    ",1,-1,0
1468621742,83618776,2023/3/15,v3.4.0-rc4,"@cloud-fan Thanks for the idea and response!

1. I don't think this issue doesn't affects `ReusedSubquery` because of how its processed and printed. The current algorithm finds all Subquery nodes (including `ReusedSubquery`) and for each Subquery, it traverses the Subquery subtree to generate the IDs if they are missing. 
Furthermore, a `ReusedSubquery` does not print the details of the Subquery it reuses whereas for ReusedExchange it does print the Exchange ID being reused. For a `ReusedSubquery`, all that is printed is this line:
```Subquery:5 Hosting operator id = 50 Hosting Expression = ReusedSubquery Subquery scalar-subquery#31, [id=#32]```
`Hosting operator ID` is the parent operator that contains the `ReusedSubquery`. The subtree of the `ReusedSubquery` is not printed anywhere and the `ReusedSubquery` node itself is not printed in the main plan tree either. Even if there are non-existing children, the issue is not surfaced in the Explain plan by default.
I guess there's still a chance it might affect Spark UI whereby the IDs in the subtree of a `ReusedSubquery` are incorrect because the IDs were generated in a previous AQE iteration... I'm not sure. I think it's best to wait and see if a ticket/bug like this is ever reported. 

2. My fix detects all the ReusedExchanges with non-existing children and generate IDs on them. I guess your question is what if multiple `ReusedExchange` reference the same non-existing `Exchange`? That's a good point and I need to account for that edge case in the code in case that is possible.

With regards to your idea for a section of non-existing `Exchanges`: we already only print each operator exactly once in the node details section. As shown in the PR description: I currently print out the plan subtree of the Non-Existing Exchange below the `ReusedExchange` (since that subtree is not shown anywhere else) and the node details while maintaining uniqueness.","1,-1    ",1,-1,0
1469073120,3182036,2023/3/15,v3.4.0-rc4,"> we already only print each operator exactly once in the node details section

What if more than one `ReusedExchange` referencing the same `Exchange`? Will we print the `Exchange` twice?","3,-1    ",3,-1,2
1469194144,83618776,2023/3/15,v3.4.0-rc4,"@cloud-fan It wouldn't because `collectOperatorsWithID` in ExplainUtils is responsible for collecting the list of nodes to print out. It uses a BitSet `collectedOperators` that's globally shared to ensure each node is ""collected"" and printed exactly once.","1,-1    ",1,-1,0
1469225186,3182036,2023/3/15,v3.4.0-rc4,So we randomly pick one `ReusedExchange` to print its corresponding `Exchange`?,"1,-1    ",1,-1,0
1478591539,83618776,2023/3/15,v3.4.0-rc4,@cloud-fan I've addressed your comments. Thanks for the review! ,"1,-1    ",1,-1,0
1478798727,3182036,2023/3/15,v3.4.0-rc4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1465291603,9700541,2023/3/15,v3.4.0-rc4,cc @wangyum and @kenny-ddd ,"1,-1    ",1,-1,0
1465298354,9700541,2023/3/15,v3.4.0-rc4,"Scala linter passed. 

![Screenshot 2023-03-12 at 1 59 52 PM](https://user-images.githubusercontent.com/9700541/224573370-cbd15aa7-0cdc-43d6-8d01-9f35d77547f6.png)
","1,-1    ",1,-1,0
1465298419,9700541,2023/3/15,v3.4.0-rc4,"Hi, @bjornjorgensen . Could you review this PR?","1,-1    ",1,-1,0
1465300338,9700541,2023/3/15,v3.4.0-rc4,"Could you review this in order to recover master branch, @sunchao ?","1,-1    ",1,-1,0
1465302694,9700541,2023/3/15,v3.4.0-rc4,"Since this is a comment only change, let me merge this to recover the master branch and PRs.

![Screenshot 2023-03-12 at 2 20 11 PM](https://user-images.githubusercontent.com/9700541/224574385-6ab72bd1-4d16-42cf-a42a-72c0459f9a73.png)
","1,-1    ",1,-1,0
1465302800,9700541,2023/3/15,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1465304288,47577197,2023/3/15,v3.4.0-rc4,"@dongjoon-hyun Thank you. Yes, this PR looks ok. ","1,-1    ",1,-1,0
1465304619,9700541,2023/3/15,v3.4.0-rc4,"Thank you so much, @bjornjorgensen .","1,-1    ",1,-1,0
1465304673,47577197,2023/3/15,v3.4.0-rc4,I have seen this one time before. Can you restart tests on the PR that fails? ,"2,-1    ",2,-1,1
1465305631,9700541,2023/3/15,v3.4.0-rc4,"No, only the PR author can do that because we use the author's GitHub Actions for that PR.
> I have seen this one time before. Can you restart tests on the PR that fails?

For Apache Spark branch, of course, you can because you are the committer.","2,-1    ",2,-1,1
1465316569,47577197,2023/3/15,v3.4.0-rc4,Seams to be working now :),"1,-1    ",1,-1,0
1465585801,1475305,2023/3/15,v3.4.0-rc4,"Thanks for your fix @dongjoon-hyun 

","1,-1    ",1,-1,0
1465358231,9700541,2023/3/15,v3.4.0-rc4,"Could you review this PR, @viirya ?","1,-1    ",1,-1,0
1465575518,9700541,2023/3/15,v3.4.0-rc4,All tests passed. Merged to master.,"1,-1    ",1,-1,0
1467120067,506656,2023/3/15,v3.4.0-rc4,"I guess we can just do the following with the comment:

```py
# The implementation of pandas_udf is embedded in pyspark.sql.function.pandas_udf
# for code reuse.
from pyspark.sql.functions import pandas_udf as pandas_udf
```

for those who want to import `pyspark.sql.connect.functions`?","1,-1    ",1,-1,0
1467126454,506656,2023/3/15,v3.4.0-rc4,"Btw, what happens if `""PYSPARK_NO_NAMESPACE_SHARE"" in os.environ`?

https://github.com/apache/spark/blob/761e0c0f6f0d00733177a869b9ecdf454e13fc9f/python/pyspark/sql/utils.py#L148-L161

Usually the env var is set, we need to explicitly import `pyspark.sql.connect.function`.","1,-1    ",1,-1,0
1467281968,47337188,2023/3/15,v3.4.0-rc4,"We didn't wrap `pyspark.sql.function.pandas_udf` with `try_remote_functions`, so  `""PYSPARK_NO_NAMESPACE_SHARE""` should be irrelevant.","1,-1    ",1,-1,0
1467349620,506656,2023/3/15,v3.4.0-rc4,"It's irrelevant, that means it's an issue, no?

E.g, could you add a test in `test_connect_function`?","1,-1    ",1,-1,0
1467493588,47337188,2023/3/15,v3.4.0-rc4,"After double thoughts, https://github.com/apache/spark/pull/40388#issuecomment-1467120067 might not be what we want, as shown below

```
>>> from pyspark.sql.connect.functions import pandas_udf
>>> spark.range(2).select(pandas_udf(lambda x: x + 1, 'int')('id')).collect()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'NoneType' object is not callable

```","1,-1    ",1,-1,0
1467510024,47337188,2023/3/15,v3.4.0-rc4,"I see your concern on `PYSPARK_NO_NAMESPACE_SHARE`, it should have affected the imports of `pandas_udf`, that is, if it is not set, `sql.functions.pandas_udf` points to `sql.connect.functions.pandas_udf`. 
Unfortunately in our case, there is no `sql.connect.functions.pandas_udf` at all.

Considering`PYSPARK_NO_NAMESPACE_SHARE` is not a heavily-used user-facing env variable, shall we merge the change proposed in this PR first for clarity? Otherwise, I am afraid that users think pandas_udf is not supported in Connect. I will work on a follow-up to support `sql.connect.functions.pandas_udf` with `PYSPARK_NO_NAMESPACE_SHARE` support.

As for `E.g, could you add a test in test_connect_function?`

The unit tests for pandas_udf are in `python/pyspark/sql/tests/connect/test_parity_pandas_udf.py` if that's your concern.

CC @ueshin ","1,-1    ",1,-1,0
1467514203,47337188,2023/3/15,v3.4.0-rc4,Also CC @HyukjinKwon ,"1,-1    ",1,-1,0
1467528741,506656,2023/3/15,v3.4.0-rc4,"> After double thoughts, https://github.com/apache/spark/pull/40388#issuecomment-1467120067 might not be what we want, as shown below

I meant:

```diff
- def pandas_udf(*args: Any, **kwargs: Any) -> None:
-     raise NotImplementedError(""pandas_udf() is not implemented."")
+ # The implementation of pandas_udf is embedded in pyspark.sql.function.pandas_udf
+ # for code reuse.
+ from pyspark.sql.functions import pandas_udf as pandas_udf
```","1,-1    ",1,-1,0
1467557107,506656,2023/3/15,v3.4.0-rc4,"> The unit tests for pandas_udf are in `python/pyspark/sql/tests/connect/test_parity_pandas_udf.py` if that's your concern.

It's not my concern.
`sql.functions.pandas_udf` and `sql.connect.functions.pandas_udf` should be usable separately, for vanilla PySpark and Spark Connect, respectively, if `PYSPARK_NO_NAMESPACE_SHARE` is set.

> Considering`PYSPARK_NO_NAMESPACE_SHARE` is not a heavily-used user-facing env variable, shall we merge the change proposed in this PR first for clarity? Otherwise, I am afraid that users think pandas_udf is not supported in Connect. I will work on a follow-up to support `sql.connect.functions.pandas_udf` with `PYSPARK_NO_NAMESPACE_SHARE` support.

I'd leave the decision to @HyukjinKwon or @zhengruifeng then.","1,-1    ",1,-1,0
1467558915,47337188,2023/3/15,v3.4.0-rc4,Good idea! I adjusted the code and PR description accordingly. Thanks @ueshin ,"1,-1    ",1,-1,0
1467582894,6477701,2023/3/15,v3.4.0-rc4,"and, `pyspark.sql.connect.*` isn't also supposed to be called directly by users. so this PR is just more about internal refactoring.","1,-1    ",1,-1,0
1469264035,6477701,2023/3/15,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1465482691,1475305,2023/3/15,v3.4.0-rc4,"Will update pr description later

","1,-1    ",1,-1,0
1469396942,1475305,2023/3/15,v3.4.0-rc4,Should we push forward this one? @HyukjinKwon ,"1,-1    ",1,-1,0
1469820687,32387433,2023/3/15,v3.4.0-rc4,"> Should we push forward this one? @HyukjinKwon

Should, I run test on local, can't passed because I build without hive. And the 2.4.0 release can't build passed without hive. 
This is mail list which others find problem. https://lists.apache.org/thread/11kdt36n6ytx87klcp2gd678lddqoknd ","1,-1    ",1,-1,0
1469844012,6477701,2023/3/15,v3.4.0-rc4,cc @grundprinzip and @hvanhovell FYI,"1,-1    ",1,-1,0
1469847935,1475305,2023/3/15,v3.4.0-rc4,"> > Should we push forward this one? @HyukjinKwon
> 
> Should, I run test on local, can't passed because I build without hive. And the 2.4.0 release can't build passed without hive. This is mail list which others find problem. https://lists.apache.org/thread/11kdt36n6ytx87klcp2gd678lddqoknd

Yes,  it will be slightly friendly to developers with this pr

","2,-1    ",2,-1,1
1469850098,6477701,2023/3/15,v3.4.0-rc4,"BTW, mind fxing https://github.com/apache/spark/pull/40389#discussion_r1136914961 though","1,-1    ",1,-1,0
1471270622,6477701,2023/3/16,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1471275097,1475305,2023/3/16,v3.4.0-rc4,Thanks @HyukjinKwon and @Hisoka-X ,"1,-1    ",1,-1,0
1472688553,47577197,2023/3/16,v3.4.0-rc4,@LuciferYang Thank you. Should we update https://github.com/apache/spark/blob/master/docs/building-spark.md with this info? ,"1,-1    ",1,-1,0
1473017525,1475305,2023/3/16,v3.4.0-rc4,"> @LuciferYang Thank you. Should we update https://github.com/apache/spark/blob/master/docs/building-spark.md with this info?

If needed, it may be more appropriate to update in https://github.com/apache/spark/blob/master/connector/connect/README.md","2,-2    ",2,-2,0
1465534836,12025282,2023/3/16,v3.4.0-rc4,"thank you @dongjoon-hyun , I'm fine to hold on this until next release. Another thought is I want to make sure all tests can be passed.","2,-4    ",2,-4,-2
1526859862,3182036,2023/3/16,v3.4.0-rc4,"Hi @dongjoon-hyun , this config was added in Spark 3.2 and we have fixed all the known regressions, I think it's time to turn it on by default in 3.5 to improve the AQE coverage. [SPARK-42101](https://issues.apache.org/jira/browse/SPARK-42101) was only for the first query access and doesn't matter that much.","1,-1    ",1,-1,0
1526962876,9700541,2023/3/16,v3.4.0-rc4,"Of course, I agree with you, @cloud-fan . Thank you for pinging me.

To @ulysses-you , could you rebase this PR?","1,-1    ",1,-1,0
1526963017,9700541,2023/3/16,v3.4.0-rc4,"Also, cc @sunchao too","1,-1    ",1,-1,0
1527065705,12025282,2023/3/16,v3.4.0-rc4,"sure, thank you @dongjoon-hyun @cloud-fan ","1,-1    ",1,-1,0
1465682073,45991044,2023/3/16,v3.4.0-rc4,"I am not sure if we should add a Executor exit code and optimize the RegisterExecutor response message in this pr.In production environment, we found sometimes only filter the exclued node when launching containers does not work as well as we want, because we found maybe driver does not request new executors for a period of time, so the execluded nodes list wont be sent to `YarnAllocator` though `requestTotalExecutorsWithPreferredLocalities`, some executors will failed and count to the failure count.

So we add the Executor exit code and optimize the RegisterExecutor response message.","2,-1    ",2,-1,1
1467201970,45991044,2023/3/16,v3.4.0-rc4,"@Ngone51 @tgravescs could you please help review this pr when you have time, thanks.","1,-1    ",1,-1,0
1465741894,9700541,2023/3/16,v3.4.0-rc4,"Hi, @HyukjinKwon . Could you review this PR when you have some time?","1,-1    ",1,-1,0
1465812124,26535726,2023/3/16,v3.4.0-rc4,"> ... for some executor pods to connect driver pods via IP.

Hi @dongjoon-hyun, I think it's quite useful, but in https://github.com/apache/spark/pull/39160#pullrequestreview-1229691638, you left a concern

> ... I have a concern. Currently, Apache Spark uses K8s Service entity via [DriverServiceFeatureStep](https://github.com/apache/spark/blob/master/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/DriverServiceFeatureStep.scala) to access Spark driver pod in K8s environment.

do you still concern that now?","1,-1    ",1,-1,0
1465870752,9700541,2023/3/16,v3.4.0-rc4,"@pan3793 .

The goal of PR is different from your PR's goal.
- Your PR tried to add `SPARK_DRIVER_POD_NAME` to `Driver Pod` to expose it to **3rd party pods**.
- This PR aims to add `SPARK_DRIVER_POD_IP` to `Executor Pod` in order to help **internal communications between Spark executors and Spark driver**.

In addition, this is a kind of propagation of the information from the driver pod to the executor pods instead of exposing the executor pods' internal information.","1,-1    ",1,-1,0
1466385779,9700541,2023/3/16,v3.4.0-rc4,"Hi, @viirya . Could you review this PR when you have some time?","2,-1    ",2,-1,1
1466547407,9700541,2023/3/16,v3.4.0-rc4,"Yes, correct, @viirya ! Thank you for the approval.","1,-1    ",1,-1,0
1466559161,9700541,2023/3/16,v3.4.0-rc4,Merged to master for Apache Spark 3.5.0.,"1,-1    ",1,-1,0
1467025042,1591700,2023/3/16,v3.4.0-rc4,+CC @otterc ,"1,-1    ",1,-1,0
1467314593,4690923,2023/3/16,v3.4.0-rc4,@Stove-hust Thank you for reporting and the patch. Would you be able to share driver logs?,"1,-1    ",1,-1,0
1467340408,52523908,2023/3/16,v3.4.0-rc4,"> @Stove-hust Thank you for reporting and the patch. Would you be able to share driver logs?

Sureï¼Add some commentsï¿½?
--- stage 10 faield 
22/10/15 10:55:58 WARN task-result-getter-1 TaskSetManager: Lost task 435.1 in stage 10.0 (TID 6822, zw02-data-hdp-dn21102.mt, executor 101): FetchFailed(null, shuffleId=3, mapIndex=-1, mapId=-1, reduceId=435, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3 partition 435
22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:386) failed in 601.792 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3 partition 435

-- resubmit stage 10 && parentStage 9
22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Resubmitting ShuffleMapStage 9 (processCmd at CliDriver.java:386) and ShuffleMapStage 10 (processCmd at CliDriver.java:386) due to fetch failure
22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Resubmitting failed stages
22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[22] at processCmd at CliDriver.java:386), which has no missing parents
22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Push-based shuffle disabled for ShuffleMapStage 9 (processCmd at CliDriver.java:386) since it is already shuffle merge finalized
22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[22] at processCmd at CliDriver.java:386) (first 15 tasks are for partitions Vector(98, 372, 690))
22/10/15 10:55:58 INFO dag-scheduler-event-loop YarnClusterScheduler: Adding task set 9.1 with 3 tasks

-- The first stage10 task completes one after another, and notifyDriverAboutPushCompletion to end stage 10, and mark finalizeTask, because the stage is not in runningStages, so the stage cannot be marked shuffleMergeFinalized.
22/10/15 10:55:58 INFO task-result-getter-0 TaskSetManager: Finished task 325.0 in stage 10.0 (TID 6166) in 154455 ms on zw02-data-hdp-dn25537.mt (executor 117) (494/500)
22/10/15 10:55:59 WARN task-result-getter-1 TaskSetManager: Lost task 325.1 in stage 10.0 (TID 6671, zw02-data-hdp-dn23160.mt, executor 47): TaskKilled (another attempt succeeded)
22/10/15 10:56:20 WARN task-result-getter-1 TaskSetManager: Lost task 358.1 in stage 10.0 (TID 6731, zw02-data-hdp-dn25537.mt, executor 95): TaskKilled (another attempt succeeded)
22/10/15 10:56:20 INFO task-result-getter-1 TaskSetManager: Task 358.1 in stage 10.0 (TID 6731) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).

--- Removed TaskSet 10.0, whose tasks have all completed
22/10/15 10:56:22 INFO task-result-getter-1 TaskSetManager: Ignoring task-finished event for 435.0 in stage 10.0 because task 435 has already completed successfully
22/10/15 10:56:22 INFO task-result-getter-1 YarnClusterScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 

--- notifyDriverAboutPushCompletion stage 10
22/10/15 10:56:23 INFO dag-scheduler-event-loop DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:386) scheduled for finalizing shuffle merge in 0 s
22/10/15 10:56:23 INFO shuffle-merge-finalizer-2 DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:386) finalizing the shuffle merge with registering merge results set to true

--- stage 9 finished 
22/10/15 10:57:51 INFO task-result-getter-1 TaskSetManager: Finished task 2.0 in stage 9.1 (TID 6825) in 112825 ms on zw02-data-hdp-dn25559.mt (executor 74) (3/3)
22/10/15 10:57:51 INFO task-result-getter-1 YarnClusterScheduler: Removed TaskSet 9.1, whose tasks have all completed, from pool 
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:386) finished in 112.832 s

--- resubmit stage 10
2/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: looking for newly runnable stages
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: running: Set(ShuffleMapStage 11, ShuffleMapStage 8)
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: waiting: Set(ShuffleMapStage 12, ShuffleMapStage 10)
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: failed: Set()
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[36] at processCmd at CliDriver.java:386), which has no missing parents
22/10/15 10:57:51 INFO dag-scheduler-event-loop OutputCommitCoordinator: Reusing state from previous attempt of stage 10.
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: Shuffle merge enabled before starting the stage for ShuffleMapStage 10 with shuffle 7 and shuffle merge 0 with 108 merger locations
22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[36] at processCmd at CliDriver.java:386) (first 15 tasks are for partitions Vector(105, 288, 447, 481))
22/10/15 10:57:51 INFO dag-scheduler-event-loop YarnClusterScheduler: Adding task set 10.1 with 4 tasks

--- stage 10 can not finished
22/10/15 10:58:18 INFO task-result-getter-1 TaskSetManager: Finished task 2.0 in stage 10.1 (TID 6857) in 26644 ms on zw02-data-hdp-dn23767.mt (executor 139) (1/4)
22/10/15 10:58:24 INFO task-result-getter-1 TaskSetManager: Finished task 3.0 in stage 10.1 (TID 6860) in 32551 ms on zw02-data-hdp-dn23729.mt (executor 42) (2/4)
22/10/15 10:58:47 INFO task-result-getter-1 TaskSetManager: Finished task 0.0 in stage 10.1 (TID 6858) in 55524 ms on zw02-data-hdp-dn20640.mt (executor 134) (3/4)
22/10/15 10:58:58 INFO task-result-getter-0 TaskSetManager: Finished task 1.0 in stage 10.1 (TID 6859) in 66911 ms on zw02-data-hdp-dn25862.mt (executor 57) (4/4)
","1,-1    ",1,-1,0
1469222238,52523908,2023/3/16,v3.4.0-rc4,"@otterc Hello, is there anything else I should add?","2,-1    ",2,-1,1
1469293356,4690923,2023/3/16,v3.4.0-rc4,@Stove-hust  Haven't had a chance to look at it yet. I'll take a look at it this week.,"1,-1    ",1,-1,0
1469402096,52523908,2023/3/16,v3.4.0-rc4,"> @Stove-hust Haven't had a chance to look at it yet. I'll take a look at it this week.

tks","1,-1    ",1,-1,0
1470501033,4690923,2023/3/16,v3.4.0-rc4,@akpatnam25 @shuwang21,"2,-1    ",2,-1,1
1473171295,1591700,2023/3/16,v3.4.0-rc4,"So this is an interesting coincidence, I literally encountered a production job which seems to be hitting this exact same issue :-)
I was in the process of creating a test case, but my intuition was along the same lines as this PR.

Can you create a test case to validate this behavior @Stove-hust ?
Essentially it should fail with current master, and succeed after this change.

Thanks for working on this fix","1,-1    ",1,-1,0
1473303194,52523908,2023/3/16,v3.4.0-rc4,"> So this is an interesting coincidence, I literally encountered a production job which seems to be hitting this exact same issue :-) I was in the process of creating a test case, but my intuition was along the same lines as this PR.
> 
> Can you create a test case to validate this behavior @Stove-hust ? Essentially it should fail with current master, and succeed after this change.
> 
> Thanks for working on this fix

No problem","3,-1    ",3,-1,2
1474162354,52523908,2023/3/16,v3.4.0-rc4,"> So this is an interesting coincidence, I literally encountered a production job which seems to be hitting this exact same issue :-) I was in the process of creating a test case, but my intuition was along the same lines as this PR.
> 
> Can you create a test case to validate this behavior @Stove-hust ? Essentially it should fail with current master, and succeed after this change.
> 
> Thanks for working on this fix

Added UT","1,-1    ",1,-1,0
1474749275,4690923,2023/3/16,v3.4.0-rc4,"@Stove-hust The main change in `DAGScheduler` looks good to me. Basically, [here](https://github.com/apache/spark/blob/11c9838283e98d5ebe6ce13b85e26217494feef2/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L762) we also check whether the parent stage is finalized and if it is not we submit that. The reason the parent stage is not getting finalized here is because it has no tasks. 
Will review the UT and take another look at the code next week. Thanks for fixing this.","1,-1    ",1,-1,0
1474782673,1591700,2023/3/16,v3.4.0-rc4,"Instead of only testing specifically for the flag - which is subject to change as the implementation evolves, we should also test for behavior here.

This is the reproducible test I was using (with some changes) to test approaches for this bug - and it mimics the case I saw in our production reasonably well.
(In DAGSchedulerSuite):
```
  for (pushBasedShuffleEnabled <- Seq(true, false)) {
    test(""SPARK-40082: recomputation of shuffle map stage with no pending partitions should not "" +
        s""hang. pushBasedShuffleEnabled = $pushBasedShuffleEnabled"") {

      if (pushBasedShuffleEnabled) {
        initPushBasedShuffleConfs(conf)
        DAGSchedulerSuite.clearMergerLocs()
        DAGSchedulerSuite.addMergerLocs(Seq(""host1"", ""host2"", ""host3"", ""host4"", ""host5""))
      }

      var taskIdCount = 0

      var completedStage: List[Int] = Nil
      val listener = new SparkListener() {
        override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {
          completedStage = completedStage :+ event.stageInfo.stageId
        }
      }
      sc.addSparkListener(listener)

      val fetchFailParentPartition = 0

      val shuffleMapRdd0 = new MyRDD(sc, 2, Nil)
      val shuffleDep0 = new ShuffleDependency(shuffleMapRdd0, new HashPartitioner(2))

      val shuffleMapRdd1 = new MyRDD(sc, 2, List(shuffleDep0), tracker = mapOutputTracker)
      val shuffleDep1 = new ShuffleDependency(shuffleMapRdd1, new HashPartitioner(2))

      val reduceRdd = new MyRDD(sc, 2, List(shuffleDep1), tracker = mapOutputTracker)

      // submit the initial mapper stage, generate shuffle output for first reducer stage.
      submitMapStage(shuffleDep0)

      // Map stage completes successfully,
      completeShuffleMapStageSuccessfully(0, 0, 3, Seq(""hostA"", ""hostB""))
      taskIdCount += 2
      assert(completedStage === List(0))

      // Now submit the first reducer stage
      submitMapStage(shuffleDep1)

      def createTaskInfo(speculative: Boolean): TaskInfo = {
        val taskInfo = new TaskInfo(
          taskId = taskIdCount,
          index = 0,
          attemptNumber = 0,
          partitionId = 0,
          launchTime = 0L,
          executorId = """",
          host = ""hostC"",
          TaskLocality.ANY,
          speculative = speculative)
        taskIdCount += 1
        taskInfo
      }

      val normalTask = createTaskInfo(speculative = false);
      val speculativeTask = createTaskInfo(speculative = true)

      // fail task 1.0 due to FetchFailed, and make 1.1 succeed.
      runEvent(makeCompletionEvent(taskSets(1).tasks(0),
        FetchFailed(makeBlockManagerId(""hostA""), shuffleDep0.shuffleId, normalTask.taskId,
          fetchFailParentPartition, normalTask.index, ""ignored""),
        result = null,
        Seq.empty,
        Array.empty,
        normalTask))

      // Make the speculative task succeed after initial task has failed
      runEvent(makeCompletionEvent(taskSets(1).tasks(0), Success,
        result = MapStatus(BlockManagerId(""hostD-exec1"", ""hostD"", 34512),
          Array.fill[Long](2)(2), mapTaskId = speculativeTask.taskId),
        taskInfo = speculativeTask))

      // The second task, for partition 1 succeeds as well.
      runEvent(makeCompletionEvent(taskSets(1).tasks(1), Success,
        result = MapStatus(BlockManagerId(""hostE-exec2"", ""hostE"", 23456),
          Array.fill[Long](2)(2), mapTaskId = taskIdCount),
      ))
      taskIdCount += 1

      sc.listenerBus.waitUntilEmpty()
      assert(completedStage === List(0, 2))

      // the stages will now get resubmitted due to the failure
      Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)

      // parent map stage resubmitted
      assert(scheduler.runningStages.size === 1)
      val mapStage = scheduler.runningStages.head

      // Stage 1 is same as Stage 0 - but created for the ShuffleMapTask 2, as it is a
      // different job
      assert(mapStage.id === 1)
      assert(mapStage.latestInfo.failureReason.isEmpty)
      // only the partition reported in fetch failure is resubmitted
      assert(mapStage.latestInfo.numTasks === 1)

      val stage0Retry = taskSets.filter(_.stageId == 1)
      assert(stage0Retry.size === 1)
      // make the original task succeed
      runEvent(makeCompletionEvent(stage0Retry.head.tasks(fetchFailParentPartition), Success,
        result = MapStatus(BlockManagerId(""hostF-exec1"", ""hostF"", 12345),
          Array.fill[Long](2)(2), mapTaskId = taskIdCount)))
      Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)

      // The retries should succeed
      sc.listenerBus.waitUntilEmpty()
      assert(completedStage === List(0, 2, 1, 2))

      // Now submit the entire dag again
      // This will add 3 new stages.
      submit(reduceRdd, Array(0, 1))
      Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)

      // Only the last stage needs to execute, and those tasks - so completed stages should not
      // change.
      sc.listenerBus.waitUntilEmpty()

      assert(completedStage === List(0, 2, 1, 2))

      // All other stages should be done, and only the final stage should be waiting
      assert(scheduler.runningStages.size === 1)
      assert(scheduler.runningStages.head.id === 5)
      assert(taskSets.count(_.stageId == 5) === 1)

      complete(taskSets.filter(_.stageId == 5).head, Seq((Success, 1), (Success, 2)))

      sc.listenerBus.waitUntilEmpty()
      assert(completedStage === List(0, 2, 1, 2, 5))
    }
  }
```

Would be good to adapt/clean it up for your PR, in addition to the existing test - so that the observed bug does not recur.

(Good news is, this PR works against it :-) )

","1,-1    ",1,-1,0
1474918516,52523908,2023/3/16,v3.4.0-rc4,"> Instead of only testing specifically for the flag - which is subject to change as the implementation evolves, we should also test for behavior here.
> 
> This is the reproducible test I was using (with some changes) to test approaches for this bug - and it mimics the case I saw in our production reasonably well. (In DAGSchedulerSuite):
> 
> ```
>   for (pushBasedShuffleEnabled <- Seq(true, false)) {
>     test(""SPARK-40082: recomputation of shuffle map stage with no pending partitions should not "" +
>         s""hang. pushBasedShuffleEnabled = $pushBasedShuffleEnabled"") {
> 
>       if (pushBasedShuffleEnabled) {
>         initPushBasedShuffleConfs(conf)
>         DAGSchedulerSuite.clearMergerLocs()
>         DAGSchedulerSuite.addMergerLocs(Seq(""host1"", ""host2"", ""host3"", ""host4"", ""host5""))
>       }
> 
>       var taskIdCount = 0
> 
>       var completedStage: List[Int] = Nil
>       val listener = new SparkListener() {
>         override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {
>           completedStage = completedStage :+ event.stageInfo.stageId
>         }
>       }
>       sc.addSparkListener(listener)
> 
>       val fetchFailParentPartition = 0
> 
>       val shuffleMapRdd0 = new MyRDD(sc, 2, Nil)
>       val shuffleDep0 = new ShuffleDependency(shuffleMapRdd0, new HashPartitioner(2))
> 
>       val shuffleMapRdd1 = new MyRDD(sc, 2, List(shuffleDep0), tracker = mapOutputTracker)
>       val shuffleDep1 = new ShuffleDependency(shuffleMapRdd1, new HashPartitioner(2))
> 
>       val reduceRdd = new MyRDD(sc, 2, List(shuffleDep1), tracker = mapOutputTracker)
> 
>       // submit the initial mapper stage, generate shuffle output for first reducer stage.
>       submitMapStage(shuffleDep0)
> 
>       // Map stage completes successfully,
>       completeShuffleMapStageSuccessfully(0, 0, 3, Seq(""hostA"", ""hostB""))
>       taskIdCount += 2
>       assert(completedStage === List(0))
> 
>       // Now submit the first reducer stage
>       submitMapStage(shuffleDep1)
> 
>       def createTaskInfo(speculative: Boolean): TaskInfo = {
>         val taskInfo = new TaskInfo(
>           taskId = taskIdCount,
>           index = 0,
>           attemptNumber = 0,
>           partitionId = 0,
>           launchTime = 0L,
>           executorId = """",
>           host = ""hostC"",
>           TaskLocality.ANY,
>           speculative = speculative)
>         taskIdCount += 1
>         taskInfo
>       }
> 
>       val normalTask = createTaskInfo(speculative = false);
>       val speculativeTask = createTaskInfo(speculative = true)
> 
>       // fail task 1.0 due to FetchFailed, and make 1.1 succeed.
>       runEvent(makeCompletionEvent(taskSets(1).tasks(0),
>         FetchFailed(makeBlockManagerId(""hostA""), shuffleDep0.shuffleId, normalTask.taskId,
>           fetchFailParentPartition, normalTask.index, ""ignored""),
>         result = null,
>         Seq.empty,
>         Array.empty,
>         normalTask))
> 
>       // Make the speculative task succeed after initial task has failed
>       runEvent(makeCompletionEvent(taskSets(1).tasks(0), Success,
>         result = MapStatus(BlockManagerId(""hostD-exec1"", ""hostD"", 34512),
>           Array.fill[Long](2)(2), mapTaskId = speculativeTask.taskId),
>         taskInfo = speculativeTask))
> 
>       // The second task, for partition 1 succeeds as well.
>       runEvent(makeCompletionEvent(taskSets(1).tasks(1), Success,
>         result = MapStatus(BlockManagerId(""hostE-exec2"", ""hostE"", 23456),
>           Array.fill[Long](2)(2), mapTaskId = taskIdCount),
>       ))
>       taskIdCount += 1
> 
>       sc.listenerBus.waitUntilEmpty()
>       assert(completedStage === List(0, 2))
> 
>       // the stages will now get resubmitted due to the failure
>       Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)
> 
>       // parent map stage resubmitted
>       assert(scheduler.runningStages.size === 1)
>       val mapStage = scheduler.runningStages.head
> 
>       // Stage 1 is same as Stage 0 - but created for the ShuffleMapTask 2, as it is a
>       // different job
>       assert(mapStage.id === 1)
>       assert(mapStage.latestInfo.failureReason.isEmpty)
>       // only the partition reported in fetch failure is resubmitted
>       assert(mapStage.latestInfo.numTasks === 1)
> 
>       val stage0Retry = taskSets.filter(_.stageId == 1)
>       assert(stage0Retry.size === 1)
>       // make the original task succeed
>       runEvent(makeCompletionEvent(stage0Retry.head.tasks(fetchFailParentPartition), Success,
>         result = MapStatus(BlockManagerId(""hostF-exec1"", ""hostF"", 12345),
>           Array.fill[Long](2)(2), mapTaskId = taskIdCount)))
>       Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)
> 
>       // The retries should succeed
>       sc.listenerBus.waitUntilEmpty()
>       assert(completedStage === List(0, 2, 1, 2))
> 
>       // Now submit the entire dag again
>       // This will add 3 new stages.
>       submit(reduceRdd, Array(0, 1))
>       Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)
> 
>       // Only the last stage needs to execute, and those tasks - so completed stages should not
>       // change.
>       sc.listenerBus.waitUntilEmpty()
> 
>       assert(completedStage === List(0, 2, 1, 2))
> 
>       // All other stages should be done, and only the final stage should be waiting
>       assert(scheduler.runningStages.size === 1)
>       assert(scheduler.runningStages.head.id === 5)
>       assert(taskSets.count(_.stageId == 5) === 1)
> 
>       complete(taskSets.filter(_.stageId == 5).head, Seq((Success, 1), (Success, 2)))
> 
>       sc.listenerBus.waitUntilEmpty()
>       assert(completedStage === List(0, 2, 1, 2, 5))
>     }
>   }
> ```
> 
> Would be good to adapt/clean it up for your PR, in addition to the existing test - so that the observed bug does not recur.
> 
> (Good news is, this PR works against it :-) )

Thank you for your advice on the UT I wrote, it was very important to me. I will delete my UT. thanks again very much","3,-1    ",3,-1,2
1474986959,1591700,2023/3/16,v3.4.0-rc4,"@Stove-hust To clarify - I meant add this as well (after you had a chance to look at it and clean it up if required - this was from my test setup).
We should keep the UT you had added - and it is important to test the specific code expectation as it stands today.","1,-1    ",1,-1,0
1475528785,52523908,2023/3/16,v3.4.0-rc4,"> @Stove-hust To clarify - I meant add this as well (after you had a chance to look at it and clean it up if required - this was from my test setup). We should keep the UT you had added - and it is important to test the specific code expectation as it stands today.

Sorry, I misunderstood what you meantãï¿½?
I think the UT  written by you is great, can I write your UT in my PR, I will mark this part of UT written by youï¿½?
I have one more question, so for this PR we will have two UTï¿½?is that rightï¿½?
1475538274,1591700,2023-03-20,Technically","2,-1    ",2,-1,1
The UT that I added will generate 2 tests - one for push based shuffle and one without.,,2023/3/16,v3.4.0-rc4,,"2,-1    ",2,-1,1
And we have the initial test you added.,,2023/3/16,v3.4.0-rc4,,"1,-1    ",1,-1,0
,,2023/3/16,v3.4.0-rc4,,"1,-1    ",1,-1,0
"You dont need to mark it as written by me ! We can include it in your PR - with any changes you make as part of the adding it.""",,2023/3/16,v3.4.0-rc4,,"1,-2    ",1,-2,-1
1475695975,52523908,2023/3/16,v3.4.0-rc4,"> Technically, 3 :-) The UT that I added will generate 2 tests - one for push based shuffle and one without. And we have the initial test you added.
> 
> You dont need to mark it as written by me ! We can include it in your PR - with any changes you make as part of the adding it.

Thanks for your answer, I have added all three UTs (including you wrote)","1,-1    ",1,-1,0
1476585572,1591700,2023/3/16,v3.4.0-rc4,"The test failure is unrelated to this PR - once the changes above are made, the reexecution should pass","1,-1    ",1,-1,0
1478833621,1591700,2023/3/16,v3.4.0-rc4,"Merged to master.
Thanks for working on this @Stove-hust !
Thanks for the review @otterc :-)","1,-1    ",1,-1,0
1478833979,1591700,2023/3/16,v3.4.0-rc4,"I could not cherry pick this into 3.4 and 3.3 - we should fix for those branches as well IMO.
Can you create a PR against those two branches as well @Stove-hust ? Thanks","1,-1    ",1,-1,0
1478849048,52523908,2023/3/17,v3.4.0-rc4,"> I could not cherry pick this into 3.4 and 3.3 - we should fix for those branches as well IMO. Can you create a PR against those two branches as well @Stove-hust ? Thanks

No problem","1,-1    ",1,-1,0
1478905577,1591700,2023/3/17,v3.4.0-rc4,Is your apache jira id `StoveM` @Stove-hust  ?,"1,-1    ",1,-1,0
1478924572,52523908,2023/3/17,v3.4.0-rc4,"@mridulm 
yepï¼it`s me
Username: StoveM
Full name: Fencheng Mei","2,-1    ",2,-1,1
1465729677,15246973,2023/3/17,v3.4.0-rc4,"cc @cloud-fan, It's appreciated if it can be reviewed in your convenience, thanks!","1,-1    ",1,-1,0
1469727065,3182036,2023/3/17,v3.4.0-rc4,"thanks, merging to master!","1,-2    ",1,-2,-1
1465860908,1475305,2023/3/17,v3.4.0-rc4,"Java 17 GA failed: https://github.com/apache/spark/actions/runs/4318647315/jobs/7537203682

```
[info] - test implicit encoder resolution *** FAILED *** (1 second, 329 milliseconds)
4429[info]   2023-03-02T23:00:20.404434 did not equal 2023-03-02T23:00:20.404434875 (SQLImplicitsTestSuite.scala:63)
4430[info]   org.scalatest.exceptions.TestFailedException:
4431[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
4432[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
4433[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
4434[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
4435[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.testImplicit$1(SQLImplicitsTestSuite.scala:63)
4436[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.$anonfun$new$2(SQLImplicitsTestSuite.scala:133)
4437[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
4438[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
4439[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
4440[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
4441[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
4442[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
4443[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
4444[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
4445[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
4446[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
4447[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
4448[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
4449[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
4450[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
4451[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
4452[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
4453[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
4454[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
4455[info]   at scala.collection.immutable.List.foreach(List.scala:431)
4456[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
4457[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
4458[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
4459[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
4460[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
4461[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
4462[info]   at org.scalatest.Suite.run(Suite.scala:1114)
4463[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
4464[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
4465[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
4466[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
4467[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
4468[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
4469[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.org$scalatest$BeforeAndAfterAll$$super$run(SQLImplicitsTestSuite.scala:34)
4470[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
4471[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
4472[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
4473[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.run(SQLImplicitsTestSuite.scala:34)
4474[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
4475[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
4476[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
4477[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
4478[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
4479[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
4480[info]   at java.base/java.lang.Thread.run(Thread.java:833) 
```","1,-1    ",1,-1,0
1465912943,1475305,2023/3/17,v3.4.0-rc4,"cc @HyukjinKwon 
also cc @bjornjorgensen who reported this issue in dev mail list

","1,-1    ",1,-1,0
1466189372,47577197,2023/3/17,v3.4.0-rc4,"@LuciferYang Thank you, sir. 
LGTM ","1,-1    ",1,-1,0
1467321228,1475305,2023/3/17,v3.4.0-rc4,friendly ping @dongjoon-hyun ,"1,-1    ",1,-1,0
1467613124,1475305,2023/3/17,v3.4.0-rc4,"https://github.com/LuciferYang/spark/actions/runs/4412451542/jobs/7732979313

<img width=""1148"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/224938008-bab8acde-7a62-46bb-b3a4-57dfb83bb12a.png"">

GA passed, status not update","1,-1    ",1,-1,0
1468368686,1475305,2023/3/17,v3.4.0-rc4,Thanks @dongjoon-hyun @HyukjinKwon @bjornjorgensen ,"1,-1    ",1,-1,0
1470433831,1475305,2023/3/17,v3.4.0-rc4,"https://github.com/apache/spark/actions/runs/4420600519

<img width=""1191"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/225388240-9f85593f-f6d6-47dd-be07-9ab906bf53a8.png"">

The latest Java 17 daily test passed. Thanks all ~

","1,-1    ",1,-1,0
1465922700,8486025,2023/3/17,v3.4.0-rc4,ping @huaxingao cc @cloud-fan @sadikovi,"1,-1    ",1,-1,0
1469076028,3182036,2023/3/17,v3.4.0-rc4,"Yea, we should mention that they can set these options to false if query fails with JDBC errors.","2,-1    ",2,-1,1
1469534683,8486025,2023/3/17,v3.4.0-rc4,"> Thank you for update. BTW, `JDBCDialect` is a documented developer API. We should not change the default value. We should keep the original default value to avoid a breaking change to 3rd party Dialect.
> 

Thank you for the reminder. But these new API only exists in master branch.

","1,-1    ",1,-1,0
1471116456,3182036,2023/3/17,v3.4.0-rc4,"thanks, merging to master!","1,-2    ",1,-2,-1
1471226525,8486025,2023/3/17,v3.4.0-rc4,@dongjoon-hyun @cloud-fan @huaxingao @sadikovi Thank you.,"1,-1    ",1,-1,0
1469911638,15246973,2023/3/17,v3.4.0-rc4,cc @cloud-fan ,"1,-1    ",1,-1,0
1478870787,15246973,2023/3/17,v3.4.0-rc4,@cloud-fan Can we merge it to master? After it I will try to refactor HiveGenericUDTF & HiveUDAFFunction. Thanks!,"1,-2    ",1,-2,-1
1478896479,3182036,2023/3/17,v3.4.0-rc4,"thanks, merging to master!","2,-1    ",2,-1,1
1468116741,822522,2023/3/17,v3.4.0-rc4,It's fine. Can you enable the tests to run?,"1,-2    ",1,-2,-1
1470420003,2623333,2023/3/17,v3.4.0-rc4,"@srowen I have enabled it, but now I don't know how to progress. Is there a ""re-run"" button to re-trigger the build? Or do I push an empty commit into this branch?","1,-1    ",1,-1,0
1474601033,822522,2023/3/17,v3.4.0-rc4,Can you push an empty commit?,"1,-1    ",1,-1,0
1466065337,3182036,2023/3/17,v3.4.0-rc4,@ulysses-you ,"1,-1    ",1,-1,0
1466080248,12025282,2023/3/17,v3.4.0-rc4,lgtm,"2,-1    ",2,-1,1
1466424185,3182036,2023/3/17,v3.4.0-rc4,"thanks for review, merging to master!","1,-1    ",1,-1,0
1479553553,27844885,2023/3/17,v3.4.0-rc4,can you spare some precious time to review? Thanks very much @cloud-fan ,"1,-1    ",1,-1,0
1488035365,3182036,2023/3/17,v3.4.0-rc4,"thanks, merging to master!","2,-1    ",2,-1,1
1467125777,44108233,2023/3/17,v3.4.0-rc4,"Seems like there are some more unchanged docstrings in several files as below:
```
spark % git grep ""Support Spark Connect""
python/pyspark/sql/column.py:            Support Spark Connect.
python/pyspark/sql/column.py:            Support Spark Connect.
python/pyspark/sql/dataframe.py:            Support Spark Connect.
python/pyspark/sql/dataframe.py:            Support Spark Connect.
python/pyspark/sql/udf.py:            Support Spark Connect.
python/pyspark/sql/udf.py:            Support Spark Connect.
```
Btw, you can change the all strings at once by running the shell command in terminal such as:
```shell
find . -name ""*.py"" -exec perl -pi -e 's/Support Spark Connect/Supports Spark Connect/g' {} \;
```","2,-1    ",2,-1,1
1467125916,44108233,2023/3/17,v3.4.0-rc4,Looks good except https://github.com/apache/spark/pull/40401#issuecomment-1467125777,"2,-1    ",2,-1,1
1467188490,7322292,2023/3/17,v3.4.0-rc4,LGTM pending CI,"2,-1    ",2,-1,1
1467252638,7322292,2023/3/17,v3.4.0-rc4,"thank you @allanf-db , merged into master/branch-3.4","2,-1    ",2,-1,1
1467212310,7322292,2023/3/17,v3.4.0-rc4,also cc @WeichenXu123 since this PR supports `df.collect` with UDT,"2,-1    ",2,-1,1
1474479783,506656,2023/3/17,v3.4.0-rc4,"@zhengruifeng Sorry, I missed your comment:

> will there be another PR for the support of UDT in `createDataFrame`?

No, this also enables UDT in `createDataFrame`.","1,-1    ",1,-1,0
1475453518,6477701,2023/3/17,v3.4.0-rc4,Merged to master and branch-3.4.,"2,-2    ",2,-2,0
1477202593,7322292,2023/3/17,v3.4.0-rc4,"@ueshin it seem that `createDataFrame` always use the underlying `sqlType` other than the UDT itself:

```
In [1]: from pyspark.ml.linalg import Vectors

In [2]: df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0,
   ...: 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0)),], [""label"", ""weight"", ""features""],)

In [3]: df.schema
Out[3]: StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('features', StructType([StructField('type', ByteType(), False), StructField('size', IntegerType(), True), StructField('indices', ArrayType(IntegerType(), False), True), StructField('values', ArrayType(DoubleType(), False), True)]), True)])

In [4]: df.collect()
Out[4]: :>                                                          (0 + 4) / 4]
[Row(label=1.0, weight=1.0, features=Row(type=1, size=None, indices=None, values=[0.0, 5.0])),
 Row(label=0.0, weight=2.0, features=Row(type=1, size=None, indices=None, values=[1.0, 2.0])),
 Row(label=1.0, weight=3.0, features=Row(type=1, size=None, indices=None, values=[2.0, 1.0])),
 Row(label=0.0, weight=4.0, features=Row(type=1, size=None, indices=None, values=[3.0, 3.0]))]
```

while in vanilla PySpark:
```
In [1]: from pyspark.ml.linalg import Vectors

In [2]: df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0,
   ...:    ...: 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0)),], [""label"", ""weight"", ""features""],)

In [3]: df.schema
Out[3]: StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('features', VectorUDT(), True)])

In [4]: df.collect()
Out[4]:                                                                         
[Row(label=1.0, weight=1.0, features=DenseVector([0.0, 5.0])),
 Row(label=0.0, weight=2.0, features=DenseVector([1.0, 2.0])),
 Row(label=1.0, weight=3.0, features=DenseVector([2.0, 1.0])),
 Row(label=0.0, weight=4.0, features=DenseVector([3.0, 3.0]))]
```


also cc @WeichenXu123 ","2,-1    ",2,-1,1
1477209035,7322292,2023/3/17,v3.4.0-rc4,"I save a df with UDT in pyspark, and then read it in python client, and it works fine. So I guess something is wrong in 
`createDataFrame`

vanilla PySpark:
```
In [1]: from pyspark.ml.linalg import Vectors

In [2]: df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0,
   ...:    ...: 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0)),], [""label"", ""weight"", ""features""],)

In [3]: df.write.parquet(""/tmp/tmp.pq"")
```

Python Client:
```
In [6]: df = spark.read.parquet(""/tmp/tmp.pq"")

In [7]: df.schema
Out[7]: StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('features', VectorUDT(), True)])

In [8]: df.collect()
Out[8]: 
[Row(label=0.0, weight=4.0, features=DenseVector([3.0, 3.0])),
 Row(label=0.0, weight=2.0, features=DenseVector([1.0, 2.0])),
 Row(label=1.0, weight=3.0, features=DenseVector([2.0, 1.0])),
 Row(label=1.0, weight=1.0, features=DenseVector([0.0, 5.0]))]
```","1,-1    ",1,-1,0
1477260665,506656,2023/3/17,v3.4.0-rc4,"@zhengruifeng ah, seems like something is wrong when the schema is a column name list.
Could you use `StructType` to specify the schema as a workaround?
I'll take a look later.","1,-1    ",1,-1,0
1477518340,7322292,2023/3/17,v3.4.0-rc4,"@ueshin Sure, thanks!

`StructType().add(""label"", DoubleType()).add(""weight"", DoubleType()).add(""features"", VectorUDT(), False)` works, but the `nullable` in column `features` must be `False`, otherwise:
```
AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `features`.`type` is nullable while it's required to be non-nullable.
```","1,-3    ",1,-3,-2
1480349942,506656,2023/3/17,v3.4.0-rc4,@zhengruifeng I submitted two PRs: #40526 and #40527.,"1,-1    ",1,-1,0
1467162461,67896261,2023/3/17,v3.4.0-rc4,cc @JoshRosen @rednaxelafx @xinrong-meng ,"1,-1    ",1,-1,0
1470085933,1475305,2023/3/17,v3.4.0-rc4,late LGTM,"1,-1    ",1,-1,0
1467317784,1097932,2023/3/17,v3.4.0-rc4,merging to master/3.4,"2,-2    ",2,-2,0
1475524024,47337188,2023/3/17,v3.4.0-rc4,"@zhengruifeng @HyukjinKwon May I get a review, please?","2,-1    ",2,-1,1
1475525966,7322292,2023/3/17,v3.4.0-rc4,"A grouped data can also be created via `df.roll/cube/pivot`, do they works with `applyInPandas` ?","1,-2    ",1,-2,-1
1475669732,47337188,2023/3/17,v3.4.0-rc4,"As long as the input relation is GroupedData, `applyInPandas` works.

An example is as shown below:
```sh
>>> df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],(""id"", ""v""))
>>> 
>>> def normalize(pdf):
...     v = pdf.v
...     return pdf.assign(v=(v - v.mean()) / v.std())
... 

>>> df.cube('id').applyInPandas(normalize, schema=""id long, v double"").show()
+---+-------------------+
| id|                  v|
+---+-------------------+
|  1|-0.7071067811865475|
|  1| 0.7071067811865475|
|  2|-0.8320502943378437|
|  2|-0.2773500981126146|
|  2| 1.1094003924504583|
+---+-------------------+

>>> df.rollup('id', df.v).applyInPandas(normalize, schema=""id long, v double"").show()
+---+----+                                                                      
| id|   v|
+---+----+
|  1|null|
|  1|null|
|  2|null|
|  2|null|
|  2|null|
+---+----+

# I didn't find `df.pivot` in vanilla PySpark.
```
The above example is verified to have the same result as vanilla PySpark.

CC @zhengruifeng ","2,-1    ",2,-1,1
1475716275,47337188,2023/3/17,v3.4.0-rc4,Rebased to the latest master; no new changes after reviews. ,"2,-1    ",2,-1,1
1476024481,6477701,2023/3/17,v3.4.0-rc4,Merged to master.,"2,-1    ",2,-1,1
1476027639,6477701,2023/3/17,v3.4.0-rc4,It has a conflict with 3.4. mind creating a PR to backport this?,"1,-2    ",1,-2,-1
1476032014,47337188,2023/3/17,v3.4.0-rc4,"Sure, I'll do. Thanks @HyukjinKwon!","1,-1    ",1,-1,0
1467431632,12025282,2023/3/17,v3.4.0-rc4,cc @cloud-fan @dongjoon-hyun ,"2,-1    ",2,-1,1
1468060649,3182036,2023/3/18,v3.4.0-rc4,"thanks, merging to master!","1,-1    ",1,-1,0
1467433147,12025282,2023/3/18,v3.4.0-rc4,"I seperate this pr because it may affect other things, e.g., a custom shuffle exchange which support both columnar and row.
cc @cloud-fan @dongjoon-hyun ","1,-1    ",1,-1,0
1468067373,3182036,2023/3/18,v3.4.0-rc4,"thanks, merging to master! @ulysses-you can we create a new PR for 3.4 to make sure all tests pass?","1,-1    ",1,-1,0
1468161014,12025282,2023/3/18,v3.4.0-rc4,@cloud-fan  created https://github.com/apache/spark/pull/40417,"3,-2    ",3,-2,1
1516268305,47577197,2023/3/18,v3.4.0-rc4,"1.9.0 is out now 
I have made a new PR https://github.com/apache/spark/pull/40878
I close this now. ","2,-1    ",2,-1,1
1469541110,8486025,2023/3/18,v3.4.0-rc4,ping @cloud-fan cc @zhengruifeng ,"1,-1    ",1,-1,0
1469878236,3182036,2023/3/18,v3.4.0-rc4,"thanks, merging to master!","1,-1    ",1,-1,0
1469932642,8486025,2023/3/18,v3.4.0-rc4,@cloud-fan @dongjoon-hyun Thank you !,"1,-1    ",1,-1,0
1470311818,9700541,2023/3/18,v3.4.0-rc4,"Thank you, @beliefer and @cloud-fan .","1,-1    ",1,-1,0
1467724593,52523908,2023/3/18,v3.4.0-rc4,@zhouyejoe can you review this PR? i would be very grateful!,"1,-1    ",1,-1,0
1469223693,52523908,2023/3/18,v3.4.0-rc4,"@mridulm Hello, can you recruit someone to help review this prãI would appreciate for your help","1,-1    ",1,-1,0
1471342162,8699921,2023/3/18,v3.4.0-rc4,Thanks for creating the PR. Will review ASAP @Stove-hust ,"3,-1    ",3,-1,2
1478751685,8699921,2023/3/18,v3.4.0-rc4,"@Stove-hust I think the changes will help resolve the issue described in the ticket. I am checking more about what could be causing to the race conditions where there are two Executor containers starting at almost the same time on the same node, and they will all enter this code section and try creating subdirs at the same time.","1,-1    ",1,-1,0
1478854808,52523908,2023/3/19,v3.4.0-rc4,"@zhouyejoe I I kept the accident scene, should be able to help youãï¼In our clustered machine environmentï¿½?1 HDDï¼creating 11 * 64 subdirectories would take longer to createï¿½?
23/02/21 10:44:09 YarnAllocator: Launching container container_184 on host hostA
23/02/21 10:44:21  YarnAllocator: Launching container container_185 on host hostA
23/02/21 10:44:21  YarnAllocator: Container container_184 on host: hostA was preempted.
","2,-1    ",2,-1,1
1534022451,52523908,2023/3/19,v3.4.0-rc4,@mridulm ,"1,-1    ",1,-1,0
1537894454,52523908,2023/3/19,v3.4.0-rc4,@cloud-fan ,"1,-1    ",1,-1,0
1547331364,52523908,2023/3/19,v3.4.0-rc4,@mridulm ,"1,-1    ",1,-1,0
1549125020,1591700,2023/3/19,v3.4.0-rc4,"So to make sure I understand the issue here @Stove-hust  - we have a container started on a node, and got immediately killed (pre-empted/etc) while it was in the process of creating the subdirs (started, but did not complete) ?

If yes, very interesting corner case - will take a pass through the PR later this week, thanks for the contribution !","1,-1    ",1,-1,0
1550984254,52523908,2023/3/20,v3.4.0-rc4,"> So to make sure I understand the issue here @Stove-hust - we have a container started on a node, and got immediately killed (pre-empted/etc) while it was in the process of creating the subdirs (started, but did not complete) ?
> 
> If yes, very interesting corner case - will take a pass through the PR later this week, thanks for the contribution !

yepï¼that's the scenario you described","2,-1    ",2,-1,1
1478874829,9616802,2023/3/20,v3.4.0-rc4,Merging to master!,"1,-1    ",1,-1,0
1467779591,52876270,2023/3/20,v3.4.0-rc4,"As last PR mentioned, @yaooqinn @shrprasa","1,-1    ",1,-1,0
1467812422,8326978,2023/3/20,v3.4.0-rc4,"Please use the jira-style prefix [SPARK-42785], not a GitHub issue-like one [SPARK #42785] like apache/kyuubi. Please also answer the rest of the questions in the PR description.","1,-1    ",1,-1,0
1467869412,109815907,2023/3/20,v3.4.0-rc4,"@zwangsheng @yaooqinn There won't be any NPE if you try to actually submit a job without the deploy-mode because if no value is provided, it defaults to ""client"".  Please refer to org.apache.spark.internal.config/package.scala.

```
  private[spark] val SUBMIT_DEPLOY_MODE = ConfigBuilder(""spark.submit.deployMode"")
    .version(""1.5.0"")
    .stringConf
    .createWithDefault(""client"")
```","2,-3    ",2,-3,-1
1467888540,52876270,2023/3/20,v3.4.0-rc4,"> @zwangsheng @yaooqinn There won't be any NPE if you try to actually submit a job without the deploy-mode because if no value is provided, it defaults to ""client"". Please refer to org.apache.spark.internal.config/package.scala.
> 
> ```
>   private[spark] val SUBMIT_DEPLOY_MODE = ConfigBuilder(""spark.submit.deployMode"")
>     .version(""1.5.0"")
>     .stringConf
>     .createWithDefault(""client"")
> ```

@shrprasa Thanks for reminder

I saw this default value, you can do local test with writing following code
![popo_2023-03-14  17-50-46](https://user-images.githubusercontent.com/52876270/224981649-5e46838a-644e-4f2e-9566-e693f82473e5.jpg)

The default value `client` you mean, will be set in `prepareSubmitEnvironment` function, which be called in `runMain()` after check 
```
args.deployMode.equals(""client"") &&
```
","3,-2    ",3,-2,1
1467890842,52876270,2023/3/20,v3.4.0-rc4,"> Please use the jira-style prefix [[SPARK-42785](https://issues.apache.org/jira/browse/SPARK-42785)], not a GitHub issue-like one [SPARK #42785] like apache/kyuubi

Sure. XD

","1,-1    ",1,-1,0
1468117200,109815907,2023/3/20,v3.4.0-rc4,I was able to reproduce the issue. Thanks @zwangsheng for fixing it.,"1,-1    ",1,-1,0
1499858366,9616802,2023/3/20,v3.4.0-rc4,@beliefer is there a chance you can this one moving again?,"1,-1    ",1,-1,0
1499887231,8486025,2023/3/20,v3.4.0-rc4,"> @beliefer is there a chance you can this one moving again?

But https://github.com/apache/spark/pull/40358 already do it.","2,-1    ",2,-1,1
1469009905,6477701,2023/3/20,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1469117434,12025282,2023/3/20,v3.4.0-rc4,"@dongjoon-hyun The reason is that we only support table cache query stage at master branch, so that test case is not valid. For branch-3.4, actually, this change only affects developers.","1,-1    ",1,-1,0
1469155631,3182036,2023/3/20,v3.4.0-rc4,"> actually, this change only affects developers.

Can we be a bit more specific? I think it's a problem for table cache because it supports both row and columnar output. But shuffle/broadcast won't have the same issue.","1,-1    ",1,-1,0
1469168690,12025282,2023/3/20,v3.4.0-rc4,"If people build a custom shuffle exchange which support both row and columnar output. Logically, we have `ShuffleExchangeLike` for developers.","1,-1    ",1,-1,0
1473029216,8486025,2023/3/20,v3.4.0-rc4,ping @huaxingao cc @cloud-fan ,"1,-1    ",1,-1,0
1476265815,822522,2023/3/20,v3.4.0-rc4,Merged to master,"1,-1    ",1,-1,0
1477169805,8486025,2023/3/20,v3.4.0-rc4,@srowen Thank you! @cloud-fan @huaxingao Thank you too!,"1,-1    ",1,-1,0
1469019263,5399861,2023/3/20,v3.4.0-rc4,cc @cloud-fan ,"1,-1    ",1,-1,0
1469020494,6477701,2023/3/20,v3.4.0-rc4,cc @itholic ,"1,-1    ",1,-1,0
1469854808,6477701,2023/3/20,v3.4.0-rc4,Cc @Yikun too if you find some time to review.,"1,-1    ",1,-1,0
1523814214,6477701,2023/3/20,v3.4.0-rc4,cc @itholic @zhengruifeng @xinrong-meng if you find some time to review.,"1,-1    ",1,-1,0
1524465392,44108233,2023/3/20,v3.4.0-rc4,Could you resolve mypy check? You can run the static analysis by running `dev/lint-python` locally.,"1,-2    ",1,-2,-1
1527398713,20475650,2023/3/20,v3.4.0-rc4,"> Could you resolve mypy check? You can run the static analysis by running `dev/lint-python` locally.

@itholic  I've fixed the issue with mypy in my changes, but I still see three problems that don't relate to changes. Should I fix them?

```
annotations failed mypy checks:
python/pyspark/pandas/namespace.py:162: error: Cannot assign multiple types to name ""_range"" without an explicit ""Type[...]"" annotation  [misc]
python/pyspark/pandas/indexes/base.py:2093: error: unused ""type: ignore"" comment
python/pyspark/pandas/indexes/base.py:2163: error: unused ""type: ignore"" comment
Found 3 errors in 2 files (checked 507 source files)
```
","1,-2    ",1,-2,-1
1528926660,44108233,2023/3/20,v3.4.0-rc4,"Can you rebase to master and try running linter again??
If the problem still exists, yes, let's fix the it. We should make the PR pass the CI anyway.","1,-1    ",1,-1,0
1468467640,6235869,2023/3/20,v3.4.0-rc4,cc @huaxingao @HeartSaVioR @cloud-fan @dongjoon-hyun @sunchao @viirya @gengliangwang,"1,-2    ",1,-2,-1
1468801728,6235869,2023/3/20,v3.4.0-rc4,Let me fix failures.,"1,-1    ",1,-1,0
1468858285,6235869,2023/3/20,v3.4.0-rc4,The error classes were not ordered. Fixed that. The other Python failure does not seem related.,"1,-1    ",1,-1,0
1472903537,13592258,2023/3/20,v3.4.0-rc4,"+1, LGTM Thanks for the PR @aokolnychyi ","1,-2    ",1,-2,-1
1473078509,9700541,2023/3/20,v3.4.0-rc4,"+1, LGTM for Apache Spark 3.5.","1,-1    ",1,-1,0
1474357322,9700541,2023/3/20,v3.4.0-rc4,"Thank you, @aokolnychyi and all. Merged to master for Apache Spark 3.5","1,-1    ",1,-1,0
1474491549,6235869,2023/3/20,v3.4.0-rc4,"Thanks for reviewing, @dongjoon-hyun @huaxingao @viirya @cloud-fan @sunchao!

I will follow up to address the comments (most likely on Monday).","1,-2    ",1,-2,-1
1469206611,1475305,2023/3/20,v3.4.0-rc4,I think we should file a jira to tracking this ,"1,-1    ",1,-1,0
1469468855,24260474,2023/3/20,v3.4.0-rc4,"> I think we should file a jira to tracking this

Done, pls check - https://issues.apache.org/jira/browse/SPARK-42803","1,-2    ",1,-2,-1
1469893777,1475305,2023/3/20,v3.4.0-rc4,The pr title should be `[SPARK-42803][CORE][SQL][ML] Use ... `,"1,-1    ",1,-1,0
1469894625,1475305,2023/3/20,v3.4.0-rc4,"@NarekDW Are there any more similar cases?

cc @srowen FYI
","1,-3    ",1,-3,-2
1470557942,24260474,2023/3/20,v3.4.0-rc4,"> @NarekDW Are there any more similar cases?
> 
> cc @srowen FYI

@LuciferYang  no, these are all","1,-1    ",1,-1,0
1474628142,822522,2023/3/20,v3.4.0-rc4,Merged to master,"2,-1    ",2,-1,1
1469047887,6477701,2023/3/20,v3.4.0-rc4,FYI: https://github.com/rithwik-db/spark/runs/12002967878,"3,-1    ",3,-1,2
1470815851,81988348,2023/3/20,v3.4.0-rc4,This ticket will be closed for now; related changes may be in a V2 of the TorchDistributor,"1,-1    ",1,-1,0
1469018169,9700541,2023/3/20,v3.4.0-rc4,"cc @hvanhovell, @HyukjinKwon ","1,-1    ",1,-1,0
1469019282,6477701,2023/3/20,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1469019919,9700541,2023/3/20,v3.4.0-rc4,"Thank you, @HyukjinKwon !","1,-1    ",1,-1,0
1469023182,9700541,2023/3/20,v3.4.0-rc4,"BTW, `branch-3.4` GitHub CI failure is still under investigation independently.","2,-1    ",2,-1,1
1469024165,6477701,2023/3/20,v3.4.0-rc4,ð ,"1,-3    ",1,-3,-2
1469048078,6477701,2023/3/20,v3.4.0-rc4,cc @HeartSaVioR ,"1,-1    ",1,-1,0
1469209415,1317309,2023/3/20,v3.4.0-rc4,Thanks! Merging to master.,"2,-1    ",2,-1,1
1469160585,6477701,2023/3/20,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1468968921,100322362,2023/3/20,v3.4.0-rc4,@HeartSaVioR - please take a look. Thx,"1,-3    ",1,-3,-2
1469335530,100322362,2023/3/20,v3.4.0-rc4,@HeartSaVioR - tests look good. Pls merge when you get a chance. Thx,"1,-3    ",1,-3,-2
1469443629,1317309,2023/3/20,v3.4.0-rc4,Thanks! Merging to master.,"1,-3    ",1,-3,-2
1469263601,6477701,2023/3/20,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-3    ",1,-3,-2
1483544934,122326661,2023/3/20,v3.4.0-rc4,"Hi @LuciferYang, @MaxGekk, could you help me review this PR? Or do you know who would be more suitable to review it? Thanks a lot!","1,-1    ",1,-1,0
1519081285,822522,2023/3/20,v3.4.0-rc4,"Looks OK pending tests, but re-run tests","2,-1    ",2,-1,1
1519097677,1475305,2023/3/20,v3.4.0-rc4,"rebased and re-run tests, let's wait ci","1,-1    ",1,-1,0
1521078012,1475305,2023/3/20,v3.4.0-rc4,All test passed,"2,-2    ",2,-2,0
1521829801,822522,2023/3/20,v3.4.0-rc4,Merged to master,"3,-1    ",3,-1,2
1521831266,1475305,2023/3/20,v3.4.0-rc4,Thanks @srowen ,"2,-1    ",2,-1,1
1469265999,9700541,2023/3/20,v3.4.0-rc4,"cc @bjornjorgensen, @yaooqinn , @srowen ","3,-1    ",3,-1,2
1469270112,9700541,2023/3/20,v3.4.0-rc4,"Thank you, @yaooqinn !","2,-1    ",2,-1,1
1469279124,9700541,2023/3/20,v3.4.0-rc4,"Thank you, @srowen .","1,-1    ",1,-1,0
1469380025,9700541,2023/3/20,v3.4.0-rc4,"Thank you, @LuciferYang .","2,-1    ",2,-1,1
1469497908,9700541,2023/3/20,v3.4.0-rc4,Thank you all! All tests passed. Merged to master/3.4/3.3/3.2.,"1,-1    ",1,-1,0
1469541755,47577197,2023/3/20,v3.4.0-rc4,"@dongjoon-hyun Thank you, sir. ð ","2,-2    ",2,-2,0
1470803805,47577197,2023/3/20,v3.4.0-rc4,"CC @panbingkun 
so you too are aware of this and hopefully don't make the same mistake I did.","1,-1    ",1,-1,0
1471187025,1475305,2023/3/20,v3.4.0-rc4,https://github.com/apache/spark/pull/40452 : I added a comment in `pom.xml` to prevent us from forgetting this,"1,-1    ",1,-1,0
1469295848,7322292,2023/3/20,v3.4.0-rc4,cc @WeichenXu123 @HyukjinKwon ,"1,-1    ",1,-1,0
1471248695,7322292,2023/3/20,v3.4.0-rc4,"`sql - slow` failed, not sure whether it is related to changes in `modules.py`, let me investigate it first","1,-1    ",1,-1,0
1471290106,19235986,2023/3/20,v3.4.0-rc4,Is it ready to merge ?,"1,-1    ",1,-1,0
1471307277,7322292,2023/3/20,v3.4.0-rc4,"@WeichenXu123 not ready.

`sql slow` failed with message related to `mllib-common`:
```
[error] /home/runner/work/spark/spark/mllib/common/src/test/scala/org/apache/spark/ml/attribute/AttributeGroupSuite.scala:35:11: exception during macro expansion: 
[error] java.util.MissingResourceException: Can't find bundle for base name org.scalactic.ScalacticBundle, locale en
[error] 	at java.util.ResourceBundle.throwMissingResourceException(ResourceBundle.java:1581)
[error] 	at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1396)
[error] 	at java.util.ResourceBundle.getBundle(ResourceBundle.java:782)
[error] 	at org.scalactic.Resources$.resourceBundle$lzycompute(Resources.scala:8)
[error] 	at org.scalactic.Resources$.resourceBundle(Resources.scala:8)
[error] 	at org.scalactic.Resources$.pleaseDefineScalacticFillFilePathnameEnvVar(Resources.scala:256)
[error] 	at org.scalactic.source.PositionMacro$PositionMacroImpl.apply(PositionMacro.scala:65)
[error] 	at org.scalactic.source.PositionMacro$.genPosition(PositionMacro.scala:85)
[error] Caused by: java.io.IOException: Stream closed
[error] 	at java.util.zip.InflaterInputStream.ensureOpen(InflaterInputStream.java:67)
```


the current test seems fine, but let's wait for the CI","1,-1    ",1,-1,0
1471423001,7322292,2023/3/20,v3.4.0-rc4,"all tests passed, merged into master","2,-1    ",2,-1,1
1469347974,44108233,2023/3/20,v3.4.0-rc4,cc @MaxGekk @cloud-fan ,"1,-1    ",1,-1,0
1469439953,1580697,2023/3/21,v3.4.0-rc4,"+1, LGTM. Merging to 3.4.
Thank you, @itholic and @cloud-fan @dongjoon-hyun @HyukjinKwon for review.","1,-1    ",1,-1,0
1469394962,9700541,2023/3/21,v3.4.0-rc4,"cc @HyukjinKwon , @hvanhovell ","1,-1    ",1,-1,0
1469417616,9700541,2023/3/21,v3.4.0-rc4,"Thank you, @HyukjinKwon and @LuciferYang . The `connect` module UT passed. Let me merge this because this is a removal of test coverage.","3,-1    ",3,-1,2
1469882767,6477701,2023/3/21,v3.4.0-rc4,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1469882286,6477701,2023/3/21,v3.4.0-rc4,cc @itholic and @Yikun if you find some time to review.,"1,-1    ",1,-1,0
1523814336,6477701,2023/3/21,v3.4.0-rc4,cc @itholic @zhengruifeng @xinrong-meng if you find some time to review.,"3,-1    ",3,-1,2
1524459310,44108233,2023/3/21,v3.4.0-rc4,"Looks pretty good., but let's wait until the [initial pandas 2.0 support](https://github.com/apache/spark/pull/40658) is done.","1,-1    ",1,-1,0
1469756155,51110188,2023/3/21,v3.4.0-rc4,"@cloud-fan Please take a look if you have time, thanks.","1,-1    ",1,-1,0
1484545471,51110188,2023/3/21,v3.4.0-rc4,"Friendly ping @cloud-fan , Please take a look if you find a time, thanks ","1,-1    ",1,-1,0
1489781086,8326978,2023/3/21,v3.4.0-rc4,"I am not sure why we must keep consistent with Hive for such a case,

1. this is just output from the command line interface, not a programming API.
2. the `hive` CLI itself is already deprecated.

Why do we not always use schemas of spark's commands as the CLI header?
","1,-1    ",1,-1,0
1489876875,3182036,2023/3/21,v3.4.0-rc4,"@yaooqinn this is a good point. If we are sure this is only for CLI display, not thriftserver protocol, I agree we don't need to follow Hive.","1,-1    ",1,-1,0
1489909241,8326978,2023/3/21,v3.4.0-rc4,"> If we are sure this is only for CLI display,

Yes. hiveResultString is only used in spark-sql CLI. The thrift server-side always uses command output schema. Maybe this is the inconsistent issue we face internally","1,-1    ",1,-1,0
1489933393,51110188,2023/3/21,v3.4.0-rc4,"Yes. `hiveResultString` is added to ensure compatibility with hive output.

`hiveResultString` is only used by the spark-sql CLI. It is used only as the CLI display.

`thriftServer` always outputs as spark's schema,
1. `hiveResultString` is not used for the thrift protocol.
2. spark-sql output from the CLI and `thriftServer` is inconsistent.

I'm not sure why spark-sql CLI has to be compatible with hive output, personally, I don't think it's necessary. Maybe we should display spark's schema as is, just like thriftSever?","1,-1    ",1,-1,0
1489962469,3182036,2023/3/21,v3.4.0-rc4,"> I'm not sure why spark-sql CLI has to be compatible with hive output, personally, I don't think it's necessary. Maybe we should display spark's schema as is, just like thriftSever?

+1","1,-1    ",1,-1,0
1493720082,51110188,2023/3/21,v3.4.0-rc4,"Please take a look, thanks @cloud-fan @yaooqinn ","2,-1    ",2,-1,1
1499051074,3182036,2023/3/21,v3.4.0-rc4,"Seems like we just need a small change: only invoke `hiveResultString` if a testing config is true, and we only set that testing config in `HiveComparisonTest`.","1,-1    ",1,-1,0
1499861262,51110188,2023/3/21,v3.4.0-rc4,"> Seems like we just need a small change: only invoke `hiveResultString` if a testing config is true, and we only set that testing config in `HiveComparisonTest`.

Return the result as a hive compatible sequence of strings of `hiveResultString`  currently does two things:
1. `HiveComparisonTest` used to compare compatibility with hive output;
2. Spark generates golden files for `SQLQueryTestSuite`;

2 is a friendly form of output, and I think we should keep it.

Edit:
Like `Array` datatype would output WrappedArray(col1, col2) instead of [col1, col2]","3,-1    ",3,-1,2
1499967555,3182036,2023/3/21,v3.4.0-rc4,"> Spark generates golden files for SQLQueryTestSuite;

I think golden files there should match `df.show` instead of hive result.","1,-1    ",1,-1,0
1500012982,51110188,2023/3/21,v3.4.0-rc4,"> > Spark generates golden files for SQLQueryTestSuite;
> 
> I think golden files there should match `df.show` instead of hive result.

This may be a bit of a regression, maybe...  first, golden generation and validation are done with `hiveResultString `, it won't have what problem, then, the output is more readable, finally, `ThriftServerQueryTestSuite` test case generation also uses the independent `hiveString`, this also needs to reconstruct. So what's the benefit of removal?","2,-1    ",2,-1,1
1500018627,51110188,2023/3/21,v3.4.0-rc4,"Like following:
<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/51110188/230561881-177790a2-3b0b-4081-ba95-7e902af6b05f.png"">

And expression describes `ExpressionInfo`, it uses NULL instead of null (It caused `ExpressionInfoSuite` to fail); 

I checked the output of `hiveResultString` and `df.show`, and the current display is just `null`
","1,-1    ",1,-1,0
1500054156,3182036,2023/3/21,v3.4.0-rc4,"I'm looking for consistency. `df.show` is what users see, and `hiveResultString` is for golden files. Shouldn't the golden file match what users really see? Why do we test something that is almost invisible to users?","1,-1    ",1,-1,0
1500073818,51110188,2023/3/21,v3.4.0-rc4,"> I'm looking for consistency. `df.show` is what users see, and `hiveResultString` is for golden files. Shouldn't the golden file match what users really see? Why do we test something that is almost invisible to users?

In fact, `SQLQueryTestSuite` does not test `df.show`, it is actually something under the `DataSet`, and `hiveResultString` only prints out the data of the `DataSet`, but I agree with you. It is better to print with spark's resultString","2,-1    ",2,-1,1
1500082745,3182036,2023/3/21,v3.4.0-rc4,also cc @AngersZhuuuu ,"1,-1    ",1,-1,0
1500113298,8326978,2023/3/21,v3.4.0-rc4,Adjusting `df.show` may need to change the output of `show` first. Some data values do not have a nice string representation yet,"3,-1    ",3,-1,2
1501408587,3182036,2023/3/21,v3.4.0-rc4,"> Some data values do not have a nice string representation yet

Let's fix `df.show` first.","1,-1    ",1,-1,0
1501660226,46485123,2023/3/21,v3.4.0-rc4,"Hmm, how about add a legacy conf to indicate whether we should follow the hive format here? and default we use false.  I'm not sure if there will be users who use this place to connect to the internal system strictly in accordance with the hive format","3,-1    ",3,-1,2
1501690683,51110188,2023/3/21,v3.4.0-rc4,Does `ThriftServerQueryTestSuite` also need to use sparkResultString golden file format?,"1,-1    ",1,-1,0
1501781527,3182036,2023/3/21,v3.4.0-rc4,@AngersZhuuuu are you programmatically parsing the output of the CLI? I thought it displays values for humans only.,"1,-1    ",1,-1,0
1506229279,3182036,2023/3/21,v3.4.0-rc4,"https://github.com/apache/spark/pull/40699 is merged, can we continue this PR?","2,-1    ",2,-1,1
1506258450,51110188,2023/3/21,v3.4.0-rc4,"> #40699 is merged, can we continue this PR?

Sure","1,-1    ",1,-1,0
1508909227,51110188,2023/3/21,v3.4.0-rc4,"@cloud-fan This PR currently makes spark-sql CLI use the same spark result string as df.show.

And remove the use of hiveResultString by non-HiveComparisonTest. Like ExpressionInfoSuite, currently, only SQLQueryTestHelper and HiveComparisonTest use hiveResultString, and SQLQueryTestHelper is used for golden file generation. I try to make SQLQueryTestHelper also use sparkResultString, but ThriftServerQueryTestSuite also uses golden files.

And Beautify the output of cast StructType to StringType, Such as {1, 2} -> {a:1, b:2}.  (It has nothing to do with the theme of this PR, Just in passing when making ExpressionInfoSuite to use sparkResultString, If you mind, I can send another PR to do this).","4,-1    ",4,-1,3
1525140564,3182036,2023/3/21,v3.4.0-rc4,"https://github.com/apache/spark/pull/40922 has been merged, @Yikf can you open a PR to update `ToPrettyString` for all the needed changes to adjust the behavior of `df.show`? ","1,-1    ",1,-1,0
1534149477,51110188,2023/3/21,v3.4.0-rc4,"> #40922 has been merged, @Yikf can you open a PR to update `ToPrettyString` for all the needed changes to adjust the behavior of `df.show`?

Sure, I will open PR to make `ToPrettyString` eval nice strings.

And, I will close this PR since I think `spark-sql` and `df.show` are two different REPLs, don't have to make them exactly the same to break compatibility.","1,-1    ",1,-1,0
1474260861,1938382,2023/3/21,v3.4.0-rc4,Overall LGTM,"1,-1    ",1,-1,0
1474261809,1475305,2023/3/21,v3.4.0-rc4,Thanks @amaliujia ,"2,-2    ",2,-2,0
1487969530,1475305,2023/3/21,v3.4.0-rc4,friendly ping @HyukjinKwon @hvanhovell ,"3,-1    ",3,-1,2
1487970449,1475305,2023/3/21,v3.4.0-rc4,rebased,"2,-1    ",2,-1,1
1493471258,6477701,2023/3/21,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1493532096,1475305,2023/3/21,v3.4.0-rc4,Thanks @HyukjinKwon @hvanhovell @amaliujia ,"2,-1    ",2,-1,1
1494305806,9616802,2023/3/21,v3.4.0-rc4,@LuciferYang thanks for doing this.,"1,-1    ",1,-1,0
1495119529,6477701,2023/3/21,v3.4.0-rc4,"BTW, @hvanhovell please take another look for a posthoc review. I just took a look to make sure the impl is matched with PySpark side so I might have missed few things.","1,-1    ",1,-1,0
1480532626,3898450,2023/3/21,v3.4.0-rc4,"@HeartSaVioR  Please help review this PR, Thanks.","2,-2    ",2,-2,0
1477991828,822522,2023/3/21,v3.4.0-rc4,Merged to master,"1,-1    ",1,-1,0
1478522804,9700541,2023/3/21,v3.4.0-rc4,Thank you all!,"3,-1    ",3,-1,2
1470010407,1475305,2023/3/21,v3.4.0-rc4,4.8.1 fixed a [compilation issue](https://github.com/davidB/scala-maven-plugin/issues/673) with Scala 3.x after 4.7.2,"3,-1    ",3,-1,2
1471000337,6477701,2023/3/21,v3.4.0-rc4,Merged to master.,"2,-3    ",2,-3,-1
1474161688,47577197,2023/3/21,v3.4.0-rc4,"I do have some problems with this upgrade. 


# with <scala-maven-plugin.version>4.8.0</scala-maven-plugin.version>
./build/mvn -DskipTests clean package

[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  2.993 s]
[INFO] Spark Project Tags ................................. SUCCESS [  5.352 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  6.497 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  6.266 s]
[INFO] Spark Project Networking ........................... SUCCESS [  8.869 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  7.967 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  9.179 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  4.749 s]
[INFO] Spark Project Core ................................. SUCCESS [01:55 min]
[INFO] Spark Project ML Local Library ..................... SUCCESS [ 22.125 s]
[INFO] Spark Project GraphX ............................... SUCCESS [ 26.436 s]
[INFO] Spark Project Streaming ............................ SUCCESS [ 39.594 s]
[INFO] Spark Project Catalyst ............................. SUCCESS [02:00 min]
[INFO] Spark Project SQL .................................. SUCCESS [02:39 min]
[INFO] Spark Project ML Common ............................ SUCCESS [ 19.772 s]
[INFO] Spark Project ML Library ........................... SUCCESS [01:46 min]
[INFO] Spark Project Tools ................................ SUCCESS [  4.027 s]
[INFO] Spark Project Hive ................................. SUCCESS [ 58.614 s]
[INFO] Spark Project REPL ................................. SUCCESS [ 15.526 s]
[INFO] Spark Project Assembly ............................. SUCCESS [  5.192 s]
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SUCCESS [ 13.213 s]
[INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 19.317 s]
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SUCCESS [ 28.580 s]
[INFO] Spark Project Examples ............................. SUCCESS [ 33.875 s]
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [ 11.099 s]
[INFO] Spark Avro ......................................... SUCCESS [ 25.772 s]
[INFO] Spark Project Connect Common ....................... SUCCESS [ 33.313 s]
[INFO] Spark Project Connect Server ....................... SUCCESS [ 27.918 s]
[INFO] Spark Project Connect Client ....................... SUCCESS [ 30.611 s]
[INFO] Spark Protobuf ..................................... SUCCESS [ 28.972 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  16:38 min
[INFO] Finished at: 2023-03-17T18:09:50+01:00
[INFO] ------------------------------------------------------------------------







# with <scala-maven-plugin.version>4.8.1</scala-maven-plugin.version>
./build/mvn -DskipTests clean package

[INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ spark-core_2.12 ---
[INFO] Not compiling main sources
[INFO] 
[INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---
[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar
[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)
[INFO] compiling 597 Scala sources and 103 Java sources to /home/bjorn/github/spark/core/target/scala-2.12/classes ...
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: value sun
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: value sun
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: value Signal
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: value Signal
[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: value Signal
[ERROR] 19 errors found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  3.848 s]
[INFO] Spark Project Tags ................................. SUCCESS [ 12.106 s]
[INFO] Spark Project Sketch ............................... SUCCESS [ 10.685 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  8.743 s]
[INFO] Spark Project Networking ........................... SUCCESS [  9.362 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  7.828 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  9.071 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  4.776 s]
[INFO] Spark Project Core ................................. FAILURE [ 17.228 s]
[INFO] Spark Project ML Local Library ..................... SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Common ............................ SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] Spark Project Connect Common ....................... SKIPPED
[INFO] Spark Project Connect Server ....................... SKIPPED
[INFO] Spark Project Connect Client ....................... SKIPPED
[INFO] Spark Protobuf ..................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  01:24 min
[INFO] Finished at: 2023-03-17T18:13:26+01:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile (scala-compile-first) on project spark-core_2.12: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile failed: org.apache.commons.exec.ExecuteException: Process exited with an error: 255 (Exit value: 255) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :spark-core_2.12","1,-1    ",1,-1,0
1474239673,1475305,2023/3/21,v3.4.0-rc4,"hmm... I run `./build/mvn -DskipTests clean package` three times with `4.8.1` on my Mac, they all executed successfully ... I can't reproduce your issue ... 

So, in what environment did you fail to compile? @bjornjorgensen ","1,-1    ",1,-1,0
1474246307,47577197,2023/3/21,v3.4.0-rc4,"hmm.. ok. 
On one PC it's manjaro with java -version
openjdk version ""17.0.6"" 2023-01-17
OpenJDK Runtime Environment (build 17.0.6+10)
OpenJDK 64-Bit Server VM (build 17.0.6+10, mixed mode)
 
and the other that have the same problem it's ubuntu with openjdk 17. This one is running on github https://github.com/bjornjorgensen/jupyter-spark-master-docker/actions/runs/4448461787/jobs/7811305871#step:7:42144 ","1,-1    ",1,-1,0
1474260571,1475305,2023/3/21,v3.4.0-rc4,"When use Java 17.0.6 with 4.8.1, I can reproduce this issue

```
[INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---
[INFO] Compiler bridge file: /Users/yangjie01/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__61.0-1.8.0_20221110T195421.jar
[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)
[INFO] compiling 597 Scala sources and 103 Java sources to /Users/yangjie01/SourceCode/git/spark-mine-12/core/target/scala-2.12/classes ...
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: value sun
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: value sun
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: value Signal
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: value Signal
[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: value Signal
[ERROR] 19 errors found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  1.591 s]
[INFO] Spark Project Tags ................................. SUCCESS [  3.560 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  4.014 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  4.817 s]
[INFO] Spark Project Networking ........................... SUCCESS [  6.146 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  5.401 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  5.420 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  3.365 s]
[INFO] Spark Project Core ................................. FAILURE [  9.098 s]
```

also cc @HyukjinKwon @panbingkun ","1,-3    ",1,-3,-2
1474274756,1475305,2023/3/21,v3.4.0-rc4,"I think GA can pass due to `-Djava.version=${JAVA_VERSION/-ea}`, I run `./build/mvn -DskipTests -Djava.version=17 package` with Java 17 can build pass @bjornjorgensen ","1,-2    ",1,-2,-1
1474284172,47577197,2023/3/21,v3.4.0-rc4,"./build/mvn -DskipTests -Djava.version=17 clean package

[INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-tags_2.12 ---
[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar
[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)
[INFO] compiling 2 Scala sources and 8 Java sources to /home/bjorn/github/spark/common/tags/target/scala-2.12/classes ...
[ERROR] '17' is not a valid choice for '-release'
[ERROR] bad option: '-release'
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  3.074 s]
[INFO] Spark Project Tags ................................. FAILURE [  1.211 s]
[INFO] Spark Project Sketch ............................... SKIPPED","1,-2    ",1,-2,-1
1474286736,1475305,2023/3/21,v3.4.0-rc4,"```
[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar
```

@bjornjorgensen You need to make sure that the Java you're using is really 17(should be 2.12.17__61.0-1.8.0, not 55.0)","2,-2    ",2,-2,0
1474297683,47577197,2023/3/21,v3.4.0-rc4,"archlinux-java status
Available Java environments:
  java-11-openjdk
  java-17-openjdk (default)
[bjorn@amd7g ~]$ java --version
openjdk 17.0.6 2023-01-17
OpenJDK Runtime Environment (build 17.0.6+10)
OpenJDK 64-Bit Server VM (build 17.0.6+10, mixed mode)

This problem is why I wrote the email ""Failed to build master google protobuf protoc-3.22.0-linux-x86_64.exe"" to dev@spark.org 


","2,-4    ",2,-4,-2
1474819534,47577197,2023/3/21,v3.4.0-rc4,I open a ticket for this at https://github.com/davidB/scala-maven-plugin/issues/684,"1,-2    ",1,-2,-1
1475249063,1475305,2023/3/21,v3.4.0-rc4,"> I open a ticket for this at [davidB/scala-maven-plugin#684](https://github.com/davidB/scala-maven-plugin/issues/684)

Next week, I will try to find a minimum case to reproducer problem. `Spark` is too complex for others :)


","1,-1    ",1,-1,0
1475641117,1475305,2023/3/21,v3.4.0-rc4,"<img width=""1118"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/226252485-27d82655-fdda-48dd-9611-dce44de69d66.png"">


@HyukjinKwon @bjornjorgensen @panbingkun In addition to the issues @bjornjorgensen  mentioned, when compiling using Java 8+Scala 2.13, I saw compilation errors as shown in the figure: `[ERROR] -release is only supported on Java 9 and higher
`, It seems that 4.8.1 and Java 8 are not compatible well, although it does not cause compilation failures


More, I saw https://github.com/davidB/scala-maven-plugin/issues/686, So I think we should revert this pr @HyukjinKwon 

","1,-1    ",1,-1,0
1475646370,1475305,2023/3/21,v3.4.0-rc4,https://github.com/apache/spark/pull/40482 I give a pr to revert this one,"1,-1    ",1,-1,0
1471393621,6477701,2023/3/21,v3.4.0-rc4,"All tests passed.

Merged to master and branch-3.4.","1,-1    ",1,-1,0
1470098995,26535726,2023/3/21,v3.4.0-rc4,"cc @slothspot @dongjoon-hyun @yaooqinn, please take a look when you get time, thanks.","1,-3    ",1,-3,-2
1473844570,26535726,2023/3/21,v3.4.0-rc4,"@dongjoon-hyun UT is added, please take a look again, thanks.","1,-1    ",1,-1,0
1477299823,26535726,2023/3/21,v3.4.0-rc4,Kindly ping @dongjoon-hyun,"2,-2    ",2,-2,0
1477316443,26535726,2023/3/21,v3.4.0-rc4,"@dongjoon-hyun thank you, I updated the log message to
```
""Application $appName with application ID $appId and submission ID $sId finished""
```","1,-1    ",1,-1,0
1478133626,9700541,2023/3/21,v3.4.0-rc4,"Merged to master branch for Apache Spark 3.5.0.
Thank you, @pan3793 and @yaooqinn .","1,-1    ",1,-1,0
1478154866,26535726,2023/3/22,v3.4.0-rc4,"Thank you, @dongjoon-hyun and @yaooqinn ","1,-1    ",1,-1,0
1473130566,1475305,2023/3/22,v3.4.0-rc4,Thanks @dongjoon-hyun ,"2,-2    ",2,-2,0
1471123749,12025282,2023/3/22,v3.4.0-rc4,cc @viirya @cloud-fan thank you,"1,-2    ",1,-2,-1
1479648779,3182036,2023/3/22,v3.4.0-rc4,"thanks, merging to master!","1,-1    ",1,-1,0
1478275510,1938382,2023/3/22,v3.4.0-rc4,LGTM,"2,-1    ",2,-1,1
1478787974,9700541,2023/3/22,v3.4.0-rc4,"Merged to master/branch-3.4. Thank you, @grundprinzip and all!","1,-1    ",1,-1,0
1470498081,4690923,2023/3/22,v3.4.0-rc4,@mridulm @xkrogen @akpatnam25 @shuwang21 Please help review. ,"1,-1    ",1,-1,0
1471014495,6477701,2023/3/22,v3.4.0-rc4,AppVeyor failure (`continuous-integration/appveyor/pr`) should be fine to ignore for now.,"1,-1    ",1,-1,0
1471270199,9700541,2023/3/22,v3.4.0-rc4,cc @mridulm ,"2,-2    ",2,-2,0
1471288187,8847816,2023/3/22,v3.4.0-rc4,LGTM. thanks!,"1,-1    ",1,-1,0
1472768397,9700541,2023/3/22,v3.4.0-rc4,Thank you for update. Merged to master/3.4.,"3,-1    ",3,-1,2
1472770014,4690923,2023/3/22,v3.4.0-rc4,Thank you @dongjoon-hyun @HyukjinKwon @shuwang21 @yaooqinn ,"1,-1    ",1,-1,0
1470712114,99207096,2023/3/22,v3.4.0-rc4,Hi @gengliangwang this should be ready for a first look!,"2,-1    ",2,-1,1
1470801546,1097932,2023/3/22,v3.4.0-rc4,"@dtenedor Since we already have `SQLQueryTestSuite` which has good basic Spark SQL features coverage, shall we combine both? E.g. let `SQLQueryTestSuite` show analyzed plan/optimized plan/execution results for comparison, and port more tests from [zetasql](https://github.com/google/zetasql/tree/master/zetasql/analyzer/testdata)
What is the reason for having a new analysis test only?","3,-1    ",3,-1,2
1470823193,99207096,2023/3/22,v3.4.0-rc4,"@gengliangwang Sure, I was thinking about this too. We can reuse the same input SQL query files if we want, and just generate and test against different analyzer test output files. Let me update the PR to do that and I can ping the thread again.","1,-2    ",1,-2,-1
1470824639,99207096,2023/3/22,v3.4.0-rc4,"@gengliangwang from past experience we will want to keep the query plans separate from the SQL results, otherwise the SQL results become hard to read. I will put the analyzer results in separate files.","3,-1    ",3,-1,2
1470835260,1097932,2023/3/22,v3.4.0-rc4,"> I will put the analyzer results in separate files.

Sounds great! Thanks for the work!","1,-1    ",1,-1,0
1470971482,99207096,2023/3/22,v3.4.0-rc4,"@gengliangwang alright I made this change, please look again when you are ready.","1,-1    ",1,-1,0
1472785577,1097932,2023/3/22,v3.4.0-rc4,LGTM except for minor comments. Thanks for the work!,"1,-1    ",1,-1,0
1476082725,3182036,2023/3/22,v3.4.0-rc4,"thanks, merging to master!","2,-2    ",2,-2,0
1476418435,1475305,2023/3/22,v3.4.0-rc4,"Seems GA break after this one:

- https://github.com/apache/spark/actions/runs/4467843445/jobs/7847830877
- https://github.com/apache/spark/actions/runs/4468075788/jobs/7848369978
- https://github.com/apache/spark/actions/runs/4468868780/jobs/7850199229

<img width=""874"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/226384714-ee30f2c5-495c-4709-8a82-78361e1d35b3.png"">
","2,-1    ",2,-1,1
1476427922,3182036,2023/3/22,v3.4.0-rc4,I'm AFK at this time. @gengliangwang can you help to revert it if @dtenedor can't fix the failure soon?,"1,-2    ",1,-2,-1
1476437795,1475305,2023/3/22,v3.4.0-rc4,"Seems would be ok to regenerate the golden files

","1,-1    ",1,-1,0
1476454408,1475305,2023/3/22,v3.4.0-rc4,https://github.com/apache/spark/pull/40492,"1,-1    ",1,-1,0
1476470933,99207096,2023/3/22,v3.4.0-rc4,Looks like some extra tests got added just as this was getting merged! Thanks @LuciferYang for this fix ð,"1,-1    ",1,-1,0
1471147451,6477701,2023/3/22,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1471199868,7322292,2023/3/22,v3.4.0-rc4,Late LGTM,"1,-1    ",1,-1,0
1471228184,7322292,2023/3/22,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1471278961,1475305,2023/3/22,v3.4.0-rc4,Thanks @dongjoon-hyun @yaooqinn ,"1,-1    ",1,-1,0
1471274601,8326978,2023/3/22,v3.4.0-rc4,"LGTM, thanks @williamhyun @dongjoon-hyun ","1,-1    ",1,-1,0
1471314521,9700541,2023/3/22,v3.4.0-rc4,"It seems that there is some GitHub Action setting issue on William side.

Actually, I was the release manager of Apache ORC 1.8.3 and tested this here in my repo.
- https://github.com/dongjoon-hyun/spark/pull/13","1,-2    ",1,-2,-1
1471315962,9700541,2023/3/22,v3.4.0-rc4,Let me merge this~ Merged to master/3.4.,"1,-1    ",1,-1,0
1472312946,1475305,2023/3/22,v3.4.0-rc4,"This function a bit special, It was discussed a long time ago ...
https://github.com/apache/spark/pull/37862#issuecomment-1249419779","1,-1    ",1,-1,0
1472972319,15246973,2023/3/22,v3.4.0-rc4,"> This function a bit special, It was discussed a long time ago ... [#37862 (comment)](https://github.com/apache/spark/pull/37862#issuecomment-1249419779)

If it's a public API, we should mark it; If not, we should correct our problem sooner or later.

cc @srowen ","1,-1    ",1,-1,0
1473029179,1475305,2023/3/22,v3.4.0-rc4,"Personally, I think it's definitely possible to change it in Spark 4.0, but I wasn't sure before.

","1,-1    ",1,-1,0
1473042762,822522,2023/3/22,v3.4.0-rc4,Not a public API but probably not worth 'fixing' before spark 4 indeed,"1,-1    ",1,-1,0
1473058514,15246973,2023/3/22,v3.4.0-rc4,"> Not a public API but probably not worth 'fixing' before spark 4 indeed

OK, Let's fix it after spark 4 release.
I will close it.","2,-1    ",2,-1,1
1471393159,100322362,2023/3/22,v3.4.0-rc4,@HeartSaVioR - pls take a look. Thx,"1,-1    ",1,-1,0
1471721625,1317309,2023/3/22,v3.4.0-rc4,"https://github.com/anishshri-db/spark/actions/runs/4434196358/jobs/7780002037

Looks like PySpark build is stuck but this change does not involve impact on PySpark. (If there is one, it should have failed in SQL/SS tests.)","1,-1    ",1,-1,0
1471722425,1317309,2023/3/22,v3.4.0-rc4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1471582823,6477701,2023/3/22,v3.4.0-rc4,cc @cloud-fan @hvanhovell FYI,"2,-2    ",2,-2,0
1473076150,9700541,2023/3/22,v3.4.0-rc4,"BTW, according to the JIRA, this is only for Apache Spark 3.5?","2,-1    ",2,-1,1
1476121551,6477701,2023/3/22,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1473067918,8326978,2023/3/22,v3.4.0-rc4,"cc @HyukjinKwon @cloud-fan @dongjoon-hyun, thanks","2,-1    ",2,-1,1
1473069824,9700541,2023/3/22,v3.4.0-rc4,"Thank you for pinging me, @yaooqinn .","1,-1    ",1,-1,0
1473073457,8326978,2023/3/22,v3.4.0-rc4,thank you @dongjoon-hyun ,"1,-1    ",1,-1,0
1473073721,9700541,2023/3/22,v3.4.0-rc4,You're welcome!,"1,-1    ",1,-1,0
1473084358,3182036,2023/3/22,v3.4.0-rc4,late LGTM,"2,-1    ",2,-1,1
1472225271,112507318,2023/3/22,v3.4.0-rc4,"Instead of proposing that the user uses another PySpark version, I think it's better to suggest that the user creates a Spark Driver session instead of a Spark Connect session.

So instead of:
[JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `_jsc` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, use the original PySpark instead of Spark Connect.

Perhaps something more like:
[JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `_jsc` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session.","2,-1    ",2,-1,1
1472234887,44108233,2023/3/22,v3.4.0-rc4,"> [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute _jsc is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session.

Sounds good. Just applied the comments on error message.
Thanks!","1,-1    ",1,-1,0
1473012048,6477701,2023/3/22,v3.4.0-rc4,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1472945603,6477701,2023/3/22,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1472616495,1124207,2023/3/22,v3.4.0-rc4,@ueshin who last touched the type hints,"2,-2    ",2,-2,0
1473464387,5399861,2023/3/22,v3.4.0-rc4,cc @cloud-fan ,"2,-2    ",2,-2,0
1474925826,15109987,2023/3/22,v3.4.0-rc4,"Can you please explain more on scenarios when rebalancepartitions becomes child of locallimit? i tried  
SELECT  * FROM t WHERE id > 1 LIMIT 5; with spark 2.4.4 version and rebalancepartitions is not part of the plan. ","2,-1    ",2,-1,1
1475522025,5399861,2023/3/22,v3.4.0-rc4,"> Can you please explain more on scenarios when rebalancepartitions becomes child of locallimit? i tried SELECT * FROM t WHERE id > 1 LIMIT 5; with spark 2.4.4 version and rebalancepartitions is not part of the plan.

Spark 2.4.4  does not have `RebalancePartitions`. Please see: https://issues.apache.org/jira/browse/SPARK-35786","1,-1    ",1,-1,0
1479847214,15109987,2023/3/22,v3.4.0-rc4,"> > Can you please explain more on scenarios when rebalancepartitions becomes child of locallimit? i tried SELECT * FROM t WHERE id > 1 LIMIT 5; with spark 2.4.4 version and rebalancepartitions is not part of the plan.
> 
> Spark 2.4.4 does not have `RebalancePartitions`. Please see: https://issues.apache.org/jira/browse/SPARK-35786
Got it.thanks!","3,-1    ",3,-1,2
1479987729,9700541,2023/3/22,v3.4.0-rc4,Merged to master for Apache Spark 3.5.,"1,-1    ",1,-1,0
1482318942,3182036,2023/3/22,v3.4.0-rc4,"I don't quite get the rationale. For `SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5;`, the user explicitly requires to do a rebalance before limit, why do we remove it? It's a physical hint and we should respect it.","1,-1    ",1,-1,0
1484408565,5399861,2023/3/22,v3.4.0-rc4,"rebalance before limit feels meaningless.
If the user really wants to take n rows from each partition, the SQL should look like this:
```sql
SELECT *
FROM   (SELECT *,
               Row_number() OVER(partition BY pmod(Hash(id, 42), 200) ORDER BY id) AS rn
        FROM   tbl)
WHERE  rn <= 5;
```","1,-1    ",1,-1,0
1484415289,3182036,2023/3/22,v3.4.0-rc4,"The optimizer can remove meanless logical operators as it won't change the query result and it's usually more efficient to remove operators. But for physical APIs like rebalance and repartition, the users explicitly ask to take control of performance tuning and we should trust them. Besides, I don't think this is an always-beneficial optimization, what if the limit is very large? We should let the user make the decision: they can remove the rebalance hint if they believe it makes the perf worse.  What do you think? @wangyum @dongjoon-hyun ","1,-1    ",1,-1,0
1484592416,5399861,2023/3/22,v3.4.0-rc4,"Could we push the `LocalLimit` through `RebalancePartitions`?
```
== Optimized Logical Plan ==
GlobalLimit 5
+- LocalLimit 5
   +- RebalancePartitions
      +- LocalLimit 5
         +- Filter (isnotnull(id#0L) AND (id#0L > 1))
            +- Relation spark_catalog.default.tbl[id#0L] parquet
```","1,-1    ",1,-1,0
1484657163,3182036,2023/3/22,v3.4.0-rc4,I don't think so. Local limit is per partition and rebalance changes partitioning.,"3,-1    ",3,-1,2
1484659332,3182036,2023/3/22,v3.4.0-rc4,"oh wait, I thought the hint applies as the last operator. `SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5;` should rebalance be the root node for this query?","3,-1    ",3,-1,2
1485539305,9700541,2023/3/22,v3.4.0-rc4,"`HINT` is a part of `selectClause`.
https://github.com/apache/spark/blob/a3d9e0ae0f95a55766078da5d0bf0f74f3c3cfc3/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4#L530-L532

That's the reason why `LIMIT` is the root of the query.
```
scala> sql(""SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5"").explain(true)
== Parsed Logical Plan ==
'GlobalLimit 5
+- 'LocalLimit 5
   +- 'UnresolvedHint REBALANCE
      +- 'Project [*]
         +- 'Filter ('id > 1)
            +- 'UnresolvedRelation [t], [], false
```","2,-1    ",2,-1,1
1486144770,5399861,2023/3/22,v3.4.0-rc4,"If such queries cannot be optimized, the performance of such queries will be very poor. We use a partition to fetch data from MySQL, and increase its parallelism for downstream computing after fetching the data:

```sql
CREATE VIEW full_query_log
AS
SELECT h.* FROM query_log_hdfs h
UNION ALL
SELECT /*+ REBALANCE */ q.*, DATE(start) FROM query_log_mysql q;

SELECT * FROM full_query_log limit 5;
```

","3,-1    ",3,-1,2
1486163555,3182036,2023/3/22,v3.4.0-rc4,"OK now I got the use case. At the time when we add the rebalance hint, we don't know what the final query is. Shall we make this optimization a bit more conservative to match both global and local limit? I still think it's risky to remove rebalance below local limit.","3,-1    ",3,-1,2
1486832610,9700541,2023/3/23,v3.4.0-rc4,+1 for @cloud-fan 's suggestion.,"1,-1    ",1,-1,0
1486833076,9700541,2023/3/23,v3.4.0-rc4,"Could you make a follow-up, @wangyum ?","1,-1    ",1,-1,0
1487849388,5399861,2023/3/23,v3.4.0-rc4,"How about reverting this PR directly? Otherwise, we need to change `LimitPushDown`, which is not elegant:
```scala
case Limit(exp, u: Union) if u.children.exists(_.isInstanceOf[HasPartitionExpressions]) =>
  val newChildren = u.children.map {
    case r: RebalancePartitions =>
      maybePushLocalLimit(exp, r.child)
    case r: RepartitionByExpression
        if r.partitionExpressions.nonEmpty && r.partitionExpressions.forall(_.deterministic) =>
      maybePushLocalLimit(exp, r.child)
    case o =>
      maybePushLocalLimit(exp, o)
  }
  Limit(exp, u.copy(children = newChildren))
```","1,-1    ",1,-1,0
1487859134,3182036,2023/3/23,v3.4.0-rc4,why do we need to change `LimitPushDown`? We can't push limit through repartition.,"1,-1    ",1,-1,0
1487867844,5399861,2023/3/23,v3.4.0-rc4,"It is meaningless to only optimize this case, because the user can manually remove the hint:
```sql
SELECT /*+ REBALANCE */ * FROM tbl LIMIT 5;
```

I'd like to optimize this case, since the user cannot manually remove the hint, because the view may be maintained by others:
```sql
CREATE VIEW full_query_log
AS
SELECT h.* FROM query_log_hdfs h
UNION ALL
SELECT /*+ REBALANCE */ q.*, DATE(start) FROM query_log_mysql q;

SELECT * FROM full_query_log limit 5;
```","1,-1    ",1,-1,0
1487993457,5399861,2023/3/23,v3.4.0-rc4,Reverted this commit: https://github.com/apache/spark/commit/58facc16536cb2784aace38d6cca77bd055a3053.,"1,-1    ",1,-1,0
1487995895,9700541,2023/3/23,v3.4.0-rc4,"Thank you for the decision, @wangyum .","1,-2    ",1,-2,-1
1473439603,9616802,2023/3/23,v3.4.0-rc4,@LuciferYang I am trying to get to your PRs in the next couple of days. My apologies for the delay.,"1,-2    ",1,-2,-1
1474224705,1475305,2023/3/23,v3.4.0-rc4,"> I am trying to get to your PRs in the next couple of days. My apologies for the delay.

No problem, thanks ~","1,-1    ",1,-1,0
1474272247,1938382,2023/3/23,v3.4.0-rc4,LGTM!,"2,-2    ",2,-2,0
1476196794,9616802,2023/3/23,v3.4.0-rc4,Merging.,"1,-1    ",1,-1,0
1476199144,1475305,2023/3/23,v3.4.0-rc4,Thanks @hvanhovell @amaliujia ,"1,-1    ",1,-1,0
1474006026,3182036,2023/3/23,v3.4.0-rc4,This is very trivial so probably doesn't need a JIRA ticket. We can add [MINOR] in the title.,"3,-1    ",3,-1,2
1475437896,265981,2023/3/23,v3.4.0-rc4,"> This is very trivial so probably doesn't need a JIRA ticket. We can add [MINOR] in the title.

Thank you @cloud-fan. Done!","3,-1    ",3,-1,2
1475459557,6477701,2023/3/23,v3.4.0-rc4,Merged to master.,"1,-3    ",1,-3,-2
1474186791,129050,2023/3/23,v3.4.0-rc4,cc @dongjoon-hyun @sunchao @viirya ,"2,-1    ",2,-1,1
1474188526,9700541,2023/3/23,v3.4.0-rc4,"Thank you for pinging me, @kazuyukitanimura .","1,-2    ",1,-2,-1
1474189149,9700541,2023/3/23,v3.4.0-rc4,"cc @cloud-fan and @HyukjinKwon , too.","1,-2    ",1,-2,-1
1474360017,9700541,2023/3/23,v3.4.0-rc4,"Thank you, @kazuyukitanimura and all.
Merged to master for Apache Spark 3.5.","1,-1    ",1,-1,0
1474365976,129050,2023/3/23,v3.4.0-rc4,Thank you!,"3,-1    ",3,-1,2
1476189112,3182036,2023/3/23,v3.4.0-rc4,late LGTM,"1,-1    ",1,-1,0
1474277110,1938382,2023/3/23,v3.4.0-rc4,Just a general question which is for my self education: do we expect the results of `Column.explain` are stable?,"3,-1    ",3,-1,2
1474622202,8486025,2023/3/23,v3.4.0-rc4,"> Just a general question which is for my self education: do we expect the results of `Column.explain` are stable?

Personally, I think Spark should keep the stable output and do not break change for users.","2,-1    ",2,-1,1
1475479633,8486025,2023/3/23,v3.4.0-rc4,ping @cloud-fan ,"1,-1    ",1,-1,0
1481119954,3182036,2023/3/23,v3.4.0-rc4,"Can we use the existing golden file framework? I think we just need to add a bunch of test queries like `SELECT a + b`.

EDIT: sorry I just realized that this PR is to test the result of `Column.explain`. Why do we care about its stability? The API doc says `Prints the expression to the console for debugging purposes`. We don't have stability tests for the plan EXPLAIN result either.","1,-1    ",1,-1,0
1482193211,8486025,2023/3/23,v3.4.0-rc4,"> EDIT: sorry I just realized that this PR is to test the result of `Column.explain`. Why do we care about its stability? The API doc says `Prints the expression to the console for debugging purposes`. We don't have stability tests for the plan EXPLAIN result either.

Got it. I will close this PR.","2,-1    ",2,-1,1
1476014661,8486025,2023/3/23,v3.4.0-rc4,ping @hvanhovell cc @HyukjinKwon @zhengruifeng @amaliujia @ueshin @LuciferYang ,"3,-1    ",3,-1,2
1476589894,1938382,2023/3/23,v3.4.0-rc4,Do we need python side compatible API?,"1,-1    ",1,-1,0
1477176898,8486025,2023/3/23,v3.4.0-rc4,"> Do we need python side compatible API?

Maybe. But PySpark does not have the API now.","1,-1    ",1,-1,0
1543136737,8486025,2023/3/23,v3.4.0-rc4,https://github.com/apache/spark/pull/40528 used to replace this one.,"1,-1    ",1,-1,0
1475460773,6477701,2023/3/23,v3.4.0-rc4,Mind filing a JIRA please? cc @MaxGekk @itholic FYI,"1,-1    ",1,-1,0
1475510753,1475305,2023/3/23,v3.4.0-rc4,"SQLQueryTestSuite failed, need re-generate the golden files","1,-1    ",1,-1,0
1475618318,44108233,2023/3/23,v3.4.0-rc4,"Found JIRA: SPARK-42838.

Can you attach the JIRA number to title?

e.g.
""[SPARK-42838][SQL] changed error class name _LEGACY_ERROR_TEMP_2000""","1,-1    ",1,-1,0
1478174533,80632333,2023/3/23,v3.4.0-rc4,"> Found JIRA: [SPARK-42838](https://issues.apache.org/jira/browse/SPARK-42838).
> 
> Can you attach the JIRA number to title?
> 
> e.g. ""[[SPARK-42838](https://issues.apache.org/jira/browse/SPARK-42838)][SQL] changed error class name _LEGACY_ERROR_TEMP_2000""
@itholic 
done!","1,-1    ",1,-1,0
1478757584,6477701,2023/3/23,v3.4.0-rc4,Mind filling the PR description as well?,"1,-1    ",1,-1,0
1474270757,1938382,2023/3/23,v3.4.0-rc4,Nit: typo `DataFraem` in PR title and PR description :) ,"1,-1    ",1,-1,0
1475453909,6477701,2023/3/23,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1475357466,1938382,2023/3/23,v3.4.0-rc4,LGTM!,"1,-1    ",1,-1,0
1475454133,6477701,2023/3/23,v3.4.0-rc4,Merged to master and branch-3.4.,"3,-1    ",3,-1,2
1475460559,7322292,2023/3/23,v3.4.0-rc4,late lgtm,"3,-1    ",3,-1,2
1474496775,1097932,2023/3/23,v3.4.0-rc4,cc @sigmod as well,"1,-1    ",1,-1,0
1475462499,6477701,2023/3/23,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1475454267,6477701,2023/3/23,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1475455925,6477701,2023/3/23,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1474562711,107834,2023/3/23,v3.4.0-rc4,"cc @peter-toth @cloud-fan 
Also cc @xinrong-meng for this being a potential Spark 3.4.0 release blocker.","1,-1    ",1,-1,0
1474774246,7253827,2023/3/23,v3.4.0-rc4,"Thanks @rednaxelafx for the fix and pinging me.
I think you are right that `EquivalentExpressions.addExpr()` should be guarded by `supportedExpression()` if we guard `getExprState()`. But, I'm not sure it is right that we don't deduplicate the `max(transform(array(id), x -> x))` in your example query.
Probably the real issue here is that in `PhysicalAggregation` the class `EquivalentExpressions` is used for simply deduplicating whole expressions while on executors we use it for common subexpression elimination. In the former case we don't need the `LambdaVariable` guard but in the latter one we need it. So maybe we should add a argument to `EquivalentExpressions` to enable/disable the guards and in `PhysicalAggregation` we should disable it?","1,-1    ",1,-1,0
1474848224,3536454,2023/3/23,v3.4.0-rc4,"Just seeing this group of PRs (most notably https://github.com/apache/spark/pull/39046), was there a real reason `NamedLambdaVariable` was added into all this mix? If I understand right, it effectively eliminates all subexpression elimination involving any expressions containing higher-order functions at any nested level, even though it's perfectly valid to pull out a complete high-order function, you just can't pull out the `LambdaFunction` by itself. 

Currently the check for `CodegenFallback` is what prevents the `LambdaFunction`'s from being considered for subexpression elimination. Quick plug for my 1.5 year old PR for adding codegen to HOFs https://github.com/apache/spark/pull/34558 simply adds `HigherOrderFunction` as a special case to only consider the arguments and not the functions themselves for subexpression elimination","1,-1    ",1,-1,0
1474880345,7253827,2023/3/23,v3.4.0-rc4,"Hm, I think you are right @Kimahriman, `LambdaVariable` and `NamedLambdaVariable` are very different and `NamedLambdaVariable` seem to be used only in `LambdaFunction`s, so https://github.com/apache/spark/pull/39046 doesn't make sense and actually it can prevent pulling out higher order functions and so cause performance regression... I think that PR should be reverted.
Update: I've filed a revert PR here: https://github.com/apache/spark/pull/40475


But I feel that is orthogonal to the issue that we use `EquivalentExpressions` for different purposes in `PhysicalAggregation` (the only place where we use `.addExpr()`) and in executors (`.addExprTree()` for subexpression elimination).
","1,-1    ",1,-1,0
1477132202,107834,2023/3/23,v3.4.0-rc4,@peter-toth could you please clarify why `supportedExpression()` was needed in `getExprState()` in the first place? i.e. why isn't it sufficient to add it to `addExprTree()`?,"3,-1    ",3,-1,2
1477136150,107834,2023/3/23,v3.4.0-rc4,@Kimahriman I'd love to see a good CSE implementation for higher-order functions too. But for backporting the fix (which is this PR's primary intent) that would have been too much. For this one (or the one @peter-toth forked off) we're just aiming for a narrow fix that allows the aggregate to work again.,"1,-1    ",1,-1,0
1477140427,3536454,2023/3/23,v3.4.0-rc4,"> @Kimahriman I'd love to see a good CSE implementation for higher-order functions too. But for backporting the fix (which is this PR's primary intent) that would have been too much. For this one (or the one @peter-toth forked off) we're just aiming for a narrow fix that allows the aggregate to work again.

Yeah I was just commenting on the related PR that broke CSE for anything using a HOF. I had plans for trying to do CSE inside a HOF but that stalled when I didn't get any traction on the initial adding codegen support","1,-1    ",1,-1,0
1477165259,3182036,2023/3/24,v3.4.0-rc4,"The check was added to `getExprState` in https://github.com/apache/spark/pull/39010, which is to avoid canonicalizing a subquery expression and leading to NPE.

I agree that we should be consistent and this PR LGTM. Can we update the test case to use `LambdaVariable` as `NamedLambdaVariable` has been removed in https://github.com/apache/spark/pull/40475 ?","1,-1    ",1,-1,0
1477837727,3182036,2023/3/24,v3.4.0-rc4,"thanks, merging to master/3.4!","2,-1    ",2,-1,1
1475465175,6477701,2023/3/24,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1475465317,6477701,2023/3/24,v3.4.0-rc4,Thanks for filing a separate JIRA.,"1,-1    ",1,-1,0
1475879149,7253827,2023/3/24,v3.4.0-rc4,"> Thanks for filing a separate JIRA.

Thanks @HyukjinKwon for the quick review!
Actually I've just noticed that the SPARK-41468 follow-up PR was only merged to `master` (3.4 at that time) only: https://github.com/apache/spark/pull/39046#issuecomment-1347975823.
So probably a simple revert commit (wihtout a new ticket) on 3.4 and `master` would have been sufficient... For the same reason I'm removing the 3.3.2 affect version from SPARK-42852. Please let me know if you disagree.","1,-1    ",1,-1,0
1475970030,6477701,2023/3/24,v3.4.0-rc4,Yeah so I just reverted it from master and 3.4 ;-).,"1,-1    ",1,-1,0
1476258173,822522,2023/3/24,v3.4.0-rc4,Merged to master,"1,-2    ",1,-2,-1
1477819886,13965087,2023/3/24,v3.4.0-rc4,@HyukjinKwon @LuciferYang @MaxGekk  @cloud-fan  cc,"1,-1    ",1,-1,0
1475377220,6235869,2023/3/24,v3.4.0-rc4,cc @cloud-fan @dongjoon-hyun ,"2,-1    ",2,-1,1
1476057828,3182036,2023/3/24,v3.4.0-rc4,"thanks, merging to master!","1,-2    ",1,-2,-1
1476581464,9700541,2023/3/24,v3.4.0-rc4,"Thank you, @cloud-fan !","1,-1    ",1,-1,0
1476769291,6235869,2023/3/24,v3.4.0-rc4,"Thanks, @dongjoon-hyun @cloud-fan!","2,-1    ",2,-1,1
1475472825,6477701,2023/3/24,v3.4.0-rc4,cc @zhengruifeng FYI,"1,-1    ",1,-1,0
1475527135,6477701,2023/3/24,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1475614276,1475305,2023/3/24,v3.4.0-rc4,All passed,"1,-1    ",1,-1,0
1475836801,7322292,2023/3/24,v3.4.0-rc4,merged to master,"1,-1    ",1,-1,0
1475837376,1475305,2023/3/24,v3.4.0-rc4,Thanks @zhengruifeng @HyukjinKwon ,"2,-1    ",2,-1,1
1475646223,1475305,2023/3/24,v3.4.0-rc4,cc @HyukjinKwon @panbingkun @bjornjorgensen FYI,"1,-1    ",1,-1,0
1475804184,8326978,2023/3/24,v3.4.0-rc4,"Thanks, merged to master","2,-1    ",2,-1,1
1475807520,1475305,2023/3/24,v3.4.0-rc4,Thanks @yaooqinn @HyukjinKwon @bjornjorgensen @panbingkun ,"3,-1    ",3,-1,2
1475992867,6477701,2023/3/24,v3.4.0-rc4,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1476093903,5399861,2023/3/24,v3.4.0-rc4,cc @cloud-fan ,"1,-1    ",1,-1,0
1476254129,1475305,2023/3/24,v3.4.0-rc4,"also cc @beliefer , https://github.com/apache/spark/pull/40355 maybe need rebase after this one","1,-1    ",1,-1,0
1476846845,6477701,2023/3/24,v3.4.0-rc4,Merged to master.,"3,-1    ",3,-1,2
1477271752,1938382,2023/3/24,v3.4.0-rc4,late LGTM,"1,-1    ",1,-1,0
1476051208,47337188,2023/3/24,v3.4.0-rc4,"This PR is almost a pure cherry-pick of https://github.com/apache/spark/commit/fee47c77e5d31bce592bf6e2bd33c2dabfc57bd3, with minor changes on tests to adapt to branch-3.4.","1,-1    ",1,-1,0
1477393696,6477701,2023/3/24,v3.4.0-rc4,Merged to branch-3.4.,"1,-1    ",1,-1,0
1477553890,47337188,2023/3/24,v3.4.0-rc4,Thanks!,"1,-2    ",1,-2,-1
1480372425,47337188,2023/3/24,v3.4.0-rc4,May I get a review please @zhengruifeng @HyukjinKwon ?,"1,-1    ",1,-1,0
1480372625,47337188,2023/3/24,v3.4.0-rc4,cc @LuciferYang  thanks!,"1,-1    ",1,-1,0
1480555708,6477701,2023/3/25,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1480556026,6477701,2023/3/25,v3.4.0-rc4,It has a conflict w/ branch-3.4. mind creating a backport PR please?,"2,-1    ",2,-1,1
1480687213,1475305,2023/3/25,v3.4.0-rc4,late LGTM,"2,-1    ",2,-1,1
1476454588,7253827,2023/3/25,v3.4.0-rc4,"@rednaxelafx, @cloud-fan let me know it this PR is a viable alternative to https://github.com/apache/spark/pull/40473. Or maybe if I should do a little cleanup like https://github.com/peter-toth/spark/commit/90421cb74dd0d0cfe2693cab39a25cd7892c0d45 in this or in a follow-up PR...","1,-1    ",1,-1,0
1477131544,107834,2023/3/25,v3.4.0-rc4,"Before the recent rounds of changes to EquivalentExpressions, the old `addExprTree` used to call `addExpr` in its core:
https://github.com/apache/spark/blob/branch-2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala#L90
That was still the case when `PhysicalAggregation` started using this mechanism to deduplicate expressions. I guess it started becoming ""detached"" from the main path when the recent refactoring happened that allows updating a separate equivalence map instead of the ""main"" one.

Your proposed PR here further orphans that function from any actual use. Which is okay for keeping binary compatibility as much as possible.
The inlined dedup logic in `PhysicalAggregation` looks rather ugly though. I don't have a strong opinion about that as long as other reviewers are okay. I'd prefer still retaining some sort of encapsulated collection for the dedup usage.

BTW I updated my PR's test case because it makes more sense to check the return value from `addExpr` on the second invocation rather than on the first (both ""not supported expression"" and actual new expression cases would have gotten a `false` return value if we add that guard to the `addExpr()` function).
https://github.com/apache/spark/pull/40473/commits/28d101ee6765c5453189fa62d6b8ade1568d99d2","1,-1    ",1,-1,0
1477972190,822522,2023/3/25,v3.4.0-rc4,@LuciferYang if you can resolve the conflict (I merged your other change) I'll merge this,"1,-1    ",1,-1,0
1478005750,1475305,2023/3/25,v3.4.0-rc4,"done, let's wait CI","1,-1    ",1,-1,0
1478757952,6477701,2023/3/25,v3.4.0-rc4,Merged to master.,"2,-1    ",2,-1,1
1478824499,1475305,2023/3/25,v3.4.0-rc4,Thanks @srowen @HyukjinKwon ,"1,-1    ",1,-1,0
1477970912,822522,2023/3/26,v3.4.0-rc4,Merged to master,"1,-1    ",1,-1,0
1477971639,1475305,2023/3/27,v3.4.0-rc4,Thanks @srowen @HyukjinKwon ,"1,-3    ",1,-3,-2
1478567572,9700541,2023/3/27,v3.4.0-rc4,"cc @viirya , too.","1,-1    ",1,-1,0
1478569986,68855,2023/3/27,v3.4.0-rc4,Looks good to me. +1,"1,-1    ",1,-1,0
1478821629,1475305,2023/3/27,v3.4.0-rc4,Thanks @dongjoon-hyun @viirya ~,"1,-1    ",1,-1,0
1486822133,7286705,2023/3/27,v3.4.0-rc4,"Hi, thanks @dongjoon-hyun for looking at it.
Our use case: we have an application which is using the `InProcessLauncher` class several times (to launch multiple spark jobs).
The first one is launched correctly but the next ones fail, trying to use the same configMaps created for the previous job.

I think the call chain is `InProcessLauncher` -> `InProcessSparkSubmit` -> `SparkSubmit` -> `KubernetesClientApplication` -> [Client](https://github.com/apache/spark/blob/ba1badd2adfd175c6680dff90e14b8aaa6cecd78/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L105) -> `KubernetesClientUtils`","3,-3    ",3,-3,0
1486827716,9700541,2023/3/27,v3.4.0-rc4,Got it. Thank you.,"1,-1    ",1,-1,0
1495644811,7286705,2023/3/27,v3.4.0-rc4,"Hi, sorry for the delay.

I will add some Unit Testing in order to validate the changes as you mention.","1,-1    ",1,-1,0
1501405550,9700541,2023/3/27,v3.4.0-rc4,"Any updates, @DHKold ?","2,-1    ",2,-1,1
1537865489,2387705,2023/3/27,v3.4.0-rc4,"> Any updates, @DHKold ?

Hello. We have the same issue (i'm the one who reported the bug in the JIRA issue). We have to maintain a fork with the change from val to def, and would be great if this fix was in the original source code.

If there is no update maybe we can help. Thx","4,-1    ",4,-1,3
1539248438,9700541,2023/3/27,v3.4.0-rc4,"Thank you for sharing, @ejblanco .","1,-1    ",1,-1,0
1542112169,4374603,2023/3/27,v3.4.0-rc4,"To collect more feedback, we are in the same situation as @ejblanco . Can also offer helping hand in case of need. ","1,-1    ",1,-1,0
1547710656,7286705,2023/3/27,v3.4.0-rc4,"Hi, sorry, Iw was away for some time. What remains to do:
- add some Unit Tests for the propose changes (I'll try to do that this week)
- check if the configmap for the driver should also be generated dynamically (I was not able to determine it, on our scenario it does not seem to produce any error)

If someone can investigate the second point, that would help.

Thanks","2,-1    ",2,-1,1
1547714727,4374603,2023/3/27,v3.4.0-rc4,On our side we duplicated the code also because of driver config map being hard-coded as well. We are using same launcher to start multiple spark sessions hence it would lead to conflict ,"1,-1    ",1,-1,0
1476455400,1475305,2023/3/27,v3.4.0-rc4,cc @cloud-fan @gengliangwang @dtenedor ,"1,-1    ",1,-1,0
1476757882,1475305,2023/3/27,v3.4.0-rc4,GA passed,"2,-1    ",2,-1,1
1476767577,1580697,2023/3/27,v3.4.0-rc4,"+1, LGTM. Merging to master.
Thank you, @LuciferYang and @dongjoon-hyun @gengliangwang @dtenedor for review.","1,-1    ",1,-1,0
1477180380,1475305,2023/3/27,v3.4.0-rc4,Thanks All ~,"1,-1    ",1,-1,0
1476637999,47577197,2023/3/27,v3.4.0-rc4,"@srowen FYI 
","1,-1    ",1,-1,0
1477300074,13155472,2023/3/27,v3.4.0-rc4,"I did enable action on my forked repo and rebase to `apache/spark` master. 
Can anyone re-run the workflows?","1,-1    ",1,-1,0
1477389181,6477701,2023/3/27,v3.4.0-rc4,Mind double checking? Seems they are not running https://github.com/sudoliyang/spark/actions,"1,-1    ",1,-1,0
1478786416,13155472,2023/3/27,v3.4.0-rc4,"@HyukjinKwon Thanks. I ran the tests at https://github.com/sudoliyang/spark/pull/2. 
I would like to rebase and force push again to run all tests here.","1,-1    ",1,-1,0
1478792526,6477701,2023/3/27,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1476856830,6477701,2023/3/27,v3.4.0-rc4,LGTM if tests pass,"1,-1    ",1,-1,0
1478658342,1938382,2023/3/27,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1478835534,99207096,2023/3/27,v3.4.0-rc4,"@HyukjinKwon the tests are passing now, this is ready to merge if you are ready :)","2,-2    ",2,-2,0
1478841111,1475305,2023/3/27,v3.4.0-rc4,@dtenedor Wait a few minutes for me to check with Scala 2.13 manually,"2,-2    ",2,-2,0
1478858139,1475305,2023/3/27,v3.4.0-rc4,"> @dtenedor Wait a few minutes for me to check with Scala 2.13 manually

done, should be ok ~","1,-1    ",1,-1,0
1479017911,6477701,2023/3/27,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1482557118,6477701,2023/3/27,v3.4.0-rc4,"I think ANSI test fails after this PR:

```
[info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (31 milliseconds)
[info]   timestampNTZ/datetime-special.sql_analyzer_test
[info]   Expected ""...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ..."", but got ""...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ..."" Result did not match for query #1
[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Assertions.assertResult(Assertions.scala:847)
[info]   at org.scalatest.Assertions.assertResult$(Assertions.scala:842)
[info]   at org.scalatest.funsuite.AnyFunSuite.assertResult(AnyFunSuite.scala:1564)
[info]   at org.apache.spark.sql.SQLQueryTestSuite.$anonfun$readGoldenFileAndCompareResults$3(SQLQueryTestSuite.scala:777)
[info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
```

https://github.com/apache/spark/actions/runs/4496107425/jobs/7910457741

@dtenedor mind taking a look please? cc @gengliangwang ","1,-1    ",1,-1,0
1483097427,99207096,2023/3/27,v3.4.0-rc4,"Sure, I can take a look.

On Fri, Mar 24, 2023 at 3:12â¯AM Hyukjin Kwon ***@***.***>
wrote:

> I think ANSI test fails after this PR:
>
> [info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (31 milliseconds)
> [info]   timestampNTZ/datetime-special.sql_analyzer_test
> [info]   Expected ""...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ..."", but got ""...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ..."" Result did not match for query #1
> [info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)
> [info]   org.scalatest.exceptions.TestFailedException:
> [info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
> [info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
> [info]   at org.scalatest.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)
> [info]   at org.scalatest.Assertions.assertResult(Assertions.scala:847)
> [info]   at org.scalatest.Assertions.assertResult$(Assertions.scala:842)
> [info]   at org.scalatest.funsuite.AnyFunSuite.assertResult(AnyFunSuite.scala:1564)
> [info]   at org.apache.spark.sql.SQLQueryTestSuite.$anonfun$readGoldenFileAndCompareResults$3(SQLQueryTestSuite.scala:777)
> [info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
>
> https://github.com/apache/spark/actions/runs/4496107425/jobs/7910457741
>
> @dtenedor <https://github.com/dtenedor> mind taking a look please? cc
> @gengliangwang <https://github.com/gengliangwang>
>
> ï¿½?> Reply to this email directly, view it on GitHub
> <https://github.com/apache/spark/pull/40496#issuecomment-1482557118>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AXU4PODW66JIEV5CNJWXEALW5VXQJANCNFSM6AAAAAAWBOMAJQ>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>


-- 
Please write anonymous feedback for Daniel at any time (form
<https://docs.google.com/forms/d/e/1FAIpQLSc-Nd9JOncZTMb2hlhj9GxQO0igStEBgtFclLXFsQleamA0ag/viewform?vc=0&c=0&w=1&flr=0>
).
","1,-1    ",1,-1,0
1483263177,99207096,2023/3/27,v3.4.0-rc4,@HyukjinKwon I ran the test locally and it passes. Maybe it is fixed at head now?,"2,-2    ",2,-2,0
1483683148,1475305,2023/3/27,v3.4.0-rc4,"```
[info] - timestampNTZ/datetime-special.sql_analyzer_test *** FAILED *** (11 milliseconds)
[info]   timestampNTZ/datetime-special.sql_analyzer_test
[info]   Expected ""...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ..."", but got ""...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ..."" Result did not match for query #1
[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)
[info]   org.scalatest.exceptions.TestFailedException:
```

@dtenedor

Seems not fixed,  run 

`SPARK_ANSI_SQL_MODE=true build/sbt ""sql/testOnly org.apache.spark.sql.SQLQueryTestSuite""`

can reproduce the failure","1,-1    ",1,-1,0
1483703189,99207096,2023/3/27,v3.4.0-rc4,Looks like @LuciferYang fixed it with https://github.com/apache/spark/pull/40552. Thanks so much for the fix!,"1,-1    ",1,-1,0
1477168751,7322292,2023/3/27,v3.4.0-rc4,merged to master/branch-3.4,"1,-1    ",1,-1,0
1477332325,1938382,2023/3/27,v3.4.0-rc4,@hvanhovell ,"1,-1    ",1,-1,0
1478120860,9616802,2023/3/27,v3.4.0-rc4,@amaliujia can you add a test that checks if the options are properly propagated from client to server?,"1,-1    ",1,-1,0
1478220179,1938382,2023/3/27,v3.4.0-rc4,"@hvanhovell existing codebase uses this to verify the options:
 
```
  test(""SPARK-32844: DataFrameReader.table take the specified options for V1 relation"") {
    withSQLConf(SQLConf.USE_V1_SOURCE_LIST.key -> ""parquet"") {
      withTable(""t"") {
        sql(""CREATE TABLE t(i int, d double) USING parquet OPTIONS ('p1'='v1', 'p2'='v2')"")

        val df = spark.read.option(""P2"", ""v2"").option(""p3"", ""v3"").table(""t"")
        val options = df.queryExecution.analyzed.collectFirst {
          case r: LogicalRelation => r.relation.asInstanceOf[HadoopFsRelation].options
        }.get
        assert(options(""p2"") == ""v2"")
        assert(options(""p3"") == ""v3"")
      }
    }
  }
  ```
  
  However this won't work for Spark Connect. Do you know if we have another way to achieve it?","2,-1    ",2,-1,1
1478228229,1938382,2023/3/27,v3.4.0-rc4,"@hvanhovell 

oh are you thinking about golden file based tests?","1,-1    ",1,-1,0
1480663504,6477701,2023/3/27,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1477436589,3182036,2023/3/27,v3.4.0-rc4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1477438241,3182036,2023/3/27,v3.4.0-rc4,also cc @xinrong-meng this is from api auditing,"1,-1    ",1,-1,0
1477276287,47337188,2023/3/27,v3.4.0-rc4,"LGTM, thank you @zhengruifeng !","1,-1    ",1,-1,0
1477304948,7322292,2023/3/27,v3.4.0-rc4,@xinrong-meng  it seems this PR was already merged? ,"2,-1    ",2,-1,1
1477308745,47337188,2023/3/27,v3.4.0-rc4,"Merged to branch-3.4, thanks!","1,-1    ",1,-1,0
1477386014,6477701,2023/3/27,v3.4.0-rc4,cc @viirya FYI,"1,-2    ",1,-2,-1
1477403287,10219731,2023/3/27,v3.4.0-rc4,"Yarn NM injects spark.yarn.app.container.log.dir as a system property, so we use ${sys:xxx} to refer it during logging initialization. 

https://logging.apache.org/log4j/2.x/manual/lookups.html#system-properties-lookup","2,-1    ",2,-1,1
1478906144,822522,2023/3/27,v3.4.0-rc4,Merged to master,"1,-1    ",1,-1,0
1478755239,6477701,2023/3/28,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1477426004,1475305,2023/3/28,v3.4.0-rc4,cc @wangyum @cloud-fan FYI,"1,-1    ",1,-1,0
1479567656,15246973,2023/3/28,v3.4.0-rc4,"cc @cloud-fan @wangyum @LuciferYang 
I run benchmark - org.apache.spark.sql.execution.datasources.json.JsonBenchmark, result as follow:

- CodeGen for get_json_object (on github)
https://github.com/panbingkun/spark/actions/runs/4489492515/jobs/7895384637
<img width=""922"" alt=""image"" src=""https://user-images.githubusercontent.com/15246973/226918094-4a943e82-151f-4d70-b29c-a16ef4087f30.png"">

- CodeGen for get_json_object (on local):
<img width=""899"" alt=""image"" src=""https://user-images.githubusercontent.com/15246973/226931986-8d8892b3-4f39-4e86-bd61-17f73f324184.png"">

- No CodeGen for get_json_object (on github)
https://github.com/panbingkun/spark/actions/runs/4489490118/jobs/7895379568
<img width=""920"" alt=""image"" src=""https://user-images.githubusercontent.com/15246973/226918535-df933355-bc2e-4b07-bbed-7f6010bcd42b.png"">

- No CodeGen for get_json_object (on local)
<img width=""898"" alt=""image"" src=""https://user-images.githubusercontent.com/15246973/226931458-9f71d338-0b09-439a-8007-24c102ff5db6.png"">

","1,-1    ",1,-1,0
1479578855,1475305,2023/3/28,v3.4.0-rc4,hmm... I think we should refactor `JsonBenchmark` to make get_json_object run w/ and w/o  code gen in one,"1,-1    ",1,-1,0
1479624519,15246973,2023/3/28,v3.4.0-rc4,"> hmm... I think we should refactor `JsonBenchmark` to make get_json_object run w/ and w/o code gen in one

Ok, Let me do it.","1,-3    ",1,-3,-2
1480692403,1475305,2023/3/28,v3.4.0-rc4,"@panbingkun I think we should also update `JsonBenchmark-jdk11-results.txt`, `JsonBenchmark-jdk17-results.txt` and `JsonBenchmark-results.txt` in this pr due to `JsonBenchmark` updated ","1,-3    ",1,-3,-2
1480968971,15246973,2023/3/28,v3.4.0-rc4,"> @panbingkun I think we should also update `JsonBenchmark-jdk11-results.txt`, `JsonBenchmark-jdk17-results.txt` and `JsonBenchmark-results.txt` in this pr due to `JsonBenchmark` updated

Done","1,-1    ",1,-1,0
1563033876,15246973,2023/3/28,v3.4.0-rc4,"> @panbingkun Could you resolve conflicts, please.

Let me update the results of `JsonBenchmark` again. Waiting for it.
- https://github.com/panbingkun/spark/actions/runs/5080796638
- https://github.com/panbingkun/spark/actions/runs/5080799367
- https://github.com/panbingkun/spark/actions/runs/5080801858

Thank you for reviewing it! @MaxGekk ","1,-1    ",1,-1,0
1563673976,15246973,2023/3/28,v3.4.0-rc4,"> @panbingkun Could you resolve conflicts, please.

This is done.","3,-1    ",3,-1,2
1478755845,6477701,2023/3/28,v3.4.0-rc4,Merged to master.,"2,-1    ",2,-1,1
1477781116,1580697,2023/3/28,v3.4.0-rc4,"@cloud-fan @grundprinzip @HyukjinKwon Could you review this PR, please.","1,-1    ",1,-1,0
1484535748,1580697,2023/3/28,v3.4.0-rc4,"Merging to master/3.4. Thank you, @cloud-fan @srielau @grundprinzip for review.","1,-1    ",1,-1,0
1478279790,1938382,2023/3/28,v3.4.0-rc4,LGTM,"1,-1    ",1,-1,0
1478454379,9700541,2023/3/28,v3.4.0-rc4,"Thank you, @amaliujia . 

Also, could you review this PR, @viirya ?","2,-2    ",2,-2,0
1478564576,9700541,2023/3/28,v3.4.0-rc4,"Thank you, @viirya . Merged to master for Apache Spark 3.5.","3,-1    ",3,-1,2
1478790850,6477701,2023/3/28,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1478711103,227407,2023/3/28,v3.4.0-rc4,"@dongjoon-hyun , may I ask for your review, since you did the original import of the GCS connector in [SPARK-33605](https://issues.apache.org/jira/browse/SPARK-33605)/#37745? Thank you!","3,-1    ",3,-1,2
1478780794,9700541,2023/3/28,v3.4.0-rc4,"Merged to master/3.4 for Apache Spark 3.4.0. This will be a part of next Apache Spark 3.4.0 RC.
I added you to the Apache Spark contributor group and assigned SPARK-42888 to you.
Welcome to the Apache Spark community, @cnauroth .

Also, cc @sunchao ","1,-1    ",1,-1,0
1478787651,506679,2023/3/28,v3.4.0-rc4,"LGTM too, thanks @cnauroth @dongjoon-hyun !","2,-1    ",2,-1,1
1480084498,227407,2023/3/28,v3.4.0-rc4,"@dongjoon-hyun and @sunchao , thank you for the commit and the warm welcome!","1,-1    ",1,-1,0
1478801856,1938382,2023/3/28,v3.4.0-rc4,@hvanhovell @cloud-fan ,"1,-1    ",1,-1,0
1479016875,6477701,2023/3/28,v3.4.0-rc4,Merged to branch-3.4.,"1,-1    ",1,-1,0
1478884594,7322292,2023/3/28,v3.4.0-rc4,"would you mind adding a simple test here? 
https://github.com/apache/spark/blob/149e020a5ca88b2db9c56a9d48e0c1c896b57069/python/pyspark/sql/tests/connect/test_connect_function.py#L1077-L1089","1,-1    ",1,-1,0
1479016249,6477701,2023/3/28,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1480012982,9616802,2023/3/28,v3.4.0-rc4,"@dongjoon-hyun officially is a bit a broad term. As far as I am concerned ammonite is just a way to use the connect JVM client, it is not meant as a change for all of Spark (although I do think it might make sense). We use other libraries all the time, and I don't think we have ever discussed those in the mailing list.

As it stands there are two reasons for using ammonite:
- It is a better and easier to use REPL than the Scala REPL ever was.
- It is easier to customize. Connect is a good illustration, for the Scala REPL I will probably need to the same thing as we did for the spark REPL project and that is fork the source code. In ammonite this was a minor modification.

I do intent to add support for the regular REPL at some point, this was just easier to get started with.

If you feel strongly about this then I am happy to send a message to the dev-list.","1,-1    ",1,-1,0
1480034164,9700541,2023/3/28,v3.4.0-rc4,"Ya, I'm not against this nice improvement. Just please shoot one email to the dev mailing list to give a headup. That's what I'm thinking that we need.","1,-1    ",1,-1,0
1480397842,9616802,2023/3/28,v3.4.0-rc4,@dongjoon-hyun I send the email.,"1,-1    ",1,-1,0
1480398301,9700541,2023/3/28,v3.4.0-rc4,"Thank you so much, @hvanhovell .","1,-4    ",1,-4,-3
1481296666,1475305,2023/3/28,v3.4.0-rc4,"Interesting!

","1,-1    ",1,-1,0
1483149265,9616802,2023/3/28,v3.4.0-rc4,Merging this one.,"1,-1    ",1,-1,0
1478944069,1475305,2023/3/28,v3.4.0-rc4,cc @HyukjinKwon @hvanhovell FYI,"1,-1    ",1,-1,0
1479928027,9700541,2023/3/28,v3.4.0-rc4,"Merged to master/3.4.
Thank you, @LuciferYang and @HyukjinKwon .","1,-1    ",1,-1,0
1480402878,1475305,2023/3/28,v3.4.0-rc4,Thanks @dongjoon-hyun @HyukjinKwon ,"1,-1    ",1,-1,0
1478990235,7322292,2023/3/28,v3.4.0-rc4,cc @HyukjinKwon @WeichenXu123 ,"1,-1    ",1,-1,0
1479160189,7322292,2023/3/28,v3.4.0-rc4,merged into master,"1,-1    ",1,-1,0
1480402574,1475305,2023/3/28,v3.4.0-rc4,rebase due to https://github.com/apache/spark/pull/40516 merged,"1,-1    ",1,-1,0
1480408180,506656,2023/3/28,v3.4.0-rc4,@LuciferYang nit: you need to update the PR description. There is an old file name `storage_level.proto`.,"1,-1    ",1,-1,0
1480437248,1475305,2023/3/28,v3.4.0-rc4,"> @LuciferYang nit: you need to update the PR description. There is an old file name `storage_level.proto`.

Thanks ~ fixed","1,-1    ",1,-1,0
1480438169,1475305,2023/3/28,v3.4.0-rc4,"> @LuciferYang . This looks worthy of having a new JIRA. Please create a new JIRA for this PR and use it. This PR is a good contribution of yours. ð

@dongjoon-hyun Thanks for your suggestion ~ I created SPARK-42901 and updated the pr title ð","1,-1    ",1,-1,0
1480556858,1475305,2023/3/28,v3.4.0-rc4,GA passed ~,"1,-2    ",1,-2,-1
1480558008,6477701,2023/3/28,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1480591952,1475305,2023/3/28,v3.4.0-rc4,Thanks @HyukjinKwon @dongjoon-hyun @ueshin ,"1,-1    ",1,-1,0
1481795559,9700541,2023/3/28,v3.4.0-rc4,"+1, late LGTM.","1,-1    ",1,-1,0
1479933229,9700541,2023/3/28,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1479940012,9700541,2023/3/28,v3.4.0-rc4,branch-3.4 is handled via https://github.com/apache/spark/pull/40500 yesterday.,"1,-1    ",1,-1,0
1480471132,7322292,2023/3/28,v3.4.0-rc4,thanks for reivews,"1,-1    ",1,-1,0
1480428307,19235986,2023/3/28,v3.4.0-rc4,"To address @HyukjinKwon 's concern about optimizer, 

can we add `is_barrier` attribute into `UnaryNode`,
and if optimizer find a node marking `is_barrier` as True, then skip all optimizations around the node.

CC @cloud-fan @mengxr WDYT ?


Barrier mode is only used in specific ML case, i.e. in model training routine, we will only use it in one pattern:

`dataset.mapInPandas(..., is_barrier=True).collect()`

and we don't need complex optimization for it.","1,-1    ",1,-1,0
1480433026,3182036,2023/3/28,v3.4.0-rc4,"hmmm why do we need to care about the optimizer? The optimizer is not sensitive to the physical execution engine, e.g. Presto, Spark, Flink have many similar SQL optimizations.","2,-1    ",2,-1,1
1480433501,19235986,2023/3/28,v3.4.0-rc4,"> hmmm why do we need to care about the optimizer? The optimizer is not sensitive to the physical execution engine, e.g. Preso, Spark, Flink have many similar SQL optimizations.

I am not familier with optimizer details but it is concern from @HyukjinKwon 
But note this PR also changes logical plan operator like `MapInPandas`","1,-1    ",1,-1,0
1480443966,6477701,2023/3/28,v3.4.0-rc4,"Predicate pushdown is just an example. e.g., you might want to combine adjacent `MapInPandas`s but it would need a special handling if `is_barrier` flag is added. ","1,-1    ",1,-1,0
1480444902,6477701,2023/3/28,v3.4.0-rc4,"I am saying that real power of Catalyst optimizer is to optimize/reorder these logical plans, and I believe that's the reason why barrier execution wasn't introduced in SQL. The barrier has to be created exactly when the call is invoked, in which basically it requires something like https://github.com/apache/spark/pull/19873 to have a sound implementation.

However, I am fine with having this as an exception if you guys are fine with this.","1,-1    ",1,-1,0
1480490580,7322292,2023/3/28,v3.4.0-rc4,"> Barrier mode is only used in specific ML case, i.e. in model training routine, we will only use it in one pattern:
> 
> dataset.mapInPandas(..., is_barrier=True).collect()

> To simply the implementation, we can implement a barrierMapInPandasAndCollect instead, and define a execution plan stage like BarrierMapInPandasAndCollectExec

If it is the only usage case, i think it will be safe to add dedicated logical plan and physical plan for it.","1,-1    ",1,-1,0
1480510947,19235986,2023/3/28,v3.4.0-rc4,"> I am saying that real power of Catalyst optimizer is to optimize/reorder these logical plans, and I believe that's the reason why barrier execution wasn't introduced in SQL. The barrier has to be created exactly when the call is invoked, in which basically it requires something like #19873 to have a sound implementation.
> 
> However, I am fine with having this as an exception if you guys are fine with this.

@cloud-fan What do you think of this ?","1,-1    ",1,-1,0
1480533198,3182036,2023/3/29,v3.4.0-rc4,"From a SQL engine's point of view, running all tasks at once or batch by batch doesn't matter. It doesn't change the semantics of the SQL operator, and the optimizer doesn't care about it. However, `mapInPandas` is a public API and you are free to define what's the expectation of the `is_barrier` parameter. To make our life easier, we can just define it as ""the tasks of running the pandas function must all be launched at once"", and it's not a barrier to the SQL operators. Then I think it's fine to just add a flag to the existing `MapInPandas` operator.","1,-1    ",1,-1,0
1484344658,19235986,2023/3/29,v3.4.0-rc4,merged to master :),"1,-1    ",1,-1,0
1479538696,26707386,2023/3/29,v3.4.0-rc4,"@zhengruifeng Hi, Would you mind have a review?","1,-1    ",1,-1,0
1480411932,6477701,2023/3/29,v3.4.0-rc4,Those are actually not real JIRAs or TODOs. These are the pointers of the original fix or ticket (that contains examples or code change). So I guess it's fine as is.,"1,-1    ",1,-1,0
1480513582,7322292,2023/3/29,v3.4.0-rc4,"I think it is fine if we don't have avaliable ticket link.
It seems that those links point to issues before moving Kolas to Apache Spark.","1,-1    ",1,-1,0
1480537486,26707386,2023/3/29,v3.4.0-rc4,"Thanks for your review. I think some examples that cannot be linked to the correct places, which can make confused, and the original points provides some clear references.","1,-1    ",1,-1,0
1486003187,6477701,2023/3/29,v3.4.0-rc4,Merged to master.,"1,-1    ",1,-1,0
1486035003,9700541,2023/3/29,v3.4.0-rc4,"Got it. Thanks. +1, LGTM too.","1,-1    ",1,-1,0
1479609724,3182036,2023/3/29,v3.4.0-rc4,cc @ulysses-you ,"2,-1    ",2,-1,1
1480466232,12025282,2023/3/29,v3.4.0-rc4,lgtm,"1,-1    ",1,-1,0
1482290465,3182036,2023/3/29,v3.4.0-rc4,"thanks for review, merging to master!","1,-1    ",1,-1,0
1486090996,3626747,2023/3/29,v3.4.0-rc4,"Hi, @c21 @cloud-fan  this seems to be SMJ full outer join codegen bug, could you have a look at this issue ? Thanks","1,-1    ",1,-1,0
1480055444,3187938,2023/3/29,v3.4.0-rc4,"LGTM, I would just add a unit test to CastSuite with relevant asserts for `Cast.needsTimeZone(from, to)` to prevent regressions","1,-1    ",1,-1,0
1483436510,3441321,2023/3/29,v3.4.0-rc4,"@MaxGekk This might take a little while. I don't want to delete the test, but I also cannot just switch the data types over to Timestamp because the partition string is not always stored in a format that is compatible with TimestampType. So I ma going to have to spend some more time understanding the test to see if there is a good way to update it for TimestampType.","1,-1    ",1,-1,0
1480119115,44108233,2023/3/29,v3.4.0-rc4,"The remaining task at hand is to address [numerous mypy annotation issues](https://github.com/itholic/spark/actions/runs/4493385095/jobs/7904557221). If you have any good ideas for resolving linter, please feel free to let me know at any time :-)","1,-1    ",1,-1,0
1489771881,44108233,2023/3/29,v3.4.0-rc4,"CI passed. cc @HyukjinKwon @ueshin @xinrong-meng @zhengruifeng PTAL when you find some time.

I summarized key changes into PR description for review.","1,-1    ",1,-1,0
1495297078,44108233,2023/3/29,v3.4.0-rc4,Applied the comments. Thanks @HyukjinKwon and @zhengruifeng  for the review.,"1,-1    ",1,-1,0
1495403113,6477701,2023/3/29,v3.4.0-rc4,LGTM if tests pass,"1,-1    ",1,-1,0
1500814281,6477701,2023/3/29,v3.4.0-rc4,Merged to master.,"1,-2    ",1,-2,-1
1500961292,47577197,2023/3/29,v3.4.0-rc4,"@itholic Thank you, great work :) 

After this PR 
`from pyspark import pandas as ps `

ModuleNotFoundError                       Traceback (most recent call last)
File /opt/spark/python/pyspark/sql/connect/utils.py:45, in require_minimum_grpc_version()
     44 try:
---> 45     import grpc
     46 except ImportError as error:

ModuleNotFoundError: No module named 'grpc'

The above exception was the direct cause of the following exception:

ImportError                               Traceback (most recent call last)
Cell In[1], line 11
      9 import pyarrow
     10 from pyspark import SparkConf, SparkContext
---> 11 from pyspark import pandas as ps
     12 from pyspark.sql import SparkSession
     13 from pyspark.sql.functions import col, concat, concat_ws, expr, lit, trim

File /opt/spark/python/pyspark/pandas/__init__.py:59
     50     warnings.warn(
     51         ""'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to ""
     52         ""set this environment variable to '1' in both driver and executor sides if you use ""
   (...)
     55         ""already launched.""
     56     )
     57     os.environ[""PYARROW_IGNORE_TIMEZONE""] = ""1""
---> 59 from pyspark.pandas.frame import DataFrame
     60 from pyspark.pandas.indexes.base import Index
     61 from pyspark.pandas.indexes.category import CategoricalIndex

File /opt/spark/python/pyspark/pandas/frame.py:88
     85 from pyspark.sql.window import Window
     87 from pyspark import pandas as ps  # For running doctests and reference resolution in PyCharm.
---> 88 from pyspark.pandas._typing import (
     89     Axis,
     90     DataFrameOrSeries,
     91     Dtype,
     92     Label,
     93     Name,
     94     Scalar,
     95     T,
     96     GenericColumn,
     97 )
     98 from pyspark.pandas.accessors import PandasOnSparkFrameMethods
     99 from pyspark.pandas.config import option_context, get_option

File /opt/spark/python/pyspark/pandas/_typing.py:25
     22 from pandas.api.extensions import ExtensionDtype
     24 from pyspark.sql.column import Column as PySparkColumn
---> 25 from pyspark.sql.connect.column import Column as ConnectColumn
     26 from pyspark.sql.dataframe import DataFrame as PySparkDataFrame
     27 from pyspark.sql.connect.dataframe import DataFrame as ConnectDataFrame

File /opt/spark/python/pyspark/sql/connect/column.py:19
      1 #
      2 # Licensed to the Apache Software Foundation (ASF) under one or more
      3 # contributor license agreements.  See the NOTICE file distributed with
   (...)
     15 # limitations under the License.
     16 #
     17 from pyspark.sql.connect.utils import check_dependencies
---> 19 check_dependencies(__name__)
     21 import datetime
     22 import decimal

File /opt/spark/python/pyspark/sql/connect/utils.py:35, in check_dependencies(mod_name)
     33 require_minimum_pandas_version()
     34 require_minimum_pyarrow_version()
---> 35 require_minimum_grpc_version()

File /opt/spark/python/pyspark/sql/connect/utils.py:47, in require_minimum_grpc_version()
     45     import grpc
     46 except ImportError as error:
---> 47     raise ImportError(
     48         ""grpc >= %s must be installed; however, "" ""it was not found."" % minimum_grpc_version
     49     ) from error
     50 if LooseVersion(grpc.__version__) < LooseVersion(minimum_grpc_version):
     51     raise ImportError(
     52         ""gRPC >= %s must be installed; however, ""
     53         ""your version was %s."" % (minimum_grpc_version, grpc.__version__)
     54     )

ImportError: grpc >= 1.48.1 must be installed; however, it was not found.        

`pip install grpc`

Collecting grpc
  Downloading grpc-1.0.0.tar.gz (5.2 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  Ã python setup.py egg_info did not run successfully.
  ï¿½?exit code: 1
  â°â> [6 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-vp4d8s4c/grpc_c0f1992ad8f7456b8ac09ecbaeb81750/setup.py"", line 33, in <module>
          raise RuntimeError(HINT)
      RuntimeError: Please install the official package with: pip install grpcio
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

Ã Encountered error while generating package metadata.
â°â> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
Note: you may need to restart the kernel to use updated packages.

After 
`pip install grpcio`  
then it works. 


I don't think that ever pandas users that try pandas API on spark will use spark connect. So can we change this back? ","1,-2    ",1,-2,-1
1501047078,44108233,2023/3/29,v3.4.0-rc4,"Thank you for the feedback, @bjornjorgensen !

IMHO, it seems more reasonable to add `grpcio` as a dependency for the Pandas API on Spark instead of reverting all this change back (Oh, btw seems like you already open a https://github.com/apache/spark/pull/40716 for fixing the existing issue ð)

From my understanding, the purpose of Spark Connect is to allow users to use existing PySpark project without any code changes through a remote client. Therefore, if a user is using the `pyspark.pandas` module in their existing code, it should work the same way through the remote client as well.

So I think we should support all the functionality of PySpark as much as possible including pandas API on Spark, since nobody can not sure whether existing PySpark users will use the Pandas API on Spark through Spark Connect or not at this point, not only the existing pandas users.

Alternatively, we might be able to create completely separate package path for the Pandas API on Spark for Spark Connect. This would allow the existing Pandas API on Spark to be used without installing `grpcio`, but IMHO it would be much more overhead than simply changing the policy to add one package as an additional installation.

WDYT? also cc @HyukjinKwon @grundprinzip @ueshin @zhengruifeng FYI","1,-1    ",1,-1,0
1501047901,6477701,2023/3/29,v3.4.0-rc4,The problem is that you don't use Spark Connect but it complains that it needs `grpcio`,"2,-1    ",2,-1,1
1501048286,6477701,2023/3/29,v3.4.0-rc4,"For example, you can't just import `from pyspark.sql.connect.column import Column as ConnectColumn` at `pyspark/pandas/_typing.py`. Even you don't use Spark connect, it will check the dependency. Can we avoid this?","1,-1    ",1,-1,0
1501097772,44108233,2023/3/29,v3.4.0-rc4,"Got it. Then I think we need to modify the current code to import the `ConnectColumn` only when `is_remote()` is `True` across all code paths. For example:

**Before**
```python
from pyspark.sql.column import Column as PySparkColumn
from pyspark.sql.connect.column import Column as ConnectColumn
from pyspark.sql.utils import is_remote

def example():
    Column = ConnectColumn if is_remote() else PySparkColumn
    return Column.__eq__
```

**After**
```python
from pyspark.sql.column import Column as PySparkColumn
from pyspark.sql.utils import is_remote

def example():
    if is_remote():
        from pyspark.sql.connect.column import Column as ConnectColumn
        Column = ConnectColumn
    else:
        Column = PySparkColumn
    return Column.__eq__
```

Also, we should remove `GenericColumn` and `GenericDataFrame`.

Can you happen to think of a better solution? As of now, I haven't come up with a better way other than this.

Thanks!","2,-1    ",2,-1,1
1501106942,47577197,2023/3/29,v3.4.0-rc4,"""The problem is that you don't use Spark Connect but it complains that it needs grpcio""
Yes, this is correct :) 
https://github.com/apache/spark/pull/40716 was for a typo so users of connect will know how to install the package.

The is_remote() can probably work. 
Perhaps we can look at how pyspark did solve this?   ","2,-1    ",2,-1,1
1501118977,3421,2023/3/29,v3.4.0-rc4,"We need to be careful in changing the imports too early or too late. The way that the is remote functionality works checks for an environment variable. If it's not set during the import setting it later yields undesired consequences. 

Is it so had to add the dependency for grpc when using the pandas API? 

What are we achieving with this? GRPC is a stable protocol and not a random library. It's available throughout all platforms. 

What's the benefit of trying this pure approach?","1,-1    ",1,-1,0
1501121526,47577197,2023/3/29,v3.4.0-rc4,"One of the key thing with pandas API on spark is that users only have to change `pd` to `ps` in the import. But now they also have to install GRPC. 

Does this PR introduce any user-facing change?
Yes, every pandas API on spark user need to install GRPC. ","1,-1    ",1,-1,0
1501126555,3421,2023/3/29,v3.4.0-rc4,"That's only partially true, they still have to install PySpark. If they pip install PySpark[connect] the dependencies are properly resolved.","2,-1    ",2,-1,1
1501127314,3421,2023/3/29,v3.4.0-rc4,"Actually even today Pandas on Spark users have to install specific dependencies that are not part of the default PySpark installation. 

See https://github.com/apache/spark/blob/master/python/setup.py

Now, the quickest fix is to simply add grpcio to the pandas on spark dependency list. 

Given that we're forcing Pandas users into specific Java versions etc adding grpcio does not make it worse in my opinion. 

The user surface remains the same. ","1,-1    ",1,-1,0
1501132941,47577197,2023/3/29,v3.4.0-rc4,"Those modules that pandas Api on spark needs are pandas, pyarrow, numpy. Witch every pandas users already have installed. 
And now they have to add connect, grpcio, grpcio-status and googleapis-common-protos.

Why does pandas API on spark need such high coupling on the connect module?   ","1,-1    ",1,-1,0
1501140136,3421,2023/3/29,v3.4.0-rc4,"IIUC only grpcio is needed. The rest is localized to the spark connect client. 

","1,-1    ",1,-1,0
1501261686,6477701,2023/3/29,v3.4.0-rc4,"> Is it so had to add the dependency for grpc when using the pandas API?

It's not super hard. But it's a bit odd to add this alone to pandas API on Spark. We should probably think about adding grpcio as a hard dependency for whole PySpark project, but definitely not alone for pandas API on Spark.

> What are we achieving with this? GRPC is a stable protocol and not a random library. It's available throughout all platforms.
> What's the benefit of trying this pure approach?

So for the current status, we're trying to add the dependencies that the module need so users won't need to install the unnecessary dependency. In addition, adding the dependency breaks existing applications when they migrate from 3.4 to 3.5. It matters when PySpark is installed without `pip` (which is actually the official release channel of Apache Spark).

","1,-1    ",1,-1,0
1501262560,6477701,2023/3/29,v3.4.0-rc4,So my proposal is to keep it consistent for now (that dose not require `grpcio` when we don't use Spark Connect). And separately discuss if we should switch them to a hard required dependency.,"1,-1    ",1,-1,0
1501265948,44108233,2023/3/29,v3.4.0-rc4,"> We should probably think about adding grpcio as a hard dependency for whole PySpark project, but definitely not alone for pandas API on Spark

I think this point is particularly crucial even though all of your opinions make sense to me.
To summarize, I will made a fix following the suggestion in https://github.com/apache/spark/pull/40525#issuecomment-1501097772 so that the Pandas API on Spark can work without `grpcio` for now. Then, we can discuss adding the `grpcio` dependency to the entire PySpark project if needed in the future.
Thank you for all the feedback!","1,-1    ",1,-1,0
1480485696,6477701,2023/3/29,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1480536077,3182036,2023/3/29,v3.4.0-rc4,late LGTM,"1,-1    ",1,-1,0
1480472824,7322292,2023/3/29,v3.4.0-rc4,also cc @WeichenXu123 ,"1,-1    ",1,-1,0
1480796168,7322292,2023/3/29,v3.4.0-rc4,"thank you @ueshin  for resolve this issue !

merged into master/branch-3.4","1,-1    ",1,-1,0
1480509383,8486025,2023/3/29,v3.4.0-rc4,"This PR has not been implemented yet. But we can see the implementation method.
@hvanhovell Could you take a look? Can this meet your expectations?

cc @HyukjinKwon @ueshin @zhengruifeng ","2,-1    ",2,-1,1
1484985770,8486025,2023/3/29,v3.4.0-rc4,ping @hvanhovell ,"2,-1    ",2,-1,1
1517382928,8486025,2023/3/29,v3.4.0-rc4,ping @hvanhovell Could you have time to take a look?,"2,-1    ",2,-1,1
1558462353,8486025,2023/3/29,v3.4.0-rc4,@hvanhovell Could you take a look again?,"2,-1    ",2,-1,1
1480663695,6477701,2023/3/29,v3.4.0-rc4,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1480688891,8326978,2023/3/29,v3.4.0-rc4,cc @HyukjinKwon @cloud-fan @dongjoon-hyun thanks,"3,-1    ",3,-1,2
1482330150,8326978,2023/3/29,v3.4.0-rc4,"thanks, merged to master and 3.4","1,-2    ",1,-2,-1
1481110368,6477701,2023/3/29,v3.4.0-rc4,Merged to master and branch-3.4.,"3,-1    ",3,-1,2
1480723613,26535726,2023/3/29,v3.4.0-rc4,cc @dongjoon-hyun @yaooqinn @Yikun ,"2,-1    ",2,-1,1
1483677257,26535726,2023/3/29,v3.4.0-rc4,"> To @pan3793 and @yaooqinn . IMO, what we need is only one additional line at the end of the replacement. WDYT?
> 
> ```scala
> .replaceAll(""^[0-9]"", ""x"")
> ```

Yes, this approach does not introduce breaking change, updated as suggested","1,-2    ",1,-2,-1
1483728354,8326978,2023/3/30,v3.4.0-rc5,"Hi @dongjoon-hyun thanks for pinging me, SGTM","1,-1    ",1,-1,0
1485948727,9700541,2023/3/30,v3.4.0-rc5,Merged to master/3.4/3.3/3.2.,"1,-1    ",1,-1,0
1486246281,6477701,2023/3/30,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1480782795,7322292,2023/3/30,v3.4.0-rc5,"It doesn't have UT in vanilla PySpark, so I only add the doctest","1,-1    ",1,-1,0
1487795766,6477701,2023/3/30,v3.4.0-rc5,Merged to master.,"3,-1    ",3,-1,2
1487796203,6477701,2023/3/30,v3.4.0-rc5,It has a conflict with branch-3.4. mind creating a backport please?,"2,-1    ",2,-1,1
1487819994,7322292,2023/3/30,v3.4.0-rc5,"@HyukjinKwon Thanks for reviews. What about only adding it to master? since it changes the dependency, so I want to be a bit conservative :)","1,-1    ",1,-1,0
1487842406,6477701,2023/3/30,v3.4.0-rc5,Sure that's fine.,"3,-2    ",3,-2,1
1488120272,6477701,2023/3/30,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1482143524,6477701,2023/3/30,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1483184075,1938382,2023/3/30,v3.4.0-rc5,Late LGTM!,"1,-1    ",1,-1,0
1482078921,506656,2023/3/30,v3.4.0-rc5,cc @itholic ,"1,-1    ",1,-1,0
1482086635,44108233,2023/3/30,v3.4.0-rc5,"btw, the examples in ""Does this PR introduce any user-facing change?"" are the same??","1,-2    ",1,-2,-1
1482126322,506656,2023/3/30,v3.4.0-rc5,"> the examples in ""Does this PR introduce any user-facing change?"" are the same??

No, previously we still see `py4j.protocol.Py4JJavaError` or `SparkConnectGrpcException` and now we only see the actual exception classes.","1,-3    ",1,-3,-2
1482146445,44108233,2023/3/30,v3.4.0-rc5,Ah I see. One for regular Spark session and the other for remote Spark session.,"1,-2    ",1,-2,-1
1482558850,6477701,2023/3/30,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1482560940,6477701,2023/3/30,v3.4.0-rc5,@ueshin it has a conflict w/ branch-3.4. would you mind creating a backport PR?,"1,-1    ",1,-1,0
1483241667,506656,2023/3/30,v3.4.0-rc5,@HyukjinKwon #40547,"3,-1    ",3,-1,2
1482442079,47337188,2023/3/30,v3.4.0-rc5,"Merged to branch-3.4, thanks!","4,-1    ",4,-1,3
1482222273,6477701,2023/3/30,v3.4.0-rc5,cc @zhengruifeng ,"1,-1    ",1,-1,0
1482441552,44108233,2023/3/30,v3.4.0-rc5,Thanks for the review @zhengruifeng ! Will address the comments soon,"1,-1    ",1,-1,0
1483670465,44108233,2023/3/30,v3.4.0-rc5,Updated!,"1,-1    ",1,-1,0
1484293536,6477701,2023/3/30,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1484293829,6477701,2023/3/30,v3.4.0-rc5,@itholic it has a conflict against branch-3.4. Mind creating a backport PR please,"1,-1    ",1,-1,0
1484321936,44108233,2023/3/30,v3.4.0-rc5,Backport: https://github.com/apache/spark/pull/40554,"2,-1    ",2,-1,1
1482252582,3182036,2023/3/30,v3.4.0-rc5,@xinrong-meng ,"1,-1    ",1,-1,0
1482332207,8326978,2023/3/30,v3.4.0-rc5,"thanks, merged to master and 3.4","1,-1    ",1,-1,0
1482531153,8326978,2023/3/30,v3.4.0-rc5,cc @cloud-fan @HyukjinKwon @dongjoon-hyun thanks,"1,-1    ",1,-1,0
1486625345,8326978,2023/3/30,v3.4.0-rc5,"> If there is a DBMS-specific magic-number, shall we make a constant for that in our directs?

I agree. But the LOC may be trivial, while the behavior change is critical. So I suggest we introduce another thread for this change.","1,-1    ",1,-1,0
1504411190,8326978,2023/3/30,v3.4.0-rc5,"cc @cloud-fan @dongjoon-hyun 
After SPARK-43049 and SPARK-42943, which bidirectionally redirect StringType to other types, the round-trip char/varchar type mapping can be safely applied. Please retake a look.","1,-3    ",1,-3,-2
1506369606,8326978,2023/3/30,v3.4.0-rc5,"thanks, merged to master","1,-1    ",1,-1,0
1482580099,8326978,2023/3/30,v3.4.0-rc5,cc @cloud-fan @HyukjinKwon @dongjoon-hyun thanks,"2,-1    ",2,-1,1
1482882715,8326978,2023/3/30,v3.4.0-rc5,"thanks, merged to master and 3.4","3,-1    ",3,-1,2
1491879620,3182036,2023/3/30,v3.4.0-rc5,"thanks, merging to master!","1,-1    ",1,-1,0
1484424269,3182036,2023/3/30,v3.4.0-rc5,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1484292338,6477701,2023/3/30,v3.4.0-rc5,Merged to branch-3.4.,"2,-1    ",2,-1,1
1483720147,1591700,2023/3/30,v3.4.0-rc5,There is a minor build failure (unused import) - but please feel free to merge after fixing it and CI passes.,"1,-1    ",1,-1,0
1483915663,1097932,2023/3/30,v3.4.0-rc5,"@mridulm Thanks for the review.
Merging to master","1,-1    ",1,-1,0
1484302193,6477701,2023/3/30,v3.4.0-rc5,Merged to master and branch-3.4.,"2,-2    ",2,-2,0
1484327671,7322292,2023/3/30,v3.4.0-rc5,late LGTM,"2,-1    ",2,-1,1
1483701403,1475305,2023/3/30,v3.4.0-rc5,cc @dtenedor @HyukjinKwon FYI,"3,-1    ",3,-1,2
1483702380,99207096,2023/3/31,v3.4.0-rc5,I am so sorry to break the build again :| thanks for fixing it! It looks like we need separate regular and ANSI test cases now!,"1,-1    ",1,-1,0
1484383367,6477701,2023/3/31,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1484384779,1475305,2023/3/31,v3.4.0-rc5,Thanks @HyukjinKwon @dtenedor ,"1,-1    ",1,-1,0
1484000722,15109987,2023/3/31,v3.4.0-rc5,@rxin @ScrapCodes @maropu  can you please review . thanks!,"1,-1    ",1,-1,0
1489811022,992952,2023/3/31,v3.4.0-rc5,"Hi @VindhyaG, this might be useful - may be we can benefit from the usecase you have for this. Is it just for logging? 
Not sure what others think, it might be good to limit the API surface. ","1,-1    ",1,-1,0
1490227613,15109987,2023/3/31,v3.4.0-rc5,"> Hi @VindhyaG, this might be useful - may be we can benefit from the usecase you have for this. Is it just for logging? Not sure what others think, it might be good to limit the API surface.

@ScrapCodes thanks for reviewing. Bug lists three ways API could be used which i have added in PR description above. Do you have any suggestions to limit API surface?","1,-1    ",1,-1,0
1490453383,992952,2023/3/31,v3.4.0-rc5,"I see this as developer facing API, So just having 
```
def getString(numRows: Int, truncate: Int): String =
     getString(numRows, truncate, vertical = false)
```
would suffice.","1,-1    ",1,-1,0
1490457386,992952,2023/3/31,v3.4.0-rc5,"Do you think, a more interesting way can be returning a JSON representation?","1,-1    ",1,-1,0
1490821588,15109987,2023/3/31,v3.4.0-rc5,"> I see this as developer facing API, So just having
> 
> ```
> def getString(numRows: Int, truncate: Int): String =
>      getString(numRows, truncate, vertical = false)
> ```
> 
> would suffice.

my intention was to keep it consistent with show() where if numrows and truncate have default values ","1,-1    ",1,-1,0
1490823634,15109987,2023/3/31,v3.4.0-rc5,"> Do you think, a more interesting way can be returning a JSON representation?
@ScrapCodes  If use case is to send the it via rest api(as listed in use case above) JSON would make more sense but for logging i suppose string in tabular form is more useful? may be have a separate API for that ?","1,-1    ",1,-1,0
1497599094,15109987,2023/3/31,v3.4.0-rc5,"@ScrapCodes can you pls suggest how can we go ahead on this?
@jaceklaskowski  have updated the versions. can you pls review again.","1,-1    ",1,-1,0
1484321981,44108233,2023/3/31,v3.4.0-rc5,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1484345121,6477701,2023/3/31,v3.4.0-rc5,Oh im sorry. Actually we don't need this for 3.4 since pandas API in spark will be in 3.5 only,"1,-1    ",1,-1,0
1484345630,44108233,2023/3/31,v3.4.0-rc5,np!,"1,-1    ",1,-1,0
1492793200,9700541,2023/3/31,v3.4.0-rc5,"Shall we close this PR because Apache Parquet 1.12.4 vote seems to fail due to the technical issue.

For the record, the release manage creates it from the wrong branch.
- https://github.com/apache/parquet-mr/commit/22069e58494e7cb5d50e664c7ffa1cf1468404f8
```
- <version>1.13.0-SNAPSHOT</version>
+ <version>1.12.4</version>
```","1,-1    ",1,-1,0
1499901243,9700541,2023/3/31,v3.4.0-rc5,Thank you for updating.,"1,-1    ",1,-1,0
1501603107,5399861,2023/3/31,v3.4.0-rc5,"TPC-DS benchmark result:
Query | Parquet 1.13.0(first time) | Parquet 1.12.3(first time) | Parquet 1.13.0(second time) | Parquet 1.12.3(second time) | Parquet 1.13.0(third time) | Parquet 1.12.3(third time)
-- | -- | -- | -- | -- | -- | --
q1.sql | 37.819 | 37.786 | 36.322 | 37.59 | 37.772 | 36.776
q2.sql | 42.132 | 41.513 | 43.189 | 42.274 | 42.859 | 42.605
q3.sql | 5.933 | 6.1 | 6.082 | 6.071 | 6.128 | 6.094
q4.sql | 335.051 | 319.173 | 322.396 | 320.977 | 324.464 | 326.822
q5.sql | 78.41 | 76.631 | 76.841 | 76.37 | 78.257 | 76.502
q6.sql | 9.006 | 9.11 | 8.737 | 8.577 | 8.729 | 9.05
q7.sql | 12.881 | 12.731 | 12.685 | 12.662 | 12.606 | 12.675
q8.sql | 10.122 | 10.092 | 10.035 | 10.853 | 10.277 | 10.841
q9.sql | 72.562 | 71.942 | 73.649 | 73.04 | 72.899 | 72.01
q10.sql | 14.127 | 13.075 | 14.276 | 13.913 | 13.281 | 13.229
q11.sql | 111.334 | 111.612 | 110.952 | 110.776 | 111.686 | 112.27
q12.sql | 3.138 | 3.854 | 3.187 | 3.613 | 3.437 | 3.306
q13.sql | 13.131 | 12.676 | 12.516 | 12.417 | 12.739 | 12.987
q14a.sql | 217.664 | 213.632 | 214.655 | 213.333 | 217.601 | 213.341
q14b.sql | 191.553 | 182.775 | 184.35 | 187.004 | 188.313 | 189.876
q15.sql | 10.308 | 10.46 | 10.304 | 9.901 | 10.175 | 10.307
q16.sql | 81.97 | 82.059 | 82.41 | 81.263 | 83.179 | 82.042
q17.sql | 28.876 | 28.905 | 30.41 | 29.573 | 29.555 | 28.837
q18.sql | 14.183 | 13.929 | 14.11 | 14.466 | 13.969 | 14.022
q19.sql | 6.611 | 7.593 | 6.652 | 6.659 | 6.446 | 6.533
q20.sql | 3.263 | 3.701 | 3.56 | 3.503 | 3.53 | 3.627
q21.sql | 2.252 | 2.188 | 2.249 | 2.128 | 2.161 | 2.252
q22.sql | 14.809 | 14.715 | 14.324 | 14.266 | 14.567 | 14.123
q23a.sql | 554.385 | 544.75 | 546.213 | 542.194 | 553.784 | 547.388
q23b.sql | 781.236 | 768.367 | 770.584 | 776.065 | 776.502 | 776.006
q24a.sql | 196.806 | 193.989 | 197.608 | 194.416 | 194.71 | 192.817
q24b.sql | 176.56 | 183.084 | 177.486 | 177.936 | 177.776 | 177.389
q25.sql | 22.323 | 22.089 | 22.665 | 22.049 | 22.248 | 22.317
q26.sql | 8.574 | 8.356 | 8.174 | 8.753 | 8.186 | 8.302
q27.sql | 9.056 | 8.252 | 8.37 | 8.319 | 8.516 | 8.38
q28.sql | 102.185 | 102.382 | 102.344 | 103.058 | 102.024 | 102.786
q29.sql | 75.655 | 75.604 | 75.217 | 75.532 | 75.835 | 76.024
q30.sql | 12.476 | 12.966 | 13.039 | 14.108 | 12.19 | 13.143
q31.sql | 26.343 | 27.632 | 26.337 | 26.791 | 26.74 | 26.098
q32.sql | 3.251 | 3.41 | 3.378 | 3.333 | 3.371 | 3.516
q33.sql | 7.143 | 6.125 | 6.85 | 6.718 | 7.067 | 6.615
q34.sql | 8.53 | 8.656 | 8.536 | 8.866 | 8.358 | 8.589
q35.sql | 35.212 | 35.571 | 35.659 | 37.631 | 36.292 | 35.603
q36.sql | 9.264 | 9.166 | 9.748 | 9.488 | 9.45 | 9.469
q37.sql | 36.368 | 35.881 | 37.023 | 36.578 | 35.823 | 36.7
q38.sql | 74.58 | 73.472 | 72.926 | 73.823 | 71.097 | 73.329
q39a.sql | 8.596 | 7.637 | 8.036 | 7.984 | 7.849 | 7.88
q39b.sql | 7.233 | 6.641 | 6.278 | 7.06 | 6.595 | 6.691
q40.sql | 17.34 | 16.558 | 16.448 | 16.864 | 16.432 | 16.413
q41.sql | 1.223 | 1.105 | 1.103 | 1.182 | 1.232 | 1.304
q42.sql | 2.464 | 2.441 | 2.554 | 2.544 | 2.314 | 2.393
q43.sql | 7.477 | 7.396 | 7.394 | 7.764 | 7.381 | 7.534
q44.sql | 30.228 | 30.516 | 30.859 | 31.057 | 30.372 | 29.008
q45.sql | 9.93 | 10.089 | 9.874 | 10.075 | 9.802 | 9.838
q46.sql | 9.544 | 9.949 | 9.503 | 9.755 | 9.395 | 9.25
q47.sql | 27.322 | 26.952 | 26.974 | 26.83 | 27.087 | 26.991
q48.sql | 14.266 | 14.39 | 14.517 | 14.684 | 14.471 | 14.61
q49.sql | 21.279 | 21.733 | 20.286 | 20.945 | 22.388 | 21.52
q50.sql | 191.416 | 194.256 | 196.701 | 194.113 | 193.354 | 191.004
q51.sql | 37.552 | 37.767 | 38.317 | 37.731 | 37.369 | 38.187
q52.sql | 2.206 | 2.406 | 2.235 | 2.362 | 2.337 | 2.278
q53.sql | 5.282 | 5.131 | 5.465 | 5.137 | 5.142 | 5.069
q54.sql | 13.039 | 12.655 | 13.047 | 12.382 | 12.992 | 12.988
q55.sql | 2.534 | 2.39 | 2.375 | 2.867 | 2.623 | 2.546
q56.sql | 7.365 | 7.087 | 6.902 | 7.406 | 7.586 | 7.081
q57.sql | 18.064 | 17.945 | 18.699 | 17.664 | 18.362 | 18.222
q58.sql | 6.198 | 6.702 | 6.109 | 6.211 | 5.9 | 6.101
q59.sql | 28.266 | 28.195 | 27.876 | 28.748 | 29.027 | 28.543
q60.sql | 6.847 | 7.143 | 7.322 | 7.1 | 7.207 | 7.215
q61.sql | 7.258 | 7.62 | 7.317 | 7.781 | 7.616 | 7.669
q62.sql | 10.334 | 11.523 | 10.389 | 10.378 | 10.072 | 10.583
q63.sql | 4.631 | 4.944 | 4.947 | 5.124 | 4.61 | 4.865
q64.sql | 249.694 | 252.117 | 254.359 | 254.813 | 253.236 | 250.401
q65.sql | 78.742 | 79.184 | 78.559 | 78.305 | 78.985 | 78.515
q66.sql | 14.98 | 14.854 | 14.794 | 14.767 | 14.781 | 14.696
q67.sql | 1019.744 | 1048.439 | 987.894 | 972.062 | 927.566 | 1002.206
q68.sql | 8.903 | 8.915 | 8.277 | 8.709 | 9.349 | 9.178
q69.sql | 13.097 | 13.01 | 14.352 | 12.036 | 12.302 | 12.843
q70.sql | 21.175 | 21.085 | 21.102 | 20.471 | 20.129 | 19.678
q71.sql | 15.13 | 15.526 | 14.929 | 15.231 | 15.406 | 15.487
q72.sql | 76.463 | 75.851 | 72.002 | 72.356 | 72.676 | 74.798
q73.sql | 5.894 | 6.09 | 5.877 | 6.051 | 6.365 | 6.634
q74.sql | 99.106 | 99.356 | 100.291 | 99.51 | 96.766 | 97.292
q75.sql | 126.625 | 128.094 | 127.364 | 128.575 | 127.418 | 125.806
q76.sql | 35.172 | 33.601 | 34.752 | 34.764 | 34.228 | 35.748
q77.sql | 8.394 | 8.01 | 7.951 | 8.061 | 7.839 | 8.348
q78.sql | 289.061 | 287.508 | 283.615 | 288.768 | 288.448 | 288.661
q79.sql | 10.048 | 9.251 | 9.396 | 9.81 | 8.607 | 8.341
q80.sql | 59.68 | 59.458 | 60.234 | 60.415 | 61.325 | 60.744
q81.sql | 17.822 | 18.815 | 18.488 | 18.95 | 17.911 | 18.113
q82.sql | 64.781 | 63.957 | 63.621 | 64.38 | 63.637 | 64.488
q83.sql | 4.686 | 4.922 | 4.635 | 4.827 | 4.678 | 5.071
q84.sql | 10.987 | 10.629 | 10.841 | 11.151 | 10.646 | 10.6
q85.sql | 12.689 | 13.304 | 13.362 | 13.19 | 13.779 | 12.657
q86.sql | 6.48 | 6.491 | 6.722 | 6.667 | 6.833 | 6.52
q87.sql | 77.589 | 77.377 | 77.177 | 77.011 | 78.339 | 78.399
q88.sql | 83.876 | 83.676 | 84.044 | 83.761 | 84.201 | 84.089
q89.sql | 6.741 | 6.564 | 6.755 | 6.708 | 6.704 | 6.794
q90.sql | 7.79 | 7.812 | 7.882 | 7.88 | 7.875 | 7.854
q91.sql | 4.072 | 3.728 | 3.883 | 3.976 | 4.151 | 4.035
q92.sql | 3.05 | 3.155 | 3.336 | 3.067 | 2.942 | 3.099
q93.sql | 356.412 | 360.731 | 358.14 | 356 | 356.108 | 358.011
q94.sql | 43.202 | 43.561 | 44.63 | 44.486 | 43.993 | 42.693
q95.sql | 197.185 | 199.657 | 193.975 | 195.843 | 201.801 | 196.113
q96.sql | 12.765 | 12.481 | 12.682 | 12.799 | 12.528 | 12.505
q97.sql | 82.895 | 82.067 | 81.754 | 82.799 | 81.788 | 81.572
q98.sql | 7.338 | 7.066 | 7.133 | 7.005 | 7.254 | 7.047
q99.sql | 18.431 | 17.874 | 17.826 | 17.861 | 17.705 | 17.878
total | 7105.675 | 7091.391 | 7030.209 | 7021.7 | 6992.413 | 7047.295

","1,-1    ",1,-1,0
1502361181,9700541,2023/3/31,v3.4.0-rc5,Thank you for sharing!,"1,-1    ",1,-1,0
1502365927,9700541,2023/3/31,v3.4.0-rc5,"As the first glance, there is no noticeable significant perf difference (in both directions: speedup or regression). What is your opinion, @wangyum ?","1,-1    ",1,-1,0
1502427643,5399861,2023/3/31,v3.4.0-rc5,@dongjoon-hyun Yes. It's no noticeable significant perf difference.,"1,-1    ",1,-1,0
1502428223,9700541,2023/4/1,v3.4.0-rc5,Thank you for the confirmation.,"1,-1    ",1,-1,0
1502429680,9700541,2023/4/1,v3.4.0-rc5,"BTW, if you mind, please revise the PR description.
1. Removing `Maybe it can improve read performance.` from the PR description.
2. Coping https://github.com/apache/spark/pull/40555#issuecomment-1501603107 to the PR descrition.","1,-1    ",1,-1,0
1502458598,5399861,2023/4/1,v3.4.0-rc5,"> BTW, if you mind, please revise the PR description.
> 
> 1. Removing `Maybe it can improve read performance.` from the PR description.
> 2. Coping [[SPARK-42926][BUILD][SQL] Upgrade Parquet to 1.13.0Â #40555 (comment)](https://github.com/apache/spark/pull/40555#issuecomment-1501603107) to the PR descrition.

OK.","1,-1    ",1,-1,0
1509444887,5399861,2023/4/1,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1486554747,1591700,2023/4/1,v3.4.0-rc5,"Looks fine to me, but +CC @srowen ","1,-1    ",1,-1,0
1486963461,822522,2023/4/1,v3.4.0-rc5,Merged to master/3.4,"1,-1    ",1,-1,0
1486964688,1475305,2023/4/1,v3.4.0-rc5,Thanks @mridulm @srowen ,"1,-2    ",1,-2,-1
1484537242,66282705,2023/4/1,v3.4.0-rc5,cc @cloud-fan ,"1,-1    ",1,-1,0
1486452930,3182036,2023/4/1,v3.4.0-rc5,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1485580609,25225043,2023/4/1,v3.4.0-rc5,@cloud-fan ,"3,-2    ",3,-2,1
1486139796,3182036,2023/4/1,v3.4.0-rc5,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1484624612,7322292,2023/4/1,v3.4.0-rc5,I guess you forgot to run `dev/connect-gen-protos.sh` ?,"1,-1    ",1,-1,0
1484841467,19235986,2023/4/1,v3.4.0-rc5,Merged to master,"1,-1    ",1,-1,0
1485482919,1475305,2023/4/1,v3.4.0-rc5,Thanks @dongjoon-hyun @HyukjinKwon ,"1,-1    ",1,-1,0
1485658926,1097932,2023/4/1,v3.4.0-rc5,"+1, @LuciferYang Thanks for the work!","1,-1    ",1,-1,0
1486109613,1475305,2023/4/1,v3.4.0-rc5,Thanks @gengliangwang ~,"1,-1    ",1,-1,0
1484700118,1317309,2023/4/2,v3.4.0-rc5,cc. @zsxwing @viirya @rangadi Please take a look. Thanks in advance!,"1,-1    ",1,-1,0
1485819017,1317309,2023/4/2,v3.4.0-rc5,"The error only occurred from linter - it now does not allow a new PR to introduce a new public API ""without adding to spark-connect"". This PR intentionally postpones addressing PySpark in separate JIRA ticket, hence addressing spark-connect should go to there as well.","1,-1    ",1,-1,0
1485834545,1317309,2023/4/2,v3.4.0-rc5,Just added a dummy implementation.,"1,-1    ",1,-1,0
1485964292,1317309,2023/4/3,v3.4.0-rc5,"@HyukjinKwon @amaliujia 
Would you mind if I ask what happens with the mima check for this PR? 
https://github.com/HeartSaVioR/spark/actions/runs/4536405777/jobs/7993077860

Is it required to add PySpark API in this PR to pass Spark connect check? At least MiMa check failed in Scala codebase.","1,-1    ",1,-1,0
1485996207,1938382,2023/4/3,v3.4.0-rc5,I think you need update at connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/CheckConnectJvmClientCompatibility.scala,"1,-1    ",1,-1,0
1485999022,1317309,2023/4/3,v3.4.0-rc5,"I was wondering what is different from dropDuplicates and this one. I don't see dropDuplicates being handled separately. Is it because the PySpark implementation of dropDuplicates is available?

If this method has to be excluded, could you please guide how to do that? Thanks in advance.","2,-1    ",2,-1,1
1486006675,1938382,2023/4/3,v3.4.0-rc5,"hmm I am not sure what you already did but I am thinking if you don't add anything into https://github.com/apache/spark/blob/master/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/Dataset.scala, then you need update `CheckConnectJvmClientCompatibility`.

","2,-2    ",2,-2,0
1486008365,1317309,2023/4/3,v3.4.0-rc5,Sigh I didn't indicate we already took a step of Scala API with Spark connect. I thought there's only in PySpark. Thanks for correcting me.,"2,-1    ",2,-1,1
1487880188,1317309,2023/4/3,v3.4.0-rc5,cc. @zsxwing @viirya @rangadi Friendly reminder.,"2,-1    ",2,-1,1
1491105348,1317309,2023/4/3,v3.4.0-rc5,"> What is the decision about batch support?

I just added support of batch in the latest commit. It needs be more test coverage for batch query support so that's why we have new FIXMEs. All FIXMEs should be resolved before merging.","1,-1    ",1,-1,0
1495329109,1317309,2023/4/3,v3.4.0-rc5,"cc. @zsxwing @viirya @rangadi Could you please review this again? I feel this is very close to the final shape.
","1,-1    ",1,-1,0
1495445673,1317309,2023/4/3,v3.4.0-rc5,Just filed another JIRA ticket https://issues.apache.org/jira/browse/SPARK-43027 to support PySpark. Once we merge this in I'll work on PySpark side.,"1,-1    ",1,-1,0
1497194248,1317309,2023/4/3,v3.4.0-rc5,"I just added the pyspark implementation in this PR. It doesn't seem to be worthwhile to have another round of review specifically for pyspark, given that the review phase is not going fast enough.","2,-1    ",2,-1,1
1500752714,1317309,2023/4/3,v3.4.0-rc5,The last update is to rebase with master branch - just to make sure CI is happy with the change before merging this.,"1,-1    ",1,-1,0
1500894854,1317309,2023/4/3,v3.4.0-rc5,Confirmed CI passed for last commit. https://github.com/HeartSaVioR/spark/runs/12606973127,"1,-1    ",1,-1,0
1500894916,1317309,2023/4/3,v3.4.0-rc5,Thanks all for reviewing! Merging to master.,"1,-1    ",1,-1,0
1525860735,50596795,2023/4/3,v3.4.0-rc5,"@HeartSaVioR Confirming that the documentation in the initial PR description is accurate and up to date, as I'll be using it at the example I base my doc updates on.

Also: I don't see these doc changes live in the SS programming guide. What Spark versions should they go live with?","1,-1    ",1,-1,0
1526809418,1317309,2023/4/3,v3.4.0-rc5,@dstrodtman-db We will release this feature in Spark 3.5.0. We don't have the tentative date to release Spark 3.5.0.,"1,-1    ",1,-1,0
1484891627,26707386,2023/4/3,v3.4.0-rc5,"@HyukjinKwon @zhengruifeng Hi, Would you mind have a review?","1,-1    ",1,-1,0
1486303566,26707386,2023/4/3,v3.4.0-rc5,"Thanks all first, I had made a global check about key words which is 'reference/api/pyspark.pandas', and hasn't find another instance like this.","2,-1    ",2,-1,1
1486307165,6477701,2023/4/3,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1487887194,8486025,2023/4/3,v3.4.0-rc5,ping @zhengruifeng @HyukjinKwon @dongjoon-hyun cc @infoankitp @navinvishy ,"1,-1    ",1,-1,0
1492810135,7322292,2023/4/3,v3.4.0-rc5,ping @cloud-fan ,"1,-1    ",1,-1,0
1495822244,8486025,2023/4/3,v3.4.0-rc5,cc @MaxGekk ,"2,-3    ",2,-3,-1
1499056223,3182036,2023/4/3,v3.4.0-rc5,Can we update the PR description? `array_append` is untouched now.,"2,-3    ",2,-3,-1
1499063099,3182036,2023/4/3,v3.4.0-rc5,"After a second thought, this makes the performance worse. We should improve `ArrayInsert` to generate more efficient code if the position argument is constant. For example, if the position argument is constant 1, then we don't need to generate code to check if position is negative or not.","1,-1    ",1,-1,0
1499895163,8486025,2023/4/3,v3.4.0-rc5,"> Can we update the PR description? `array_append` is untouched now.

Updated.","2,-1    ",2,-1,1
1499895556,8486025,2023/4/3,v3.4.0-rc5,"> After a second thought, this makes the performance worse. We should improve `ArrayInsert` to generate more efficient code if the position argument is constant. For example, if the position argument is constant 1, then we don't need to generate code to check if position is negative or not.

You means create another PR to simplify the code of `ArrayInsert`.","2,-1    ",2,-1,1
1519867307,3182036,2023/4/3,v3.4.0-rc5,"thanks, merging to master!","1,-1    ",1,-1,0
1533956516,8486025,2023/4/3,v3.4.0-rc5,@cloud-fan Thanks !,"1,-1    ",1,-1,0
1487518958,1938382,2023/4/3,v3.4.0-rc5,cc @zhenlineo can you take a look?,"1,-1    ",1,-1,0
1492754667,4190164,2023/4/3,v3.4.0-rc5,cc @LuciferYang Can we merge this?,"1,-1    ",1,-1,0
1492772726,1475305,2023/4/3,v3.4.0-rc5,The pr title should be `[SPARK-42519][CONNECT][TESTS] ...`,"1,-1    ",1,-1,0
1492773043,1475305,2023/4/3,v3.4.0-rc5,"@zhenlineo Thanks for ping me ~ Let me do some manual checks
","1,-1    ",1,-1,0
1492878798,1475305,2023/4/3,v3.4.0-rc5,"```
### How was this patch tested?
Yes
```

I think You can say `Add new tests`
","1,-1    ",1,-1,0
1493565139,1475305,2023/4/3,v3.4.0-rc5,cc @hvanhovell @HyukjinKwon @zhengruifeng FYI,"1,-1    ",1,-1,0
1493686717,6477701,2023/4/3,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1493689271,32387433,2023/4/3,v3.4.0-rc5,Thanks @zhenlineo @LuciferYang @HyukjinKwon ,"1,-1    ",1,-1,0
1488876848,1580697,2023/4/3,v3.4.0-rc5,"Highly likely, the GA `continuous-integration/appveyor/pr` is not related to my changes. I am going to merge this PR.","2,-2    ",2,-2,0
1488877352,1580697,2023/4/3,v3.4.0-rc5,"Merging to master. Thank you, @cloud-fan for review.","1,-1    ",1,-1,0
1485155792,1475305,2023/4/3,v3.4.0-rc5,"cc @dongjoon-hyun FYI

If you have time, please help verify this change. I am not sure if only my environment can reproduce this issue. Thanks ~

","1,-1    ",1,-1,0
1485412215,9700541,2023/4/3,v3.4.0-rc5,"Thank you for pinging me, @LuciferYang .","1,-1    ",1,-1,0
1485469907,9700541,2023/4/3,v3.4.0-rc5,I verified this manually via Maven. Merged to master/3.4/3.3/3.2.,"1,-1    ",1,-1,0
1485485240,1475305,2023/4/3,v3.4.0-rc5,Thanks very much @dongjoon-hyun ð,"3,-1    ",3,-1,2
1487823925,10149923,2023/4/3,v3.4.0-rc5,cc @dongjoon-hyun @cloud-fan @viirya @yaooqinn thank you,"3,-1    ",3,-1,2
1485666995,1591700,2023/4/3,v3.4.0-rc5,+CC @srowen,"1,-1    ",1,-1,0
1485705422,822522,2023/4/3,v3.4.0-rc5,I think it's fine. These do look like better usages of RNGs. Let's see what tests say.,"1,-2    ",1,-2,-1
1485827152,9700541,2023/4/3,v3.4.0-rc5,"According to the `Affected Version` in JIRA, I also agree with backporting to the applicable release branches.","2,-1    ",2,-1,1
1486162373,822522,2023/4/3,v3.4.0-rc5,Merged to master/3.4/3.3,"3,-1    ",3,-1,2
1486467736,1591700,2023/4/3,v3.4.0-rc5,"Thanks for the reviews everyone !
And thanks for merging it @srowen :-)","1,-2    ",1,-2,-1
1486516135,1475305,2023/4/3,v3.4.0-rc5,late LGTM,"1,-1    ",1,-1,0
1486087962,6477701,2023/4/3,v3.4.0-rc5,cc @allisonwang-db ,"1,-1    ",1,-1,0
1486268124,21131848,2023/4/3,v3.4.0-rc5,I updated the description to mention that the issue occurs only when both wholestage codegen and adaptive execution is disabled.,"1,-2    ",1,-2,-1
1486295355,12025282,2023/4/3,v3.4.0-rc5,"lgtm, it seems we missed update non-AQE part when we optimize out the DPP broadcast value in https://github.com/apache/spark/pull/34051.","2,-1    ",2,-1,1
1486818021,9700541,2023/4/3,v3.4.0-rc5,"According to the Affected Versions in Jira, I merged this to master/3.4/3.3.","2,-1    ",2,-1,1
1486155106,6477701,2023/4/4,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1486086978,6477701,2023/4/4,v3.4.0-rc5,cc @WeichenXu123 and @ueshin ,"1,-2    ",1,-2,-1
1486153075,6477701,2023/4/4,v3.4.0-rc5,Merged to master.,"1,-3    ",1,-3,-2
1486149209,84573424,2023/4/4,v3.4.0-rc5,"Discussed in https://github.com/apache/spark/pull/35278#issuecomment-1033927506, PTAL. @HyukjinKwon Thanks a lot.
","2,-2    ",2,-2,0
1486238107,6477701,2023/4/4,v3.4.0-rc5,Merged to master.,"1,-2    ",1,-2,-1
1486304054,84573424,2023/4/4,v3.4.0-rc5,Thanks all !,"1,-1    ",1,-1,0
1486195050,8326978,2023/4/4,v3.4.0-rc5,"cc @cloud-fan @dongjoon-hyun @HyukjinKwon, thanks","2,-1    ",2,-1,1
1486755262,8326978,2023/4/4,v3.4.0-rc5,"https://github.com/yaooqinn/spark/runs/12325165688 action passed, and the last one which blocks hours for python test, removed tailing `:` only. 

Merged to master, thanks","1,-1    ",1,-1,0
1486270562,12025282,2023/4/4,v3.4.0-rc5,cc @dongjoon-hyun @cloud-fan @viirya @yaooqinn thank you,"1,-1    ",1,-1,0
1486431066,6477701,2023/4/4,v3.4.0-rc5,Nice!,"1,-1    ",1,-1,0
1486984947,9616802,2023/4/4,v3.4.0-rc5,@allisonwang-db can we do something similar for the scala side?,"1,-1    ",1,-1,0
1487510024,1938382,2023/4/4,v3.4.0-rc5,"> @allisonwang-db can we do something similar for the scala side?

Same question: do we plan to do the same on the scala side?","1,-1    ",1,-1,0
1512438468,66282705,2023/4/4,v3.4.0-rc5,@grundprinzip I added a config for the stack trace size (default 4096). Could you take another look at this PR?,"1,-1    ",1,-1,0
1512439357,66282705,2023/4/4,v3.4.0-rc5,@amaliujia Scala side does not have an existing SQL conf. We can certainly add this with a new config on the Scala side as well.,"1,-1    ",1,-1,0
1515505880,6477701,2023/4/4,v3.4.0-rc5,Merged to master.,"1,-3    ",1,-3,-2
1515733014,7322292,2023/4/4,v3.4.0-rc5,@HyukjinKwon @allisonwang-db  it seems `UserDefinedFunctionE2ETestSuite` keeps failing after this PR,"1,-1    ",1,-1,0
1515739073,6477701,2023/4/4,v3.4.0-rc5,Reverting this out for now.,"1,-1    ",1,-1,0
1517215527,66282705,2023/4/4,v3.4.0-rc5,"> @HyukjinKwon @allisonwang-db it seems `UserDefinedFunctionE2ETestSuite` keeps failing after this PR

This is because the default max HTTP header size is 8k and having 2k messages and 4k stack trace exceeds this limit. For now I changed the default max stack trace size to be 2k. We should consider moving to a different error framework to remove such constraints.","1,-1    ",1,-1,0
1519228572,7322292,2023/4/4,v3.4.0-rc5,merged to master again,"1,-1    ",1,-1,0
1486626442,8326978,2023/4/4,v3.4.0-rc5,cc @HyukjinKwon @gengliangwang @cloud-fan thanks,"1,-1    ",1,-1,0
1487934105,8326978,2023/4/4,v3.4.0-rc5,"thanks, merge to master/3.4","1,-3    ",1,-3,-2
1486823487,13622031,2023/4/4,v3.4.0-rc5,"@srowen Can you help review this PR? We found this problem in the use of connecting AD for authentication. This problem is very similar to [36784](https://github.com/apache/spark/pull/36784), because STS LDAP authentication lacks some functions.","1,-1    ",1,-1,0
1486915870,822522,2023/4/4,v3.4.0-rc5,It's hard to make out the change because you moved the code. What is the important change?,"1,-1    ",1,-1,0
1487018810,1475305,2023/4/4,v3.4.0-rc5,"Was this file originally copied from hive? Is there a corresponding fix in the hive?

","1,-1    ",1,-1,0
1487033055,1475305,2023/4/4,v3.4.0-rc5,"> Was this file originally copied from hive? Is there a corresponding fix in the hive?

difficult to synchronize, this file has changed too much in hive

","3,-1    ",3,-1,2
1487034185,13622031,2023/4/4,v3.4.0-rc5,"> It's hard to make out the change because you moved the code. What is the important change?

The original code will continue to construct principals according to the DN pattern after processing the domain. After I modify it, when encountering a domain, it will no longer go to the logic of the DN pattern, and the createCandidatePrincipals function will return directly.","1,-1    ",1,-1,0
1487036226,13622031,2023/4/4,v3.4.0-rc5,"> > Was this file originally copied from hive? Is there a corresponding fix in the hive?
> 
> difficult to synchronize, this file has changed too much in hive

You can check this file.   [LdapUtils](https://github.com/apache/hive/blob/4d21ba11c893038dbecd98c512a81ccf1d4fc6d0/service/src/java/org/apache/hive/service/auth/ldap/LdapUtils.java#L203)","1,-3    ",1,-3,-2
1487039909,13622031,2023/4/4,v3.4.0-rc5,"I think the community needs to fully synchronize the latest code of hive in the end, but at present this PR can solve LDAP authentication including domain.","1,-1    ",1,-1,0
1487040596,26535726,2023/4/4,v3.4.0-rc5,"The major LDAP enhancement in Apache Hive occurred in HIVE-14713, which also brought UT. There is a Scala version porting in Apache Kyuubi https://github.com/apache/kyuubi/pull/4152. If wanted, I can contribute those parts to Spark, the change should be similar w/ Kyuubi.","1,-1    ",1,-1,0
1529403614,13622031,2023/4/4,v3.4.0-rc5,"> cc @sunchao too

Can this modification be merged?","1,-1    ",1,-1,0
1487925221,26535726,2023/4/4,v3.4.0-rc5,cc @ulysses-you @cloud-fan ,"2,-1    ",2,-1,1
1487963292,12025282,2023/4/4,v3.4.0-rc5,lgtm,"1,-1    ",1,-1,0
1488023804,3182036,2023/4/4,v3.4.0-rc5,"thanks, merging to master!","1,-1    ",1,-1,0
1488024167,3182036,2023/4/4,v3.4.0-rc5,"thanks, merging to master!","1,-1    ",1,-1,0
1487513366,1938382,2023/4/4,v3.4.0-rc5,LGTM,"1,-1    ",1,-1,0
1487776982,6477701,2023/4/4,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1488139292,1097932,2023/4/4,v3.4.0-rc5,Merging to master,"1,-1    ",1,-1,0
1487814020,4190164,2023/4/4,v3.4.0-rc5,cc @xinrong-meng ,"1,-1    ",1,-1,0
1487847175,1475305,2023/4/4,v3.4.0-rc5,"```
TODO: Need to fix the test for maven.
```

Should we make this one 3.5.0 only? I think Maven test failure will become a blocker for 3.4.0 release



","1,-1    ",1,-1,0
1499487211,9616802,2023/4/4,v3.4.0-rc5,Merging this one.,"1,-1    ",1,-1,0
1488273071,7322292,2023/4/4,v3.4.0-rc5,"thank you for reviews, merged into master","1,-1    ",1,-1,0
1491249761,8326978,2023/4/4,v3.4.0-rc5,cc @cloud-fan @HyukjinKwon ,"1,-1    ",1,-1,0
1493844696,8326978,2023/4/4,v3.4.0-rc5,"thanks, merged to master","1,-1    ",1,-1,0
1487949308,1475305,2023/4/4,v3.4.0-rc5,cc @HyukjinKwon @hvanhovell ,"1,-1    ",1,-1,0
1487985703,1475305,2023/4/4,v3.4.0-rc5,"`avro/functions` not in sql module, so `CheckConnectJvmClientCompatibility` cannot perform mima check between `client.jar` and `avro.jar`, created SPARK-42958 to tracking this

","1,-1    ",1,-1,0
1488053877,1475305,2023/4/4,v3.4.0-rc5,GA passed,"3,-1    ",3,-1,2
1488070518,6477701,2023/4/4,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1488080391,1475305,2023/4/4,v3.4.0-rc5,Thanks @HyukjinKwon @yaooqinn ,"1,-1    ",1,-1,0
1487972613,9700541,2023/4/4,v3.4.0-rc5,cc @HyukjinKwon and @xinrong-meng ,"1,-1    ",1,-1,0
1488003787,9700541,2023/4/4,v3.4.0-rc5,"Could you review this PR, @viirya ?","1,-1    ",1,-1,0
1488017066,9700541,2023/4/4,v3.4.0-rc5,"Thank you so much, @viirya . Merged to master/3.4.","1,-1    ",1,-1,0
1488114753,9700541,2023/4/4,v3.4.0-rc5,"Thank you for checking.

Yes, JSON was fine.
```
spark-rm@1ea0f8a3e397:/opt/spark-rm/output/spark/spark-repo-glCsK/org/apache/spark$ find . | grep json
./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.json
./spark-graphx_2.13/3.4.1-SNAPSHOT/spark-graphx_2.13-3.4.1-SNAPSHOT-cyclonedx.json
./spark-kvstore_2.13/3.4.1-SNAPSHOT/spark-kvstore_2.13-3.4.1-SNAPSHOT-cyclonedx.json
...
```

I found that I missed that `maven-metadata-local.xml`. 
```
spark-rm@1ea0f8a3e397:/opt/spark-rm/output/spark/spark-repo-glCsK/org/apache/spark$ find . | grep xml
./spark-core_2.13/3.4.1-SNAPSHOT/maven-metadata-local.xml
./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.xml
./spark-core_2.13/maven-metadata-local.xml
```

Let me make a follow-up to use `cyclonedx.xml` and `cyclonedx.json` specifically. Thank you so much, @viirya and @HyukjinKwon .","2,-1    ",2,-1,1
1488040738,502522,2023/4/5,v3.4.0-rc5,"cc: @HyukjinKwon, @pengzhon-db, @WweiL, @grundprinzip 
This is ready for review. I will fix the conflicts. ","1,-2    ",1,-2,-1
1492687197,10248890,2023/4/5,v3.4.0-rc5,"I have less expertises in protobuf, otherwise LGTM","1,-1    ",1,-1,0
1496448770,1938382,2023/4/5,v3.4.0-rc5,The proto side overall looks good.,"1,-1    ",1,-1,0
1498440221,6477701,2023/4/5,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1524690074,7322292,2023/4/5,v3.4.0-rc5,"@rangadi It seems the doctest `pyspark.sql.connect.dataframe.DataFrame.writeStream` is not stable, would you mind taking a look?

https://github.com/apache/spark/actions/runs/4815479364/jobs/8574219355

https://github.com/apache/spark/actions/runs/4805357601/jobs/8551705966

https://github.com/apache/spark/actions/runs/4783998036/jobs/8504986700","1,-1    ",1,-1,0
1488130579,9700541,2023/4/5,v3.4.0-rc5,"Thank you, @HyukjinKwon . I'll merge this because GitHub Action doesn't cover this.","1,-1    ",1,-1,0
1488154261,9700541,2023/4/5,v3.4.0-rc5,"For the record, to sum up, SPARK-42957 made the following change. `cyclonedx` didn't exist before across all file names and only `-cyclonedx.json` and `-cyclonedx.xml` are newly generated for SBOM according to our snapshot result.
```
   # Remove any extra files generated during install
-  find . -type f |grep -v \.jar |grep -v \.pom | xargs rm
+  find . -type f |grep -v \.jar |grep -v \.pom |grep -v cyclonedx | xargs rm
```","1,-1    ",1,-1,0
1490660361,9700541,2023/4/5,v3.4.0-rc5,"I verified that Apache Spark 3.4.0 RC5 successfully has SBOM artifacts.
- https://repository.apache.org/content/repositories/orgapachespark-1439/org/apache/spark/spark-core_2.12/3.4.0/spark-core_2.12-3.4.0-cyclonedx.json
- https://repository.apache.org/content/repositories/orgapachespark-1439/org/apache/spark/spark-core_2.13/3.4.0/spark-core_2.13-3.4.0-cyclonedx.json

Thank you, @HyukjinKwon , @viirya , @xinrong-meng .","1,-1    ",1,-1,0
1490740514,68855,2023/4/5,v3.4.0-rc5,Cool. Thanks @dongjoon-hyun ,"1,-1    ",1,-1,0
1494075879,8326978,2023/4/5,v3.4.0-rc5,cc @cloud-fan @HyukjinKwon @dongjoon-hyun thanks,"1,-1    ",1,-1,0
1495556789,8326978,2023/4/5,v3.4.0-rc5,"thanks, merged to master","1,-2    ",1,-2,-1
1488464221,12025282,2023/4/5,v3.4.0-rc5,cc @cloud-fan @dongjoon-hyun @yaooqinn ,"1,-2    ",1,-2,-1
1492257092,9700541,2023/4/5,v3.4.0-rc5,"cc @sunchao , too","1,-1    ",1,-1,0
1495056158,9700541,2023/4/5,v3.4.0-rc5,"Oh my bad. I realized that @ulysses-you re-used SPARK-38697 mistakenly.
Let me revert this.","1,-1    ",1,-1,0
1495057403,9700541,2023/4/5,v3.4.0-rc5,"This is reverted from master branch.
```
$ git log --oneline -n3
9287a5e7db (HEAD -> master, apache/master, apache/HEAD) Revert ""[SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE query stage optimizer""
6f8b068151 [SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE query stage optimizer
```

To @ulysses-you , please create a new JIRA and new PR.","1,-1    ",1,-1,0
1495059461,9700541,2023/4/5,v3.4.0-rc5,"Just for the record, SPARK-38697 was a commit of one-year ago.
```
$ git show 883596a4ba | head
commit 883596a4bab36ddf0e1a5af0ba98325ca8582550
Author: ulysses-you <ulyssesyou18@gmail.com>
Date:   Fri Apr 15 16:02:00 2022 +0800

    [SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE Optimizer
```","1,-1    ",1,-1,0
1495211687,12025282,2023/4/5,v3.4.0-rc5,"oh sorry, not sure why that happened, I actually have created https://issues.apache.org/jira/browse/SPARK-42963.

created a new pr https://github.com/apache/spark/pull/40653","1,-1    ",1,-1,0
1489129059,1938382,2023/4/5,v3.4.0-rc5,LGTM,"1,-1    ",1,-1,0
1489181163,9616802,2023/4/5,v3.4.0-rc5,Merging.,"1,-1    ",1,-1,0
1491064274,1097932,2023/4/5,v3.4.0-rc5,Merging to master/3.4/3.3/3.2,"1,-1    ",1,-1,0
1490430902,1580697,2023/4/5,v3.4.0-rc5,"Merging to master. Thank you, @cloud-fan for review.","1,-1    ",1,-1,0
1489520920,6477701,2023/4/5,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1489522478,6477701,2023/4/5,v3.4.0-rc5,It has a conflict with branch-3.4. Would you mind creating a backport please?,"1,-1    ",1,-1,0
1489567987,506656,2023/4/5,v3.4.0-rc5,#40595,"1,-1    ",1,-1,0
1491102080,6477701,2023/4/5,v3.4.0-rc5,Merged to branch-3.4.,"1,-2    ",1,-2,-1
1489600591,1475305,2023/4/5,v3.4.0-rc5,cc @HyukjinKwon @sadikovi ,"1,-1    ",1,-1,0
1489687927,6477701,2023/4/5,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1489741560,1475305,2023/4/5,v3.4.0-rc5,Thanks @HyukjinKwon @sadikovi ,"1,-1    ",1,-1,0
1489742010,1475305,2023/4/5,v3.4.0-rc5,cc @sadikovi @srowen @HyukjinKwon ,"1,-1    ",1,-1,0
1489721124,6477701,2023/4/5,v3.4.0-rc5,Merged to master.,"1,-1    ",1,-1,0
1489721186,100322362,2023/4/5,v3.4.0-rc5,"@HeartSaVioR - PTAL when you get a chance. Thx
","1,-1    ",1,-1,0
1490244143,1317309,2023/4/5,v3.4.0-rc5,Thanks! Merging to master.,"1,-2    ",1,-2,-1
1489866478,8326978,2023/4/5,v3.4.0-rc5,"This change makes sense to me. This is a breaking change, then shall we add a migration guide for it?","1,-2    ",1,-2,-1
1491166217,3182036,2023/4/5,v3.4.0-rc5,"The change makes sense, but I'd say this is a legacy feature and the existing behavior doesn't make sense at all. For string +/- internal, the string can be timestamp, timestamp_ntz and interval, there is no correct behavior here.

My suggestion is don't touch it to keep legacy workloads running. We should update the SQL queries to not use String so extensively.","1,-2    ",1,-2,-1
1491166746,3182036,2023/4/5,v3.4.0-rc5,"Or we should probably fail it in ANSI mode, cc @gengliangwang ","2,-2    ",2,-2,0
1491229091,5399861,2023/4/5,v3.4.0-rc5,+1 for fail it in ANSI mode.,"2,-1    ",2,-1,1
1491232046,1097932,2023/4/5,v3.4.0-rc5,"> My suggestion is don't touch it to keep legacy workloads running. We should update the SQL queries to not use String so extensively.

>we should probably fail it in ANSI mode

+1, totally agree!","2,-1    ",2,-1,1
1491192780,8326978,2023/4/5,v3.4.0-rc5,cc @cloud-fan @HyukjinKwon thanks,"2,-1    ",2,-1,1
1491489837,8326978,2023/4/5,v3.4.0-rc5,"thanks, merged to master","2,-1    ",2,-1,1
1492973410,3421,2023/4/6,v3.4.0-rc5,Ping on this @HyukjinKwon or @hvanhovell?,"2,-1    ",2,-1,1
1498771199,7322292,2023/4/6,v3.4.0-rc5,Another question: will the raw data in `LocalRelation` also shown in Spark UI?   It might be sensitive,"1,-1    ",1,-1,0
1498775987,3421,2023/4/6,v3.4.0-rc5,"> Another question: will the raw data in `LocalRelation` also shown in Spark UI? It might be sensitive

This is similar to what we do for SQL queries we put the literal string into the description.","1,-1    ",1,-1,0
1502561858,6477701,2023/4/6,v3.4.0-rc5,Actually it would also have a security concern as it exposes the local data as is.,"1,-1    ",1,-1,0
1502562037,6477701,2023/4/6,v3.4.0-rc5,Let me make a PR to redact it for now at least.,"1,-1    ",1,-1,0
1489923055,3182036,2023/4/6,v3.4.0-rc5,cc @ulysses-you ,"1,-2    ",1,-2,-1
1489923733,3182036,2023/4/6,v3.4.0-rc5,"also cc @xinrong-meng , this is not a blocker but it's better if we can make it into 3.4.0.","1,-1    ",1,-1,0
1490156737,3182036,2023/4/6,v3.4.0-rc5,"thanks for review, merging to master/3.4!","1,-1    ",1,-1,0
1490405810,9700541,2023/4/6,v3.4.0-rc5,"+1 for reverting decision. Thank you, @cloud-fan and all.","1,-1    ",1,-1,0
1491252507,1475305,2023/4/6,v3.4.0-rc5,GA passed,"1,-1    ",1,-1,0
1508027751,1475305,2023/4/6,v3.4.0-rc5,"@HyukjinKwon @hvanhovell @yaooqinn Can this on move forward?

","1,-1    ",1,-1,0
1537836851,6477701,2023/4/6,v3.4.0-rc5,@LuciferYang is it good to go?,"1,-1    ",1,-1,0
1537849572,1475305,2023/4/6,v3.4.0-rc5,"> @LuciferYang is it good to go?

Yes, I think so","1,-1    ",1,-1,0
1550268865,9616802,2023/4/6,v3.4.0-rc5,Alright let's merge it then.,"1,-2    ",1,-2,-1
1493081969,822522,2023/4/6,v3.4.0-rc5,Was this opened by mistake?,"1,-1    ",1,-1,0
1493082220,3421,2023/4/6,v3.4.0-rc5,Yes.,"1,-1    ",1,-1,0
1493485269,7322292,2023/4/6,v3.4.0-rc5,cc @WeichenXu123 @HyukjinKwon  I think it is ready to review,"1,-1    ",1,-1,0
1495655168,19235986,2023/4/6,v3.4.0-rc5,"Followup tasks:

* We should replace `mapInPandas` with `mapInArrow` for better performance
* For each spark task, we should save each partition data into local disk (in arrow format) before starting torch process, and provide utility reading function (returning `torch.utils.data.Dataset` instance) that torch program (running as a child process) can invoke it to iterate the partition data inside torch program.","1,-1    ",1,-1,0
1495768111,7322292,2023/4/6,v3.4.0-rc5,"```
pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.UNKNOWN
	details = ""Java heap space""
	debug_error_string = ""UNKNOWN:Error received from peer ipv4:127.0.0.1:39429 {grpc_message:""Java heap space"", grpc_status:2, created_time:""2023-04-04T10:54:38.384319228+00:00""}""
>
```
it fails again even I switch back to the initial approach, the error is raised in the server side, so should not be related to the way to concat. I guess there is no enough RAM.

cc @HyukjinKwon are those UTs tested in parallel or one by one?","2,-1    ",2,-1,1
1495858338,7322292,2023/4/6,v3.4.0-rc5,"the GA status is missing, https://github.com/zhengruifeng/spark/actions/runs/4607449033","1,-1    ",1,-1,0
1496734980,7322292,2023/4/6,v3.4.0-rc5,"In my local env, the failed test can pass with even bigger model size.
but let me try to reduce the model size for GA to see what will happen.","1,-1    ",1,-1,0
1497315263,7322292,2023/4/6,v3.4.0-rc5,"I am hitting a weird failure of `TorchDistributorDistributedUnitTestsOnConnect.test_parity_torch_distributor`, it appeared after I rebase this PR yesterday, but I don't find any suspicious commits merged recently.

```
======================================================================
ERROR [18.362s]: test_end_to_end_run_distributedly (pyspark.ml.tests.connect.test_parity_torch_distributor.TorchDistributorDistributedUnitTestsOnConnect)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/ml/torch/tests/test_distributor.py"", line 457, in test_end_to_end_run_distributedly
    output = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False).run(
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 749, in run
    output = self._run_distributed_training(framework_wrapper_fn, train_object, *args)
  File ""/__w/spark/spark/python/pyspark/ml/torch/distributor.py"", line 607, in _run_distributed_training
    self.spark.range(start=0, end=self.num_tasks, step=1, numPartitions=self.num_tasks)
  File ""/__w/spark/spark/python/pyspark/sql/connect/dataframe.py"", line 1354, in collect
    table, schema = self._session.client.to_table(query)
  File ""/__w/spark/spark/python/pyspark/sql/connect/client.py"", line 668, in to_table
    table, schema, _, _, _ = self._execute_and_fetch(req)
  File ""/__w/spark/spark/python/pyspark/sql/connect/client.py"", line 982, in _execute_and_fetch
    for response in self._execute_and_fetch_as_iterator(req):
  File ""/__w/spark/spark/python/pyspark/sql/connect/client.py"", line 963, in _execute_and_fetch_as_iterator
    self._handle_error(error)
  File ""/__w/spark/spark/python/pyspark/sql/connect/client.py"", line 1055, in _handle_error
    self._handle_rpc_error(error)
  File ""/__w/spark/spark/python/pyspark/sql/connect/client.py"", line 1095, in _handle_rpc_error
    raise SparkConnectGrpcException(str(rpc_error)) from None
pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:
	status = StatusCode.UNKNOWN
	details = ""Java heap space""
	debug_error_string = ""UNKNOWN:Error received from peer ipv4:127.0.0.1:35071 {created_time:""2023-04-05T10:13:52.254507275+00:00"", grpc_status:2, grpc_message:""Java heap space""}""
>
```


In my local env, I can only repro this by decreasing the driver memory (e.g. ""spark.driver.memory"", ""512M""), And this issue can be simply resolve by increase the driver memory to 1024M.
I tests different combinations locally like:
`spark.driver.memory=1024M, spark.executor.memory=512M`
`spark.driver.memory=1024M, spark.executor.memory=1024M`
etc
and they also works as expected.

But in Github Action (this resource limitation seems to be https://github.com/apache/spark/blob/0b45a5278026c2ea9ce2b127333514f7a7a933f4/.github/workflows/build_and_test.yml#L1028), no matter how larger driver memory I set (3G, 4G), this test just keeps failing with this error message.

Do you have any thoughts on this? @WeichenXu123 @HyukjinKwon 
","1,-1    ",1,-1,0
1498386497,19235986,2023/4/6,v3.4.0-rc5,@HyukjinKwon WDYT ? Can we increase CI bot memory capacity ?,"2,-1    ",2,-1,1
1499783849,7322292,2023/4/6,v3.4.0-rc5,"Thanks all for the reviews, merged into master","1,-2    ",1,-2,-1
1490470454,4701453,2023/4/6,v3.4.0-rc5,@holdenk @MaxGekk ,"1,-1    ",1,-1,0
1490659794,4700574,2023/4/6,v3.4.0-rc5,LGTM +1,"1,-1    ",1,-1,0
1493036019,4701453,2023/4/6,v3.4.0-rc5,"@HyukjinKwon Changed the implementation to `df.explain(""codegen"")` and added it to the connector","1,-1    ",1,-1,0
1501412496,4701453,2023/4/6,v3.4.0-rc5,Hi @HyukjinKwon Does the PR approach make sense to you?,"1,-1    ",1,-1,0
1497057684,1580697,2023/4/6,v3.4.0-rc5,"@Hisoka-X Please, update PR's description and fix the error class name.

Is `fanjia` at https://issues.apache.org/jira/browse/SPARK-42316 your JIRA account ?","1,-1    ",1,-1,0
1497067995,32387433,2023/4/6,v3.4.0-rc5,"> @Hisoka-X Please, update PR's description and fix the error class name.

Done

> Is `fanjia` at https://issues.apache.org/jira/browse/SPARK-42316 your JIRA account ?

Yes
","1,-1    ",1,-1,0
1497075225,1580697,2023/4/6,v3.4.0-rc5,"+1, LGTM. Merging to master.
Thank you, @Hisoka-X.","1,-1    ",1,-1,0
1497076338,32387433,2023/4/6,v3.4.0-rc5,Thank @MaxGekk ,"1,-2    ",1,-2,-1
1491250312,1475305,2023/4/6,v3.4.0-rc5,"```
2023-03-30T16:09:39.9363333Z [0m[[0m[0minfo[0m] [0m[0m[31m- Dataset result destructive iterator *** FAILED *** (84 milliseconds)[0m[0m
2023-03-30T16:09:39.9382605Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.vectorized.ColumnarBatch@3f5ed920 equaled org.apache.spark.sql.vectorized.ColumnarBatch@3f5ed920 (ClientE2ETestSuite.scala:819)[0m[0m
2023-03-30T16:09:39.9383550Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.scalatest.exceptions.TestFailedException:[0m[0m
2023-03-30T16:09:39.9384640Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)[0m[0m
2023-03-30T16:09:39.9385936Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)[0m[0m
2023-03-30T16:09:39.9387002Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)[0m[0m
2023-03-30T16:09:39.9388072Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)[0m[0m
2023-03-30T16:09:39.9389136Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$106(ClientE2ETestSuite.scala:819)[0m[0m
2023-03-30T16:09:39.9390070Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)[0m[0m
2023-03-30T16:09:39.9391014Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)[0m[0m
2023-03-30T16:09:39.9392246Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m[0m
2023-03-30T16:09:39.9393644Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Transformer.apply(Transformer.scala:22)[0m[0m
2023-03-30T16:09:39.9394518Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Transformer.apply(Transformer.scala:20)[0m[0m
2023-03-30T16:09:39.9395634Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)[0m[0m
2023-03-30T16:09:39.9396587Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)[0m[0m
2023-03-30T16:09:39.9397490Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)[0m[0m
2023-03-30T16:09:39.9398540Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)[0m[0m
2023-03-30T16:09:39.9399811Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)[0m[0m
2023-03-30T16:09:39.9400900Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)[0m[0m
2023-03-30T16:09:39.9401857Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)[0m[0m
2023-03-30T16:09:39.9402904Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)[0m[0m
2023-03-30T16:09:39.9403910Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)[0m[0m
2023-03-30T16:09:39.9405036Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)[0m[0m
2023-03-30T16:09:39.9406066Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)[0m[0m
2023-03-30T16:09:39.9407048Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)[0m[0m
2023-03-30T16:09:39.9407986Z [0m[[0m[0minfo[0m] [0m[0m[31m  at scala.collection.immutable.List.foreach(List.scala:431)[0m[0m
2023-03-30T16:09:39.9409109Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)[0m[0m
2023-03-30T16:09:39.9410244Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)[0m[0m
2023-03-30T16:09:39.9411594Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)[0m[0m
2023-03-30T16:09:39.9412839Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)[0m[0m
2023-03-30T16:09:39.9414688Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)[0m[0m
2023-03-30T16:09:39.9416225Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)[0m[0m
2023-03-30T16:09:39.9416768Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Suite.run(Suite.scala:1114)[0m[0m
2023-03-30T16:09:39.9417907Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.Suite.run$(Suite.scala:1096)[0m[0m
2023-03-30T16:09:39.9419535Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)[0m[0m
2023-03-30T16:09:39.9420225Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)[0m[0m
2023-03-30T16:09:39.9421710Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.SuperEngine.runImpl(Engine.scala:535)[0m[0m
2023-03-30T16:09:39.9422826Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)[0m[0m
2023-03-30T16:09:39.9423713Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)[0m[0m
2023-03-30T16:09:39.9436163Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.ClientE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:41)[0m[0m
2023-03-30T16:09:39.9437232Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)[0m[0m
2023-03-30T16:09:39.9455327Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)[0m[0m
2023-03-30T16:09:39.9456678Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)[0m[0m
2023-03-30T16:09:39.9457909Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:41)[0m[0m
2023-03-30T16:09:39.9459138Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)[0m[0m
2023-03-30T16:09:39.9460291Z [0m[[0m[0minfo[0m] [0m[0m[31m  at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)[0m[0m
2023-03-30T16:09:39.9461481Z [0m[[0m[0minfo[0m] [0m[0m[31m  at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)[0m[0m
2023-03-30T16:09:39.9462543Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m[0m
2023-03-30T16:09:39.9463721Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m[0m
2023-03-30T16:09:39.9464883Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m[0m
2023-03-30T16:09:39.9465861Z [0m[[0m[0minfo[0m] [0m[0m[31m  at java.lang.Thread.run(Thread.java:750)[0m[0m
```
@ivoson The new test failed","1,-1    ",1,-1,0
1507952082,1475305,2023/4/6,v3.4.0-rc5,@ivoson any update of this one ?,"1,-1    ",1,-1,0
1509728135,15122230,2023/4/6,v3.4.0-rc5,Latest commits addressed the comments above. cc @hvanhovell @LuciferYang please take a look when you have time. Thanks.,"1,-1    ",1,-1,0
1510302202,15122230,2023/4/6,v3.4.0-rc5,looking into the test failure.,"1,-1    ",1,-1,0
1510850421,1475305,2023/4/6,v3.4.0-rc5,"It is not related to the current PR. It seems that the `SparkResult ` instance is not thread-safe, do we need to consider this? @hvanhovell 


","1,-1    ",1,-1,0
1520993028,15122230,2023/4/6,v3.4.0-rc5,"Hi @hvanhovell , could you please take a look at this PR? Thanks.","1,-1    ",1,-1,0
1558516547,1475305,2023/4/6,v3.4.0-rc5,"friendly ping @hvanhovell , is this one ok?","1,-1    ",1,-1,0
1558517257,1475305,2023/4/6,v3.4.0-rc5,"@ivoson Can you resolve the conflict?

","1,-1    ",1,-1,0
1491101671,6477701,2023/4/6,v3.4.0-rc5,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1491119721,7322292,2023/4/6,v3.4.0-rc5,LGTM,"1,-1    ",1,-1,0
1492862165,1475305,2023/4/6,v3.4.0-rc5,friendly ping @srowen @sadikovi ,"1,-1    ",1,-1,0
1493559038,1475305,2023/4/6,v3.4.0-rc5,"Can we push forward this one ? @srowen @sadikovi 
","1,-1    ",1,-1,0
1494327133,822522,2023/4/6,v3.4.0-rc5,Merged to master,"1,-1    ",1,-1,0
1494341174,1475305,2023/4/6,v3.4.0-rc5,"@srowen Does this not need to be merged into branch-3.4?

","2,-1    ",2,-1,1
1494344667,822522,2023/4/6,v3.4.0-rc5,I don't see a strong reason to. We don't have a specific issue this solves right?,"1,-1    ",1,-1,0
1494353798,1475305,2023/4/6,v3.4.0-rc5,"@srowen I think there are two reasons:
1. branch-3.3 and the master both use `spark.util.ShutdownHookManager`, but 3.4 not, I think they should keep the same.
2. @sadikovi mentioned earlier(https://github.com/apache/spark/pull/36529#discussion_r1152687590) `DeleteOnExit functionality can cause performance issues due memory leaks under heavy load in certain JDK versions because while the temporary files are deleted, the entries in the map are never cleared.`

So I think branch-3.4 may be worth having this one

WDYT  @sadikovi ?","1,-1    ",1,-1,0
1494361090,822522,2023/4/6,v3.4.0-rc5,"Right, but we haven't observed this actual problem as far as I know.
I'm not against back-porting, go for it","1,-1    ",1,-1,0
1491167649,8001253,2023/4/7,v3.4.0,I'have corrected the problem that cause build error in github workflow,"2,-3    ",2,-3,-1
1493486005,6477701,2023/4/7,v3.4.0,cc @SandishKumarHN and @rangadi,"1,-1    ",1,-1,0
1494738923,8001253,2023/4/7,v3.4.0,@rangadi The adition of `any` should be added in wich table? Protobuf -> Spark SQL or Spark SQL -> Protobuf?,"1,-1    ",1,-1,0
1495086264,8001253,2023/4/7,v3.4.0,@rangadi the first two fixes were made,"1,-1    ",1,-1,0
1495143531,502522,2023/4/7,v3.4.0,"@lucaspompeun thanks for the updates. LGTM.

`Any` is for 'Protobuf -> Spark SQL' type table. ","1,-1    ",1,-1,0
1495147474,8001253,2023/4/7,v3.4.0,@rangadi Done!,"1,-1    ",1,-1,0
1495161387,502522,2023/4/7,v3.4.0,"Thanks.
@HyukjinKwon please merge when you get a chance.","1,-1    ",1,-1,0
1501344560,6477701,2023/4/7,v3.4.0,Merged to master.,"2,-1    ",2,-1,1
1500518140,17288675,2023/4/7,v3.4.0,"@dtenedor FYI, I updated the tests and am just missing one for empty input table, and one for merging sparse/dense sketches. Once I get the build to be green, I'm going to remove the WIP tag from the PR and send an e-mail back on that initial spark-dev thread (or maybe start a new thread) letting everyone know that the implementation is open for review. I think renaming functions is still do-able at this point, so let me know if you'd like to setup another sync to discuss updated function names?","1,-1    ",1,-1,0
1502323531,17288675,2023/4/7,v3.4.0,"Hi @mkaravel thank you for the review! I'll respond to your comments in-line.

>more in favor of an aggregate function that merges sketches and returns the merged sketch

I'm not opposed to building out an agg function that merges sketches without estimating the cardinality; I think this would be beneficial for multi-stage re-aggregations. I think this topic (unfortunately) begs the question of function naming, and here's the resultant naming scheme I'd propose:

Aggregate functions:
- HllSketch (IntegerType|LongType|StringType|...) -> BinaryType
- HllUnion (BinaryType) -> BinaryType

Normal functions
- HllSketchEstimate (BinaryType) -> LongType

I think this API is simple and the lack of an aggregate function that returns the estimated cardinality is fine. What do you think?","1,-1    ",1,-1,0
1505627742,6397014,2023/4/7,v3.4.0,"> Hi @mkaravel thank you for the review! I'll respond to your comments in-line.
> 
> > more in favor of an aggregate function that merges sketches and returns the merged sketch
> 
> I'm not opposed to building out an agg function that merges sketches without estimating the cardinality; I think this would be beneficial for multi-stage re-aggregations. I think this topic (unfortunately) begs the question of function naming, and here's the resultant naming scheme I'd propose:
> 
> Aggregate functions:
> 
> * HllSketch (IntegerType|LongType|StringType|...) -> BinaryType
> * HllUnion (BinaryType) -> BinaryType
> 
> Normal functions
> 
> * HllSketchEstimate (BinaryType) -> LongType
> 
> I think this API is simple and the lack of an aggregate function that returns the estimated cardinality is fine. What do you think?

Semantically I like this API. There is one more scalar expression that I would add, it's purpose being to merge two sketches. Let me give some more details below.

In terms of naming I am more in favor of dropping the ""Sketch"" part from the names and using an underscore to separate the ""Hll"" part from the rest. More specifically this is what I would choose (this is a personal opinion, and I am not reflecting opinions of anybody else other than myself):

Aggregate functions:

* hll_collect (IntegerType|LongType|StringType|BinaryType|...) -> BinaryType
* {hll_merge_agg, hll_union_agg}(BinaryType) -> BinaryType

Scalar functions:
* {hll_merge, hll_union}(BinaryType, BinaryType) -> BinaryType
* hll_estimate(BinaryType) -> LongType

Above I propose hll_merge* or hll_union* for the functions that perform the union (or merge operation) on sketches. I have a slight preference for the ""merge"" versions.","1,-1    ",1,-1,0
1509053727,17288675,2023/4/7,v3.4.0,"Hello @mkaravel ! 

I've updated the PR to provide the following functions:

Aggregate functions:
- hll_sketch_agg(IntegerType|LongType|StringType|BinaryType) -> BinaryType
- hll_union_agg(BinaryType) -> BinaryType

Scalar functions
- hll_sketch_estimate(BinaryType) -> LongType
- hll_union(BinaryType, BinaryType) -> BinaryType

Naming wise, I felt it was valuable to keep the function names aligned with the Datasketches objects they utilize, and be explicit about the operation being applied. Hopefully these function names are a good middle ground for us? I'll continue working on getting all the tests to pass, and then open the PR up for wide review.

","1,-1    ",1,-1,0
1515134305,17288675,2023/4/7,v3.4.0,A few tests are failing due to some connectivity issues unrelated to the changes in this PR - is there an easy way to re-run without pushing a new commit?,"1,-1    ",1,-1,0
1516584672,6397014,2023/4/7,v3.4.0,"> A few tests are failing due to some connectivity issues unrelated to the changes in this PR - is there an easy way to re-run without pushing a new commit?

I am not aware how to do that. @gatorsmile any ideas?","3,-1    ",3,-1,2
1516614822,11567269,2023/4/7,v3.4.0,"Are you able to see the button ""Re-run all jobs"" ?
<img width=""1206"" alt=""image"" src=""https://user-images.githubusercontent.com/11567269/233427975-c8ecf0cd-d1c2-43cf-b367-1183c8911d44.png"">
","2,-1    ",2,-1,1
1516899001,17288675,2023/4/7,v3.4.0,"@mkaravel regarding your comment about 'mixing sketches with different lgk values',[ this is the Union implementation which handles merging sketches with different configs](https://github.com/apache/datasketches-java/blob/master/src/main/java/org/apache/datasketches/hll/Union.java#L317-L340); my assumption is that the functionality is tested and stable, but let me know if you think we should try to limit the union operation to only support sketches with the same config?","1,-1    ",1,-1,0
1517360699,6397014,2023/4/7,v3.4.0,"> @mkaravel regarding your comment about 'mixing sketches with different lgk values',[ this is the Union implementation which handles merging sketches with different configs](https://github.com/apache/datasketches-java/blob/master/src/main/java/org/apache/datasketches/hll/Union.java#L317-L340); my assumption is that the functionality is tested and stable, but let me know if you think we should try to limit the union operation to only support sketches with the same config?

I am not questioning the correctness of the DataSketches implementation.

My concern is accidental mistakes that can happen if the user does not pay attention to the `lgk` values of the input sketches. I would argue that merging two sketches with different `lgk` values is ""advanced"" usage of sketches, and the user should be aware that mixing such sketches comes with caveats (loosing precision with respect to the sketch with higher `lgk`). The current API hides this complexity and caveats.

Let's consider another alternative (I want your opinion on this): Let's say we have two overloads (we can extend this to the aggregate version)
* hll_merge(sketch1: BinaryType, sketch2: BinaryType)
* hll_merge(sketch1: BinaryType, sketch2: BinaryType, allowDifferentLgKs: BooleanType)

The first errors out if the `lgk` values are different.
The second errors out if the `lgk` values are different and `allowDifferentLgKs` is `false`. However, if `allowDifferentLgk` is `true` then the second overload behaves as your current implementation.

Clearly, I am talking about adding a third boolean argument, with the default value being `false`. With these two overloads, if the user tries to merge two sketches with different precision, the query will fail. If they really need to merge them, they have the opportunity to do that by means of  setting the third argument to `true`, and it will not happen accidentally without them noticing. It will happen because they force it this way, they will be proactive about the loss of precision. A nice error message for the first overload and good documentation will make it very clear what is going on.","2,-1    ",2,-1,1
1520502797,17288675,2023/4/7,v3.4.0,">about adding a third boolean argument, with the default value being false

I'm open to making this change, as this is in-line with the limitation I was planning on imposing on the original approx_count_distinct_sketch merge functionality. @mkaravel do you have an opinion on whether we should preserve the lgMaxK argument supported by the union operation, given this change? [Here's a ref to the docs](https://github.com/apache/datasketches-java/blob/b2c62bf050d8eb37b20e523a35bcd4f8c9a6cdf2/src/main/java/org/apache/datasketches/hll/Union.java#L45-L49). I'm leaning towards removing this argument in favor of relying on the default behavior, to further reduce complexity.","2,-1    ",2,-1,1
1523694528,17288675,2023/4/7,v3.4.0,FYI it looks like [another similar implementation of a Datasketches/Spark integration already exists in its own repo ](https://github.com/Gelerion/spark-sketches/tree/spark-3.0)- I've invited the owner of that implementation to provide feedback on this PR.,"1,-1    ",1,-1,0
1523999640,17288675,2023/4/7,v3.4.0,"@mkaravel I've updated the implementation based on your review comments. We're now returning the updatable binary representation, no longer support the tgtHllType parameter, and defer initialization of the Union instance until we've ingested the first HllSketch such that we can throw an exception when union/sketch lgConfigKs don't match. Let me know when you've had chance to re-review?","1,-1    ",1,-1,0
1528267013,6397014,2023/4/7,v3.4.0,"@RyanBerti nit in the PR description:
> Yes, this PR introduces two new aggregate functions, and two new non-aggregate functions:

Maybe use ""scalar"" instead of ""non-aggregate"".","1,-1    ",1,-1,0
1528770943,12531020,2023/4/7,v3.4.0,"While browsing the dev-mailing list, I came across this PR and I am excited to see that data-sketches will be built-in in Spark. Interestingly, a year ago, I created a similar [project](https://github.com/Gelerion/spark-sketches) that adds sketch support for Spark versions 2.4 to 3.4. Even the implementation path is very similar :)","1,-1    ",1,-1,0
1528840439,6397014,2023/4/7,v3.4.0,"@RyanBerti I created a PR against this PR for updating `sql-expression-schema.md` (I regenerated the golden file).
Feel free to either merge my PR into yours, or just copy the file in my PR into your code manually.
Just trying to help here :)

My PR: https://github.com/RyanBerti/spark/pull/4","1,-1    ",1,-1,0
1530822680,17288675,2023/4/7,v3.4.0,"@mkaravel @dtenedor Finally got all the tests passing, thanks for all your help! Think I covered all of the most recent review comments, let me know if you need anything else from me for the merge?","1,-1    ",1,-1,0
1531272567,6477701,2023/4/7,v3.4.0,Merged to master.,"3,-1    ",3,-1,2
1531802864,99207096,2023/4/7,v3.4.0,@RyanBerti this is super awesome. Just wanted to thank you again for working on this. I'm sure lots of users will appreciate it.,"2,-1    ",2,-1,1
1531857727,17288675,2023/4/7,v3.4.0,"@dtenedor thanks for all the help, excited to be able to utilize sketches natively in Spark!","1,-1    ",1,-1,0
1541016431,21131848,2023/4/7,v3.4.0,"As a follow-up, should you add a check to ensure a foldable `lgConfigK`?:

```
spark-sql (default)> create or replace temp view v1 as
select * from values
(1, 12),
(1, 12),
(2, 12),
(2, 12),
(3, 12)
as tab(col, logk);
Time taken: 1.665 seconds
spark-sql (default)> select hex(hll_sketch_agg(col, logk)) from v1;
23/05/09 16:25:25 ERROR Executor: Exception in task 4.0 in stage 0.0 (TID 4)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$4(InternalRow.scala:138)
	at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$4$adapted(InternalRow.scala:138)
	at org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:40)
	at org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.lgConfigK$lzycompute(datasketchesAggregates.scala:65)
	at org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.lgConfigK(datasketchesAggregates.scala:64)
	at org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.createAggregationBuffer(datasketchesAggregates.scala:116)
	at org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.createAggregationBuffer(datasketchesAggregates.scala:55)
...
```","1,-1    ",1,-1,0
1542840734,17288675,2023/4/7,v3.4.0,"@bersprockets good catch - I thought relying on ExpectsInputTypes would be sufficient. Looking at other example functions like ApproximatePercentile, I see that the foldable check is applied after super.checkInputDataTypes() in an overridden checkInputDataTypes() implementation. I'll follow this pattern and open up a new PR.","1,-1    ",1,-1,0
1491411368,5399861,2023/4/7,v3.4.0,cc @cloud-fan @gengliangwang ,"1,-1    ",1,-1,0
1494792083,1097932,2023/4/7,v3.4.0,"@wangyum Do you mind writing more details about the reason why making this change in the PR description? 
On second thought: as I will propose enabling ANSI mode by default in Spark 4.0, this one adds more gaps between the default behavior and ANSI SQL mode.","1,-1    ",1,-1,0
1517869092,5399861,2023/4/7,v3.4.0,@gengliangwang Has updated the description.,"1,-1    ",1,-1,0
1521086780,44108233,2023/4/7,v3.4.0,"Applied the comments, thanks @zhengruifeng !","1,-1    ",1,-1,0
1522823970,7322292,2023/4/7,v3.4.0,merged to master,"1,-1    ",1,-1,0
1493081730,822522,2023/4/7,v3.4.0,Can you make a little JIRA for this and https://github.com/apache/spark/pull/40620 ? they're minor bugs,"1,-3    ",1,-3,-2
1494324368,822522,2023/4/7,v3.4.0,Merged to master/3.4,"1,-1    ",1,-1,0
1494743599,506656,2023/4/7,v3.4.0,@thyecust Good catch! Thanks for fixing it!,"1,-2    ",1,-2,-1
1494800031,506656,2023/4/7,v3.4.0,I submitted a PR #40650 to fix the failed tests.,"1,-1    ",1,-1,0
1493081780,822522,2023/4/7,v3.4.0,"Indeed a bug. It happens to work for values that aren't 0, I think","1,-1    ",1,-1,0
1494322069,822522,2023/4/7,v3.4.0,Merged to master/3.4/3.3/3.2,"1,-1    ",1,-1,0
1491457693,62563545,2023/4/7,v3.4.0,"@Ngone51 @VasilyKolpakov 
please help me review the PR","2,-1    ",2,-1,1
1494623895,62563545,2023/4/7,v3.4.0,"@Ngone51 @VasilyKolpakov
would  you please help me review the PR?","2,-1    ",2,-1,1
1498695731,62563545,2023/4/7,v3.4.0,"@HyukjinKwon would you please help me review this PR?
","2,-1    ",2,-1,1
1499201846,1591700,2023/4/7,v3.4.0,+CC @HeartSaVioR ,"1,-1    ",1,-1,0
1491509278,6477701,2023/4/7,v3.4.0,"Hey, I think all these typo PRs should have a test if this is an actual issue.","2,-1    ",2,-1,1
1491509670,6477701,2023/4/7,v3.4.0,"And please file a JIRA if this is an issue, see also https://spark.apache.org/contributing.html","1,-1    ",1,-1,0
1492468583,47577197,2023/4/7,v3.4.0,"@thyecust 
The failed test is not related to this, can you restart failed test? 
","1,-1    ",1,-1,0
1493367769,49089605,2023/4/7,v3.4.0,"



> Yeah it's more of a bug fix, good catch. File a JIRA and link in the title please

Thank you, this is my first time submitting code for Spark. I have applied for a JIRA account and I will file a bug later.","1,-1    ",1,-1,0
1493488541,6477701,2023/4/7,v3.4.0,cc @tgravescs ,"1,-3    ",1,-3,-2
1493595928,822522,2023/4/7,v3.4.0,Merged to master/3.4/3.3/3.2,"2,-1    ",2,-1,1
1496383011,4563792,2023/4/7,v3.4.0,"definitely looks like a typo, thanks for catching and fixing","1,-1    ",1,-1,0
1495253738,3182036,2023/4/7,v3.4.0,"@grundprinzip Spark Connect is not released yet, I think we can still change it? This PR should go to Spark 3.4. cc @xinrong-meng ","1,-1    ",1,-1,0
1495260097,3421,2023/4/7,v3.4.0,If it's guaranteed to be included in 3.4 it's not a breaking change.,"1,-1    ",1,-1,0
1496174032,3182036,2023/4/7,v3.4.0,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1496177514,3182036,2023/4/7,v3.4.0,"It has conflicts with 3.4, @MaxGekk can you create a backport PR? Thanks!","1,-1    ",1,-1,0
1496220564,1580697,2023/4/7,v3.4.0,@cloud-fan I am working on the backport ...,"2,-1    ",2,-1,1
1496368421,1580697,2023/4/7,v3.4.0,Here is the backport to `branch-3.4`: https://github.com/apache/spark/pull/40666,"1,-1    ",1,-1,0
1497270264,6477701,2023/4/7,v3.4.0,Merged to master.,"1,-1    ",1,-1,0
1492778516,7322292,2023/4/7,v3.4.0,"Thanks, merged into master/branch-3.4","2,-1    ",2,-1,1
1493484560,6477701,2023/4/7,v3.4.0,cc @xinrong-meng ,"4,-1    ",4,-1,3
1499733331,47337188,2023/4/7,v3.4.0,Thanks for working on that! We will enable `foreach` and `foreachPartition` in Python Client's DataFrame based on that.,"3,-1    ",3,-1,2
1499822466,9616802,2023/4/7,v3.4.0,"Merging, thanks!","3,-1    ",3,-1,2
1494736492,22358241,2023/4/7,v3.4.0,Hi @cloud-fan @jiangxb1987  could you pls help review this?,"1,-1    ",1,-1,0
1494785221,1591700,2023/4/7,v3.4.0,"It is unclear to me why we need this from the description, what the existing issues being solved are, and how much this approach will help.
Btw, if the variable being broadcasted is that small - simply inline it in the task and remove the roundtrip entirely ?","2,-1    ",2,-1,1
1495187524,3182036,2023/4/7,v3.4.0,"@mridulm the use case we found so far is the broadcasting of hadoop conf: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L161-L162

Simply inline the variable does not improve the perf because:
1. hadoop conf is not that small
2. if the executor is powerful like 16 cores, the hadoop conf has 16 copies in the executor JVM, which is a waste.

We might find more cases in the future, as SQL operators need to broadcast small data (but not as small as a single integer) sometimes.","2,-1    ",2,-1,1
1498430883,1591700,2023/4/7,v3.4.0,"Why not handle a single chunk case in `TorrentBroadcast` to inline it ?
For the cases mentioned, this should come within 4mb.

This will prevent adding a new codepath, while also making all cases of broadcast faster for smaller blocks.
It also allows us to seemlessly go from smaller to larger payloads without changing api's in code.","2,-1    ",2,-1,1
1498460524,1591700,2023/4/7,v3.4.0,"A strawman proposal:
```
diff --git a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala
index 7b430766851..d9632964e3d 100644
--- a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala
+++ b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala
@@ -29,6 +29,7 @@ import scala.util.Random
 import org.apache.spark._
 import org.apache.spark.internal.{config, Logging}
 import org.apache.spark.io.CompressionCodec
+import org.apache.spark.network.util.JavaUtils
 import org.apache.spark.serializer.Serializer
 import org.apache.spark.storage._
 import org.apache.spark.util.{KeyLock, Utils}
@@ -95,12 +96,16 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO
 
   private val broadcastId = BroadcastBlockId(id)
 
-  /** Total number of blocks this broadcast variable contains. */
-  private val numBlocks: Int = writeBlocks(obj)
-
   /** The checksum for all the blocks. */
   private var checksums: Array[Int] = _
 
+  /** Total number of blocks this broadcast variable contains. */
+  private val (singleBlockData: Array[Byte], numBlocks: Int) = writeBlocks(obj)
+  assert(1 != numBlocks || null != singleBlockData)
+  assert(1 == numBlocks || null == singleBlockData)
+  assert(null != checksums || null != singleBlockData)
+
+
   override protected def getValue() = synchronized {
     val memoized: T = if (_value == null) null.asInstanceOf[T] else _value.get
     if (memoized != null) {
@@ -135,7 +140,23 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO
    * @param value the object to divide
    * @return number of blocks this broadcast variable is divided into
    */
-  private def writeBlocks(value: T): Int = {
+  private def writeBlocks(value: T): (Array[Byte], Int) = {
+
+    val blocks = {
+      try {
+        TorrentBroadcast.blockifyObject(value, blockSize, SparkEnv.get.serializer, compressionCodec)
+      } catch {
+        case t: Throwable =>
+          logError(s""Store broadcast $broadcastId failed, cannot serialize object"")
+          throw t
+      }
+    }
+
+    if (1 == blocks.length) {
+      // no checksum
+      return (JavaUtils.bufferToArray(blocks(0)), 1)
+    }
+
     import StorageLevel._
     val blockManager = SparkEnv.get.blockManager
     if (serializedOnly && !isLocalMaster) {
@@ -156,8 +177,6 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO
       }
     }
     try {
-      val blocks =
-        TorrentBroadcast.blockifyObject(value, blockSize, SparkEnv.get.serializer, compressionCodec)
       if (checksumEnabled) {
         checksums = new Array[Int](blocks.length)
       }
@@ -172,7 +191,7 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO
             s""in local BlockManager"")
         }
       }
-      blocks.length
+      (null, blocks.length)
     } catch {
       case t: Throwable =>
         logError(s""Store broadcast $broadcastId fail, remove all pieces of the broadcast"")
@@ -186,6 +205,14 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO
     // Fetch chunks of data. Note that all these chunks are stored in the BlockManager and reported
     // to the driver, so other executors can pull these chunks from this executor as well.
     val blocks = new Array[BlockData](numBlocks)
+    if (null != singleBlockData) {
+      assert(1 == numBlocks)
+      blocks(0) = new ByteBufferBlockData(
+        new ChunkedByteBuffer(ByteBuffer.wrap(singleBlockData)), false)
+      return blocks
+    }
+
+    assert(1 != numBlocks)
     val bm = SparkEnv.get.blockManager
 
     for (pid <- Random.shuffle(Seq.range(0, numBlocks))) {

```","1,-1    ",1,-1,0
1505873421,22358241,2023/4/7,v3.4.0,"I think inline it in TorrentBroadcast still suffer from the multiple copies issue. Need to send a id and fetch-when-needed. Might be possible to implement on TorrentBroadcast with slightly API change, but not sure whether this will add more complexity, since there are not too much that we would like to share with current TorrentBroadcast implementation","1,-1    ",1,-1,0
1506428610,1591700,2023/4/7,v3.4.0,"Did you check the implementation proposal above @liuzqt ? There should not be a multiple copies issue - it will also get inlined into the task binary - which is already broadcast.
If I am missing something here, please let me know the details.","1,-1    ",1,-1,0
1514174607,1591700,2023/4/7,v3.4.0,"Circling back to this - to make sure I clarify: please feel free to use the proposal I detailed above (or some variation of it) @liuzqt !
Please ping me when you are done, and I will help review it.","1,-1    ",1,-1,0
1516765543,22358241,2023/4/7,v3.4.0,"Hi @mridulm  sorry for the late reply. I'm still confused about the mutiple copies issue, the taskBinary broadcast is created [here](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1554), when inlining into small broadcast, the taskBinary contains the real data block, and will be sent to executor per-task. Pls correct me if I understand it wrong...","2,-1    ",2,-1,1
1517162992,3182036,2023/4/7,v3.4.0,"yea it seems like the same as not using broadcast at all, then the multi-copy issue will appear. We need to still use the block manager to transfer the broadcast data.","1,-1    ",1,-1,0
1517342791,1591700,2023/4/7,v3.4.0,"A few points to consider:

a) Task binary is already broadcasted - so the inlined versoin should not have an overhead by itself.

Ignore (a) for time being, 

b) the poc above was a strawman proposal to inline, we can use other strategies for the small block case in torrent broadcast itself - either inline (as in the example), or fetch from block manager directly, etc.

Essentially, what I am trying to get to is, having users explicitly try to reason about whether their broadcast data is small or large is brittle - this is something that needs to be handled automatically by the broadcast impl seemlessly.
When data is small, use more efficient paths, and progressively move to more expensive options when data is larger - without any user code change.","1,-1    ",1,-1,0
1522520810,22358241,2023/4/7,v3.4.0,"Hi @mridulm agree that the broadcast impl under the hood should not be exposed to user if possible, let me see how we can inline the small broadcast within current broadcast code path based on your poc","2,-1    ",2,-1,1
1492767210,6235869,2023/4/7,v3.4.0,cc  @cloud-fan @dongjoon-hyun @viirya @huaxingao @sunchao,"1,-1    ",1,-1,0
1495066151,9700541,2023/4/7,v3.4.0,"FYI, this PR landed only at master branch because SPARK-42997 is filed as 'Improvement`. 

![Screenshot 2023-04-03 at 3 26 17 PM](https://user-images.githubusercontent.com/9700541/229640458-0cbecb85-a8ae-4b2f-84f6-913dfbc6ad51.png)
","3,-1    ",3,-1,2
1495211916,6235869,2023/4/7,v3.4.0,"Thanks for reviewing, @dongjoon-hyun @cloud-fan @huaxingao @viirya!","1,-1    ",1,-1,0
1541071974,32387433,2023/4/7,v3.4.0,kindly ping @MaxGekk ,"1,-1    ",1,-1,0
1493472727,5399861,2023/4/7,v3.4.0,cc @cloud-fan ,"1,-1    ",1,-1,0
1493483913,6477701,2023/4/7,v3.4.0,cc @gengliangwang too,"1,-1    ",1,-1,0
1493924751,13050963,2023/4/7,v3.4.0,"@MaxGekk Plz review this PR, thx","1,-1    ",1,-1,0
1498582185,1580697,2023/4/8,v3.4.0,"@Leibnizhu Could you fix PR's description and title according to your actual changes, please.","1,-1    ",1,-1,0
1498985698,13050963,2023/4/8,v3.4.0,"> @Leibnizhu Could you fix PR's description and title according to your actual changes, please.

done","3,-1    ",3,-1,2
1499019506,1580697,2023/4/8,v3.4.0,"+1, LGTM. Merging to master.
Thank you, @Leibnizhu.","1,-1    ",1,-1,0
1499021280,1580697,2023/4/8,v3.4.0,@Leibnizhu Congratulations with your first contribution to Apache Spark!,"1,-1    ",1,-1,0
1499054849,13050963,2023/4/8,v3.4.0,"> @Leibnizhu Congratulations with your first contribution to Apache Spark!

thx ððð","3,-2    ",3,-2,1
1493483499,6477701,2023/4/8,v3.4.0,">  But generating optimised logical plan sometimes takes more time.

Why don't you just print out `df.queryExecution.analyzed`? This sounds not very common case to have a separate user-facing API for this.","2,-1    ",2,-1,1
1494285099,27844885,2023/4/8,v3.4.0,"> > But generating optimised logical plan sometimes takes more time.
> 
> Why don't you just print out `df.queryExecution.analyzed`? This sounds not very common case to have a separate user-facing API for this.

This is a unresolved task that I found on Jira [SPARK-42860](https://issues.apache.org/jira/browse/SPARK-42860), which some people ""Start watching this issue"". Additionally, its
priority is set to ""Blocked"", so I completed it. In fact, I also have a little doubt about the usage scenario of
this API.","1,-1    ",1,-1,0
1494287118,27844885,2023/4/8,v3.4.0,"> > But generating optimised logical plan sometimes takes more time.
> 
> Why don't you just print out `df.queryExecution.analyzed`? This sounds not very common case to have a separate user-facing API for this.

In your opinion, should I close this prï¿½?
1493483696,6477701,2023-04-03,Mind filing a JIRA please? See also https://spark.apache.org/contributing.html.
1496640034,6961317,2023-04-04,Thanks @ShreyeshArangath for this! I think it helps clear a lot of unnecessary noise from user logs and keeps the logs manageable.","1,-1    ",1,-1,0
,,2023/4/8,v3.4.0,,"2,-1    ",2,-1,1
One thing I noticed is that we set `spark.yarn.report.logging.frequency` to `30` by default. I think it is a much more sensible default, especially given a) Spark jobs submitted to YARN would usually run longer than a minute including the overhead of launching Spark driver b) We always report any state change immediately e.g. `ACCEPTED` -> `RUNNING`. I would be pro defaulting to `30`. But if we want to eliminate behavior change,2023/4/8,v3.4.0,,"1,-1    ",1,-1,0
,,2023/4/8,v3.4.0,,"1,-1    ",1,-1,0
I saw the tests are failing with what seems to be a transient Github connection issue.,,2023/4/8,v3.4.0,,"1,-2    ",1,-2,-1
```,,2023/4/8,v3.4.0,,"1,-1    ",1,-1,0
/usr/bin/git -c protocol.version=2 fetch --prune --progress --no-recurse-submodules origin +refs/heads/*:refs/remotes/origin/* +refs/tags/*:refs/tags/*,,2023/4/8,v3.4.0,,"1,-1    ",1,-1,0
  Error: fatal: unable to access 'https://github.com/apache/spark/': The requested URL returned error: 429,,2023/4/8,v3.4.0,,"1,-1    ",1,-1,0
  The process '/usr/bin/git' failed with exit code 128,,2023/4/8,v3.4.0,,"1,-2    ",1,-2,-1
```,,2023/4/8,v3.4.0,,"1,-1    ",1,-1,0
"Can you push an empty commit to re-run the tests?""",,2023/4/8,v3.4.0,,"1,-3    ",1,-3,-2
1497981336,6961317,2023/4/9,v3.4.0,"The GIthub Actions tests now pass. The AppVeyor build is failing, but it looks to be unrelated.

Tagging folks with recent commits/reviews of this file
cc: @sunchao @dongjoon-hyun @mridulm @tgravescs Can you help take a look?","1,-1    ",1,-1,0
1497998935,4563792,2023/4/9,v3.4.0,"how is this different from:
```

  private[spark] val REPORT_INTERVAL = ConfigBuilder(""spark.yarn.report.interval"")
    .doc(""Interval between reports of the current app status."")
    .version(""0.9.0"")
    .timeConf(TimeUnit.MILLISECONDS)
    .createWithDefaultString(""1s"")

```

?","1,-1    ",1,-1,0
1498070306,5604993,2023/4/9,v3.4.0,"> how is this different from:
>
>  private[spark] val REPORT_INTERVAL = ConfigBuilder(""spark.yarn.report.interval"")
>   .doc(""Interval between reports of the current app status."")
>    .version(""0.9.0"")
>    .timeConf(TimeUnit.MILLISECONDS)
>    .createWithDefaultString(""1s"")
>
> ?

@tgravescs The new property allows for the ability to reduce the log noise without reducing the responsiveness when something changes because the status check interval is not reduced like if `spark.yarn.report.interval` was used alone. For example, if a job failed right away waiting 30 seconds to get that feedback slows down the iteration cycle.","1,-1    ",1,-1,0
1498073102,47700876,2023/4/9,v3.4.0,"> how is this different from:
> 
> ```
> 
>   private[spark] val REPORT_INTERVAL = ConfigBuilder(""spark.yarn.report.interval"")
>     .doc(""Interval between reports of the current app status."")
>     .version(""0.9.0"")
>     .timeConf(TimeUnit.MILLISECONDS)
>     .createWithDefaultString(""1s"")
> ```
> 
> ?

@tgravescs this property dictates the how often we poll yarn for an application report. We still want to check for the application state every second, if we change this property then we might not know of a change in state until the `x` seconds have elapsed. 

This is why, I decided to create a new logging specific property so that the current behavior of polling application report every second is not affected.
","1,-1    ",1,-1,0
1499419716,4563792,2023/4/9,v3.4.0,lgtm to me pending build/test,"1,-1    ",1,-1,0
1503789133,47700876,2023/4/9,v3.4.0,"@dongjoon-hyun / @HyukjinKwon, could you help me merge this PR? thanks :) ","1,-1    ",1,-1,0
1503796800,9700541,2023/4/9,v3.4.0,"Got it, @ShreyeshArangath .","1,-1    ",1,-1,0
1503800710,9700541,2023/4/9,v3.4.0,"Merged to master for Apache Spark 3.5.0.

Thank you, @ShreyeshArangath , @HyukjinKwon , @shardulm94, @tgravescs , @robreeves .

Also, welcome to the Apache Spark community, @ShreyeshArangath !
I added you to the Apache Spark contributor group and assigned SPARK-43002 to you.","1,-1    ",1,-1,0
1498287802,27844885,2023/4/9,v3.4.0,"I applied the suggested code changes. can you review the code one more time in your spare time, thanks a lot @jaceklaskowski 

","1,-2    ",1,-2,-1
1501887451,27844885,2023/4/9,v3.4.0,Could you please take a look when you have time? thanks! @cloud-fan ,"1,-1    ",1,-1,0
1512288253,27844885,2023/4/9,v3.4.0,"I applied the suggested code changes. can you review the code one more time in your spare time, thanks a lot @jaceklaskowski","1,-1    ",1,-1,0
1495674620,1475305,2023/4/9,v3.4.0,"Thanks @dongjoon-hyun 
OK~ let's wait for some more days","1,-1    ",1,-1,0
1499944650,9700541,2023/4/9,v3.4.0,Merged to master. Thank you for waiting.,"1,-1    ",1,-1,0
1499945844,1317309,2023/4/9,v3.4.0,"Sorry for post-review. It'd be nice if you don't mind running below benchmark as well.
sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/StateStoreBasicOperationsBenchmark.scala
The test is more likely about performance of WriteBatch rather than RocksDB itself but that's something we use in RocksDB state store provider.","1,-1    ",1,-1,0
1499946937,9700541,2023/4/9,v3.4.0,"Thank you, @HeartSaVioR . To @LuciferYang , could you address the above comment?","1,-1    ",1,-1,0
1500006415,1475305,2023/4/9,v3.4.0,"> Thank you, @HeartSaVioR . To @LuciferYang , could you address the above comment?

OK, Let me update the results of `StateStoreBasicOperationsBenchmark ` in a follow-up

","1,-1    ",1,-1,0
1500025387,1475305,2023/4/9,v3.4.0,"Need to update later because

<img width=""1200"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/230563757-cf26102c-3f87-43a9-95f3-84f8a65b530c.png"">
","1,-2    ",1,-2,-1
1519081998,822522,2023/4/10,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1519097453,1475305,2023/4/10,v3.2.4,Thanks @srowen ,"1,-1    ",1,-1,0
1494110470,7322292,2023/4/10,v3.2.4,cc @cloud-fan @itholic ,"1,-1    ",1,-1,0
1495243707,7322292,2023/4/10,v3.2.4,@cloud-fan @MaxGekk would you mind taking another look?,"1,-1    ",1,-1,0
1495471186,1580697,2023/4/10,v3.2.4,"+1, LGTM. Merging to master/3.4.
Thank you, @zhengruifeng and @cloud-fan @itholic for review.","1,-1    ",1,-1,0
1498422397,6477701,2023/4/10,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1498737212,6477701,2023/4/10,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1494395904,1580697,2023/4/10,v3.2.4,"Merging to master/3.4. Thank you, @cloud-fan @HyukjinKwon @LuciferYang for review.","1,-1    ",1,-1,0
1495341213,88070094,2023/4/10,v3.2.4,Failed tests are irrelevant,"1,-1    ",1,-1,0
1495345568,88070094,2023/4/10,v3.2.4,cc @yaooqinn @dongjoon-hyun ,"1,-1    ",1,-1,0
1495658982,88070094,2023/4/10,v3.2.4,"> I understand what you aim to achieve in this PR. In case of K8s Cluster Mode, the driver pod has a driver `ConfigMap` containing `spark.app.submitTime` already. So, you want to keep that. However, this is not safe because of the timezone between the user machine and the K8s control plane server.
> 
> If we want to propose this improvement to include the driver pod pending time, we need to add a configuration to control this feature, @zhouyifan279 . Could you try to add a configuration and add an integration test case too?

@dongjoon-hyun Can you elaborate about how timezone affects the computed driver pod pending time.

As I understand, both `spark.app.submitTime` and `spark.app.startTime` is milliseconds between the current time and midnight, January 1, 1970 UTC.
","1,-1    ",1,-1,0
1496541323,9700541,2023/4/10,v3.2.4,"Sorry for misleading you. You are right about timezone. What I imagined was more like the following case.
```
$ docker run -it --rm --cap-add SYS_TIME openjdk:latest bash

bash-4.4# date -s '@2147483647'
Tue Jan 19 03:14:07 UTC 2038

bash-4.4# jshell
|  Welcome to JShell -- Version 18.0.2.1
|  For an introduction type: /help intro

jshell> java.lang.System.currentTimeMillis()
$1 ==> 2147483657595
```","1,-1    ",1,-1,0
1500887774,88070094,2023/4/10,v3.2.4,"> Any updates, @zhouyifan279 ?

@dongjoon-hyun sorry for response late. 

If I did not misunderstand, you are talking about user changes system time by intention.

Speaking from my limited experience, I think very few users may encounter this case. 
We'd better not to handle this case in Spark's code. 

Besides, if user changed system time by intention, he can handle it by himself easily:
1. Inject a custom spark property, such as `spark.app.systemTimeBias` into SparkConf. 
2. The accurate driver pod pending time would be: `startTime - systemTimeBias - submitTime`.

","1,-1    ",1,-1,0
1501206457,9700541,2023/4/10,v3.2.4,"No because it will be a new regression in terms of the vulnerability. You are aiming to fix one thing and open another bug.
> We'd better not to handle this case in Spark's code.

That's the reason why I counter-propose you to add `a configuration` to support this proposed behavior and the existing behavior, @zhouyifan279 .
- https://github.com/apache/spark/pull/40645#pullrequestreview-1370202973
","1,-1    ",1,-1,0
1504982149,88070094,2023/4/10,v3.2.4,"@dongjoon-hyun Added a new configuration `spark.kubernetes.setSubmitTimeInDriver` to control which `submitTime` Spark should use.
Now Spark will set `spark.app.submitTime` in the following cases:

1. The application is deployed using spark-submit in client/cluster mode. 
SubmitTime is the time spark-submit executed.
2. The application is deployed by other ways, such as [spark-on-k8s-operator](https://googlecloudplatform.github.io/spark-on-k8s-operator/)
SubmitTime is the time driver pod started.
3. The application is deployed using spark-submit in cluster mode and `spark.kubernetes.setSubmitTimeInDriver` is true. 
SubmitTime is the time driver pod started.","1,-1    ",1,-1,0
1514065355,88070094,2023/4/10,v3.2.4,@dongjoon-hyun do you have any more inputs on this PR?,"1,-1    ",1,-1,0
1535595335,88070094,2023/4/10,v3.2.4,Gently ping @dongjoon-hyun ,"1,-1    ",1,-1,0
1540367826,9700541,2023/4/10,v3.2.4,"Unfortunately, it seems to fail again.","2,-1    ",2,-1,1
1540395802,88070094,2023/4/10,v3.2.4,"> Unfortunately, it seems to fail again.

Let me trigger it again. ","1,-1    ",1,-1,0
1540497458,9700541,2023/4/10,v3.2.4,"Thank you, @zhouyifan279 .

BTW, when I said the following.
> Please re-trigger the failed test pipeline.

It means this. `Re-run failed jobs` is always faster.
![Screen Shot 2023-05-09 at 9 25 51 AM](https://github.com/apache/spark/assets/9700541/59d15792-5d5e-4ee5-9ab8-61341391cb23)
","1,-1    ",1,-1,0
1541170207,88070094,2023/4/10,v3.2.4,"> Thank you, @zhouyifan279 .
> 
> BTW, when I said the following.
> 
> > Please re-trigger the failed test pipeline.
> 
> It means this. `Re-run failed jobs` is always faster. ![Screen Shot 2023-05-09 at 9 25 51 AM](https://user-images.githubusercontent.com/9700541/237159446-59d15792-5d5e-4ee5-9ab8-61341391cb23.png)

Thanks for the information. Didn't know that before. ð ","1,-1    ",1,-1,0
1541993717,88070094,2023/4/10,v3.2.4,"> This PR never passes the unit tests. According to the GitHub Action log, you had better verify the following test suite locally, @zhouyifan279 . In the community, we are unable to merge a PR with UT failures.
> 
> ```
> [info] ReplE2ESuite:
> sh: 1: cannot open /dev/tty: No such device or address
> sh: 1: cannot open /dev/tty: No such device or address
> sh: 1: cannot open /dev/tty: No such device or address
> sh: 1: cannot open /dev/tty: No such device or address
> sh: 1: cannot open /dev/tty: No such device or address
> sh: 1: cannot open /dev/tty: No such device or address
> ```
@dongjoon-hyun CI succeeded after I rebase on master branch.
","2,-1    ",2,-1,1
1542515219,9700541,2023/4/10,v3.2.4,"Great! Thank you, @zhouyifan279 .","1,-1    ",1,-1,0
1494374948,1475305,2023/4/10,v3.2.4,cc @wangyum FYI,"3,-1    ",3,-1,2
1494379813,1475305,2023/4/10,v3.2.4,"> Add configuration spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.

hmm... what happens if machines that support AVX 512 are prohibited from using AVX 512?","1,-1    ",1,-1,0
1494383506,1475305,2023/4/10,v3.2.4,"```
[error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala:4512:7: method parquetAggregatePushDown is defined twice;
[error]   the conflicting method parquetAggregatePushDown was defined at line 4510:7
[error]   def parquetAggregatePushDown: Boolean = getConf(PARQUET_VECTOR512_READ_ENABLED)
[error]       ^
[error] one error found
```

compile failed","1,-1    ",1,-1,0
1495204917,12368495,2023/4/10,v3.2.4,"> > Add configuration spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.
> 
> hmm... what happens if machines that support AVX 512 are prohibited from using AVX 512?

@LuciferYang this is a good question.  As far as I know, Cascade Lake currently used by most users. Compared with Ice lake, Cascade Lake is Intel's previous generation CPU and it does not contains avx512vbmi & avx512_vbmi2 instruction set.  If running the parquet vector optimization on Cascade Lake, it will become very slowly(0.01x), but there is a JDK [Patch](https://bugs.openjdk.org/browse/JDK-8290322) to fix this problem, with the patch, it will be 1.7x.  Under normal situation, it will be 5.5x on Ice Lake with Java17. The patch has been merged Java17.","1,-1    ",1,-1,0
1495217320,12368495,2023/4/10,v3.2.4,"> ```
> [error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala:4512:7: method parquetAggregatePushDown is defined twice;
> [error]   the conflicting method parquetAggregatePushDown was defined at line 4510:7
> [error]   def parquetAggregatePushDown: Boolean = getConf(PARQUET_VECTOR512_READ_ENABLED)
> [error]       ^
> [error] one error found
> ```
> 
> compile failed

@LuciferYang Thanks, I will fix it","1,-1    ",1,-1,0
1498395411,10219731,2023/4/10,v3.2.4,"Since avx256 has been supported by more types of x86 servers, could this PR also supports avx256 ?","1,-1    ",1,-1,0
1494373721,1475305,2023/4/10,v3.2.4,cc @srowen @sadikovi backport to 3.4,"2,-1    ",2,-1,1
1495472893,9700541,2023/4/10,v3.2.4,Merged to branch-3.4.,"1,-1    ",1,-1,0
1495474531,1475305,2023/4/10,v3.2.4,thanks @dongjoon-hyun @sadikovi ,"1,-1    ",1,-1,0
1495107424,6477701,2023/4/10,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1495247655,1475305,2023/4/10,v3.2.4,Thanks @HyukjinKwon @hvanhovell ,"1,-1    ",1,-1,0
1494564101,32387433,2023/4/10,v3.2.4,@grundprinzip @zhenlineo @LuciferYang Can you help me see if there is a problem? Thanks.,"1,-1    ",1,-1,0
1495241836,1475305,2023/4/10,v3.2.4,"I suggest placing the design doc on Google doc and initiating discussions in the dev mail list for more people to participate. 

Additionally, Spark Connect is not limited to Scala clients, so Python clients should also be considered.

Meanwhile, there is still a lot of unfinished work on Spark Connect (in order to maintain the same behavior as the native Spark API),  so I am not sure if everyone has the energy to discuss this new feature at the moment.

","1,-1    ",1,-1,0
1495374508,32387433,2023/4/10,v3.2.4,"> I suggest placing the design doc on Google doc and initiating discussions in the dev mail list for more people to participate.
> 
> Additionally, Spark Connect is not limited to Scala clients, so Python clients should also be considered.
> 
> Meanwhile, there is still a lot of unfinished work on Spark Connect (in order to maintain the same behavior as the native Spark API), so I am not sure if everyone has the energy to discuss this new feature at the moment.

Thanks for suggestion, I will add python design and move doc to google doc later. Then send mail. Before start to do this feature, I will try to do other Spark Connect missing features that need to be added","1,-1    ",1,-1,0
1496739368,9616802,2023/4/10,v3.2.4,"@Hisoka-X thanks for the write up. We should be able to support most of this at the moment. GRPC supports this type of execution out of the box. The reason we did not really go for this, is because of API compatibility. The `SparkResult` does support incremental collect and can collect results in the background though.

The thing that Martin was getting at in the ticket is more about what to do when disconnect happen. You probably want to reconnect in these cases, this does require some architectural rework. We are discussing how we should do this, there are quite a few trade offs here. Do you mind shelving this until we can provide a bit more clarity? Please let me know if you want in on these conversations.","1,-1    ",1,-1,0
1497019620,32387433,2023/4/10,v3.2.4,"> @Hisoka-X thanks for the write up. We should be able to support most of this at the moment. GRPC supports this type of execution out of the box. The reason we did not really go for this, is because of API compatibility. The `SparkResult` does support incremental collect and can collect results in the background though.
> 
> The thing that Martin was getting at in the ticket is more about what to do when disconnect happen. You probably want to reconnect in these cases, this does require some architectural rework. We are discussing how we should do this, there are quite a few trade offs here. Do you mind shelving this until we can provide a bit more clarity? Please let me know if you want in on these conversations.

Ok for me. I would be happy if I could join the discussion","1,-1    ",1,-1,0
1494811441,822522,2023/4/10,v3.2.4,Oh darn the tests passed in the original PR. OK I'll merge this when tests finish.,"1,-1    ",1,-1,0
1495098701,6477701,2023/4/10,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1498291680,1938382,2023/4/10,v3.2.4,This PR should be ready to review now.,"1,-1    ",1,-1,0
1499830325,9616802,2023/4/10,v3.2.4,Merging. Please address remaining comments (if any) in a follow-up,"2,-1    ",2,-1,1
1496340472,99207096,2023/4/10,v3.2.4,Hi @gengliangwang here is the correctness bug fix ð ,"1,-1    ",1,-1,0
1498470838,1097932,2023/4/10,v3.2.4,"Thanks, merging to master/branch-3.4
cc @xinrong-meng ","1,-1    ",1,-1,0
1495283854,1475305,2023/4/10,v3.2.4,Wait https://github.com/apache/spark/pull/40605 to add mima check,"1,-1    ",1,-1,0
1511606127,1475305,2023/4/10,v3.2.4,Wait https://github.com/apache/spark/pull/40605 to add mima check,"1,-1    ",1,-1,0
1537836364,6477701,2023/4/10,v3.2.4,cc @rangadi fyi,"1,-1    ",1,-1,0
1550897944,1475305,2023/4/10,v3.2.4,hmm... `SparkConnectPlanner ` has had another conflict... I will fix it and merge this pr as soon as possible,"1,-1    ",1,-1,0
1552308089,1475305,2023/4/10,v3.2.4,Merged to master. Thanks @hvanhovell @HyukjinKwon @rangadi ,"1,-1    ",1,-1,0
1495296706,6235869,2023/4/10,v3.2.4,cc @gengliangwang @cloud-fan @dongjoon-hyun @viirya @huaxingao @sunchao,"1,-1    ",1,-1,0
1496292594,6235869,2023/4/10,v3.2.4,"@dongjoon-hyun, let me look into test failures.","1,-1    ",1,-1,0
1496675818,6235869,2023/4/10,v3.2.4,"Ok, all tests have been adapted. This PR is ready for a detailed review.","1,-1    ",1,-1,0
1496724854,1097932,2023/4/10,v3.2.4,"@aokolnychyi @cloud-fan  I am +0 for changing the behavior since I haven't heard complaints about this from end-users.  Instead, relaxing the strict compiler check can bring complaints.
 
Do we consider other alternatives? For example, we can have a new function `as_not_null` appending on the input value/columns to bypass the static checks? E.g.
```
insert into target select as_not_null(null_column_name) from source
```","1,-1    ",1,-1,0
1496774141,6235869,2023/4/10,v3.2.4,"@gengliangwang, this PR is based on the consensus we reached in [this](https://github.com/apache/spark/pull/40308#discussion_r1127081206) thread. Each approach has its own pros/cons. The primary problem is that our behavior is not consistent (e.g. inserts and updates behave differently). In that thread, it seemed the best way forward is to use runtime checks everywhere. If I were to pick one approach, I think runtime checks are a bit better as they only fail if we really have null values. Otherwise, we rely on null propagation, which may not be that reliable. The primary motivation for this PR is to have consistent behavior rather than replace static checks with runtime checks as those are better.

Let me know what you think!","1,-1    ",1,-1,0
1496846899,1097932,2023/4/10,v3.2.4,@aokolnychyi Yes I got it. My concern was around the behavior change. I am OK with the idea and merging this one.,"1,-1    ",1,-1,0
1496932417,6235869,2023/4/10,v3.2.4,"@gengliangwang, got it. I was initially concerned as well but I believe this is the right thing to do after we discussed it. Thanks for taking a look!","1,-1    ",1,-1,0
1497100207,3182036,2023/4/10,v3.2.4,"thanks, merging to master","1,-3    ",1,-3,-2
1495309716,1475305,2023/4/10,v3.2.4,This one just for Spark 3.5.0,"1,-1    ",1,-1,0
1500518103,1938382,2023/4/10,v3.2.4,late LGTM!,"3,-1    ",3,-1,2
1496046743,35164941,2023/4/10,v3.2.4,"@MaxGekk , I have updated the PR for all your review except the ""Could you trigger the error from user code, please. For example sql(""select parse_url(...)"")"".
I can't use sql function in StringExpressionsSuite.scala, don't know how to import the sql function. 
I have tried in UrlFunctionsSuite.scala using sql function. The test result reported:
```
[info] - url parse_url function *** FAILED *** (3 seconds, 922 milliseconds)
[info]   Expected exception org.apache.spark.SparkIllegalArgumentException to be thrown, but no exception was thrown (UrlFunctionsSuite.scala:75)
```  

Actually I firstly added test case in UrlFunctionSuite like I do now.scala to try to get the error, but I failed. I have tried to assign the sql code result to a variable and then call show method. The test case reported following error:
```
[info] - url parse_url function *** FAILED *** (4 seconds, 65 milliseconds)
[info]   Expected exception org.apache.spark.SparkIllegalArgumentException to be thrown, but org.apache.spark.SparkException was thrown (UrlFunctionsSuite.scala:75)
```

Then I found StringExpressionsSuite.scala and tried in it. The error can be reproduced. So I don't know it can't be reproduced from user space or my method is wrong.","2,-1    ",2,-1,1
1496094989,1580697,2023/4/10,v3.2.4,"@liang3zy22 You need to run an action to trigger the execution exception like:
```scala
    withSQLConf(SQLConf.ANSI_ENABLED.key -> ""true"") {
      val url = ""inva lid://user:pass@host/file;param?query;p2""
      checkError(
        exception = intercept[SparkException] {
          sql(s""SELECT parse_url('$url', 'HOST')"").collect()
        }.getCause.asInstanceOf[SparkThrowable],
        errorClass = ""INVALID_URL"",
        parameters = Map(
          ""url"" -> url,
          ""ansiConfig"" -> toSQLConf(SQLConf.ANSI_ENABLED.key)))
    }
```","1,-1    ",1,-1,0
1498579830,1580697,2023/4/10,v3.2.4,@liang3zy22 Do you have an account at https://issues.apache.org/ ? I would like to resolve it and assign it to you.,"1,-1    ",1,-1,0
1498590596,35164941,2023/4/10,v3.2.4,"> @liang3zy22 Do you have an account at https://issues.apache.org/ ? I would like to resolve it and assign it to you.

I have. I already added comment in the JIRA issue SPARK-42844.
","2,-1    ",2,-1,1
1498595729,1580697,2023/4/10,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @liang3zy22 and @itholic for review.","1,-1    ",1,-1,0
1498597356,1580697,2023/4/10,v3.2.4,@liang3zy22 Congratulations with your first contribution to Apache Spark!,"1,-1    ",1,-1,0
1498600699,35164941,2023/4/10,v3.2.4,"@MaxGekk and @itholic  , thanks for your patient review.","1,-1    ",1,-1,0
1519710374,47577197,2023/4/10,v3.2.4,https://pandas.pydata.org/pandas-docs/version/2.0.1/whatsnew/v2.0.1.html ,"1,-1    ",1,-1,0
1519900926,44108233,2023/4/10,v3.2.4,"Thanks, @bjornjorgensen !","1,-1    ",1,-1,0
1527304592,7322292,2023/4/10,v3.2.4,"just FYI, 2.0.1 has been released","1,-1    ",1,-1,0
1528925001,44108233,2023/4/10,v3.2.4,"Thanks for the notice. Let me address 2.0.0 first, and made a follow-up for 2.0.1 right after then to avoid extra complexity.","2,-1    ",2,-1,1
1552517228,44108233,2023/4/10,v3.2.4,"I just have opened a [new PR](https://github.com/apache/spark/pull/41211) that does not include any behavior changes related to the pandas API on Spark.

Since there are already lots of behavior changes included in this PR, let me resume working on it during a future major release Instead of reverting all the changes.

Also changed the PR title from ""Upgrade pandas to 2.0.0"" to ""Matching the behavior of pandas API on Spark to pandas 2.0.0""","2,-2    ",2,-2,0
1552522173,44108233,2023/4/10,v3.2.4,"Let me close this PR for now, will revisit when we ready for the next Apache Spark major release.","1,-1    ",1,-1,0
1495467060,12025282,2023/4/10,v3.2.4,cc @dongjoon-hyun @cloud-fan  thank you,"1,-1    ",1,-1,0
1495950647,3182036,2023/4/10,v3.2.4,"thanks, merging to master!","1,-2    ",1,-2,-1
1498441276,66282705,2023/4/11,v3.2.4,cc @srielau ,"1,-3    ",1,-3,-2
1498585569,1580697,2023/4/11,v3.2.4,"@allisonwang-db Could you fix the R test:
```
ââ Failed ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
ââ 1. Failure ('test_sparkSQL.R:3963'): Setting and getting config on SparkSessi
`sparkR.conf(""completely.dummy"")` threw an error with unexpected message.
Expected match: ""Config 'completely.dummy' is not set""
Actual message: ""Unknown error: Error in handleErrors(returnStatus, conn): org.apache.spark.SparkNoSuchElementException: 
```","1,-1    ",1,-1,0
1503093154,1580697,2023/4/11,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @allisonwang-db and @srielau for review.","1,-1    ",1,-1,0
1534242428,8486025,2023/4/11,v3.2.4,ping @cloud-fan ,"1,-1    ",1,-1,0
1534274671,3182036,2023/4/11,v3.2.4,"I think @peter-toth did something similar before, can you share some ideas @peter-toth ?","1,-1    ",1,-1,0
1534538671,8486025,2023/4/11,v3.2.4,"> I think @peter-toth did something similar before, can you share some ideas @peter-toth ?

I guess @peter-toth did the similar thing for scalar subquery, but this one try to fix non-scalar subquery.","1,-1    ",1,-1,0
1536035613,7253827,2023/4/11,v3.2.4,"> > I think @peter-toth did something similar before, can you share some ideas @peter-toth ?
> 
> I guess @peter-toth did the similar thing for scalar subquery, but this one try to fix non-scalar subquery.

Sorry, I haven't got time to fully review the PR (maybe next week) but at first sight it seems to copy some fuctions (e.g. `checkIdenticalPlans()`, `mergeNamedExpressions()`) from `MergeScalarSubqueries` so there seems to be some room for improvement and we could share the common functions. Also, some names (e.g. `subqueryIndex`) might need some changes here.

This PR combines UNION ALL legs if they return disjoint set of rows from the same source node. I think this makes sense in those cases when there are overlaping scans in the legs (despite the disjoint filters), and by ""overlapping"" I mean that the scans use some common set of files.
So seems like the only case when this change doesn't bring improvement is when the filter is a pushed-down partitioning/bucketing column filter and the scans in union legs doesn't overlap. But even in that case I'm not sure if this PR has any disadvantage, just doesn't improve anything...

BTW, `MergeScalarSubqueries` (https://github.com/apache/spark/pull/32298) does very similar merging, but we run that only once because merging can be costly when there are many candidates. Do we need `EliminateUnions` in `operatorOptimizationBatch`?
Sidenote: `MergeScalarSubqueries` doesn't work with different filters currently. This is because merging filters in subqueries is more comlicated as we need to propogate the filters up to an aggregate, and because it can cause performance degradation when we have non-overlapping scans. (See this WIP PR: https://github.com/apache/spark/pull/37630).","1,-1    ",1,-1,0
1536060234,8486025,2023/4/11,v3.2.4,"@peter-toth Thank you for your first look.
In fact, this PR references some functions as you mentioned above (e.g. `checkIdenticalPlans`). If we can share these functions, that be good.

The partitioning/bucketing column filter seems doesn't improve anything. I will optimize it in further.","1,-1    ",1,-1,0
1496091280,3182036,2023/4/11,v3.2.4,cc @gengliangwang @viirya @allisonwang-db ,"1,-1    ",1,-1,0
1497146642,3182036,2023/4/11,v3.2.4,"@viirya sorry I wrote it wrong. The attr IDs do not change. It should be
> However, with metadata columns, the analyzer will copy a scan relation with new output attributes to include metadata cols.","1,-1    ",1,-1,0
1499900796,3182036,2023/4/11,v3.2.4,"thanks for the review, merging to master (it doesn't fix any actual bug so no need to backport)!","1,-1    ",1,-1,0
1497382000,73838248,2023/4/11,v3.2.4,"Ping @LuciferYang @JoshRosen that previously discussed this issue in https://github.com/apache/spark/pull/37206 let me know if would be a better approach to reopen the old PR rather then creating a new one. And/or if my description of the data race does not sounds accurate to you.
","1,-1    ",1,-1,0
1497404693,1475305,2023/4/11,v3.2.4,"It's ok to me to further discussion in this one 
","1,-1    ",1,-1,0
1497963991,1475305,2023/4/11,v3.2.4,"I found that before Scala 2.13.6(include) seems no this issue and the new test will failed after 2.13.7.  

@eejbyfeldt I am not sure if this is caused by change of https://github.com/scala/scala/pull/9258, as it has been added to Scala 2.13.4.

also cc @srowen @mridulm and @xinrong-meng 
","1,-1    ",1,-1,0
1497998251,73838248,2023/4/11,v3.2.4,"> I found that before Scala 2.13.6(include) seems no this issue and the new test will failed after 2.13.7.
> 
> @eejbyfeldt I am not sure if this is caused by change of [scala/scala#9258](https://github.com/scala/scala/pull/9258), as it has been added to Scala 2.13.4.


Thanks for looking in to that closer. Should probably formulated myself more clearly that was only a guess on my part and not something I had verified. But now that you narrowed it down to 2.13.7 another possible candidate change could be https://github.com/scala/scala/pull/9786 that changed how that mutations are tracked in ArrayBuffer. 

EDIT: Even if [scala/scala#9258](https://github.com/scala/scala/pull/9258) was tagged milestone 2.13.4 at some point it looks to me like it actually landed in 2.13.7 with this commit https://github.com/scala/scala/commit/5f250021e7e55dd36e45d0b224a6cb6967e67ebd

","1,-1    ",1,-1,0
1498310261,1591700,2023/4/11,v3.2.4,"This makes sense.
The TaskRunner is visible to heartbeater since it gets added to `runningTasks` before the task binary is deserialized.
During `TaskRunner.run`, we are registering the accumulators as part of the deserialization - which can result in race with the heartbearter reading the metrics.

So this is essentially a bug waiting to happen for two reasons:

* `externalAccums` is getting used in an MT-unsafe way by the task thread and the reporter thread.
* This is broken in 2.12 as well - we were just not aware of it.
  * There always was potential visibility issues in heartbeat for external accumulators.

Note, the task itself is visible only after the deserialization is complete, since it is volatile - but the registration is not covered by it.
","1,-2    ",1,-2,-1
1498625968,1475305,2023/4/11,v3.2.4,"@eejbyfeldt `unused-imports` check failed, please fix it

```
[error] /home/runner/work/spark/spark/core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala:48:41: Unused import
[error] import org.apache.spark.internal.config.Network
[error]                                         ^
[error] one error found
```","1,-2    ",1,-2,-1
1498628326,1475305,2023/4/11,v3.2.4,"> I found that before Scala 2.13.6(include) seems no this issue and the new test will failed after 2.13.7.
> 
> @eejbyfeldt I am not sure if this is caused by change of [scala/scala#9258](https://github.com/scala/scala/pull/9258), as it has been added to Scala 2.13.4.
> 
> also cc @srowen @mridulm and @xinrong-meng

also cc @dongjoon-hyun due to Spark 3.2.x also use Scala 2.13.8 and maybe Spark 3.2.4 should include this one","1,-1    ",1,-1,0
1499814517,6477701,2023/4/11,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1499820415,1475305,2023/4/11,v3.2.4,"@HyukjinKwon branch-3.3 and branch-3.2 may also require this one, they also use Scala 2.13.8, need @eejbyfeldt  to submit an independent PRs?","1,-2    ",1,-2,-1
1499907848,9700541,2023/4/11,v3.2.4,"Thank you, @eejbyfeldt , @mridulm , @HyukjinKwon , and @LuciferYang .
I also backported to branch-3.3.

However, we need a new PR for branch-3.2 due to the compilation error, @LuciferYang and @eejbyfeldt .
```
[error] /Users/dongjoon/APACHE/spark-merge/core/src/test/scala/org/apache/spark/executor/ExecutorSuite.scala:33:19: object logging is not a member of package org.apache
[error] import org.apache.logging.log4j._
```","1,-2    ",1,-2,-1
1499982811,9700541,2023/4/11,v3.2.4,"To @LuciferYang  and all.

After double-checking, I found that Apache Spark 3.2.x is not affected because it uses Scala 2.13.5. 

https://github.com/apache/spark/blob/7773740e4141444bf78ba75dcee9f3fade7f6e11/pom.xml#L3389

SPARK-35496 (Scala 2.13.7) landed at Apache Spark 3.3.0+. We don't need to backport this to branch-3.2.
","1,-1    ",1,-1,0
1499983771,9700541,2023/4/11,v3.2.4,"Please let me know if this is still valid in `branch-3.2`.
> branch-3.3 and branch-3.2 may also require this one, they also use Scala 2.13.8","1,-1    ",1,-1,0
1500005046,1475305,2023/4/11,v3.2.4,@dongjoon-hyun Scala 2.13.5 does not require this fix. I apologize for providing incorrect information earlier,"1,-2    ",1,-2,-1
1500756781,9700541,2023/4/11,v3.2.4,"No problem at all. Thank you always, @LuciferYang !","1,-2    ",1,-2,-1
1500810809,1591700,2023/4/11,v3.2.4,"Thanks for checking @dongjoon-hyun and @LuciferYang !
Great to finally have this issue fixed :-)","1,-1    ",1,-1,0
1496750694,6477701,2023/4/11,v3.2.4,cc @itholic @zhengruifeng @xinrong-meng @Yikun if you find some time to review.,"1,-1    ",1,-1,0
1496753586,6477701,2023/4/11,v3.2.4,Merged to branch-3.4.,"1,-1    ",1,-1,0
1496856632,6477701,2023/4/11,v3.2.4,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1496775494,6477701,2023/4/11,v3.2.4,cc @zhengruifeng @ueshin FYI,"1,-1    ",1,-1,0
1496811319,6477701,2023/4/11,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1496816167,6477701,2023/4/11,v3.2.4,cc @allanf-db @zhengruifeng @ueshin FYI,"1,-2    ",1,-2,-1
1496960528,6477701,2023/4/11,v3.2.4,Merged to master and bracnh-3.4.,"1,-2    ",1,-2,-1
1497083617,6477701,2023/4/11,v3.2.4,Nice. cc @itholic ,"1,-1    ",1,-1,0
1499780072,6477701,2023/4/11,v3.2.4,Will merge this in few days. Please let me know if there's any concern.,"1,-1    ",1,-1,0
1501048604,6477701,2023/4/11,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1497349346,6477701,2023/4/11,v3.2.4,"Seems working fine.

Merged to master.","1,-1    ",1,-1,0
1511331257,9616802,2023/4/11,v3.2.4,Merging,"1,-1    ",1,-1,0
1512936430,1475305,2023/4/11,v3.2.4,"Find an error related to ReplE2ESuite on [GA test task](https://pipelines.actions.githubusercontent.com/serviceHosts/c184045e-b556-4e78-b8ef-fb37b2eda9a3/_apis/pipelines/1/runs/69161/signedlogcontent/23?urlExpires=2023-04-18T10%3A18%3A24.5673026Z&urlSigningMethod=HMACV1&urlSignature=IZ4kWbB8mtkvxvyxojX3%2FxIz43j%2FVRKl7Ghp2Y52nnE%3D):

```
023-04-18T03:29:20.8938544Z [0m[[0m[0minfo[0m] [0m[0m[32mReplE2ESuite:[0m[0m
2023-04-18T03:29:24.5685770Z sh: 1: cannot open /dev/tty: No such device or address
...
2023-04-18T03:29:25.5551653Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-18T03:29:25.5631256Z sh: 1: cannot create /dev/tty: No such device or address
...
2023-04-18T03:29:43.5473148Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.lang.RuntimeException: REPL Timed out while running command: [0m[0m
2023-04-18T03:29:43.5473697Z [0m[[0m[0minfo[0m] [0m[0m[31mclass A(x: Int) { def get = x * 5 + 19 }[0m[0m
2023-04-18T03:29:43.5474147Z [0m[[0m[0minfo[0m] [0m[0m[31mdef dummyUdf(x: Int): Int = new A(x).get[0m[0m
2023-04-18T03:29:43.5474578Z [0m[[0m[0minfo[0m] [0m[0m[31mval myUdf = udf(dummyUdf _)[0m[0m
2023-04-18T03:29:43.5480701Z [0m[[0m[0minfo[0m] [0m[0m[31mspark.range(5).select(myUdf(col(""id""))).as[Int].collect()[0m[0m
2023-04-18T03:29:43.5481161Z [0m[[0m[0minfo[0m] [0m[0m[31m      [0m[0m
2023-04-18T03:29:43.5481539Z [0m[[0m[0minfo[0m] [0m[0m[31mConsole output: [0m[0m
2023-04-18T03:29:43.5482081Z [0m[[0m[0minfo[0m] [0m[0m[31mSpark session available as 'spark'.[0m[0m
2023-04-18T03:29:43.5484862Z [0m[[0m[0minfo[0m] [0m[0m[31m   _____                  __      ______                            __[0m[0m
2023-04-18T03:29:43.5488022Z [0m[[0m[0minfo[0m] [0m[0m[31m  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_[0m[0m
2023-04-18T03:29:43.5491144Z [0m[[0m[0minfo[0m] [0m[0m[31m  \__ \/ __ \/ __ `/ ___/ //_/  / /   / __ \/ __ \/ __ \/ _ \/ ___/ __/[0m[0m
2023-04-18T03:29:43.5494244Z [0m[[0m[0minfo[0m] [0m[0m[31m ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_[0m[0m
2023-04-18T03:29:43.5497421Z [0m[[0m[0minfo[0m] [0m[0m[31m/____/ .___/\__,_/_/  /_/|_|   \____/\____/_/ /_/_/ /_/\___/\___/\__/[0m[0m
2023-04-18T03:29:43.5500566Z [0m[[0m[0minfo[0m] [0m[0m[31m    /_/[0m[0m
2023-04-18T03:29:43.5502973Z sh: 1: cannot open /dev/tty: No such device or address
...
2023-04-18T03:29:43.7154910Z [0m[[0m[0minfo[0m] [0m[0m[31mError output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc[0m[0m
2023-04-18T03:29:43.7155688Z [0m[[0m[0minfo[0m] [0m[0m[31mCompiling /home/runner/work/spark/spark/connector/connect/client/jvm/(console)[0m[0m
2023-04-18T03:29:43.7156273Z [0m[[0m[0minfo[0m] [0m[0m[31mjava.lang.RuntimeException: Nonzero exit value: 2[0m[0m
2023-04-18T03:29:43.7156688Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.package$.error(package.scala:30)[0m[0m
2023-04-18T03:29:43.7157121Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:138)[0m[0m
2023-04-18T03:29:43.7157538Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:108)[0m[0m
2023-04-18T03:29:43.7158104Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.TTY$.stty(Utils.scala:103)[0m[0m
2023-04-18T03:29:43.7158794Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.TTY$.withSttyOverride(Utils.scala:114)[0m[0m
2023-04-18T03:29:43.7160239Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.Terminal$.readLine(Terminal.scala:41)[0m[0m
2023-04-18T03:29:43.7162012Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.AmmoniteFrontEnd.readLine(AmmoniteFrontEnd.scala:133)[0m[0m
2023-04-18T03:29:43.7163249Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.AmmoniteFrontEnd.action(AmmoniteFrontEnd.scala:28)[0m[0m
2023-04-18T03:29:43.7163556Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.$anonfun$action$4(Repl.scala:194)[0m[0m
2023-04-18T03:29:43.7163863Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.$anonfun$flatMap$1(Signaller.scala:45)[0m[0m
2023-04-18T03:29:43.7164158Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Signaller.apply(Signaller.scala:28)[0m[0m
2023-04-18T03:29:43.7171303Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.flatMap(Signaller.scala:45)[0m[0m
2023-04-18T03:29:43.7172151Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.flatMap$(Signaller.scala:45)[0m[0m
2023-04-18T03:29:43.7172894Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Signaller.flatMap(Signaller.scala:16)[0m[0m
2023-04-18T03:29:43.7174248Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.$anonfun$action$2(Repl.scala:178)[0m[0m
2023-04-18T03:29:43.7176993Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.util.Catching.flatMap(Res.scala:115)[0m[0m
2023-04-18T03:29:43.7177811Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.action(Repl.scala:170)[0m[0m
2023-04-18T03:29:43.7178715Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.loop$1(Repl.scala:212)[0m[0m
2023-04-18T03:29:43.7179009Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.run(Repl.scala:227)[0m[0m
2023-04-18T03:29:43.7179272Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.Main.$anonfun$run$1(Main.scala:236)[0m[0m
2023-04-18T03:29:43.7179543Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.Option.getOrElse(Option.scala:189)[0m[0m
2023-04-18T03:29:43.7179795Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.Main.run(Main.scala:224)[0m[0m
2023-04-18T03:29:43.7184102Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:100)[0m[0m
2023-04-18T03:29:43.7185205Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.application.ReplE2ESuite$$anon$1.run(ReplE2ESuite.scala:59)[0m[0m
2023-04-18T03:29:43.7185569Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[0m[0m
2023-04-18T03:29:43.7185885Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m[0m
2023-04-18T03:29:43.7191750Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m[0m
2023-04-18T03:29:43.7192249Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m[0m
2023-04-18T03:29:43.7192662Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.lang.Thread.run(Thread.java:750)[0m[0m
```

I am currently unsure why this error occurred

","1,-1    ",1,-1,0
1513102087,21010250,2023/4/11,v3.2.4,"@LuciferYang It looks related to the TTY issues (https://github.com/com-lihaoyi/Ammonite/issues/276) we were hitting in the CI pipelines earlier. (`2023-04-18T03:29:24.5685770Z sh: 1: cannot open /dev/tty: No such device or address` in the logs is the indicator) 

[These](https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253) changes (taken from [here](https://github.com/com-lihaoyi/Ammonite/issues/276#issuecomment-439273906)) (helped us mitigate the issue for the CI pipelines but perhaps the GA test pipeline uses a different config? Any idea about the pipeline config @HyukjinKwon?

Also for further clarification, this issue only appears during the CI tests. Local testing does not face this issue.","1,-1    ",1,-1,0
1515507340,6477701,2023/4/11,v3.2.4,Is this just being flaky? All GA jobs share the same thing (that you fixed),"1,-1    ",1,-1,0
1515510014,4190164,2023/4/12,v3.2.4,"@HyukjinKwon I would not think it is flaky. It is probably the GA builds also missing the TTY setup in the build steps. So we need to copy the following changes to the steps:
```
      run: |
        # Fix for TTY related issues when launching the Ammonite REPL in tests.
        export TERM=vt100 && script -qfc 'echo exit | amm -s' && rm typescript
```
https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253","1,-1    ",1,-1,0
1515545618,6477701,2023/4/12,v3.2.4,@LuciferYang which GitHub job failed? The link seems stale now so I can't see. It is a scheduled build?,"1,-1    ",1,-1,0
1515558035,1475305,2023/4/12,v3.2.4,"> @LuciferYang which GitHub job failed? The link seems stale now so I can't see. It is a scheduled build?

I found this issue from GA task of user pr

The last failed build on https://github.com/apache/spark/pull/40783  also had this issue, I don't know if @rangadi  can provide a new link","1,-1    ",1,-1,0
1515558746,1475305,2023/4/12,v3.2.4,@HyukjinKwon https://github.com/rangadi/spark/actions/runs/4737137341/jobs/8409588363 this one,"1,-1    ",1,-1,0
1515586188,6477701,2023/4/12,v3.2.4,"Ah, i think it just happened because of unsynced fork. It should be fixed if their fork is synced to the lastest master branch.","1,-1    ",1,-1,0
1515617336,1475305,2023/4/12,v3.2.4,"Hmm... GA should merge the current pr to the latest master before testing, right? 

It's okay, Let's wait and see if there are any new cases happening.

","1,-1    ",1,-1,0
1515619326,26535726,2023/4/12,v3.2.4,"> @HyukjinKwon I would not think it is flaky. It is probably the GA builds also missing the TTY setup in the build steps. So we need to copy the following changes to the steps:
> 
> ```
>       run: |
>         # Fix for TTY related issues when launching the Ammonite REPL in tests.
>         export TERM=vt100 && script -qfc 'echo exit | amm -s' && rm typescript
> ```
> 
> https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253

@zhenlineo is there some reference about such magic command? better to briefly explain the mechanism or leave a link in comments","1,-1    ",1,-1,0
1515644305,6477701,2023/4/12,v3.2.4,"> Hmm... GA should merge the current pr to the latest master before testing, right?

Yes except the workflow file. For the changes in workflow, they have to manually update to the latest master branch.","1,-1    ",1,-1,0
1515645182,1475305,2023/4/12,v3.2.4,"> > Hmm... GA should merge the current pr to the latest master before testing, right?
> 
> Yes except the workflow file. For the changes in workflow, they have to manually update to the latest master branch.

Got it ~","1,-2    ",1,-2,-1
1517995794,1475305,2023/4/12,v3.2.4,"@vicennial I found `ReplE2ESuite` always failed in Java 17 GA daily test:

- https://github.com/apache/spark/actions/runs/4726264540/jobs/8385681548
- https://github.com/apache/spark/actions/runs/4737365554/jobs/8410097712
- https://github.com/apache/spark/actions/runs/4748319019/jobs/8434392414
- https://github.com/apache/spark/actions/runs/4759278349/jobs/8458399201

<img width=""1307"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/233674106-5cf0c4cf-ed4f-4d75-be42-3b7c39dc2936.png"">
","1,-1    ",1,-1,0
1497551347,25019163,2023/4/12,v3.2.4,@zhenlineo @hvanhovell ,"1,-1    ",1,-1,0
1497582797,25019163,2023/4/12,v3.2.4,"The client script still does
```
CONNECT_CLASSPATH=""$(build/sbt ""${SCALA_ARG}"" -DcopyDependencies=false ""export connect-client-jvm/fullClasspath"" | grep jar | tail -n1)""
SQL_CLASSPATH=""$(build/sbt ""${SCALA_ARG}"" -DcopyDependencies=false ""export sql/fullClasspath"" | grep jar | tail -n1)""
```
and these sbt commands still take almost a minute (20-30s each) before it drops me into the client shell...
I wonder if something is wrong with my setup.","1,-3    ",1,-3,-2
1498716991,25019163,2023/4/12,v3.2.4,Updated PR description.,"2,-2    ",2,-2,0
1499095448,25019163,2023/4/12,v3.2.4,Also added possibility to provide saved classpath without waiting 1 minute for sbt to compute it.,"1,-1    ",1,-1,0
1504419536,6477701,2023/4/12,v3.2.4,Hm .. this only got into the master branch but the original fix wend down to branch-3.4. (https://github.com/apache/spark/pull/40257). Would probably need a separate JIRA in such case next time ;-).,"1,-1    ",1,-1,0
1505087804,25019163,2023/4/12,v3.2.4,I think it's fine in master only. It's dev scripts and probably majority of dev will be in master moving forward.,"1,-2    ",1,-2,-1
1506144424,6477701,2023/4/12,v3.2.4,"Yeah, merging it to master only is fine. The point I was making was to have a different JIRA so we know which version the fix has landed to (so as to make it easier when we revert, etc.)","1,-3    ",1,-3,-2
1504498086,3182036,2023/4/12,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1498401644,7788766,2023/4/12,v3.2.4,cc @cloud-fan @dongjoon-hyun @beliefer ,"1,-1    ",1,-1,0
1498981627,3182036,2023/4/12,v3.2.4,also cc @yaooqinn ,"1,-1    ",1,-1,0
1535607944,3182036,2023/4/12,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1498078371,6235869,2023/4/12,v3.2.4,"Let me also check `NoSuchIndexException`, `IndexAlreadyExistsException`. It may be a bit harder to restore those.","1,-1    ",1,-1,0
1498101815,6235869,2023/4/12,v3.2.4,cc @dongjoon-hyun @viirya @huaxingao @gengliangwang @cloud-fan @gatorsmile @sunchao,"1,-1    ",1,-1,0
1498344000,9616802,2023/4/12,v3.2.4,Just curious. Why wasn't this caught by MiMa?,"1,-1    ",1,-1,0
1498377624,6235869,2023/4/12,v3.2.4,"@hvanhovell, I think because the Catalyst package is excluded in MiMa checks.

```
ProblemFilters.exclude[Problem](""org.apache.spark.sql.catalyst.*""),
```

Exceptions are in `org.apache.spark.sql.catalyst.analysis`.","1,-1    ",1,-1,0
1498378134,6235869,2023/4/12,v3.2.4,I'll check test failures in a bit.,"1,-1    ",1,-1,0
1498500411,1097932,2023/4/12,v3.2.4,"Thanks, merging to master/3.4","1,-1    ",1,-1,0
1498573509,6235869,2023/4/12,v3.2.4,"Thanks for reviewing, @gengliangwang!","1,-1    ",1,-1,0
1498249353,6477701,2023/4/12,v3.2.4,let me just directly revert.,"1,-1    ",1,-1,0
1499818206,7322292,2023/4/12,v3.2.4,"thanks, merged to master","1,-1    ",1,-1,0
1499832054,8326978,2023/4/12,v3.2.4,cc @dongjoon-hyun @cloud-fan thanks,"1,-1    ",1,-1,0
1499880602,8326978,2023/4/12,v3.2.4,"thanks, merged to master","1,-2    ",1,-2,-1
1499869942,1938382,2023/4/12,v3.2.4,LGTM,"1,-2    ",1,-2,-1
1501601564,32387433,2023/4/12,v3.2.4,"@HyukjinKwon @amaliujia @hvanhovell Hi, I add support for python. PTAL again. Thanks!","1,-1    ",1,-1,0
1508001349,32387433,2023/4/12,v3.2.4,"@HyukjinKwon @hvanhovell Hi, kindly ask, can merge this PR now? ","1,-1    ",1,-1,0
1517122121,32387433,2023/4/12,v3.2.4,kindly ping @HyukjinKwon ,"1,-2    ",1,-2,-1
1517134131,6477701,2023/4/12,v3.2.4,I am fine. but I would defer to @hvanhovell to merge.,"1,-2    ",1,-2,-1
1525614789,32387433,2023/4/12,v3.2.4,kindly ping @hvanhovell ,"1,-1    ",1,-1,0
1538334844,32387433,2023/4/12,v3.2.4,"Hi, can we merge this PR now? @HyukjinKwon @hvanhovell @xinrong-meng ð","1,-1    ",1,-1,0
1538365093,9616802,2023/4/12,v3.2.4,Merging.,"1,-2    ",1,-2,-1
1538371068,32387433,2023/4/12,v3.2.4,Thanks! @HyukjinKwon @amaliujia @hvanhovell @grundprinzip @xinrong-meng ,"1,-2    ",1,-2,-1
1499692742,5399861,2023/4/12,v3.2.4,cc @cloud-fan,"1,-1    ",1,-1,0
1501607154,5399861,2023/4/12,v3.2.4,@dongjoon-hyun Both branch-2.x and branch-3.x have this issue.,"1,-1    ",1,-1,0
1501992099,3182036,2023/4/12,v3.2.4,"3.2 will be EOL soon, shall we backport to 3.3+?","1,-1    ",1,-1,0
1501994635,3182036,2023/4/12,v3.2.4,or shall we include this fix in 3.2 as it's the last chance? I don't have a strong opinion. @dongjoon-hyun can you make a call here since you are the release manager of 3.2.4?,"1,-2    ",1,-2,-1
1502016243,9700541,2023/4/12,v3.2.4,"I'd prefer to consider this for `branch-3.3`+ because this was not a regression at 3.2, @cloud-fan ~","1,-1    ",1,-1,0
1502016856,9700541,2023/4/12,v3.2.4,"BTW, are you going to block Apache Spark 3.4 RC7, @cloud-fan ?

cc @xinrong-meng , too","1,-1    ",1,-1,0
1502553970,3182036,2023/4/12,v3.2.4,"Since it's not a regression, we don't need to block 3.4 either.","1,-2    ",1,-2,-1
1502557611,9700541,2023/4/12,v3.2.4,"Ya, I think so too~","1,-2    ",1,-2,-1
1502558052,9700541,2023/4/12,v3.2.4,This patch can wait for Apache Spark 3.4.1 and 3.3.3.,"1,-1    ",1,-1,0
1509470713,5399861,2023/4/13,v3.2.4,"Merged to master, branch-3.4 and branch-3.3.","1,-1    ",1,-1,0
1499428796,1134574,2023/4/13,v3.2.4,cc @dongjoon-hyun @huaxingao @viirya to take a look. Thanks for contribution.,"1,-1    ",1,-1,0
1523054485,1002986,2023/4/13,v3.2.4,"adding @rangadi as well, as they seem to have been a recent committer on a lot of the protobuf parsing logic!","1,-1    ",1,-1,0
1526548109,502522,2023/4/13,v3.2.4,FYI: I sent another improvement for Protobuf here: https://github.com/apache/spark/pull/40983,"1,-1    ",1,-1,0
1526729416,1002986,2023/4/13,v3.2.4,"@rangadi thanks for the comments! i've updated the pr quite a bit, namely i've split it up into two commits:
- commit 1: add tests that show the current state
- commit 2: add flag + modify tests to show what changed

and i addressed many of your comments as well along the way ð ","1,-1    ",1,-1,0
1528873292,1002986,2023/4/13,v3.2.4,"Thanks a bunch @pang-wu and @rangadi for the thoughtful discussion, really appreciate all the feedback and input ð ","1,-1    ",1,-1,0
1531258227,1002986,2023/4/13,v3.2.4,"okay @rangadi @pang-wu (and cc @viirya)  i think i've addressed all comments; i've referenced the external libraries and also removed the mentions of ""ambiguity"" in the documentation. i've also renamed the option `emit.default.values` which i think is clearer than `materialize.default.values`. would y'all be able to take another look? ","1,-1    ",1,-1,0
1532446807,502522,2023/4/13,v3.2.4,@justaparth thanks for the PR. Please ping @gengliangwang to merge this once the tests pass (or @viirya if he can merge).,"1,-1    ",1,-1,0
1532749008,1002986,2023/4/13,v3.2.4,"thanks @rangadi!

cc @viirya or @gengliangwang , the tests have passed! do you mind taking a look and helping us merge this? thanks a bunch!","1,-1    ",1,-1,0
1533676324,502522,2023/4/13,v3.2.4,@HeartSaVioR could you merge this? You don't need to review.,"1,-1    ",1,-1,0
1534070113,6477701,2023/4/13,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1499296690,1591700,2023/4/13,v3.2.4,QQ: When does description actually become `null` ? (other than synthetic cases like tests),"1,-1    ",1,-1,0
1499302582,1633312,2023/4/13,v3.2.4,"> QQ: When does description actually become `null` ? (other than synthetic cases like tests)

I don't know when filename would be null. My guess would be compiler removed filename info or different JVM implementation. I just saw this in real event log. Doing defensive null handling is no harm here.","1,-1    ",1,-1,0
1499303226,1591700,2023/4/13,v3.2.4,"Filename can be null, my query was about description :-) ","1,-1    ",1,-1,0
1499306408,1633312,2023/4/13,v3.2.4,"> Filename can be null, my query was about description :-)

What do you mean by ""description"" here?","1,-1    ",1,-1,0
1502409840,1633312,2023/4/13,v3.2.4,"> BTW, according to JIRA, is this a regression at Apache Spark 3.3.2, @warrenzhu25 ?

I don't think so.","1,-2    ",1,-2,-1
1502418739,9700541,2023/4/13,v3.2.4,"Do you happen to know when this bug starts, @warrenzhu25 ?","1,-1    ",1,-1,0
1502423743,1633312,2023/4/13,v3.2.4,"> Do you happen to know when this bug starts, @warrenzhu25 ?

Sorry, I have no idea. It's 1st time I have seen this.","1,-1    ",1,-1,0
1502424431,9700541,2023/4/13,v3.2.4,"Thank you for your answers, @warrenzhu25 .","1,-1    ",1,-1,0
1516820300,1633312,2023/4/13,v3.2.4,@dongjoon-hyun @mridulm Do you have more comments on this?,"1,-1    ",1,-1,0
1517319566,1591700,2023/4/13,v3.2.4,Looks like my comments were not addressed ?,"1,-1    ",1,-1,0
1517342815,1633312,2023/4/13,v3.2.4,"> Looks like my comments were not addressed ?

Sorry, forget to push. Updated.","1,-1    ",1,-1,0
1517348391,1591700,2023/4/13,v3.2.4,"+CC @dongjoon-hyun for review, since you took a look at this PR before","2,-2    ",2,-2,0
1524683595,1591700,2023/4/13,v3.2.4,"Merging to master.
Thanks for fixing this @warrenzhu25 
Thanks for the review @HyukjinKwon, @srowen :-)","1,-1    ",1,-1,0
1499974575,9700541,2023/4/13,v3.2.4,"cc @imback82, @cloud-fan , @viirya , @sunchao ","1,-2    ",1,-2,-1
1500003144,13965087,2023/4/13,v3.2.4,"> We may need adding test case.

yeah , i will add UT later","1,-3    ",1,-3,-2
1500479527,13965087,2023/4/13,v3.2.4,"One more question , it time to make the default value of `SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED`  as true ?","1,-1    ",1,-1,0
1500503549,9700541,2023/4/13,v3.2.4,"Maybe, no? If this is not working properly before, we cannot enable this configuration at Apache Spark 3.5.0. Since we need to wait for one release cycle, we may be able to do that at Apache Spark 3.6.0 if we want.
> One more question , it time to make the default value of `SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED` as true ?

","1,-1    ",1,-1,0
1500796714,13965087,2023/4/13,v3.2.4,"> Maybe, no? If this is not working properly before, we cannot enable this configuration at Apache Spark 3.5.0. Since we need to wait for one release cycle, we may be able to do that at Apache Spark 3.6.0 if we want.
> 
> > One more question , it time to make the default value of `SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED` as true ?

Yes, this is the more logical way.","1,-1    ",1,-1,0
1500796913,13965087,2023/4/13,v3.2.4,"The CI build failure doesn't seem to be caused by this patch, can you take a look?

@dongjoon-hyun @viirya ","1,-1    ",1,-1,0
1501394034,9700541,2023/4/13,v3.2.4,"Please rebase to the `master` branch once more, @zzzzming95 .
> The CI build failure doesn't seem to be caused by this patch, can you take a look?","1,-1    ",1,-1,0
1503491070,13965087,2023/4/13,v3.2.4,@cloud-fan @dongjoon-hyun @viirya  Please merge to master . Thanks ~,"1,-1    ",1,-1,0
1503821063,9700541,2023/4/13,v3.2.4,"To be clear, this PR didn't get any approval yet, @zzzzming95 .
> Please merge to master . Thanks ~","1,-1    ",1,-1,0
1507873131,3182036,2023/4/13,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1528783879,4223091,2023/4/13,v3.2.4,"I was the OP of the issue in the jira.

Thank you for the fix, but I discovered a weird behavior when hints applied and I don't know how to interpret it. Please check [SPARK-43326](https://issues.apache.org/jira/browse/SPARK-43326) I filled","1,-1    ",1,-1,0
1528784672,13965087,2023/4/13,v3.2.4,"> I was the OP of the issue in the jira.
> 
> Thank you for the fix, but I discovered a weird behavior when hints applied and I don't know how to interpret it. Please check [SPARK-43326](https://issues.apache.org/jira/browse/SPARK-43326) I filled

Okay, I will follow up on this issue","1,-1    ",1,-1,0
1502177124,502522,2023/4/13,v3.2.4,cc: @HyukjinKwon please merge. ,"1,-1    ",1,-1,0
1502484037,6477701,2023/4/13,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1500652566,4784782,2023/4/13,v3.2.4,"This happens on a benchmark job generating a large number of very tiny blocks. When the job is finished, the cluster tries to shutdown the idle executors and migrate all the blocks to other active executors, the driver acts like hanging, then resumed after a while.","1,-1    ",1,-1,0
1500810568,1591700,2023/4/13,v3.2.4,"You are right about the null -> Int, should have checked better :-)","1,-1    ",1,-1,0
1533324077,9700541,2023/4/13,v3.2.4,"Since this is `Performance` PR, do you think you can contribute a micro-benchmark which is similar to your case? Maybe, as an independent JIRA issue?
> This happens on a benchmark job generating a large number of very tiny blocks. When the job is finished, the cluster tries to shutdown the idle executors and migrate all the blocks to other active executors, the driver acts like hanging, then resumed after a while.

","1,-2    ",1,-2,-1
1548728418,4784782,2023/4/13,v3.2.4,@dongjoon-hyun I created https://issues.apache.org/jira/browse/SPARK-43515 as a followup task to add a micro-benchmark.,"1,-2    ",1,-2,-1
1550174613,4784782,2023/4/13,v3.2.4,"Merged to master/3.4, thanks all!","1,-1    ",1,-1,0
1550286780,9700541,2023/4/13,v3.2.4,"Also, +1 for backporting decision of @jiangxb1987 on this improvement PR.","1,-1    ",1,-1,0
1500623778,10248890,2023/4/13,v3.2.4,CC @rangadi @pengzhon-db,"1,-2    ",1,-2,-1
1502715640,10248890,2023/4/13,v3.2.4,Hi @HyukjinKwon could you please take another look? Thanks!,"1,-2    ",1,-2,-1
1505663707,10248890,2023/4/13,v3.2.4,@HyukjinKwon can you merge this? Thank you!,"1,-1    ",1,-1,0
1506121449,6477701,2023/4/13,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1499782987,6477701,2023/4/13,v3.2.4,cc @zhengruifeng ,"1,-2    ",1,-2,-1
1499815273,7322292,2023/4/13,v3.2.4,"Just FYI, vanilla PySpark's DataFrame.toPandas also has this issue https://issues.apache.org/jira/browse/SPARK-41971
Is it possible to move the changes to `ArrowUtils` to fix them all?","1,-2    ",1,-2,-1
1499840711,506656,2023/4/13,v3.2.4,"> Just FYI, vanilla PySpark's DataFrame.toPandas also has this issue [issues.apache.org/jira/browse/SPARK-41971](https://issues.apache.org/jira/browse/SPARK-41971)
Is it possible to move the changes to ArrowUtils to fix them all?

Yes, I'm aware of the issue, but let me hold on it to the following PRs.
(Thanks for filing the ticket, btw. ð )

TL;DR

Actually this PR still has an issue with `toPandas`.

```py
>>> spark.sql(""values (1, struct(1 as a, 2 as a)) as t(x, y)"").toPandas()
   x                     y
0  1  {'a_0': 1, 'a_1': 2}
```

The duplicated fields have suffix `_1`, `_2`, and so on.

Also, handling struct type in `toPandas` was not well-defined and there are behavior difference even between Arrow enabled/disabled in PySpark.

```py
>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)
>>> spark.sql(""values (1, struct(1 as a, 2 as b)) as t(x, y)"").toPandas()
   x       y
0  1  (1, 2)
>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)
>>> spark.sql(""values (1, struct(1 as a, 2 as b)) as t(x, y)"").toPandas()
   x                 y
0  1  {'a': 1, 'b': 2}
```

Currently PySpark with Arrow enabled, and Spark Connect, use a map for the struct type object as a result, whereas `Row` object in PySpark without Arrow.

The options are:

1. It's ok to be different, also with suffix.
    - In this case, the suffix is a must because a map object will hold only one value for the duplicates.
2. `Row` object should be used for the struct.
    - In this case, we will lose the benefit of Arrow -> pandas fast conversion.","1,-1    ",1,-1,0
1504715146,6477701,2023/4/13,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1499981247,1938382,2023/4/13,v3.2.4,@hvanhovell @cloud-fan ,"1,-1    ",1,-1,0
1503248755,3182036,2023/4/13,v3.2.4,"thanks, merging to master!","1,-2    ",1,-2,-1
1501443519,6477701,2023/4/13,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1500819117,7322292,2023/4/13,v3.2.4,cc @WeichenXu123 @HyukjinKwon ,"1,-1    ",1,-1,0
1501260234,19235986,2023/4/13,v3.2.4,"We also need to change `TorchDistributor._run_local_training` implementation.

The existing code it executes pytorch code in client side, but in spark connect case, we should execute pytorch code in server side (we can reuse _run_distributed_training code in the case, but local mode spark job does not support GPU schdeduling, instead, we can broadcast the selected driver GPU list to all tasks and each task select its GPU id via task rank). ","1,-2    ",1,-2,-1
1501269024,19235986,2023/4/13,v3.2.4,"On second thought,

I propose to make TorchDistributor._run_local_training only supports spark legacy mode,

but for ` TorchDistributor._run_distributed_training` , we should make it support both legacy mode and spark connect mode, i.e., when running on spark local mode cluster, but user set `TorchDistributor.local_mode=False`, it executes `TorchDistributor._run_distributed_training`, in this case, current master code does not handle GPU allocation correctly, you need to fix it (we can broadcast the selected driver GPU list to all tasks and each task select its GPU id via task rank). ","1,-2    ",1,-2,-1
1501300769,7322292,2023/4/13,v3.2.4,"> The existing code it executes pytorch code in client side, but in spark connect case, we should execute pytorch code in server side

yes, I feel it is non-trivial to execute pytorch code in server side, since we need to launch a new Python process in the server side and then communicate with it.

> I propose to make TorchDistributor._run_local_training only supports spark legacy mode

agree

> running on spark local mode cluster, but user set TorchDistributor.local_mode=False

why not just failing it?

","1,-1    ",1,-1,0
1501312338,19235986,2023/4/13,v3.2.4,"> running on spark local mode cluster, but user set TorchDistributor.local_mode=False

I think we need to support this, because for spark ML algorithm implemented atop TorchDistributor, we hope to support either spark local mode or spark cluster mode.","1,-1    ",1,-1,0
1501315354,19235986,2023/4/13,v3.2.4,"> yes, I feel it is non-trivial to execute pytorch code in server side, since we need to launch a new Python process in the server side and then communicate with it.

I think it does not require too much work, we can reuse most code of `TorchDistributor._run_distributed_training`, we just need to fix one issue:
current master code does not handle GPU allocation correctly, we can broadcast the selected driver GPU list to all tasks and each task select its GPU id via task rank.","1,-1    ",1,-1,0
1501338874,19235986,2023/4/14,v3.2.4,"Summary: for spark connect mode:

If torchDistributor.local_mode is True, raise error saying no support.

If torchDistributor.local_mode is False, and spark server side is spark local mode, we need to fix the issue https://github.com/apache/spark/pull/40695#issuecomment-1501315354 

If torchDistributor.local_mode is False, and spark server side is spark cluster mode, current master code works fine either with GPU or without GPU config.","1,-1    ",1,-1,0
1501379283,3421,2023/4/14,v3.2.4,What is local mode and why would you not support it on the client?,"1,-1    ",1,-1,0
1501384634,19235986,2023/4/14,v3.2.4,"> What is local mode

Let me clarify it,

TorchDistributer has ""local mode"" configure, if true, it just run torch program in client side, if False, it launches a spark job to run the torch program. So the TorchDistributer side ""local mode"" has nothing to do with spark master local mode.


> why would you not support it on the client?

I assume you mean to run torch program in spark connect client machine, we can support this, but I think it is less meaningful, because client machine should usually run lightweight workloads, but torch programs are heavy workloads and they often requires GPU, which client machine are hard to satisfy the condition.


","1,-1    ",1,-1,0
1501388453,3421,2023/4/14,v3.2.4,"> I assume you mean to run torch program in spark connect client machine, we can support this, but I think it is less meaningful, because client machine should usually run lightweight workloads, but torch programs are heavy workloads and they often requires GPU, which client machine are hard to satisfy the condition.

I don't think this is a valid assumption. With Spark Connect you can actually build an environment in which you have GPU locally but don't have a GPU on your cluster. In this case you still want to leverage the same execution flow. I've previously talked to users that were looking for an EC2 setup with a GPU attached and running the workloads from there against Sprk using Spark Connect. 

This is very similar to running sklearn locally on the client side.

It's not the only way, but it's a very valid way.","1,-1    ",1,-1,0
1501397222,19235986,2023/4/14,v3.2.4,"> I don't think this is a valid assumption. With Spark Connect you can actually build an environment in which you have GPU locally but don't have a GPU on your cluster. In this case you still want to leverage the same execution flow. I've previously talked to users that were looking for an EC2 setup with a GPU attached and running the workloads from there against Sprk using Spark Connect.


OK make sense, we can support it too @zhengruifeng , in this case, we can read client side environment variable `CUDA_VISIBLE_DEVICES` to determine which GPU devices we can use.","1,-1    ",1,-1,0
1502498964,7322292,2023/4/14,v3.2.4,@grundprinzip would you mind taking another look at the changes in protos?,"1,-1    ",1,-1,0
1504351407,7322292,2023/4/14,v3.2.4,@WeichenXu123 mind taking another look?,"1,-1    ",1,-1,0
1504825054,7322292,2023/4/14,v3.2.4,merged to master,"1,-1    ",1,-1,0
1499909905,100322362,2023/4/14,v3.2.4,@HeartSaVioR - PTAL. Thanks,"1,-1    ",1,-1,0
1499982120,1317309,2023/4/14,v3.2.4,"CI fails with odd errors.
@anishshri-db Could you please rebase so that CI is retriggered? If the new trial fails again, maybe good to post to dev@ and see whether someone encountered this before, and/or someone is willing to volunteer to fix if that's not a transient issue.","1,-1    ",1,-1,0
1500446559,100322362,2023/4/14,v3.2.4,">  Could you please rebase so that CI is retriggered? If the new trial fails again, maybe good to post to dev@ and see whether someone encountered this before, and/or someone is willing to volunteer to fix if that's not a transient issue.

Seems like a known issue someone already posted to dev channel - https://github.com/sbt/sbt/issues/7202","1,-1    ",1,-1,0
1500555522,100322362,2023/4/14,v3.2.4,@HeartSaVioR - all tests passed. Please merge when you get a chance. Thx,"1,-1    ",1,-1,0
1501422540,1317309,2023/4/14,v3.2.4,Missed the notification. Thanks! Merging to master.,"1,-1    ",1,-1,0
1499958423,3182036,2023/4/14,v3.2.4,cc @viirya @dongjoon-hyun @yaooqinn @beliefer ,"1,-1    ",1,-1,0
1499958805,9700541,2023/4/14,v3.2.4,"Thank you for pinging me, @cloud-fan .","1,-1    ",1,-1,0
1499960023,9700541,2023/4/14,v3.2.4,"Also, cc @sunchao ","1,-1    ",1,-1,0
1499977213,3182036,2023/4/14,v3.2.4,"@dongjoon-hyun This is a bit of a low-level change and ideally we should run all end-to-end tests twice with the config on and off. However, it seems not worthwhile to double the test resource for this change. How about we turn it on by default after we have enough SQL operator coverage?","1,-1    ",1,-1,0
1499984943,9700541,2023/4/14,v3.2.4,Got it. +1 for the testing plan.,"1,-1    ",1,-1,0
1499999785,8486025,2023/4/14,v3.2.4,@cloud-fan Thank you for ping me. What is the performance before or after this PR?,"1,-1    ",1,-1,0
1500011078,8326978,2023/4/14,v3.2.4,"I'm just curious about the prefix Task for Evaluator. Is it more specific to Partition, Split for SQL/DataFrame, or RDD? `Task` more likely belongs to the scheduler. Before reviewing the PR description and implementation, I thought it was to evaluate the cost of task execution or something. :)","1,-1    ",1,-1,0
1500034764,3182036,2023/4/14,v3.2.4,"@beliefer This is not a performance feature. It's just to avoid people making mistakes referencing extra objects in the closure, which can slow down task serialization and increase query latency.

@yaooqinn I don't have a strong opinion. how about PartitionEvaluator?","1,-1    ",1,-1,0
1500115645,8326978,2023/4/14,v3.2.4,"`PartitionEvaluator` looks better to me, altho I don't have a strong option either.","1,-1    ",1,-1,0
1500156450,8486025,2023/4/14,v3.2.4,"> @beliefer This is not a performance feature. It's just to avoid people making mistakes referencing extra objects in the closure, which can slow down task serialization and increase query latency.

Got it.
","1,-1    ",1,-1,0
1502007270,3182036,2023/4/14,v3.2.4,"thanks for the review, merging to master!","1,-1    ",1,-1,0
1502174667,9700541,2023/4/14,v3.2.4,"Thank you, @cloud-fan and all!","1,-2    ",1,-2,-1
1500814404,6477701,2023/4/14,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1500823598,1475305,2023/4/14,v3.2.4,@Yikf Can you re-trigger GA?,"1,-1    ",1,-1,0
1503400401,3182036,2023/4/14,v3.2.4,"The change LGTM. Can we also check other systems like Preso/Trino/Impala and see how they display null values?

cc @HyukjinKwon @viirya ","1,-1    ",1,-1,0
1503408659,1475305,2023/4/14,v3.2.4,"> The change LGTM. Can we also check other systems like Preso/Trino/Impala and see how they display null values?

@Yikf Can you help check  Preso/Trino ? Also check PostgreSQL?

","1,-1    ",1,-1,0
1504461118,51110188,2023/4/14,v3.2.4,"> The change LGTM. Can we also check other systems like Preso/Trino/Impala and see how they display null values?
> 
> cc @HyukjinKwon @viirya


> > The change LGTM. Can we also check other systems like Preso/Trino/Impala and see how they display null values?
> 
> @Yikf Can you help check Preso/Trino ? Also check PostgreSQL?

Sure, As following,
**Trino**
```shell
trino> select ARRAY[1, null];
   _col0
-----------
 [1, NULL]
(1 row)

Query 20230412_023104_00007_unnu6, FINISHED, 1 node
Splits: 1 total, 1 done (100.00%)
0.03 [0 rows, 0B] [0 rows/s, 0B/s]

trino> select null;
 _col0
-------
 NULL
(1 row)

Query 20230412_023108_00008_unnu6, FINISHED, 1 node
Splits: 1 total, 1 done (100.00%)
0.07 [0 rows, 0B] [0 rows/s, 0B/s]
```
**PostgreSQL**
```shell
postgres=# select array[1, null];
  array
----------
 {1,NULL}
(1 row)
```","1,-1    ",1,-1,0
1506228355,3182036,2023/4/14,v3.2.4,"thanks, merging to master!","1,-2    ",1,-2,-1
1517271113,7788766,2023/4/14,v3.2.4,"Actually, this is a breaking change for CAST expression. It affects the string representation of complex types and not just `df.show` which could cause issues when comparing the string representation of a row:

```sql
select cast(struct(null) as string) as c0, null as c1
```

```
-- df.show()
+------+----+
|    c0|  c1|
+------+----+
|{NULL}|NULL|
+------+----+

-- df.collect.toList
List([{NULL},null])
```

It also affects CSV writes:
```scala
val df = spark.sql(
  """"""
  select cast(array(1,2) as string) as c0, 1 as c1
  union all
  select cast(array(3,null) as string) as c0, null as c1
  """"""
).coalesce(1)
df.write.csv(""file:/tmp/ivan_test.csv"")
```

Before this PR:
```
""[1, 2]"",1
""[3, null]"",
```

After this PR:
```
""[1, 2]"",1
""[3, NULL]"",
```

And there is no option to re-enable null format.

On a broader side of things: It would be good to clarify why spark-sql shell is expected to be visually compatible with spark-shell, those are two different REPLs with their own ways of displaying data. For example, we don't display a table the same way in those REPLs.","1,-1    ",1,-1,0
1517271441,7788766,2023/4/14,v3.2.4,cc @cloud-fan @dongjoon-hyun ,"1,-1    ",1,-1,0
1517277895,3182036,2023/4/14,v3.2.4,"@sadikovi CSV does not accept struct/array/map columns, your example generates a string column and writes it to CSV. That said, any behavior change of that ""string generation"" function will change the value we write to CSV, but I wouldn't call it a CSV behavior change.

It's intentional that the cast struct/array/map to string behavior is changed w.r.t. nulls. If third-party libraries rely on this behavior, then we need to revisit this PR.

BTW, I think PR description should not say it's for consistency with spark-sql shell. `df.show` displays top-level null value as `NULL`, but inner field null value as `null`. This is the major motivation of the fix.","1,-1    ",1,-1,0
1517305453,7788766,2023/4/14,v3.2.4,"I just tried without the change and df.show() seems to return correct results: 
```scala
spark.sql(
  """"""
  select struct('a'), map('a', 1), array(1, 2), 'a'
  union all
  select struct(null), map('a', null), array(null, null), null 
  """""").show()
```

```
+---------+-----------+------------+----+
|struct(a)|  map(a, 1)| array(1, 2)|   a|
+---------+-----------+------------+----+
|      {a}|   {a -> 1}|      [1, 2]|   a|
|   {null}|{a -> null}|[null, null]|null|
+---------+-----------+------------+----+
```

Could you provide an example of inconsistency in df.show() that this PR addresses?

The reason I brought the CSV example is because that is typically how users would write structs/arrays/maps to a CSV file, they would transform that to a string, potentially via CAST. That is why I think this is a behaviour change in writing a CSV file.","1,-1    ",1,-1,0
1517330580,51110188,2023/4/14,v3.2.4,"> BTW, I think PR description should not say it's for consistency with spark-sql shell. `df.show` displays top-level null value as `NULL`, but inner field null value as `null`. This is the major motivation of the fix.

Actually, previously, the top-level is consistent with the inner null display with `null`. This PR mainly thinks that the mainstream database uses `NULL` instead of `null` when displaying null, and `NULL` is a better nice string representation.

If it's affecting something else, I think we should look at it again, sorry ~ : )


> For example, we don't display a table the same way in those REPLs.

BTW, For apache spark, spark-sql and spark-shell are two different REPLs, but they are only user interaction interfaces. Shouldn't the displayed content be the same? At least in terms of the result set.","2,-1    ",2,-1,1
1517335589,51110188,2023/4/14,v3.2.4,"> which could cause issues when comparing the string representation of a row:

Sorry, I'm not sure what that means, does it mean compared to what was written before this PR? Any problems if cast(null as string) with NULL, I see that trino is also NULL
``` SQL
trino> select cast(null as varchar) as c0, null as c1;
  c0  |  c1
------+------
 NULL | NULL
(1 row)
```","1,-1    ",1,-1,0
1517376514,3182036,2023/4/14,v3.2.4,"@sadikovi Sorry I was wrong, `Dataset.show` also explicitly prints null as `null` before this PR.

Being consistent with other databases is good, but if people rely on the cast behavior to write struct/array/map to CSV, then it's more important to not break it.

Shall we revert this PR then? We can change spark-sql shell to print `null` to be consistent with `df.show` instead.","2,-1    ",2,-1,1
1517405789,51110188,2023/4/14,v3.2.4,"+1 for the revert.

BTW, Should spark-sql keep consistent with df.show, this also seems to break a lot of things, since spark-sql previously used hiveResultString, not only null display.","1,-1    ",1,-1,0
1517417915,3182036,2023/4/14,v3.2.4,"If people programmatically parse the output of spark-sql shell, then we shouldn't change it. If it's only for display and consumed by humans, I think it's OK to change.","1,-1    ",1,-1,0
1517427635,51110188,2023/4/14,v3.2.4,"> If people programmatically parse the output of spark-sql shell

I want to keep it the way it is. Look at what other people think.","1,-1    ",1,-1,0
1518270247,7788766,2023/4/14,v3.2.4,"To be honest, I don't understand why spark-sql shell is expected to be consistent with spark-shell or pyspark shell. Can someone elaborate? I can see making spark-sql shell consistent with Presto/Trino/MySQL/Postgres, etc. but I don't understand why Scala REPL should be visually consistent with SQL terminal in terms of displaying results - they serve different purposes.

If you would like to have a consistent visual behaviour for NULLs/nulls, it is also fine just as long as it does not break other features like Cast or `collect.toString`. Maybe we could simply add a conversion method to display values in a DataFrame in whatever format we need when calling `.show` instead of changing Cast. In fact, we can refactor it into a separate class and reuse it in spark-sql and spark-shell. 

","1,-1    ",1,-1,0
1500048705,94670132,2023/4/14,v3.2.4,@maropu Could you help to review this pr? Thanks,"1,-1    ",1,-1,0
1501306465,6477701,2023/4/14,v3.2.4,Mind retriggering https://github.com/caican00/spark/runs/12587259773?,"1,-1    ",1,-1,0
1501446573,6477701,2023/4/14,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1501505795,1475305,2023/4/14,v3.2.4,late LGTM,"2,-1    ",2,-1,1
1500071438,46485123,2023/4/14,v3.2.4,ping @cloud-fan ,"3,-1    ",3,-1,2
1500331931,3182036,2023/4/14,v3.2.4,https://github.com/apache/spark/pull/40437 might be related. We want to remove `hiveResultString` from CLI and only use it in hive compatibility tests.,"1,-1    ",1,-1,0
1501012548,46485123,2023/4/14,v3.2.4,ping @cloud-fan ,"2,-3    ",2,-3,-1
1508825010,506679,2023/4/14,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1508910038,9700541,2023/4/14,v3.2.4,Thank you all!,"1,-2    ",1,-2,-1
1500814524,6477701,2023/4/14,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1500869567,1317309,2023/4/14,v3.2.4,Thanks for reviewing and merging!,"2,-1    ",2,-1,1
1503520883,1580697,2023/4/14,v3.2.4,"@cloud-fan @HyukjinKwon @gatorsmile @srielau @entong @hvanhovell Could you review this PR when you have time, please.","1,-2    ",1,-2,-1
1505231846,1580697,2023/4/14,v3.2.4,"Merging to master. Thank you, @cloud-fan for review.","2,-2    ",2,-2,0
1500261827,1317309,2023/4/14,v3.2.4,"This is introduced from 3.4 hence ideal to land the fix to 3.4, but the possibility to trigger the bug is relatively very low, hence probably not urgent.","1,-1    ",1,-1,0
1500982175,1317309,2023/4/14,v3.2.4,I'll just merge this since the fix is super straightforward and CI passed.,"1,-1    ",1,-1,0
1501572484,1475305,2023/4/14,v3.2.4,"> This is introduced from 3.4 hence ideal to land the fix to 3.4, but the possibility to trigger the bug is relatively very low, hence probably not urgent.

So this is not a blocker released in 3.4, is it?

","1,-1    ",1,-1,0
1501709748,1317309,2023/4/14,v3.2.4,No blocker for current 3.4.0 release.,"1,-1    ",1,-1,0
1501443455,6477701,2023/4/14,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1501315619,6477701,2023/4/14,v3.2.4,cc @Ngone51 @jiangxb1987 too FYI,"1,-2    ",1,-2,-1
1511354240,27844885,2023/4/14,v3.2.4,"According to the two ideas provided by @cloud-fan  on how to differentiate user-facing errors and user-triggered errors ([have a special prefix] or [create a base trait for transient errors]), in the implementation process, I think having a special prefix may be a more good idea.
I defined a new `SparkThrowable#isTransientError` method with reference to `SparkThrowable#isInternalError`, and decided whether to skip the retry logic based on the return value of the `SparkThrowable#isTransientError`.
```scala
  def isInternalError(errorClass: String): Boolean = {
    errorClass == ""INTERNAL_ERROR""
  }

  def isTransientError(errorClass: String): Boolean = {
    errorClass.startsWith(""TRANSIENT"")
  }
```

can you re-review the code when you are free, and make some comments. @cloud-fan @aokolnychyi ","1,-1    ",1,-1,0
1512709703,3182036,2023/4/14,v3.2.4,"OK, I'm on the fence now. On one hand, the number of transient errors should be much smaller than the number of user-triggered errors, so it's better to find out these transient errors and mark them. On the other hand, not retrying the task can be a regression that leads to job failure, so we should make sure we only skip task retry when the error is definitely user-triggered.

To be conservative, now I'm leaning towards picking some errors and marking them as ""can skip task retry"". I like the idea from @aokolnychyi that we can add a JSON field for it.","1,-1    ",1,-1,0
1512731238,1591700,2023/4/14,v3.2.4,"I would be on the conservative side and skip retry when we are absolutely certain that a retry will not help.
SerDe failures, for example, are good candidates (which is already handled), and similar ... note that in non deterministic tasks, a retry can succeed which earlier failed with a user exception ","1,-1    ",1,-1,0
1513639084,6235869,2023/4/14,v3.2.4,+1 for being safe ,"1,-1    ",1,-1,0
1513933182,27844885,2023/4/15,v3.2.4,"> OK, I'm on the fence now. On one hand, the number of transient errors should be much smaller than the number of user-triggered errors, so it's better to find out these transient errors and mark them. On the other hand, not retrying the task can be a regression that leads to job failure, so we should make sure we only skip task retry when the error is definitely user-triggered.
> 
> To be conservative, now I'm leaning towards picking some errors and marking them as ""can skip task retry"". I like the idea from @aokolnychyi that we can add a JSON field for it.

I'm trying to change the code now","1,-1    ",1,-1,0
1513960560,27844885,2023/4/15,v3.2.4,"> I would be on the conservative side and skip retry when we are absolutely certain that a retry will not help. SerDe failures, for example, are good candidates (which is already handled), and similar ... note that in non deterministic tasks, a retry can succeed which earlier failed with a user exception

I see, I'm trying to change the code now","1,-1    ",1,-1,0
1536925100,27844885,2023/4/15,v3.2.4,"I have modified the code, can you re-review the code when you are free, and make some comments. @cloud-fan @aokolnychyi @mridulm ","1,-1    ",1,-1,0
1537508145,27844885,2023/4/15,v3.2.4,"According to the suggestions provided by @cloud-fan @aokolnychyi .I modified the code.
I added the `isTransient ` attribute to some `error_classes` 
such as:
-  `AMBIGUOUS_LATERAL_COLUMN_ALIAS`
- `CANNOT_PARSE_DECIMAL`
- `DATATYPE_MISMATCH` 
- `DIVIDE_BY_ZERO`
- `_LEGACY_ERROR_TEMP_3043(npe)`
```java
  ""CANNOT_PARSE_DECIMAL"" : {
    ""message"" : [
      ""Cannot parse decimal.""
    ],
    ""sqlState"" : ""22018"",
    ""isTransient"" : false
  },
```

When these errors occur, the retry logic is skipped.
```java
   if (!ef.isTransient) {
        // if the exception has an error class which means a non-transient error, not retry
        logError(s""$task has a non-transient exception: ${ef.description}; not retrying"")
        sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info)
        abort(s""$task has a non-transient exception: ${ef.description}"", ef.exception)
        return
    }
```
hope you leave some comments in your free time. @cloud-fan @aokolnychyi @mridulm, thanks a lot.



","1,-2    ",1,-2,-1
1546797443,8545796,2023/4/15,v3.2.4,"useful feature, any updates here?","1,-1    ",1,-1,0
1547045506,27844885,2023/4/15,v3.2.4,"> useful feature, any updates here?

I modified the code according to the suggestions provided by @cloud-fan @aokolnychyi @mridulm , next step may be to seek help from people who worked on the error framework. Can you give some suggestions on the next work?","1,-1    ",1,-1,0
1500561170,9700541,2023/4/15,v3.2.4,"Could you review this, @viirya ?

Although the build system seems to be recovering now, I want to reduce the chance of failures in the future by switching the repo.
- https://github.com/apache/spark/commits/master

```
- addSbtPlugin(""com.typesafe.sbteclipse"" % ""sbteclipse-plugin"" % ""5.2.4"")
+ addSbtPlugin(""com.github.sbt"" % ""sbt-eclipse"" % ""6.0.0"")
```","2,-1    ",2,-1,1
1500583121,68855,2023/4/15,v3.2.4,"> This PR aims to use set-eclipse instead of sbteclipse-plugin.

One typo `set-eclipse` in the description.","2,-1    ",2,-1,1
1500587401,9700541,2023/4/15,v3.2.4,"Thank you, @viirya . The description is fixed now.","2,-2    ",2,-2,0
1500594750,9700541,2023/4/15,v3.2.4,I tested this manually. Merged to master/3.4/3.3/3.2.,"2,-2    ",2,-2,0
1500625015,9700541,2023/4/15,v3.2.4,"Yes, correctly. Apache Spark 3.2.0+ uses SBT 1.5.0+ via SPARK-34959.","1,-1    ",1,-1,0
1500708097,9700541,2023/4/15,v3.2.4,"Documentation generation GitHub Action job passed.

![Screenshot 2023-04-07 at 3 56 39 PM](https://user-images.githubusercontent.com/9700541/230689883-597b8e03-b929-4c68-88a3-025358192793.png)
","2,-1    ",2,-1,1
1500708179,9700541,2023/4/15,v3.2.4,"Could you review this PR, @huaxingao ?","2,-1    ",2,-1,1
1500709184,9700541,2023/4/16,v3.2.4,Thank you so much!,"2,-1    ",2,-1,1
1500709472,9700541,2023/4/16,v3.2.4,"Merged to master for Apache Spark 3.5. Thank you, @huaxingao and @amaliujia ","2,-1    ",2,-1,1
1502451369,1097932,2023/4/16,v3.2.4,"Thanks, merging to master/3.4","1,-1    ",1,-1,0
1500726183,1097932,2023/4/16,v3.2.4,"I will come up with screenshots from branch-3.4.
The markdown tables in the master branch are not showing properly. cc @grundprinzip 
<img width=""919"" alt=""image"" src=""https://user-images.githubusercontent.com/1097932/230692781-af14c6a9-b458-4c21-826c-9851868c830d.png"">
","2,-1    ",2,-1,1
1500727241,1097932,2023/4/16,v3.2.4,"cc @xinrong-meng it would be great to include this in the doc of Spark 3.4.0. 
(Document changes won't fail RC vote)","1,-1    ",1,-1,0
1501304729,6477701,2023/4/16,v3.2.4,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1501285574,7322292,2023/4/16,v3.2.4,"Thanks @grundprinzip for the reviews, merged into master","1,-2    ",1,-2,-1
1500901102,1475305,2023/4/16,v3.2.4,cc @dongjoon-hyun @HeartSaVioR FYI,"1,-1    ",1,-1,0
1501241798,1317309,2023/4/16,v3.2.4,"Thanks, merging to master!","1,-1    ",1,-1,0
1501310244,1475305,2023/4/17,v3.2.4,Thanks @HeartSaVioR @dongjoon-hyun ,"1,-1    ",1,-1,0
1501048066,6477701,2023/4/17,v3.2.4,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1501256952,6477701,2023/4/17,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1501597722,8326978,2023/4/17,v3.2.4,cc @cloud-fan @HyukjinKwon @dongjoon-hyun thanks,"1,-1    ",1,-1,0
1502613838,8326978,2023/4/17,v3.2.4,"thanks, merged to master","1,-2    ",1,-2,-1
1501347591,12368495,2023/4/17,v3.2.4,"@LuciferYang @wangyum @frankliee Since parquet-mr has released 1.13.0, So I resubmit the PR. The original PR is https://github.com/apache/spark/pull/40646","1,-1    ",1,-1,0
1501350342,12368495,2023/4/17,v3.2.4,"@frankliee Sorry for delay. The PR only supports AVX512, does not support AVX256.
Your question ""Do we need to create SparkContext in static code ?""  because I want to get the SQL configuration sql.parquet.vector512.read.enabled","1,-2    ",1,-2,-1
1501461669,12368495,2023/4/17,v3.2.4,"@LuciferYang @wangyum @frankliee I have added a benchmark.

This is the result:
```
Java HotSpot(TM) 64-Bit Server VM 17.0.5+9-LTS-191 on Linux 5.15.0-60-generic
Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz
Selection:                                Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Without Java Vector API                            4696           4802          89         21.3          47.0       1.0X
With Java Vector API                               3742           3927         230         26.7          37.4       1.3X
```","1,-1    ",1,-1,0
1501580625,1475305,2023/4/17,v3.2.4,"Please wait for me to check and update the benchmark results of `ZStandardBenchmark`

","1,-2    ",1,-2,-1
1502708533,1475305,2023/4/17,v3.2.4,"> New results look reasonable.

I have been in a team meeting this morning.

It seems that the results of `ZStandardBenchmark` are somewhat related to the CPU model.","1,-1    ",1,-1,0
1502757419,1475305,2023/4/17,v3.2.4,"The CPU model used in micro-bench is the same as before, and the results have not changed much now

","2,-2    ",2,-2,0
1504423456,1475305,2023/4/17,v3.2.4,Thanks @dongjoon-hyun ,"2,-2    ",2,-2,0
1501449301,44108233,2023/4/17,v3.2.4,"cc @HyukjinKwon @bjornjorgensen FYI

We don't need to install `grpcio` for pandas API on Spark with regular session after this PR.","1,-1    ",1,-1,0
1514231816,44108233,2023/4/17,v3.2.4,CI passed. @HyukjinKwon Could you take a look when you find some time??,"1,-2    ",1,-2,-1
1521051461,44108233,2023/4/17,v3.2.4,@HyukjinKwon Test passed. There is anything else we should address here?,"1,-1    ",1,-1,0
1522850607,6477701,2023/4/17,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1523008330,47577197,2023/4/17,v3.2.4,@itholic Thank you. Great work. ,"1,-1    ",1,-1,0
1523798861,6477701,2023/4/17,v3.2.4,@itholic can you double check this https://github.com/apache/spark/pull/40722#discussion_r1170976266?,"2,-2    ",2,-2,0
1502586204,1475305,2023/4/17,v3.2.4,"> Could you file a JIRA for this, @LuciferYang ? This contribution looks enough to have a JIRA issue.

@dongjoon-hyun thanks for your suggestion ~ created SPARK-43090","1,-1    ",1,-1,0
1504420604,7322292,2023/4/17,v3.2.4,"what if there are two input datasets, one for training and one for validation?","1,-1    ",1,-1,0
1505144052,19235986,2023/4/17,v3.2.4,"> what if there are two input datasets, one for training and one for validation?

We can add a ""is_validation"" boolean column to mark it is for training or for validation. ","1,-1    ",1,-1,0
1505144497,19235986,2023/4/17,v3.2.4,@mengxr raises another suggestion: uses petastorm to load data from DBFS / HDFS /.. .(so that it can make torch distributor has a simpler interfaces). But thereâs a shortcoming that it is low performant for sparse vector features. We havenât made final decision yet.,"1,-1    ",1,-1,0
1524671277,19235986,2023/4/17,v3.2.4,"> @mengxr raises another suggestion: uses petastorm to load data from DBFS / HDFS /.. .(so that it can make torch distributor has a simpler interfaces). But thereâs a shortcoming that it is low performant for sparse vector features. We havenât made final decision yet.

Finally, after offline discussion, we decided to adopt the approach of this PR, because this PR approach has significant benefits including:

 - It does not need to dump training dataset to distributed file system but just saving partition data in local disk, which is much faster.
 - If we uses petastorm or pytorch parquet / arrow loader, we have to densify sparse feature input data, it makes data exploded before saving dataset, this causes further performance deterioration. But current approach in this PR it dumps sparse data to local disk and when loading for training, it densifies the data.

Btw, we decided to make it a private API because the API currently will be only used by pyspark MLv2 (a new module we plan to add soon) code.
","1,-1    ",1,-1,0
1524848380,7322292,2023/4/17,v3.2.4,`test_data_loader` should also be added to `modules.py`,"1,-1    ",1,-1,0
1502530555,47337188,2023/4/17,v3.2.4,"CI failed because of 
```
Run echo ""APACHE_SPARK_REF=$(git rev-parse HEAD)"" >> $GITHUB_ENV
fatal: detected dubious ownership in repository at '/__w/spark/spark'
To add an exception for this directory, call:

	git config --global --add safe.directory /__w/spark/spark
fatal: detected dubious ownership in repository at '/__w/spark/spark'
To add an exception for this directory, call:

	git config --global --add safe.directory /__w/spark/spark
Error: Process completed with exit code 128.
```","1,-1    ",1,-1,0
1512285645,47337188,2023/4/17,v3.2.4,@HyukjinKwon @zhengruifeng Would you please take a look? Thank you!,"1,-1    ",1,-1,0
1512286161,6477701,2023/4/17,v3.2.4,cc @ueshin FYI,"1,-2    ",1,-2,-1
1518443157,6477701,2023/4/17,v3.2.4,Merged to master.,"1,-2    ",1,-2,-1
1502375359,9700541,2023/4/17,v3.2.4,R failure on AppVeyor is irrelevant to this PR.,"1,-1    ",1,-1,0
1502427149,9700541,2023/4/17,v3.2.4,"Could you review this PR, @viirya ? I verified manually.

```
$ ls -alt
total 67688
-rw-r--r--@  1 dongjoon  staff      1955 Apr 10 15:27 maven-metadata-local.xml
-rw-r--r--@  1 dongjoon  staff       492 Apr 10 15:27 _remote.repositories
-rw-r--r--@  1 dongjoon  staff   4451845 Apr 10 15:27 spark-core_2.12-3.5.0-SNAPSHOT-javadoc.jar
-rw-r--r--@  1 dongjoon  staff    362584 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-cyclonedx.json
-rw-r--r--@  1 dongjoon  staff    312338 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-cyclonedx.xml
-rw-r--r--@  1 dongjoon  staff   1423015 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-test-sources.jar
-rw-r--r--@  1 dongjoon  staff   2919170 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-sources.jar
-rw-r--r--@  1 dongjoon  staff   8078262 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-tests.jar
-rw-r--r--@  1 dongjoon  staff  14038165 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT.jar
-rw-r--r--@  1 dongjoon  staff     44355 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT.pom
drwxr-xr-x@  4 dongjoon  staff       128 Apr 10 14:56 ..
drwxr-xr-x@ 12 dongjoon  staff       384 Apr 10 14:56 .

$ head spark-core_2.12-3.5.0-SNAPSHOT-cyclonedx.xml
<?xml version=""1.0"" encoding=""UTF-8""?>
<bom serialNumber=""urn:uuid:14185927-0ca0-4589-92aa-268c9c81873d"" version=""1"" xmlns=""http://cyclonedx.org/schema/bom/1.4"">
  <metadata>
    <timestamp>2023-04-10T22:26:52Z</timestamp>
    <tools>
      <tool>
        <vendor>OWASP Foundation</vendor>
        <name>CycloneDX Maven plugin makeBom compile+provided+runtime+system</name>
        <version>2.7.6</version>
        <hashes>
```","1,-1    ",1,-1,0
1502482929,6477701,2023/4/17,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1502483678,6477701,2023/4/17,v3.2.4,"Hm, for some reasons, it shows @LuciferYang as a primary author. I manually changed it to @dongjoon-hyun.","1,-1    ",1,-1,0
1502484690,9700541,2023/4/17,v3.2.4,"Oh, it was intentional https://github.com/apache/spark/pull/40726#pullrequestreview-1378012264, but thank you!

Thank you, @HyukjinKwon and @viirya !","2,-1    ",2,-1,1
1502587494,1475305,2023/4/17,v3.2.4,late LGTM ~ Thanks @dongjoon-hyun and all ~,"1,-1    ",1,-1,0
1503090288,162090,2023/4/17,v3.2.4,this working OK on the latest maven releases that homebrew is pushing out?,"1,-1    ",1,-1,0
1503684409,9700541,2023/4/17,v3.2.4,"To @steveloughran , this is done independently from Maven.
I verified in Apache ORC and Spark community only for now.","1,-1    ",1,-1,0
1502430516,9700541,2023/4/17,v3.2.4,"Could you review this PR when you have some time, @huaxingao ?","2,-1    ",2,-1,1
1502456159,9700541,2023/4/17,v3.2.4,"Thank you, @huaxingao !","2,-1    ",2,-1,1
1502463385,9700541,2023/4/17,v3.2.4,"I also confirmed the moved `*StateStoreSuite` output in the GitHub Action log on this PR.
- https://github.com/dongjoon-hyun/spark/actions/runs/4661381120/jobs/8250624115

Merged to master/3.4.","2,-1    ",2,-1,1
1507810721,4190164,2023/4/17,v3.2.4,cc @xinrong-meng @grundprinzip For the proto change.,"1,-1    ",1,-1,0
1509161614,4190164,2023/4/17,v3.2.4,CC @amaliujia @vicennial Be free to give any review feedbacks.,"1,-1    ",1,-1,0
1515123046,47337188,2023/4/17,v3.2.4,"Thanks @zhenlineo , the proto changes look nice.","1,-1    ",1,-1,0
1515124878,47337188,2023/4/17,v3.2.4,"A nit on the PR description, would you please indicate which API is supported under `Does this PR introduce any user-facing change?` section?  That would be helpful for future API auditing. Thanks!","2,-1    ",2,-1,1
1522405921,9616802,2023/4/17,v3.2.4,Merging to master.,"2,-2    ",2,-2,0
1502463722,1633312,2023/4/17,v3.2.4,@dongjoon-hyun @mridulm @Ngone51 Help take a look?,"1,-3    ",1,-3,-2
1502631790,1633312,2023/4/17,v3.2.4,"> I understand the intention but there is a chance of instability due to `OutOfDisk` and sometimes `OutOfMemory`. In addition, bin-packed executors could work slower due to the network traffic congestion. Do you have some production results about the real benefits, @warrenzhu25 ?

In prod, we have seen 20~50 percent resource save based on executor size, the more saving with more cpu cores per executor. For the issue could be caused by bin-packed executors, it's still possible to happen when all executor cores are occupied. If it indeed happen, the better solution might be large partition num. 

The purpose of change is to provide one more scheduling option, customer still can use the old way.","1,-1    ",1,-1,0
1503818955,9700541,2023/4/17,v3.2.4,"Please put your observation to the PR description too in order to make it a permanent commit log, @warrenzhu25 .","1,-1    ",1,-1,0
1506091631,1633312,2023/4/17,v3.2.4,"> Please put your observation to the PR description too in order to make it a permanent commit log, @warrenzhu25 .

Done","1,-1    ",1,-1,0
1506142915,6477701,2023/4/17,v3.2.4,I have been wondering if it's better to allow `spark.task.cpus` to set a floating number like `0.2` so I/O intensive tasks can benefit from that as well as the case like this.,"1,-2    ",1,-2,-1
1506143017,6477701,2023/4/17,v3.2.4,cc @tgravescs too FYI,"1,-1    ",1,-1,0
1507220266,4563792,2023/4/18,v3.2.4,"Overall I'm fine with adding in more scheduling algorithms, we have talked about it before but never done it.  this has been requested beofre like https://issues.apache.org/jira/browse/SPARK-17637

Ideally perhaps we could make a pluggable scheduling algorithm, but I think that is much more complex, if we make the config more generic like proposed it doesn't exclude it from happening in the future","1,-1    ",1,-1,0
1507371099,4563792,2023/4/18,v3.2.4,I'm also curious what do you have locality set to?  What if you set wait=0 as I thought that kind of got you this,"1,-1    ",1,-1,0
1502627560,5399861,2023/4/18,v3.2.4,cc @cloud-fan ,"1,-2    ",1,-2,-1
1503436680,3182036,2023/4/18,v3.2.4,there is already a PR doing the same thing: https://github.com/apache/spark/pull/40688,"2,-1    ",2,-1,1
1503470505,5399861,2023/4/18,v3.2.4,Close this one.,"3,-1    ",3,-1,2
1507656246,1097932,2023/4/18,v3.2.4,"Thanks, merging to master/3.4","1,-1    ",1,-1,0
1502632324,6477701,2023/4/18,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1502642472,6235869,2023/4/18,v3.2.4,"@huaxingao @cloud-fan @dongjoon-hyun @sunchao @HyukjinKwon @viirya @gengliangwang, could you take a look at the approach used in this PR and let me know what you think? If it seems reasonable, I'll add more tests.","1,-1    ",1,-1,0
1504401749,6235869,2023/4/18,v3.2.4,"@cloud-fan, could you take another look? I have changed the approach and updated the PR description.","1,-1    ",1,-1,0
1504945702,3182036,2023/4/18,v3.2.4,This approach LGTM. Can we also check the SQL UI manually and see if there is any problem?,"1,-1    ",1,-1,0
1505632407,6235869,2023/4/18,v3.2.4,"> Can we also check the SQL UI manually and see if there is any problem?

Checking.","3,-1    ",3,-1,2
1505784253,6235869,2023/4/18,v3.2.4,"There is a difference in terms of what is shown in the UI as the new approach uses nested execution.

**Prior to this change**

<img width=""1490"" alt=""image"" src=""https://user-images.githubusercontent.com/6235869/231557959-cb44436e-0f34-4393-88ed-90fcc141381c.png"">
<img width=""193"" alt=""image"" src=""https://user-images.githubusercontent.com/6235869/231558107-32e96225-dcb4-4379-b33d-efc767cdb36d.png"">

**After this change**

<img width=""891"" alt=""image"" src=""https://user-images.githubusercontent.com/6235869/231558265-1a14ad48-a596-4b63-ba96-d12647bea921.png"">
<img width=""270"" alt=""image"" src=""https://user-images.githubusercontent.com/6235869/231558323-98f26730-7e8f-4401-ad56-570bf659d91b.png"">

It seems reasonable as we do use nested execution. What do you think, @cloud-fan?","1,-1    ",1,-1,0
1506166432,3182036,2023/4/18,v3.2.4,yea the UI looks good!,"1,-1    ",1,-1,0
1506187667,6235869,2023/4/18,v3.2.4,Failures seem unrelated. Retrying [here](https://github.com/aokolnychyi/spark/actions/runs/4680956607).,"1,-1    ",1,-1,0
1506229131,6235869,2023/4/18,v3.2.4,"Thanks for reviewing, @cloud-fan! I updated the PR. Let me also know if you see any issues regarding [this proposal](https://github.com/apache/spark/pull/40734#discussion_r1164418931).","1,-1    ",1,-1,0
1506683474,3182036,2023/4/18,v3.2.4,"all tests passed, and the pyspark issue is unrelated: https://github.com/aokolnychyi/spark/runs/12709198717

merging to master, thanks!","1,-1    ",1,-1,0
1507248524,6235869,2023/4/18,v3.2.4,"Thanks, @cloud-fan!","2,-2    ",2,-2,0
1512268579,1938382,2023/4/18,v3.2.4,"hmm I am not sure about this. I think there are some practice already that add the API but throw unsupported exception.

so I am not sure if we want those unsupported API are just missing or add those but throws.

BTW the python side I believe add the APIs but throw.

cc @hvanhovell ","1,-1    ",1,-1,0
1521621956,7322292,2023/4/18,v3.2.4,I think the behavior of throwing `UnsupportedOperationException` other than compilation failure is also acceptable,"1,-1    ",1,-1,0
1521756074,1475305,2023/4/18,v3.2.4,"This is a minor issue, let me close this one, thanks @zhengruifeng @amaliujia 

","1,-1    ",1,-1,0
1503960956,10248890,2023/4/18,v3.2.4,"Also I think we need to add unit test for this by reusing tests here https://github.com/apache/spark/pull/37894

You can follow [my PR](https://github.com/apache/spark/pull/40691/files#r1162341092) to 
1. create a new mixin class that contains all test cases from the original one but don't extend `ReusedSQLTestCase`, and 
2. create a new class below with the original name and extend this mixin class and `ReusedSQLTestCase`
3. create a parity test class that extends this mixin class and `ReusedConnectTestCase`
","2,-1    ",2,-1,1
1505700378,94015493,2023/4/18,v3.2.4,"> Also I think we need to add unit test for this by reusing tests here #37894
> 
> You can follow [my PR](https://github.com/apache/spark/pull/40691/files#r1162341092) to
> 
> 1. create a new mixin class that contains all test cases from the original one but don't extend `ReusedSQLTestCase`, and
> 2. create a new class below with the original name and extend this mixin class and `ReusedSQLTestCase`
> 3. create a parity test class that extends this mixin class and `ReusedConnectTestCase`

@WweiL I added a test file test_parity_pandas_grouped_map_with_state.py. However, I had to skip all tests due to spark.streams not supported in connect for now","1,-1    ",1,-1,0
1509252099,10248890,2023/4/18,v3.2.4,"Seems that there is lint error, you can run `PYTHON_EXECUTABLE=python3.9 ./dev/lint-python` or just `./dev/lint-python` before commit to make sure","1,-1    ",1,-1,0
1510673506,94015493,2023/4/18,v3.2.4,@HyukjinKwon can u help merge this?,"1,-1    ",1,-1,0
1512270326,6477701,2023/4/18,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1504450475,6477701,2023/4/18,v3.2.4,Merged to master.,"2,-3    ",2,-3,-1
1504452796,1475305,2023/4/18,v3.2.4,Thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1503053830,1935105,2023/4/18,v3.2.4,"Hi, after bumping to Maven 3.9.1 in Apache Kyuubi (https://github.com/apache/kyuubi/pull/4647), there are notable increasing failures in Maven builds when resolving Maven dependencies from the central mirror.
We are trying to investigate and improve the situation in https://github.com/apache/kyuubi/pull/4692, by increasing Maven Resolver Transport connection timeout back to original default connection timeout 60000ms.","1,-1    ",1,-1,0
1503380076,1475305,2023/4/18,v3.2.4,"The property `aether.connector.connectTimeout` seems not working, and there are still maven compilation performance issues, need more investigation ...

","1,-1    ",1,-1,0
1503444531,1935105,2023/4/18,v3.2.4,Agree. Same on kyuubi.,"1,-1    ",1,-1,0
1503770505,9700541,2023/4/18,v3.2.4,"Thank you all.

FYI, @steveloughran . Apache Maven 3.9.1 seems to have issues still according to the above discussion.","1,-1    ",1,-1,0
1504437932,1475305,2023/4/18,v3.2.4,"<img width=""1293"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/231329527-32cb53b8-6724-4461-8e26-5411ff81f000.png"">

The last build took 1 hour and 7 meters, let me re-run GA more times

","1,-1    ",1,-1,0
1505253649,162090,2023/4/18,v3.2.4,"noted. interestingly for cutting hadoop releases i have had to turn off connection pooling so as to guarantee reliable builds (docker container having to download **/*.jar from maven central is too brittle otherwise). that's even older versions.

* spark &c with fixed versions should stay on the good ones
* if the latest version of the SBOR is happy on recent maven builds then we can reinstate in hadoop.","2,-1    ",2,-1,1
1511614880,1475305,2023/4/18,v3.2.4,"@dongjoon-hyun @steveloughran Adding `-Dmaven.resolver.transport=wagon` seems useful, as recent Java 11&17 Maven build have all finished within 1 hour and there have been no more timeouts.  Let me check for two more days





","1,-1    ",1,-1,0
1519081149,822522,2023/4/18,v3.2.4,"I mean, is it worth updating if 3.9.1 introduces small issues that we have to work around? or just wait?","1,-1    ",1,-1,0
1519098711,1475305,2023/4/18,v3.2.4,"Long time no see @srowen , thanks for your review ~

I am not sure if the native http client will change the default timeout value to be same with wagon in the future. But I don't object to letting us wait and see if there will be any changes of the new version of Apache Maven :)","1,-1    ",1,-1,0
1521073578,1475305,2023/4/18,v3.2.4,"@steveloughran Do you have any other questions? I am planning to close this PR and test Apache Maven 3.9.2+ in the future to check if it is possible not to add `-Dmaven.resolver.transport=wagon`. Apache Spark can continue to use 3.8.7 now.



","1,-1    ",1,-1,0
1521522776,162090,2023/4/18,v3.2.4,"me, I'm happy with whatever suits, just noting that 3.9.1 was unwelcome trouble for me as it ended up hurting the 3.3.5 release process -not the docker builds, which were still on a lower version, but my local dev builds blew up. joy","1,-1    ",1,-1,0
1521616040,1475305,2023/4/18,v3.2.4,"Thanks for your attention to this pr @bowenliang123 @dongjoon-hyun @srowen @steveloughran, close first ~
","2,-1    ",2,-1,1
1521624076,1935105,2023/4/18,v3.2.4,"Thanks for the ping. And feel free to cc me when Spark has the next attempt to use maven 3.9.x/4.x.
I was pessimistic that whether Maven will change the default timeout as it's a default config (`aether.connector.connectTimeout=10000`) of Maven Artifact Resolver not in Maven itself. And as for Maven, it has changed to Artifact Resolver as the default resolver already, which fewer improvements we could expect from that. And we are also difficult to reduce possible timeout to maven central mirrors.","1,-1    ",1,-1,0
1522123005,9700541,2023/4/18,v3.2.4,"Sorry for missing pings, @LuciferYang . I'm back from my vacation.
I'm also +1 for the decision (waiting for new better version). Thank you for closing.","1,-2    ",1,-2,-1
1526918052,3182036,2023/4/18,v3.2.4,"thanks for review, merging to master!","1,-1    ",1,-1,0
1503059170,25019163,2023/4/18,v3.2.4,cc @HyukjinKwon @hvanhovell ,"3,-1    ",3,-1,2
1504417223,6477701,2023/4/18,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1504449595,6477701,2023/4/18,v3.2.4,cc @MaxGekk ,"3,-2    ",3,-2,1
1506215271,6477701,2023/4/18,v3.2.4,cc @ueshin ,"2,-1    ",2,-1,1
1510595839,32387433,2023/4/18,v3.2.4,"Hi guys, I want implement support sql with columns first, to avoid [sql with dataframes problem](https://github.com/apache/spark/pull/40741#discussion_r1163580352) before have better solution. If ok for you please let me know. Thanks.","1,-1    ",1,-1,0
1504278565,5399861,2023/4/18,v3.2.4,cc @cloud-fan ,"1,-2    ",1,-2,-1
1509447313,5399861,2023/4/18,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1504447656,6477701,2023/4/18,v3.2.4,"qq:

> because the unwrapped expression contains a UDF

Where is the UDF, and how it's related to UDF?","1,-1    ",1,-1,0
1504453830,5399861,2023/4/18,v3.2.4,"@HyukjinKwon The `EqualNullSafe(Cast(ts, DateType), date)` shoud rewrite to `If(IsNull(ts), FalseLiteral, And(GreaterThanOrEqual(ts, Cast(date, TimestampType)), LessThan(ts, Cast(date + 1, TimestampType))))`.  It contains `If`.  So there is no benefit if rewrite `EqualNullSafe(Cast(ts, DateType), date)`.","1,-1    ",1,-1,0
1504456010,3182036,2023/4/18,v3.2.4,"@wangyum I think we should mention two things:
1. the current rewrite is incorrect for EqualNullSafe
2. even if we fix the rewrite, it will contain `If` and can't be pushed down to the data source.","1,-1    ",1,-1,0
1505211706,3182036,2023/4/18,v3.2.4,shall we still rewrite `EqualNullSafe` if column is not nullable?,"1,-1    ",1,-1,0
1509442721,5399861,2023/4/18,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1515969429,7253827,2023/4/18,v3.2.4,This PR is WIP as it contains https://github.com/apache/spark/pull/40856. Once that PR is merged I will rebase and remove the WIP flag.,"1,-1    ",1,-1,0
1525540641,7253827,2023/4/18,v3.2.4,"https://github.com/apache/spark/pull/40856 got merged and I've rebased this PR. I'm removing the WIP flag and the PR is ready for review.

cc @cloud-fan, @wangyum, @maryannxue, @sigmod ","1,-1    ",1,-1,0
1504258604,4190164,2023/4/18,v3.2.4,"~I would give the ssh repl a try too. Maybe starting the repl this way do not need the tyy?
https://github.com/com-lihaoyi/Ammonite/issues/276#issuecomment-267302917
Also see http://ammonite.io/api/sshd/ammonite/sshd/SshdRepl.html~","1,-1    ",1,-1,0
1504718977,6477701,2023/4/18,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1504221169,1097932,2023/4/18,v3.2.4,"@alexjinghn there is a JAVA17 job on the OSS and it seems working
https://github.com/apache/spark/blob/master/.github/workflows/build_java17.yml
https://github.com/apache/spark/actions/workflows/build_java17.yml

How do you reproduce the issue?","2,-1    ",2,-1,1
1504793438,8326978,2023/4/18,v3.2.4,I can reproduce this on java17 with spark-shell locally. Can we add a test for anonymous UDFs?,"1,-1    ",1,-1,0
1504849359,1475305,2023/4/19,v3.2.4,"> I can reproduce this on java17 with spark-shell locally. Can we add a test for anonymous UDFs?

+1 for add a test for anonymous UDFs","1,-1    ",1,-1,0
1505154053,6477701,2023/4/19,v3.2.4,I think it works probably we build with JDK 8 and run with JDK 17. I guess this is only reproducible when it's both built and runs with JDK 17.,"2,-1    ",2,-1,1
1506236853,8326978,2023/4/19,v3.2.4,">  I guess this is only reproducible when it's both built and runs with JDK 17.

my case was built on 8 but run on 17","1,-1    ",1,-1,0
1507662454,37601753,2023/4/19,v3.2.4,"@yaooqinn @LuciferYang Thanks for the suggestion. I've added a test and verified that the test captures the failure (fails w/o my change, passes w/ my change).

@HyukjinKwon this should be able to be reproduced as long as it runs on JDK15+ since the root cause is the classloader API behavior change.","1,-1    ",1,-1,0
1510634387,3182036,2023/4/19,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1504318218,8699921,2023/4/19,v3.2.4,cc @mridulm @thejdeep ,"1,-1    ",1,-1,0
1504922648,7322292,2023/4/19,v3.2.4,also cc @grundprinzip ,"1,-1    ",1,-1,0
1505080460,7322292,2023/4/19,v3.2.4,"ok, on second thought, I think we should narrow this PR to abbreviation only.

I think we can support redaction as followings in the future:

```
{
...
      case (field: FieldDescriptor, relation: proto.LocalRelation)
          if field.getJavaType == FieldDescriptor.JavaType.MESSAGE && relation != null =>
        builder.setField(field, redactLocalRelation(relation))

      case (field: FieldDescriptor, msg: Message)
          if field.getJavaType == FieldDescriptor.JavaType.MESSAGE && msg != null =>
      ...
...
}

private def redactLocalRelation(relation: proto.LocalRelation): proto.LocalRelation = {

....
}

```","1,-1    ",1,-1,0
1507780590,6477701,2023/4/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1517277978,7322292,2023/4/19,v3.2.4,merged to master,"2,-1    ",2,-1,1
1517284466,1475305,2023/4/19,v3.2.4,Thanks @zhengruifeng ,"1,-1    ",1,-1,0
1504717472,1938382,2023/4/19,v3.2.4,@cloud-fan ,"1,-1    ",1,-1,0
1505187077,3182036,2023/4/19,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1504808270,1475305,2023/4/19,v3.2.4,"cc @rangadi due to initially he set `shadeTestJar` of protobuf module to true

also cc @HyukjinKwon 

","1,-1    ",1,-1,0
1508246298,1475305,2023/4/19,v3.2.4,Yeah ~ all test passed,"1,-1    ",1,-1,0
1508821079,506679,2023/4/19,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1508822547,1475305,2023/4/19,v3.2.4,Thanks @sunchao @dongjoon-hyun @HyukjinKwon @rangadi ,"1,-1    ",1,-1,0
1508910565,9700541,2023/4/19,v3.2.4,Thank you all!,"1,-1    ",1,-1,0
1504877859,12025282,2023/4/19,v3.2.4,cc @beliefer @cloud-fan ,"1,-1    ",1,-1,0
1504945014,8486025,2023/4/19,v3.2.4,"@ulysses-you Thank you for ping me.
Should we put `numOutputRows += 1` into `BaseLimitIterator.next()` ?","1,-1    ",1,-1,0
1505048650,12025282,2023/4/19,v3.2.4,"yea, I'm fine move it to iterator. jsut want to make less change, though.","1,-1    ",1,-1,0
1507808567,12025282,2023/4/19,v3.2.4,the failed pyspark test seems irrelevant..,"1,-1    ",1,-1,0
1507869782,3182036,2023/4/19,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1507883876,8486025,2023/4/19,v3.2.4,LGTM.,"1,-1    ",1,-1,0
1507203842,9616802,2023/4/19,v3.2.4,cc @zhenlineo since you are working on this on the connect side.,"1,-1    ",1,-1,0
1507385397,26215547,2023/4/19,v3.2.4,"cc @cloud-fan @viirya for review, thanks!","1,-1    ",1,-1,0
1514799826,3182036,2023/4/19,v3.2.4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1514800366,3182036,2023/4/19,v3.2.4,@kings129 can you open a new PR for branch 3.3? Thanks!,"1,-1    ",1,-1,0
1515263464,26215547,2023/4/19,v3.2.4,"> @kings129 can you open a new PR for branch 3.3? Thanks!

Thanks for the quick review, @cloud-fan!
Yes, here is the pull request for branch 3.3: https://github.com/apache/spark/pull/40858","1,-1    ",1,-1,0
1507882624,5399861,2023/4/19,v3.2.4,cc @cloud-fan ,"1,-1    ",1,-1,0
1509442084,5399861,2023/4/19,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1505508356,25019163,2023/4/19,v3.2.4,One more time @HyukjinKwon ... I don't know how this chmod did not get committed last time.,"1,-1    ",1,-1,0
1506143532,6477701,2023/4/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1506020468,1938382,2023/4/19,v3.2.4,@cloud-fan ,"1,-1    ",1,-1,0
1506207463,3182036,2023/4/19,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1507197317,47577197,2023/4/19,v3.2.4,The operation was canceled after 6 hours running. ,"2,-1    ",2,-1,1
1507885566,6477701,2023/4/19,v3.2.4,Can you rerun? it should be fixed now,"2,-2    ",2,-2,0
1508321882,47577197,2023/4/19,v3.2.4,"@HyukjinKwon yes, now its works :)","1,-1    ",1,-1,0
1512408011,6477701,2023/4/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1506648960,6477701,2023/4/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1506350585,1097932,2023/4/19,v3.2.4,Merging to master,"1,-1    ",1,-1,0
1506145678,6477701,2023/4/19,v3.2.4,cc @LuciferYang FYI,"1,-1    ",1,-1,0
1506229380,1475305,2023/4/19,v3.2.4,"Some changes seems unrelated to the current pr, and not ready to review yet? @zhenlineo ","2,-1    ",2,-1,1
1508768693,4190164,2023/4/19,v3.2.4,"@LuciferYang Yeah, adding the whole client jars will fix the issue, but we are looking at how could we fix the issue with only the selected files using artifact sync. So this PR is still experimental at this moment.","2,-2    ",2,-2,0
1522471367,4190164,2023/4/19,v3.2.4,"@LuciferYang @hvanhovell @vicennial 
This fixes maven test failures to run UDF E2E tests.","1,-1    ",1,-1,0
1523680330,9616802,2023/4/19,v3.2.4,Merging to master,"1,-1    ",1,-1,0
1506647802,6477701,2023/4/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1506133199,1938382,2023/4/19,v3.2.4,1,"1,-1    ",1,-1,0
1506266279,7322292,2023/4/19,v3.2.4,merged to master,"1,-1    ",1,-1,0
1515576682,12415848,2023/4/19,v3.2.4,"I will close this PR. Though there is problematic behaviour if the column name is present in column list as well as in partition clause, and in case of hive tables, that situation should be detected . But it is not that severe a bug, as for hive table format the partition col should not be present in both the places ( i was not aware of that).","2,-1    ",2,-1,1
1506144869,6477701,2023/4/19,v3.2.4,cc @rednaxelafx FYI,"2,-2    ",2,-2,0
1512406478,6477701,2023/4/19,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1512407062,6477701,2023/4/19,v3.2.4,It has some minor conflicts against branch-3.3. @bersprockets wanna make a pr for it?,"1,-1    ",1,-1,0
1513162902,21131848,2023/4/19,v3.2.4,">It has some minor conflicts against branch-3.3. @bersprockets wanna make a pr for it?

Thanks! Will do.","1,-1    ",1,-1,0
1506535826,1317309,2023/4/19,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1506503716,8326978,2023/4/19,v3.2.4,cc @cloud-fan @dongjoon-hyun @HyukjinKwon thanks,"1,-1    ",1,-1,0
1512343373,8326978,2023/4/19,v3.2.4,"please retake a look, @cloud-fan ,thanks.","1,-1    ",1,-1,0
1512503237,3182036,2023/4/19,v3.2.4,"@srielau how do you think of this new TVF `sql_keywords`?
```
@ExpressionDescription(
  usage = """"""_FUNC_() - Get Spark SQL keywords"""""",
  examples = """"""
    Examples:
      > SELECT * FROM _FUNC_() LIMIT 2;
       ADD  false
       AFTER  false
  """""",
  since = ""3.5.0"",
  group = ""generator_funcs"")
```","2,-1    ",2,-1,1
1513550564,3514644,2023/4/19,v3.2.4,"> @srielau how do you think of this new TVF `sql_keywords`?
> 
> ```
> @ExpressionDescription(
>   usage = """"""_FUNC_() - Get Spark SQL keywords"""""",
>   examples = """"""
>     Examples:
>       > SELECT * FROM _FUNC_() LIMIT 2;
>        ADD  false
>        AFTER  false
>   """""",
>   since = ""3.5.0"",
>   group = ""generator_funcs"")
> ```

I'm not opposed to it. What is the reference to JDBC compliance? ","2,-2    ",2,-2,0
1514014308,8326978,2023/4/19,v3.2.4,"> What is the reference to JDBC compliance?

To implement java.sql.DatabaseMetaData#getSQLKeywords at thriftserver side","1,-1    ",1,-1,0
1514178105,5399861,2023/4/19,v3.2.4,@yaooqinn Could you re-run the test?,"1,-1    ",1,-1,0
1514185495,8326978,2023/4/19,v3.2.4,"thanks, @wangyum for the notification. I didn't notice there was a transient failure","1,-1    ",1,-1,0
1515674976,8326978,2023/4/20,v3.2.4,the test failures seem not relatedï¿½?can you retake a look? @wangyum @cloud-fan ,"1,-1    ",1,-1,0
1517166975,8326978,2023/4/20,v3.2.4,"thanks @cloud-fan @wangyum @srielau for the help, merged to master","1,-1    ",1,-1,0
1506420575,7322292,2023/4/20,v3.2.4,"@HyukjinKwon  do we have a timeout parameter for python unittest?

The issue happens occasionally, I want to fail the test if it is too long (maybe 10min) so that reviews at least will be aware of that the failure is unrelated

","1,-1    ",1,-1,0
1506426179,6477701,2023/4/20,v3.2.4,We don't have. We have `utils.eventually`  but I think it cannot be used in this case. Would need to implement it separately.,"1,-1    ",1,-1,0
1506482546,100322362,2023/4/20,v3.2.4,@HeartSaVioR - please take a look. Thanks,"1,-1    ",1,-1,0
1506948252,1317309,2023/4/20,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1507881376,49986318,2023/4/20,v3.2.4,"cc  @dongjoon-hyun @yaooqinn @Yikun, can anyone help review?","1,-1    ",1,-1,0
1507965774,8326978,2023/4/20,v3.2.4,"> The cpu limits are set by spark.kubernetes.{driver,executor}.limit.cores. The cpu is set by spark.{driver,executor}.cores. The memory request and limit are set by summing the values of spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead. Other resource limits are set by spark.{driver,executor}.resources.{resourceName}.* configs.

Referring to the doc, we can actually set driver pod memory alone  ","1,-1    ",1,-1,0
1507988511,49986318,2023/4/20,v3.2.4,"> > The cpu limits are set by spark.kubernetes.{driver,executor}.limit.cores. The cpu is set by spark.{driver,executor}.cores. The memory request and limit are set by summing the values of spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead. Other resource limits are set by spark.{driver,executor}.resources.{resourceName}.* configs.
> 
> Referring to the doc, we can actually set driver pod memory alone

It`s about container memory request&limit, not about the driver or exec.
Current code is setting the pod request.memory and limit.memory in the **same value** from by summing the values of spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead.
But request.memory and limit.memory are different kind of params from k8s, always keep same value may not a good practice. In most case the request memory quota we can get is always smaller than limit.memory from infrastructure team. If spark pods request.memory can only same as limit.memory, then total memory we can use is based on the smaller one, thus can not fully use the memory resource.
requests å®ä¹äºå¯¹åºçå®¹å¨æéè¦çæå°èµæºéï¿½?
limits å®ä¹äºå¯¹åºå®¹å¨æå¤§å¯ä»¥æ¶èçèµæºä¸éï¿½?
1512567363,52876270,2023-04-18,There was a discussion about this on the spark dev mailing list earlier","1,-1    ",1,-1,0
"[spark executor pod has same memory value for request and limit](https://www.mail-archive.com/search?l=dev@spark.apache.org&q=subject:%22spark+executor+pod+has+same+memory+value+for+request+and+limit%22&o=newest)""",,2023/4/20,v3.2.4,,"1,-1    ",1,-1,0
1512582985,49986318,2023/4/20,v3.2.4,"> 

@zwangsheng do we have any timeline to add this featureï¿½?
1512608100,52876270,2023-04-18,> spark executor pod has same memory value for request and limit","1,-1    ",1,-1,0
,,2023/4/20,v3.2.4,,"1,-1    ",1,-1,0
You can found this in the mail discussion:,,2023/4/20,v3.2.4,,"1,-1    ",1,-1,0
> There is a very good reason for this. It is recommended using k8s that you set ,,2023/4/20,v3.2.4,,"1,-1    ",1,-1,0
memory request and limit to the same value, set a cpu request,2023/4/20,v3.2.4,,"1,-1    ",1,-1,0
"limit. More info here https://home.robusta.dev/blog/kubernetes-memory-limit""",,2023/4/20,v3.2.4,,"1,-1    ",1,-1,0
1507804751,7322292,2023/4/20,v3.2.4,"@dongjoon-hyun  sorry, my bad, `test_parity_torch_distributor` became flaky, I haven't find the root cause for now.
After disabling the related cases (https://github.com/apache/spark/pull/40775 and https://github.com/apache/spark/pull/40787), the CI should be fine","1,-1    ",1,-1,0
1507871353,6477701,2023/4/20,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1507775612,506679,2023/4/20,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1507777201,9700541,2023/4/20,v3.2.4,"Thank you for merging, @sunchao !","2,-1    ",2,-1,1
1507825295,1475305,2023/4/20,v3.2.4,Thanks @sunchao @dongjoon-hyun ,"2,-2    ",2,-2,0
1506701455,8326978,2023/4/20,v3.2.4,"> This adds functionality similar to YARN[1] to K8s.

They are not exactly the same. Can we clarify both the similarities and differences? It seems that tracking failures on hosts has not been ported to k8s","1,-1    ",1,-1,0
1506708266,8326978,2023/4/20,v3.2.4,also cc @dongjoon-hyun ,"2,-1    ",2,-1,1
1506716422,26535726,2023/4/20,v3.2.4,"> They are not exactly the same.  ... It seems that tracking failures on hosts has not been ported to k8s

let me try to port the ""tracking failures on hosts"" feature as well

update 
------
this is kind of an independent and big feature, and seems out of the scope of this PR, documented the differences between YARN and K8s instead","1,-1    ",1,-1,0
1507865003,8326978,2023/4/20,v3.2.4,can we rename the title to `Port executor failure tracker from Spark on YARN to K8s`?,"1,-1    ",1,-1,0
1512322397,8326978,2023/4/20,v3.2.4,"thanks, merged to master","2,-1    ",2,-1,1
1512323524,26535726,2023/4/20,v3.2.4,"Thanks, @yaooqinn ","1,-1    ",1,-1,0
1506674711,7322292,2023/4/20,v3.2.4,cc @HyukjinKwon @WeichenXu123 ,"1,-1    ",1,-1,0
1506678975,6477701,2023/4/20,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1506838722,3182036,2023/4/20,v3.2.4,cc @gengliangwang ,"1,-1    ",1,-1,0
1508192537,3182036,2023/4/20,v3.2.4,"thanks for review, merging to master!","1,-1    ",1,-1,0
1506933782,7253827,2023/4/20,v3.2.4,"cc @cloud-fan, please let me know if you have a better idea.","2,-1    ",2,-1,1
1506949427,3182036,2023/4/20,v3.2.4,shall we update `ConvertToLocalRelation` to support `CommandResult` as well?,"2,-2    ",2,-2,0
1506952254,3182036,2023/4/20,v3.2.4,"or a more surgical way is to update `Dataset.getRows`, convert `CommandResult` to `LocalRelation` in the logical plan, and then execute the new logical plan.","1,-1    ",1,-1,0
1506972606,7253827,2023/4/20,v3.2.4,"> or a more surgical way is to update `Dataset.getRows`, convert `CommandResult` to `LocalRelation` in the logical plan, and then execute the new logical plan.

Thanks, that's a good idea!","2,-1    ",2,-1,1
1506998951,7253827,2023/4/20,v3.2.4,Updated to `LocalRelation` conversion in `Dataset.getRows()` in https://github.com/apache/spark/pull/40779/commits/98bf071cfb016b6461545a964ec07576419a6bb6,"1,-1    ",1,-1,0
1517096507,6477701,2023/4/20,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1517360462,7253827,2023/4/20,v3.2.4,Thanks all for the review!,"2,-1    ",2,-1,1
1506960027,32387433,2023/4/20,v3.2.4,@martin-g @HyukjinKwon PTAL. Thanks,"1,-1    ",1,-1,0
1507250353,1475305,2023/4/20,v3.2.4,"Could you add a test case for this scenario?

","1,-1    ",1,-1,0
1507251569,1475305,2023/4/20,v3.2.4,"hmm.. there is no `[SERVER]` tag, we should remove if from pr title","1,-1    ",1,-1,0
1507885274,6477701,2023/4/20,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1509103164,10248890,2023/4/20,v3.2.4,@Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`? ,"1,-1    ",1,-1,0
1509470286,32387433,2023/4/20,v3.2.4,"> @Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`?

Sorry, I don't get it. What do you mean about `drop the abbreviate`? This PR just do null check.","2,-1    ",2,-1,1
1511747342,10248890,2023/4/20,v3.2.4,"> > @Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`?
> 
> Sorry, I don't get it. What do you mean about `drop the abbreviate`? This PR just do null check.

Ah sorry! I didn't see you did the abbreviate in the top, please ignore this!","2,-2    ",2,-2,0
1511747358,10248890,2023/4/20,v3.2.4,"> > @Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`?
> 
> Sorry, I don't get it. What do you mean about `drop the abbreviate`? This PR just do null check.

Ah sorry! I didn't see you did the abbreviate in the top, please ignore this!","1,-1    ",1,-1,0
1511762972,10248890,2023/4/20,v3.2.4,"@Hisoka-X I think we also change this line?
https://github.com/apache/spark/pull/40780/files#diff-d21a96890e660df398527183a191b0d0c73521aada856e2491c33243f269fdceR128

I could add it to my ongoing PR https://github.com/apache/spark/pull/40785","1,-1    ",1,-1,0
1512370377,32387433,2023/4/20,v3.2.4,"> @Hisoka-X I think we also change this line? https://github.com/apache/spark/pull/40780/files#diff-d21a96890e660df398527183a191b0d0c73521aada856e2491c33243f269fdceR128
> 
> I could add it to my ongoing PR #40785

OK for me, but I just want to remind, add null check just want to avoid throw NPE in `setMessage`, not want to change `null` to  `""""`","1,-1    ",1,-1,0
1513609314,10248890,2023/4/20,v3.2.4,"> > @Hisoka-X I think we also change this line? https://github.com/apache/spark/pull/40780/files#diff-d21a96890e660df398527183a191b0d0c73521aada856e2491c33243f269fdceR128
> > I could add it to my ongoing PR #40785
> 
> OK for me, but I just want to remind, add null check just want to avoid throw NPE in `setMessage`, not want to change `null` to `""""`

I see. With additional look I think here it's better to keep what it is. as Status's description is nullable:

``` 
private Status(Code code, @Nullable String description, @Nullable Throwable cause) {
    this.code = checkNotNull(code, ""code"");
    this.description = description;
    this.cause = cause;
  }

public Status withDescription(String description) {
    if (Objects.equal(this.description, description)) {
      return this;
    }
    return new Status(this.code, description, this.cause);
  }

```","2,-1    ",2,-1,1
1506971666,3182036,2023/4/20,v3.2.4,cc @yaooqinn ,"1,-1    ",1,-1,0
1507774360,506679,2023/4/20,v3.2.4,"Test failures unrelated. Merged to master/branch-3.4, thanks!","1,-1    ",1,-1,0
1507777043,9700541,2023/4/20,v3.2.4,"Thank you so much, @sunchao ! Ya, I agree with you.","1,-1    ",1,-1,0
1507810640,8326978,2023/4/20,v3.2.4,late +1,"1,-1    ",1,-1,0
1507797626,8486025,2023/4/20,v3.2.4,"The failure GA is unrelated to this PR.
ping @hvanhovell @zhengruifeng cc @dongjoon-hyun @amaliujia ","1,-1    ",1,-1,0
1507802914,7322292,2023/4/20,v3.2.4,"sorry, `test_parity_torch_distributor` became flaky, I am going to disable related cases for now","1,-1    ",1,-1,0
1507878936,1938382,2023/4/20,v3.2.4,"So this happens when 
```
val df = createDataFrame(...)
df.collect()
```
?","1,-1    ",1,-1,0
1507999037,8486025,2023/4/20,v3.2.4,"> So this happens when
> 
It happens when the root plan is `LocalRelation`.


","2,-1    ",2,-1,1
1513048262,8486025,2023/4/20,v3.2.4,@hvanhovell The failed GA is unrelated to this PR.,"2,-2    ",2,-2,0
1552613772,8486025,2023/4/20,v3.2.4,"Because there are difference suggestion from @hvanhovell and @ueshin, I don't know how to continue this job.","1,-1    ",1,-1,0
1554336985,8486025,2023/4/20,v3.2.4,"@ueshin @hvanhovell Recently, https://github.com/apache/spark/pull/41064 added the rowCount statistics to `LocalRelation`. In this PR, @ueshin also suggested to add the row count as optional to `LocalRelation` message.
So I think this is a chance to add optional row count to `LocalRelation` message.","1,-1    ",1,-1,0
1560418014,8486025,2023/4/20,v3.2.4,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1507459984,502522,2023/4/20,v3.2.4,"cc: @amaliujia, @zhenlineo, @WweiL, @pengzhon-db ","1,-1    ",1,-1,0
1507743236,4190164,2023/4/20,v3.2.4,"The PR failed with formatting, just run the following command as it suggested on the failed build:
```
./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm
```","1,-1    ",1,-1,0
1507837825,1475305,2023/4/20,v3.2.4,"Good work ~ @rangadi please add new mima check to `CheckConnectJvmClientCompatibility`, thanks ","1,-1    ",1,-1,0
1508888793,1475305,2023/4/20,v3.2.4,https://github.com/apache/spark/pull/40757/files already re-chmod `connector/connect/bin/spark-connect-scala-client-classpath`,"2,-1    ",2,-1,1
1510912677,1475305,2023/4/20,v3.2.4,"```
ERROR: Comparing client jar: /__w/spark/spark/connector/connect/client/jvm/target/scala-2.12/spark-connect-client-jvm-assembly-3.5.0-SNAPSHOT.jar and and sql jar: /__w/spark/spark/sql/core/target/scala-2.12/spark-sql_2.12-3.5.0-SNAPSHOT.jar 
problems: 
method logName()java.lang.String in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method log()org.slf4j.Logger in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logInfo(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logDebug(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logTrace(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logWarning(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logError(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logInfo(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logDebug(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logTrace(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logWarning(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method logError(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method isTraceEnabled()Boolean in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method initializeLogIfNecessary(Boolean)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method initializeLogIfNecessary(Boolean,Boolean)Boolean in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
synthetic method initializeLogIfNecessary$default$2()Boolean in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
method initializeForcefully(Boolean,Boolean)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version
static method SOURCES_ALLOW_ONE_TIME_QUERY()scala.collection.Seq in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
static method SOURCE_NAME_NOOP()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
static method SOURCE_NAME_TABLE()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
static method SOURCE_NAME_CONSOLE()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
static method SOURCE_NAME_FOREACH_BATCH()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
static method SOURCE_NAME_FOREACH()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
static method SOURCE_NAME_MEMORY()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
method toTable(java.lang.String)org.apache.spark.sql.streaming.StreamingQuery in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version
Exceptions to binary compatibility can be added in 'CheckConnectJvmClientCompatibility#checkMiMaCompatibility'
```

Some compatibility issues need to be fixed

","1,-1    ",1,-1,0
1514133806,502522,2023/4/20,v3.2.4,"@LuciferYang PTAL recent updates. 
I will fix the ReplE2ESuite failure (not yet sure if that is related to this PR).","1,-1    ",1,-1,0
1514200138,1475305,2023/4/20,v3.2.4,"> I will fix the ReplE2ESuite failure (not yet sure if that is related to this PR).

This should be not related to the current PR. I told @vicennial  yesterday

https://github.com/apache/spark/pull/40675#issuecomment-1512936430
https://github.com/apache/spark/pull/40675#issuecomment-1513102087

@rangadi you can try to re-trigger the failed GA task again



","1,-1    ",1,-1,0
1514464840,21010250,2023/4/20,v3.2.4,@rangadi Could you try merging master into your branch and rerun the tests? I'm not sure if the github workflow that's running here is picking up [these](https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253) changes from the original PR (which fixed this issue in the original PR),"1,-1    ",1,-1,0
1515487762,502522,2023/4/20,v3.2.4,"@LuciferYang, @HyukjinKwon, @hvanhovell  please merge this when you get chance. All tests pass.","1,-1    ",1,-1,0
1515508104,6477701,2023/4/20,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1507483027,1938382,2023/4/20,v3.2.4,@cloud-fan ,"1,-1    ",1,-1,0
1507699796,1938382,2023/4/20,v3.2.4,In fact the compilation is failing on tests. let me fix it first.,"1,-1    ",1,-1,0
1508027205,1938382,2023/4/20,v3.2.4,Test issues should have been fixed.,"1,-1    ",1,-1,0
1510689649,3182036,2023/4/20,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1507640339,10248890,2023/4/20,v3.2.4,"@HyukjinKwon @rangadi @pengzhon-db

Hey guys, PTAL when you get a chance, thanks!","1,-1    ",1,-1,0
1513889202,10248890,2023/4/20,v3.2.4,@HyukjinKwon Can you take a look? Thanks!,"1,-1    ",1,-1,0
1515508328,6477701,2023/4/20,v3.2.4,@WweiL mind rebasing this one please?,"1,-1    ",1,-1,0
1515522281,10248890,2023/4/20,v3.2.4,Fetched and Merged with master. I noticed stack trace was added. I'll create a SPARK ticket to include it ,"1,-1    ",1,-1,0
1515544805,6477701,2023/4/20,v3.2.4,@hvanhovell or @grundprinzip actually mind taking a look please when you find some time?,"1,-1    ",1,-1,0
1515737988,6477701,2023/4/20,v3.2.4,"I see .. sorry for a bit of back and forth:

```
python/pyspark/sql/connect/streaming/query.py:72: error: Missing return statement  [return]
python/pyspark/sql/connect/streaming/query.py:108: error: Missing return statement  [return]
python/pyspark/sql/connect/streaming/query.py:140: error: Missing return statement  [return]
```

Let's add them back :-) ..","1,-1    ",1,-1,0
1517035261,10248890,2023/4/20,v3.2.4,@HyukjinKwon Can you merge this : ) Thanks!,"1,-1    ",1,-1,0
1517073954,6477701,2023/4/20,v3.2.4,Merged to master.,"2,-4    ",2,-4,-2
1507690159,506679,2023/4/20,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1507692835,9700541,2023/4/20,v3.2.4,Thank you always!,"1,-1    ",1,-1,0
1507940961,9700541,2023/4/20,v3.2.4,"For the record, I verified that this commit excluded `branch-3.2` correct like the following in the latest `Snapshot Publishing`.

- https://github.com/apache/spark/actions/runs/4694946818

![Screenshot 2023-04-13 at 10 22 09 PM](https://user-images.githubusercontent.com/9700541/231948512-81799483-8617-4039-b90b-1db0fd61231e.png)
","1,-1    ",1,-1,0
1507800484,7322292,2023/4/20,v3.2.4,also track it in https://issues.apache.org/jira/browse/SPARK-43122,"1,-1    ",1,-1,0
1507812967,7322292,2023/4/20,v3.2.4,cc @HyukjinKwon @WeichenXu123 @dongjoon-hyun ,"1,-1    ",1,-1,0
1507813457,7322292,2023/4/20,v3.2.4,"```
Starting test(python3.9): pyspark.ml.tests.connect.test_parity_torch_distributor (temp output: /__w/spark/spark/python/target/69e3939a-22ef-4a97-a09c-c5d3e66930de/python3.9__pyspark.ml.tests.connect.test_parity_torch_distributor__zke1oq_a.log)
Finished test(python3.9): pyspark.ml.tests.connect.test_parity_torch_distributor (192s) ... 10 tests were skipped
```

right now the local mode related tests are disabled in connect","1,-1    ",1,-1,0
1507878070,6477701,2023/4/20,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1507937784,7322292,2023/4/20,v3.2.4,@dongjoon-hyun I created https://issues.apache.org/jira/browse/SPARK-43122 for the disabled tests,"1,-1    ",1,-1,0
1507938724,9700541,2023/4/20,v3.2.4,"Great! Thank you, @zhengruifeng ! That's all we need in order to not to forget.","1,-1    ",1,-1,0
1507879840,1475305,2023/4/20,v3.2.4,"Test first,  need wait until 3.4 release at least 

","2,-4    ",2,-4,-2
1507924698,9700541,2023/4/20,v3.2.4,"Also, cc FYI, @sunchao ","1,-1    ",1,-1,0
1508009381,47577197,2023/4/20,v3.2.4,"> Test first, need wait until 3.4 release at least

3.4.0 is uploaded to mirrors. ","1,-1    ",1,-1,0
1508038439,1475305,2023/4/20,v3.2.4,"> > Test first, need wait until 3.4 release at least
> 
> 3.4.0 is uploaded to mirrors.


I know, but let's wait for the official website update

","1,-1    ",1,-1,0
1509118242,506679,2023/4/20,v3.2.4,"Personally I like this move, but not sure whether there are other users in the community that still depend on Hadoop 2.x. Has this been discussed before?","1,-1    ",1,-1,0
1509138558,47577197,2023/4/20,v3.2.4,"> Personally I like this move, but not sure whether there are other users in the community that still depend on Hadoop 2.x. Has this been discussed before?

yes, like this one [Dropping Apache Spark Hadoop2 Binary Distribution?](https://lists.apache.org/thread/z4jdy9959b6zj9t726zl0zcrk4hzs0xs) ","1,-1    ",1,-1,0
1509143877,506679,2023/4/20,v3.2.4,"Oh cool, it's good then. Forgot about this thread even though I replied on it ..","1,-1    ",1,-1,0
1509450150,506679,2023/4/20,v3.2.4,Just sent an email to the community in case anyone still have concerns on this.,"1,-1    ",1,-1,0
1513198232,1475305,2023/4/20,v3.2.4,friendly ping @dongjoon-hyun @sunchao ,"1,-1    ",1,-1,0
1513418991,506679,2023/4/20,v3.2.4,"Merged to master, thanks!","2,-1    ",2,-1,1
1514032756,1475305,2023/4/21,v3.2.4,Thanks @sunchao @dongjoon-hyun @bjornjorgensen ~,"2,-1    ",2,-1,1
1508294950,8486025,2023/4/21,v3.2.4,ping @infoankitp @navinvishy cc @cloud-fan ,"2,-2    ",2,-2,0
1515638251,8486025,2023/4/21,v3.2.4,"https://github.com/apache/spark/pull/40833 merged, so close this one.","1,-1    ",1,-1,0
1510593669,5399861,2023/4/21,v3.2.4,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1510633617,6477701,2023/4/21,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1508375775,6477701,2023/4/21,v3.2.4,"Not sure how CI passes, or if this only happens in my local. But I think we should exclude them in any event.

cc @LuciferYang ","1,-1    ",1,-1,0
1508799437,1475305,2023/4/21,v3.2.4,"Manually run `dev/sbt-checkstyle` can reproduce, but it seems that `dev/sbt-checkstyle` does not always execute.

https://github.com/apache/spark/blob/4c938d62d791742b9f0c6a77b66fc06a90d7c0ad/dev/run-tests.py#L611-L618

https://github.com/apache/spark/blob/4c938d62d791742b9f0c6a77b66fc06a90d7c0ad/dev/run-tests.py#L641-L647

`lint-java` use mvn checkstyle, It did not checked this error.

So should we let `lint-java` use  `sbt-checkstyle` to unify behavior?

Additionally, it seems that the generated proto java files of the connect-common module have not been checked? it is a little strange.

","1,-1    ",1,-1,0
1510285270,6477701,2023/4/21,v3.2.4,Yeah .. let me merge this in first and fix them to be consistent separately .. ,"1,-1    ",1,-1,0
1510285530,6477701,2023/4/21,v3.2.4,Let me merge this one first.,"1,-1    ",1,-1,0
1510286042,6477701,2023/4/21,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1510594855,1475305,2023/4/21,v3.2.4,"hmm...branch-3.4 not require this fix?

","2,-1    ",2,-1,1
1510617283,6477701,2023/4/21,v3.2.4,actually yeah. let's put this in branch-3.4 too.,"2,-1    ",2,-1,1
1510617463,6477701,2023/4/21,v3.2.4,Merged to branch-3.4 too.,"2,-2    ",2,-2,0
1511401471,1475305,2023/4/21,v3.2.4,"> Yeah .. let me merge this in first and fix them to be consistent separately ..

`sbt-checkstyle-plugin` will not print wrong fmt issue to the console,  we can only check the problem from `target/checkstyle-output.xml`  and they are in different module directories. On the other hand, `sbt-checkstyle-plugin`  has not been updated for a long time, It seems that no one is maintaining it ...

`maven-checkstyle-plugin` will print out the problem to the console, which is more intuitive.



","1,-1    ",1,-1,0
1508448973,7322292,2023/4/21,v3.2.4,"let me try merging commits from master several times, to see whether this fix is stable enough","1,-1    ",1,-1,0
1510991598,7322292,2023/4/21,v3.2.4,"This PR actually only change `setUp & tearDown` to `setUpClass & tearDownClass`.

In all the 6 runs, the PyTorch related tests all passed, so I think it is ready for review.

@HyukjinKwon @dongjoon-hyun @WeichenXu123 ","1,-1    ",1,-1,0
1512278648,7322292,2023/4/21,v3.2.4,"I see a failure in `pyspark.ml.clustering`, should be unrelated.
but let me re-run `pyspark.ml.clustering` and test torch one more time.
right now, I didn't see the previous torch-related issue.","1,-1    ",1,-1,0
1512379310,7322292,2023/4/21,v3.2.4,"in all the 10 runs, `test_parity_torch_distributor` and `test_distributor` passed without being hanged.
so I think it is stable enough to re-enable.","1,-1    ",1,-1,0
1512379955,7322292,2023/4/21,v3.2.4,merged to master,"1,-1    ",1,-1,0
1516006137,7870972,2023/4/21,v3.2.4,"Maybe @cloud-fan or @hvanhovell? You last reviewed changes here.

It's a one-line fix: `DslAttr.attr` now returns the wrapped `UnresolvedAttribute` instead of creating a new one.","2,-1    ",2,-1,1
1517793940,3182036,2023/4/21,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1517926432,1475305,2023/4/21,v3.2.4,"https://github.com/apache/spark/actions/runs/4765094614/jobs/8470442826

<img width=""1278"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/233662686-1bfb0633-bbd6-4c4a-a9b9-ecdd8e2f0ffc.png"">


@cloud-fan many test failed after this one, should we revert this one first?","2,-1    ",2,-1,1
1517927714,1475305,2023/4/21,v3.2.4,"> https://github.com/apache/spark/actions/runs/4765094614/jobs/8470442826
> 
> <img alt=""image"" width=""1278"" src=""https://user-images.githubusercontent.com/1475305/233662686-1bfb0633-bbd6-4c4a-a9b9-ecdd8e2f0ffc.png"">
> 
> @cloud-fan many test failed after this one, should we revert this one first?

also ping @HyukjinKwon ","2,-2    ",2,-2,0
1517960380,3182036,2023/4/21,v3.2.4,let's revert first. Seems GA wrongly reported green for this PR.,"1,-1    ",1,-1,0
1517960382,5399861,2023/4/21,v3.2.4,Reverted: https://github.com/apache/spark/commit/3523d83ac472b330bb86a442365c0a15f7e53f8c.,"1,-1    ",1,-1,0
1518283716,7870972,2023/4/21,v3.2.4,"Damn, thank you for reverting guys. Unsure why GA didn't test the last commit.","1,-1    ",1,-1,0
1518291912,7870972,2023/4/21,v3.2.4,"@cloud-fan, maybe let's consider multi-part attribute references as fine or at least separate from this? What do you think?

I opened another PR just changing `DslAttr.attr` to not break on special characters #40902.","1,-1    ",1,-1,0
1517025604,4190164,2023/4/21,v3.2.4,cc @amaliujia Could you review the agg and reduce changes. a.k.a. the last two commits.,"1,-1    ",1,-1,0
1518390479,1938382,2023/4/21,v3.2.4,Overall looks reasonable to me. I only have questions over the proto validation in the server side.,"1,-1    ",1,-1,0
1544918604,4190164,2023/4/21,v3.2.4,cc @hvanhovell @HyukjinKwon Can we merge this? Thanks!,"1,-1    ",1,-1,0
1548042425,9616802,2023/4/21,v3.2.4,Merging.,"1,-1    ",1,-1,0
1509226317,10248890,2023/4/21,v3.2.4,"@rangadi @pengzhon-db @HyukjinKwon PTAL, thank you!","1,-1    ",1,-1,0
1509937727,1938382,2023/4/21,v3.2.4,Why do you need the change in `dev/tox.ini`?,"1,-1    ",1,-1,0
1510523779,10248890,2023/4/21,v3.2.4,"> Why do you need the change in `dev/tox.ini`?

@amaliujia Ah thanks for pointing it out. I'll revert it.

The change and reason is in this PR: https://github.com/apache/spark/pull/40801","1,-1    ",1,-1,0
1510754674,6477701,2023/4/21,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1510632838,6477701,2023/4/21,v3.2.4,@DerekTBrown Mind creating a JIRA and add it to the PR title please? See also https://spark.apache.org/contributing.html,"1,-1    ",1,-1,0
1514545693,26535726,2023/4/21,v3.2.4,"@DerekTBrown I have an alternative solution https://github.com/apache/spark/pull/40831, which should cover your case","1,-1    ",1,-1,0
1520512047,6845676,2023/4/21,v3.2.4,Looks good. Closing in favor of #40831 ,"2,-1    ",2,-1,1
1509546979,104102129,2023/4/21,v3.2.4,@cloud-fan can you help review this? thank you!,"2,-2    ",2,-2,0
1514003542,6477701,2023/4/21,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1509411772,10248890,2023/4/21,v3.2.4,"Not sure if this would break github tests, let's see","1,-1    ",1,-1,0
1510629571,6477701,2023/4/21,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1509470843,26535726,2023/4/21,v3.2.4,cc @wangyum @sunchao ,"1,-1    ",1,-1,0
1509508228,506679,2023/4/21,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1510009044,5399861,2023/4/21,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1510097573,1938382,2023/4/21,v3.2.4,LGTM,"1,-1    ",1,-1,0
1510562041,7322292,2023/4/21,v3.2.4,"@gengliangwang I just success to run this `pip install --upgrade -r dev/requirements.txt` in a clean conda env, and can not reproduce the issue related to pytorch version.

@HyukjinKwon @WeichenXu123 have you seen this version issue?","1,-1    ",1,-1,0
1510625745,6477701,2023/4/21,v3.2.4,I haven't seen this before. but I think loosing the version restrictions looks fine.,"1,-1    ",1,-1,0
1510722608,1097932,2023/4/21,v3.2.4,"> @gengliangwang I just success to run this pip install --upgrade -r dev/requirements.txt in a clean conda env, and can not reproduce the issue related to pytorch version.

@zhengruifeng I am using python 3.11.2
And I am actually running with 
```
pip3 install --upgrade -r dev/requirements.txt
```","1,-1    ",1,-1,0
1510749953,6477701,2023/4/21,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1510589717,5399861,2023/4/21,v3.2.4,@cloud-fan @sunchao ,"1,-1    ",1,-1,0
1510667065,3182036,2023/4/21,v3.2.4,@wangyum can we mention the idea a bit more? Do we push down the cast in join condition to the join children?,"1,-1    ",1,-1,0
1510807373,5399861,2023/4/21,v3.2.4,"@cloud-fan OK, updated the description of the PR. We do not push down the cast to the  join children. This rule is the same as `CoalesceBucketsInJoin`. Must also must before `EnsureRequirements`. The join's children must be the filter, project and stream side of broadcast join.","1,-1    ",1,-1,0
1510825983,3182036,2023/4/21,v3.2.4,Can we fix this issue by pulling out expressions of join conditions to Projects under Join? IIRC you had a PR for it and then the join condition should only contains attributes.,"1,-1    ",1,-1,0
1511025438,5399861,2023/4/21,v3.2.4,"Pulling out join condition can't fix this issue because the [output partitioning](https://github.com/apache/spark/blob/72922adc8a78e8d31f03205a148b89291a9a4d19/sql/core/src/main/scala/org/apache/spark/sql/execution/AliasAwareOutputExpression.scala#L57) is `UnknownPartitioning(Bucket Number)` which can't satisfy the `SortMergeJoin`, and also needs to introduce shuffle.

```sql
SELECT *
FROM   (SELECT Cast(i AS DECIMAL(20, 0)) AS i FROM t2) tmp1
JOIN   (SELECT Cast(i AS DECIMAL(20, 0)) AS i FROM t3) tmp2
ON     tmp1.i = tmp2.i; 
```

```
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- SortMergeJoin [i#23], [i#24], Inner
   :- Sort [i#23 ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(i#23, 200), ENSURE_REQUIREMENTS, [plan_id=130]
   :     +- Project [cast(i#19L as decimal(20,0)) AS i#23]
   :        +- Filter isnotnull(cast(i#19L as decimal(20,0)))
   :           +- FileScan parquet spark_catalog.default.t2[i#19L] Batched: true, Bucketed: false (disabled by query planner)
   +- Sort [i#24 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(i#24, 200), ENSURE_REQUIREMENTS, [plan_id=135]
         +- Project [cast(i#20 as decimal(20,0)) AS i#24]
            +- Filter isnotnull(cast(i#20 as decimal(20,0)))
               +- FileScan parquet spark_catalog.default.t3[i#20] Batched: true, Bucketed: false (disabled by query planner)
```","3,-1    ",3,-1,2
1512508095,3182036,2023/4/21,v3.2.4,Shall we improve the partitioning propagation to support cast?,"1,-1    ",1,-1,0
1512593794,7322292,2023/4/21,v3.2.4,merged to master,"1,-1    ",1,-1,0
1510125914,5399861,2023/4/21,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1512404675,6477701,2023/4/21,v3.2.4,cc @Ngone51 @jiangxb1987 FYI,"1,-1    ",1,-1,0
1539614085,73838248,2023/4/21,v3.2.4,"Small ping on this. The PR was previously a draft since I was waiting for me to test it on a real workload, but that have now been done.

","1,-1    ",1,-1,0
1544008137,822522,2023/4/21,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1511980672,36677585,2023/4/21,v3.2.4,@itholic @MaxGekk ,"1,-1    ",1,-1,0
1519453198,1580697,2023/4/21,v3.2.4,"@kori73 Could you update the example (output) according to the recent commit, please.","1,-1    ",1,-1,0
1519612722,36677585,2023/4/21,v3.2.4,"> @kori73 Could you update the example (output) according to the recent commit, please.

updated the example according to the recent commit","1,-1    ",1,-1,0
1519620141,1580697,2023/4/21,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @kori73.","1,-1    ",1,-1,0
1519623179,1580697,2023/4/21,v3.2.4,@kori73 Congratulations with your first contribution to Apache Spark!,"3,-1    ",3,-1,2
1510490719,790409,2023/4/21,v3.2.4,@allisonwang-db @cloud-fan ,"1,-2    ",1,-2,-1
1512329066,3182036,2023/4/21,v3.2.4,is this a long-standing bug? Also cc @viirya ,"1,-1    ",1,-1,0
1513296034,790409,2023/4/21,v3.2.4,"Yes, this is a long-standing bug.","2,-2    ",2,-2,0
1513568704,790409,2023/4/21,v3.2.4,"FYI this bug affected both the current DecorrelateInnerQuery framework and the old code (with spark.sql.optimizer.decorrelateInnerQuery.enabled = false), and this PR fixes both.","1,-2    ",1,-2,-1
1514015480,3182036,2023/4/21,v3.2.4,"The streaming failure is unrelated. Thanks, merging to master/3.4!","1,-2    ",1,-2,-1
1526471166,1591700,2023/4/21,v3.2.4,"+CC @cloud-fan, @dongjoon-hyun who have reviewed work on explain output earlier.","1,-2    ",1,-2,-1
1526602319,9700541,2023/4/21,v3.2.4,"Thank you for pinging me, @mridulm .

Also, cc @sunchao ","1,-1    ",1,-1,0
1539764854,3182036,2023/4/21,v3.2.4,"I agree that we do need to copy the cached plan when getting it, but I feel the current change is hard to understand and reason about.

I think a better place to fix is `CacheManager#useCachedDataInternal`. It does copy `InMemoryRelation` but does not copy the physical plan:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L298-L306

We can add a new method `InMemoryRelation#freshCopy` which takes new output attributes and copy the physical plan, then we change `cached.cachedRepresentation.withOutput(currentFragment.output)` to `cached.cachedRepresentation.freshCopy(currentFragment.output)`","1,-1    ",1,-1,0
1542680938,5604993,2023/4/21,v3.2.4,"> I agree that we do need to copy the cached plan when getting it, but I feel the current change is hard to understand and reason about.
> 
> I think a better place to fix is `CacheManager#useCachedDataInternal`. It does copy `InMemoryRelation` but does not copy the physical plan: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L298-L306
> 
> We can add a new method `InMemoryRelation#freshCopy` which takes new output attributes and copy the physical plan, then we change `cached.cachedRepresentation.withOutput(currentFragment.output)` to `cached.cachedRepresentation.freshCopy(currentFragment.output)`

The challenge with this approach is that `InMemoryRelation` does not get the physical plan passed in directly. It is through `CachedRDDBuilder`. I originally tried making a new `CachedRDDBuilder` with a copy of the physical plan, but it broke other tests due to the private state it maintains.","2,-3    ",2,-3,-1
1543311678,3182036,2023/4/21,v3.2.4,"I see, can we change `def cachedPlan: SparkPlan = cacheBuilder.cachedPlan` to `lazy val cachedPlan: SparkPlan = cacheBuilder.cachedPlan.clone` with comments to explain it? This makes sure that a new instance of `InMemoryRelation` will have a new instance of the physical plan.","1,-1    ",1,-1,0
1552124399,5604993,2023/4/21,v3.2.4,"> 

@cloud-fan Cloning the cachedPlan is also problematic because it contains state (accumulators in private fields) when it includes a `CollectMetricsExec` operator. `CollectMetricsExec.collect` specifically looks at the `InMemoryRelation.cachedPlan` to get the stateful metrics.

I verified this is an issue by cloning `InMemoryRelation.cachedPlan`. Then I modified a unit test that uses `Dataset.observe` to cache the dataset. This breaks the unit test. When I revert cloning the cachedPlan it passes. Here is how I modified the unit test to prove the issue.
```
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
index f046daacb91..3de33e3e1b2 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
@@ -277,7 +277,7 @@ class DataFrameCallbackSuite extends QueryTest
         max($""id"").as(""max_val""),
         // Test unresolved alias
         sum($""id""),
-        count(when($""id"" % 2 === 0, 1)).as(""num_even""))
+        count(when($""id"" % 2 === 0, 1)).as(""num_even"")).cache()
       .observe(
         name = ""other_event"",
         avg($""id"").cast(""int"").as(""avg_val""))
```

So I think we should stick with only cloning it for the innerChildren since the only usage is the ExplainUtils and statefulness doesn't matter, besides the `TreeNode.tag` values.","1,-3    ",1,-3,-2
1552505341,3182036,2023/4/21,v3.2.4,"thanks, merging to master/3.4!","1,-2    ",1,-2,-1
1510544354,6477701,2023/4/21,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1510557462,6477701,2023/4/21,v3.2.4,Merged to master and branch-3.4.,"1,-2    ",1,-2,-1
1510659262,6477701,2023/4/21,v3.2.4,Merged to branch-3.3 too.,"1,-1    ",1,-1,0
1510607496,6477701,2023/4/21,v3.2.4,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1510612587,6477701,2023/4/21,v3.2.4,Sorry for a forth and back. We don't actually need this - it was my mistake. reverted.,"1,-1    ",1,-1,0
1510617073,44108233,2023/4/21,v3.2.4,cc @mingyangge-db @HyukjinKwon FYI,"1,-1    ",1,-1,0
1510761612,6477701,2023/4/21,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1510686511,35164941,2023/4/21,v3.2.4,"I can't find a method to trigger this error_class using sql command. This error_class are only thrown by ""mergeExpressions"" field of AggregateWindowFunction. But this field isn't called anywhere in all aggregate window function. Is it an internal error?","1,-1    ",1,-1,0
1514112456,35164941,2023/4/21,v3.2.4,"@MaxGekk , any comment?","3,-1    ",3,-1,2
1532747669,1580697,2023/4/21,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @liang3zy22.","1,-1    ",1,-1,0
1510908244,7322292,2023/4/21,v3.2.4,merged to master,"1,-1    ",1,-1,0
1510871407,2598924,2023/4/21,v3.2.4,"One of the tests fail but I wonder why this should not be due to import:
`starting mypy annotations test...
annotations failed mypy checks:
python/pyspark/broadcast.py:100: error: Overloaded function implementation does not accept all possible arguments of signature 3  [misc]
Found 1 error in 1 file (checked 505 source files)`

I am getting also some Java tests failing but this is really weird since I have not touched any Java code","1,-1    ",1,-1,0
1510851860,51110188,2023/4/21,v3.2.4,cc @ulysses-you ,"2,-2    ",2,-2,0
1512265126,1938382,2023/4/21,v3.2.4,Looks like the failed test is a flaky one so not relevant to this change.,"1,-1    ",1,-1,0
1512342256,51110188,2023/4/21,v3.2.4,"Yea, I can re-trigger if it's necessary.","1,-1    ",1,-1,0
1512354622,1938382,2023/4/21,v3.2.4,"I guess better to trigger to have it pass
","1,-1    ",1,-1,0
1519205090,6477701,2023/4/21,v3.2.4,Merged to master.,"2,-2    ",2,-2,0
1518209816,13213701,2023/4/21,v3.2.4,"Surprisingly after commiting of naming improvements (no logic changes) the build failed. I think it's not related to my change:
```
[error] running /home/runner/work/spark/spark/build/sbt -Phadoop-3 -Pconnect sql-kafka-0-10/test protobuf/test connect/test connect-client-jvm/test mllib/test ; received return code 1
```

It happened at [Run / Build modules: streaming, sql-kafka-0-10, streaming-kafka-0-10, mllib-local, mllib, yarn, mesos, kubernetes, hadoop-cloud, spark-ganglia-lgpl, connect, protobuf](https://github.com/woj-i/spark/actions/runs/4762682139/jobs/8474612155#logs)
Can you please help me with fixing the build?

The status result is:
```
ReplE2ESuite.Simple query
java.lang.RuntimeException: 
REPL Timed out while running command: 
spark.sql(""select 1"").collect()
      
Console output: 
Spark session available as 'spark'.
   _____                  __      ______                            __
  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_
  \__ \/ __ \/ __ `/ ___/ //_/  / /   / __ \/ __ \/ __ \/ _ \/ ___/ __/
 ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_
/____/ .___/\__,_/_/  /_/|_|   \____/\____/_/ /_/_/ /_/\___/\___/\__/
    /_/

```

I extracted some logs about summary of the failure: 
```
2023-04-21T08:59:45.6811001Z [0m[[0m[0minfo[0m] [0m[0m[36mTests: succeeded 739, failed 3, canceled 0, ignored 0, pending 0[0m[0m
2023-04-21T08:59:45.6811459Z [0m[[0m[0minfo[0m] [0m[0m[31m*** 3 TESTS FAILED ***[0m[0m
2023-04-21T08:59:45.6841148Z [0m[[0m[31merror[0m] [0m[0mFailed tests:[0m
2023-04-21T08:59:45.6845148Z [0m[[0m[31merror[0m] [0m[0m	org.apache.spark.sql.application.ReplE2ESuite[0m
2023-04-21T08:59:45.9076588Z [0m[[0m[31merror[0m] [0m[0m(connect-client-jvm / Test / [31mtest[0m) sbt.TestsFailedException: Tests unsuccessful[0m
2023-04-21T08:59:45.9270454Z [0m[[0m[31merror[0m] [0m[0mTotal time: 140 s (02:20), completed Apr 21, 2023 8:59:45 AM[0m
2023-04-21T08:59:46.7253579Z [0J[error] running /home/runner/work/spark/spark/build/sbt -Phadoop-3 -Pconnect sql-kafka-0-10/test protobuf/test connect/test connect-client-jvm/test mllib/test ; received return code 1
2023-04-21T08:59:46.9853017Z ##[error]Process completed with exit code 18.
```

and some exceptions/errors that were very frequent in this test
```
2023-04-21T08:58:47.7065800Z [0m[[0m[0minfo[0m] [0m[0m[32mUserDefinedFunctionE2ETestSuite:[0m[0m
2023-04-21T08:58:47.9937562Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed filter (225 milliseconds)[0m[0m
2023-04-21T08:58:48.0428439Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed filter - java (48 milliseconds)[0m[0m
2023-04-21T08:58:48.1508396Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed map (98 milliseconds)[0m[0m
2023-04-21T08:58:48.2447937Z [0m[[0m[0minfo[0m] [0m[0m[32m- filter with condition (92 milliseconds)[0m[0m
2023-04-21T08:58:48.3442084Z [0m[[0m[0minfo[0m] [0m[0m[32m- filter with col(*) (98 milliseconds)[0m[0m
2023-04-21T08:58:48.4067957Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed map - java (55 milliseconds)[0m[0m
2023-04-21T08:58:48.4836489Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed flat map (74 milliseconds)[0m[0m
2023-04-21T08:58:48.5347903Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed flat map - java (51 milliseconds)[0m[0m
2023-04-21T08:58:48.8260858Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed map partition (288 milliseconds)[0m[0m
2023-04-21T08:58:49.0958006Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset typed map partition - java (270 milliseconds)[0m[0m
2023-04-21T08:58:49.1921226Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset foreach (92 milliseconds)[0m[0m
2023-04-21T08:58:49.2289858Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset foreach - java (34 milliseconds)[0m[0m
2023-04-21T08:58:49.3390396Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset foreachPartition (109 milliseconds)[0m[0m
2023-04-21T08:58:49.4353655Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset foreachPartition - java (99 milliseconds)[0m[0m
2023-04-21T08:58:49.4971434Z [0m[[0m[0minfo[0m] [0m[0m[32m- Dataset foreach: change not visible to client (65 milliseconds)[0m[0m
2023-04-21T08:58:49.5121205Z [0m[[0m[0minfo[0m] [0m[0m[32mReplE2ESuite:[0m[0m
2023-04-21T08:58:53.2505694Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.2692727Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.3262467Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.3544612Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.3646754Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.3896517Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.4061277Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.4268730Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.4353509Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.4546791Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.4808386Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.4964638Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.5198604Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.5300108Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.5383343Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.5494860Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.5631736Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.5913690Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.6397803Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.6769519Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.7221075Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.7631252Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.8129701Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.8289367Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.8541965Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.8848448Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9060169Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9152965Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9257073Z sh: 1: cannot create /dev/tty: No such device or address
2023-04-21T08:58:53.9294986Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9379865Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9483798Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9579639Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9644462Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9701922Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9739622Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9775897Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9830054Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9870461Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9910296Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:53.9947627Z sh: 1: cannot open /dev/tty: No such device or address
```
...

```
2023-04-21T08:58:59.6221061Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.6255708Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.6282881Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.6349493Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.6627557Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.6787449Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.7065904Z [0m[[0m[0minfo[0m] [0m[0m[31m- Simple query *** FAILED *** (10 seconds, 75 milliseconds)[0m[0m
2023-04-21T08:58:59.7248880Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.7820090Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.7821918Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.7870724Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.7941758Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8165642Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8305760Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8376055Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8407369Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8439122Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8541147Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8651384Z sh: 1: cannot open /dev/tty: No such device or address
2023-04-21T08:58:59.8652177Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.lang.RuntimeException: REPL Timed out while running command: [0m[0m
2023-04-21T08:58:59.8653000Z [0m[[0m[0minfo[0m] [0m[0m[31mspark.sql(""select 1"").collect()[0m[0m
2023-04-21T08:58:59.8653943Z [0m[[0m[0minfo[0m] [0m[0m[31m      [0m[0m
2023-04-21T08:58:59.8654470Z [0m[[0m[0minfo[0m] [0m[0m[31mConsole output: [0m[0m
2023-04-21T08:58:59.8655695Z [0m[[0m[0minfo[0m] [0m[0m[31mSpark session available as 'spark'.[0m[0m
2023-04-21T08:58:59.8656304Z [0m[[0m[0minfo[0m] [0m[0m[31m   _____                  __      ______                            __[0m[0m
2023-04-21T08:58:59.8657167Z [0m[[0m[0minfo[0m] [0m[0m[31m  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_[0m[0m
2023-04-21T08:58:59.8657763Z [0m[[0m[0minfo[0m] [0m[0m[31m  \__ \/ __ \/ __ `/ ___/ //_/  / /   / __ \/ __ \/ __ \/ _ \/ ___/ __/[0m[0m
2023-04-21T08:58:59.8663578Z [0m[[0m[0minfo[0m] [0m[0m[31m ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_[0m[0m
2023-04-21T08:58:59.8664196Z [0m[[0m[0minfo[0m] [0m[0m[31m/____/ .___/\__,_/_/  /_/|_|   \____/\____/_/ /_/_/ /_/\___/\___/\__/[0m[0m
2023-04-21T08:58:59.8664916Z [0m[[0m[0minfo[0m] [0m[0m[31m    /_/[0m[0m
2023-04-21T08:58:59.8665558Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8666266Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8666734Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8667377Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8667825Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8668458Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8668901Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8669534Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8669960Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8670722Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.8671179Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
```
...

```
2023-04-21T08:58:59.9409151Z [0m[[0m[0minfo[0m] [0m[0m[31mError output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc[0m[0m
2023-04-21T08:58:59.9409484Z [0m[[0m[0minfo[0m] [0m[0m[31mCompiling /home/runner/work/spark/spark/connector/connect/client/jvm/(console)[0m[0m
2023-04-21T08:58:59.9409764Z [0m[[0m[0minfo[0m] [0m[0m[31mjava.lang.RuntimeException: Nonzero exit value: 2[0m[0m
2023-04-21T08:58:59.9410063Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.package$.error(package.scala:30)[0m[0m
2023-04-21T08:58:59.9410464Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:138)[0m[0m
2023-04-21T08:58:59.9410846Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:108)[0m[0m
2023-04-21T08:58:59.9411103Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.TTY$.stty(Utils.scala:103)[0m[0m
2023-04-21T08:58:59.9411387Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.TTY$.withSttyOverride(Utils.scala:114)[0m[0m
2023-04-21T08:58:59.9411672Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.Terminal$.readLine(Terminal.scala:41)[0m[0m
2023-04-21T08:58:59.9412007Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.AmmoniteFrontEnd.readLine(AmmoniteFrontEnd.scala:133)[0m[0m
2023-04-21T08:58:59.9412374Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.AmmoniteFrontEnd.action(AmmoniteFrontEnd.scala:28)[0m[0m
2023-04-21T08:58:59.9412650Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.$anonfun$action$4(Repl.scala:194)[0m[0m
2023-04-21T08:58:59.9412936Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.$anonfun$flatMap$1(Signaller.scala:45)[0m[0m
2023-04-21T08:58:59.9413217Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Signaller.apply(Signaller.scala:28)[0m[0m
2023-04-21T08:58:59.9413496Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.flatMap(Signaller.scala:45)[0m[0m
2023-04-21T08:58:59.9413765Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.flatMap$(Signaller.scala:45)[0m[0m
2023-04-21T08:58:59.9414044Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Signaller.flatMap(Signaller.scala:16)[0m[0m
2023-04-21T08:58:59.9414308Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.$anonfun$action$2(Repl.scala:178)[0m[0m
2023-04-21T08:58:59.9414621Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.util.Catching.flatMap(Res.scala:115)[0m[0m
2023-04-21T08:58:59.9414875Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.action(Repl.scala:170)[0m[0m
2023-04-21T08:58:59.9415122Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.loop$1(Repl.scala:212)[0m[0m
2023-04-21T08:58:59.9415366Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.run(Repl.scala:227)[0m[0m
2023-04-21T08:58:59.9415618Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.Main.$anonfun$run$1(Main.scala:236)[0m[0m
2023-04-21T08:58:59.9415866Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.Option.getOrElse(Option.scala:189)[0m[0m
2023-04-21T08:58:59.9416095Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.Main.run(Main.scala:224)[0m[0m
2023-04-21T08:58:59.9416446Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:100)[0m[0m
2023-04-21T08:58:59.9416845Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.application.ReplE2ESuite$$anon$1.run(ReplE2ESuite.scala:59)[0m[0m
2023-04-21T08:58:59.9417169Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[0m[0m
2023-04-21T08:58:59.9417461Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m[0m
2023-04-21T08:58:59.9417825Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m[0m
2023-04-21T08:58:59.9418212Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m[0m
2023-04-21T08:58:59.9445731Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.lang.Thread.run(Thread.java:750)[0m[0m
2023-04-21T08:58:59.9446013Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m
2023-04-21T08:58:59.9446291Z [0m[[0m[0minfo[0m] [0m[0m[31mjava.lang.RuntimeException: Nonzero exit value: 2[0m[0m
2023-04-21T08:58:59.9446689Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.package$.error(package.scala:30)[0m[0m
2023-04-21T08:58:59.9447082Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:138)[0m[0m
2023-04-21T08:58:59.9447463Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:108)[0m[0m
2023-04-21T08:58:59.9447714Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.TTY$.stty(Utils.scala:103)[0m[0m
2023-04-21T08:58:59.9448002Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.TTY$.withSttyOverride(Utils.scala:114)[0m[0m
2023-04-21T08:58:59.9448285Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.terminal.Terminal$.readLine(Terminal.scala:41)[0m[0m
2023-04-21T08:58:59.9448611Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.AmmoniteFrontEnd.readLine(AmmoniteFrontEnd.scala:133)[0m[0m
2023-04-21T08:58:59.9448994Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.AmmoniteFrontEnd.action(AmmoniteFrontEnd.scala:28)[0m[0m
2023-04-21T08:58:59.9449261Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.$anonfun$action$4(Repl.scala:194)[0m[0m
2023-04-21T08:58:59.9449548Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.$anonfun$flatMap$1(Signaller.scala:45)[0m[0m
2023-04-21T08:58:59.9449820Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Signaller.apply(Signaller.scala:28)[0m[0m
2023-04-21T08:58:59.9450091Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.flatMap(Signaller.scala:45)[0m[0m
2023-04-21T08:58:59.9450362Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Scoped.flatMap$(Signaller.scala:45)[0m[0m
2023-04-21T08:58:59.9450642Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Signaller.flatMap(Signaller.scala:16)[0m[0m
2023-04-21T08:58:59.9451184Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.$anonfun$action$2(Repl.scala:178)[0m[0m
2023-04-21T08:58:59.9451519Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.util.Catching.flatMap(Res.scala:115)[0m[0m
2023-04-21T08:58:59.9451770Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.action(Repl.scala:170)[0m[0m
2023-04-21T08:58:59.9452017Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.loop$1(Repl.scala:212)[0m[0m
2023-04-21T08:58:59.9452260Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.repl.Repl.run(Repl.scala:227)[0m[0m
2023-04-21T08:58:59.9452503Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.Main.$anonfun$run$1(Main.scala:236)[0m[0m
2023-04-21T08:58:59.9452752Z [0m[[0m[0minfo[0m] [0m[0m[31m  scala.Option.getOrElse(Option.scala:189)[0m[0m
2023-04-21T08:58:59.9452981Z [0m[[0m[0minfo[0m] [0m[0m[31m  ammonite.Main.run(Main.scala:224)[0m[0m
2023-04-21T08:58:59.9453327Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:100)[0m[0m
2023-04-21T08:58:59.9453685Z [0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.application.ReplE2ESuite$$anon$1.run(ReplE2ESuite.scala:59)[0m[0m
2023-04-21T08:58:59.9454018Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[0m[0m
2023-04-21T08:58:59.9454310Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m[0m
2023-04-21T08:58:59.9454671Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m[0m
2023-04-21T08:58:59.9455071Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m[0m
2023-04-21T08:58:59.9455325Z [0m[[0m[0minfo[0m] [0m[0m[31m  java.lang.Thread.run(Thread.java:750)[0m[0m
2023-04-21T08:58:59.9455486Z [0m[[0m[0minfo[0m] [0m[0m[31m[0m[0m

```","2,-1    ",2,-1,1
1541324494,13213701,2023/4/21,v3.2.4,"I merged this branch with the most recent master branch hoping it solves the build issue. I have another build failure, now different error, IMO still unrelated to the change:

```
ThriftServerWithSparkContextInHttpSuite.(It is not a test it is a sbt.testing.SuiteSelector)
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextInHttpSuite.beforeAll(ThriftServerWithSparkContextSuite.scala:226)
org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)
org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:67)
org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
java.util.concurrent.FutureTask.run(FutureTask.java:266)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:750)

The currently active SparkContext was created at:

(No active SparkContext.)
```
","2,-1    ",2,-1,1
1511463799,1475305,2023/4/21,v3.2.4,"> Make some special sql can be parsed. Like SELECT 1 UNION SELECT 1.

cc @wangyum FYI","1,-1    ",1,-1,0
1512395349,6477701,2023/4/21,v3.2.4,cc @cloud-fan @MaxGekk and @gengliangwang FYI,"3,-1    ",3,-1,2
1512499213,3182036,2023/4/21,v3.2.4,Can you provide more background to help people understand and review? I have no idea why catching one more exception type can fix this parser issue.,"1,-1    ",1,-1,0
1512553285,32387433,2023/4/21,v3.2.4,"> Can you provide more background to help people understand and review? I have no idea why catching one more exception type can fix this parser issue.

Thanks for point that. I updated the description.","1,-1    ",1,-1,0
1513061680,32387433,2023/4/21,v3.2.4,Replaced by https://github.com/apache/spark/pull/40835,"1,-1    ",1,-1,0
1512437973,3182036,2023/4/21,v3.2.4,Do we have a design doc? This is a big feature.,"1,-1    ",1,-1,0
1514220395,8326978,2023/4/21,v3.2.4,"> Do we have a design doc? This is a big feature.

+1","1,-1    ",1,-1,0
1511808088,1938382,2023/4/22,v3.2.4,@cloud-fan @hvanhovell ,"1,-1    ",1,-1,0
1512024390,1938382,2023/4/22,v3.2.4,"```
[error] spark-catalyst: Failed binary compatibility check against org.apache.spark:spark-catalyst_2.12:3.3.0! Found 2 potential problems (filtered 1415)
[error]  * static method canWrite(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,Boolean,scala.Function2,java.lang.String,scala.Enumeration#Value,scala.Function1)Boolean in class org.apache.spark.sql.types.DataType does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.sql.types.DataType.canWrite"")
[error]  * method canWrite(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,Boolean,scala.Function2,java.lang.String,scala.Enumeration#Value,scala.Function1)Boolean in object org.apache.spark.sql.types.DataType does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](""org.apache.spark.sql.types.DataType.canWrite"")
```

Failing because of this.

In fact this method should be marked as private sql method....","1,-1    ",1,-1,0
1512024601,1938382,2023/4/22,v3.2.4,Could we change this method as private sql?,"2,-3    ",2,-3,-1
1512273513,9616802,2023/4/22,v3.2.4,@amaliujia IMO it is fine to move this and break compatibility a bit. Let's just update the MiMa checks and move on.,"2,-3    ",2,-3,-1
1512343082,1938382,2023/4/22,v3.2.4,I updated the MIMA check.,"1,-1    ",1,-1,0
1514157595,3182036,2023/4/22,v3.2.4,"thanks, merging to master!","1,-2    ",1,-2,-1
1511902838,1938382,2023/4/22,v3.2.4,@hvanhovell @cloud-fan ,"2,-3    ",2,-3,-1
1512339258,9616802,2023/4/22,v3.2.4,Merging.,"2,-3    ",2,-3,-1
1525985365,1580697,2023/4/22,v3.2.4,"@grundprinzip @hvanhovell @zhengruifeng @HyukjinKwon Could you review this PR, please.","1,-1    ",1,-1,0
1528006747,3421,2023/4/22,v3.2.4,Are the Python changes done in a follow up?,"1,-1    ",1,-1,0
1529698798,1580697,2023/4/22,v3.2.4,"> Are the Python changes done in a follow up?

Yep, in a separate PR.
","1,-1    ",1,-1,0
1530726252,6477701,2023/4/22,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1512338872,7322292,2023/4/23,v3.2.4,"LGTM, merged to master","1,-1    ",1,-1,0
1516031342,7322292,2023/4/23,v3.2.4,"It seems that there will be behavior difference:
```
In [3]: spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)

In [4]: spark.sql(""values (1, struct(1 as a, 2 as a, 3 as b)) as t(x, y)"").toPandas()
Out[4]: 
   x          y
0  1  (1, 2, 3)
```","1,-1    ",1,-1,0
1516699784,506656,2023/4/23,v3.2.4,"That's an existing behavior difference as we discussed before at https://github.com/apache/spark/pull/40692#issuecomment-1499840711.
This PR targets to save the case where there are duplicated field names with Arrow enabled.","3,-1    ",3,-1,2
1535196428,506656,2023/4/23,v3.2.4,Close this in favor of #40988.,"1,-1    ",1,-1,0
1513208955,1475305,2023/4/23,v3.2.4,cc @dongjoon-hyun @HyukjinKwon FYI,"1,-1    ",1,-1,0
1513951127,6477701,2023/4/23,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1514032541,1475305,2023/4/23,v3.2.4,Thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1512422982,26535726,2023/4/23,v3.2.4,"`SPARK_USER_NAME` is introduced in SPARK-26015(https://github.com/apache/spark/pull/23017), and I guess supporting dynamic user name is one of the author initial intention","2,-1    ",2,-1,1
1512423177,26535726,2023/4/23,v3.2.4,cc @Yikun,"1,-1    ",1,-1,0
1512541064,26535726,2023/4/23,v3.2.4,"@Yikun I suppose it's a K8s-only feature.

As mentioned in the PR description, the main purpose is to reduce the gap between Spark on YARN and K8s, to allow users seamlessly migrate Spark jobs from YARN to K8s.

I don't have much knowledge about docker/container technology, and I agree w/ you it looks not easy to dynamically switch user based on the Docker Official Image rule","1,-1    ",1,-1,0
1512824480,26535726,2023/4/23,v3.2.4,Also cc @yaooqinn ,"1,-1    ",1,-1,0
1515680101,1736354,2023/4/23,v3.2.4,"Just for others reviewer infomation, I also wanna share some considerations about this (also include some idea in offline discussion with @pan3793 ):
1. (-0.5) As per docker official recommendation about [USER](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user), we should use `groupadd` and `useradd` to address, rather than change `/etc/passwd` directly. If we specify the USER (useradd/groupadd) in Dockerfile in future, this change will be ignored.
2. (-0.5) In theory, application users should be decoupled from container users. Such as, spark docker image should use static user `spark` (just like we done in spark-docker), and other application respect the `spark` user, or donât depends on the container user.
3. (+0.5) As per https://github.com/apache/spark/pull/23017 original design, it was intend to switch user name dynamically.
4. (+0.5) Consider the Spark case, there are many users want to migrate YARN to K8s easily, support user dynamic switch is a reasonable case.
5. (+0.5) It's a K8s only feature, not for Docker image, so 1 / 2 could be balanced in some level.

So, I am +0.5 on this PR. : )","1,-1    ",1,-1,0
1526441574,6845676,2023/4/23,v3.2.4,@pan3793 are we good to merge?,"1,-1    ",1,-1,0
1527304010,26535726,2023/4/23,v3.2.4,"> Do you think you can add an integration test case in order to prevent a future regression, @pan3793 ?

Sure, IT is added.","1,-1    ",1,-1,0
1533967009,26535726,2023/4/23,v3.2.4,@Yikun @dongjoon-hyun would you please take a look again?,"1,-1    ",1,-1,0
1512460448,26535726,2023/4/23,v3.2.4,cc @HyukjinKwon ,"2,-1    ",2,-1,1
1512515595,6477701,2023/4/23,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1514120599,8486025,2023/4/23,v3.2.4,@cloud-fan The failed GA is unrelated to this PR.,"1,-1    ",1,-1,0
1514809153,3182036,2023/4/23,v3.2.4,"thanks, merging to master!","2,-1    ",2,-1,1
1515638428,8486025,2023/4/24,v3.2.4,@cloud-fan Thank you!,"1,-1    ",1,-1,0
1513966878,109614351,2023/4/24,v3.2.4,"@rangadi a general question: when I use `dev/connect-gen-protos.sh` to generate protobuf related changes, it automatically added a lot of lines of `@typing_extensions.final` to different files, should I remove this in my PR?","2,-2    ",2,-2,0
1513995464,502522,2023/4/24,v3.2.4,"> when I use dev/connect-gen-protos.sh to generate protobuf related changes, it automatically added a lot of lines of @typing_extensions.final to different files, should I remove this in my PR?

No need. That is fine. It is a known problem. ","2,-1    ",2,-1,1
1518440451,6477701,2023/4/24,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1518440835,6477701,2023/4/24,v3.2.4,@bogao007 what's your JIRA id? I need to assign you in the JIRA ticket.,"1,-1    ",1,-1,0
1518455976,109614351,2023/4/24,v3.2.4,"> @bogao007 what's your JIRA id? I need to assign you in the JIRA ticket.

I think this might be my JIRA id `62cbecffa94a6f9c0efe1622`, let me know if it doesn't work.","1,-1    ",1,-1,0
1514330524,26535726,2023/4/24,v3.2.4,"CI passed w/ small changes in UT, kindly ping @cloud-fan ","1,-1    ",1,-1,0
1514348702,3182036,2023/4/24,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1513069449,6477701,2023/4/24,v3.2.4,"All related tests passed.

Merged to master.","1,-1    ",1,-1,0
1513951665,6477701,2023/4/24,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1514032309,1475305,2023/4/24,v3.2.4,Thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1517183669,5399861,2023/4/24,v3.2.4,cc @yaooqinn @cloud-fan ,"1,-1    ",1,-1,0
1518438939,5399861,2023/4/24,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1514504924,6477701,2023/4/24,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1513924482,49699333,2023/4/24,v3.2.4,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.","1,-1    ",1,-1,0
1514182487,4690923,2023/4/24,v3.2.4,@mridulm @tgravescs @zhouyejoe @akpatnam25 @shuwang21 Please help review,"1,-1    ",1,-1,0
1514901193,1591700,2023/4/24,v3.2.4,+CC @attilapiros ,"1,-1    ",1,-1,0
1514983363,4563792,2023/4/24,v3.2.4,"so the intention here is it still uses external shuffle service but doesn't store the secret so if node manager goes down it can't recover that, correct?
  
Personally I'm fine with adding this option, you also need to add documentation for this new configuration in .md file with explanation why you would want to disable it.  I'm wondering if we want something in the security doc about it.","2,-1    ",2,-1,1
1515102595,4690923,2023/4/24,v3.2.4,"> so the intention here is it still uses external shuffle service but doesn't store the secret so if node manager goes down it can't recover that, correct?

Yes. Since, the secret is not encrypted, it is a security violation for some applications. If the NM goes down, the metadata of these apps will not be recovered but that is acceptable. 

> you also need to add documentation for this new configuration in .md file with explanation why you would want to disable it. I'm wondering if we want something in the security doc about it.

I updated the `configuration.md` and `security.md` with the config. Let me know if I should add more explanation in the security doc.","1,-1    ",1,-1,0
1515895607,1591700,2023/4/24,v3.2.4,"Exactly that usecase @tgravescs.
We have usecases where applications require heightened security, and are fine with the recomputation cost due to lost shuffle data.","1,-1    ",1,-1,0
1517239197,2017933,2023/4/24,v3.2.4,"As we can run multiple external shuffle services on the same NM what about introducing just a new config for the whole external shuffle service controlling whether recovery is enabled.

Then those applications with higher security can use the external shuffle service where the recovery is disabled.

I think regarding code complexity and code size it would be a much better solution. WDYT?","1,-1    ",1,-1,0
1517295269,4690923,2023/4/24,v3.2.4,"> As we can run multiple external shuffle services on the same NM what about introducing just a new config for the whole external shuffle service controlling whether recovery is enabled.
> 
> Then those applications with higher security can use the external shuffle service where the recovery is disabled.

It can be done but setting up and running  another shuffle service requires much more effort at least in our production environment. 

> I think regarding code complexity and code size it would be a much better solution. WDYT?

The code change here is quite small. Agreed that disabling recovery for the whole ESS may need much smaller change but there is not much complexity with this change as well.
","1,-1    ",1,-1,0
1517841735,4563792,2023/4/24,v3.2.4,lgtm,"1,-1    ",1,-1,0
1518125522,1591700,2023/4/24,v3.2.4,"Thanks for fixing this @otterc !
Thanks for the reviews @tgravescs, @zhouyejoe :-)","1,-1    ",1,-1,0
1514390592,1475305,2023/4/24,v3.2.4,cc @HyukjinKwon FYI,"1,-1    ",1,-1,0
1537539310,9700541,2023/4/24,v3.2.4,"Merged to master for Apache Spark 3.5.0.
Thank you, @panbingkun , @LuciferYang , @Hisoka-X .","1,-1    ",1,-1,0
1537552436,15246973,2023/4/24,v3.2.4,"> To @panbingkun , if you don't mind, please don't expose your internal fork info in Apache Spark repository. For example, the PR description will be a commit log in Apache Spark repository, so we had better remove those information from your PR description like your company's Spark 4-digit version name and company-name postfix.
> 
> 
> 
> FYI, for this PR, I clean them up.

 OK,Thanks.","1,-1    ",1,-1,0
1514474772,1317309,2023/4/24,v3.2.4,"cc. @HyukjinKwon 
Could you please help looking into this, especially the point of backward compatibility on PySpark side? I don't see the way to do that in PySpark but I'm not an expert on Python so I might miss something. Thanks in advance!

cc. @zsxwing @viirya @rangadi @jerrypeng ","1,-1    ",1,-1,0
1515921604,1317309,2023/4/24,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1516024691,7322292,2023/4/24,v3.2.4,merged to master,"1,-1    ",1,-1,0
1516183143,1475305,2023/4/24,v3.2.4,Thanks @zhengruifeng @HyukjinKwon ,"2,-1    ",2,-1,1
1515550602,6477701,2023/4/24,v3.2.4,cc @sunchao ,"2,-1    ",2,-1,1
1515821660,26535726,2023/4/24,v3.2.4,"~~Does it mean we drop support for building against vanilla Hadoop3 client?~~

https://github.com/apache/spark/blob/09a43531d30346bb7c8d213822513dc35c70f82e/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L118-L124

Update: leave it, https://github.com/apache/spark/pull/33160 didn't get in, Spark does not support for building against vanilla Hadoop3 client","2,-1    ",2,-1,1
1515842174,1475305,2023/4/24,v3.2.4,"> ~Does it mean we drop support for building against vanilla Hadoop3 client?~
> 
> https://github.com/apache/spark/blob/09a43531d30346bb7c8d213822513dc35c70f82e/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L118-L124
> 
> Update: leave it, #33160 didn't get in, Spark does not support for building against vanilla Hadoop3 client

friendly ping @sunchao ","1,-1    ",1,-1,0
1516617489,506679,2023/4/24,v3.2.4,Some related JIRA: https://issues.apache.org/jira/browse/SPARK-37994,"1,-1    ",1,-1,0
1516649415,26535726,2023/4/24,v3.2.4,"@sunchao so the current supported Hadoop version is 3.2.2+ and 3.3.1+? there is some code for Hadoop 3.0 and 3.1, should we remove it then?","1,-1    ",1,-1,0
1516662806,506679,2023/4/24,v3.2.4,"yea, the shaded Hadoop client only work for Hadoop 3.2.2+ and 3.3.1+. I'm not sure if there're people that still use Hadoop 3.0/3.1 with Spark though.

I'm not aware of any code in Spark that specifically depend on Hadoop 3.0/3.1. Could you point to them for me?","1,-1    ",1,-1,0
1516692916,26535726,2023/4/24,v3.2.4,"> I'm not aware of any code in Spark that specifically depend on Hadoop 3.0/3.1. Could you point to them for me?

@sunchao I find two examples

- SPARK-13704 added a workaround for [YARN-9332](https://issues.apache.org/jira/browse/YARN-9332) (fixed in Hadoop 3.2.1/3.3.0)
- SPARK-32256 added a workaround for [HADOOP-14067](https://issues.apache.org/jira/browse/HADOOP-14067) (fixed in Hadoop 3.1.0)","1,-1    ",1,-1,0
1516693819,6570401,2023/4/24,v3.2.4,"No strong opinion on this, but we should make it clear that this PR is explicitly dropping support for Hadoop 3.0/3.1 and earlier versions of 3.2

cc @mridulm ","1,-1    ",1,-1,0
1517170281,1475305,2023/4/24,v3.2.4,"@xkrogen @sunchao @pan3793 I would like to clarify, actually, no longer using Hadoop 3.0/3.1 support for build ant test is not the original intention of this PR.

So if there is an way to build and test Hadoop 3.0/3.1 successfully before this pr, but it loses after this pr,  I think we should stop this work because Apache Spark has not previously stated on any occasion that it no longer supports Hadoop 3.0/3.1, right ?


@xkrogen @sunchao @pan3793 Can you give a command that can be used for build & test with Hadoop 3.0/3.1? I want to manually check it, thanks ~
","1,-1    ",1,-1,0
1518134382,506679,2023/4/24,v3.2.4,"> So if there is an way to build and test Hadoop 3.0/3.1 successfully before this pr, but it loses after this pr, I think we should stop this work because Apache Spark has not previously stated on any occasion that it no longer supports Hadoop 3.0/3.1, right ?

Yes, I think that's probably a sensible thing to do. 

> @xkrogen @sunchao @pan3793 Can you give a command that can be used for build & test with Hadoop 3.0/3.1? I want to manually check it, thanks ~

You can check this JIRA for the command to build: https://issues.apache.org/jira/browse/SPARK-37994","1,-3    ",1,-3,-2
1518931685,1475305,2023/4/24,v3.2.4,"> > So if there is an way to build and test Hadoop 3.0/3.1 successfully before this pr, but it loses after this pr, I think we should stop this work because Apache Spark has not previously stated on any occasion that it no longer supports Hadoop 3.0/3.1, right ?
> 
> Yes, I think that's probably a sensible thing to do.
> 
> > @xkrogen @sunchao @pan3793 Can you give a command that can be used for build & test with Hadoop 3.0/3.1? I want to manually check it, thanks ~
> 
> You can check this JIRA for the command to build: https://issues.apache.org/jira/browse/SPARK-37994

I encountered the following error while compiling `hadoop-cloud` module during build with hadoop 3.1.x:

```
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:34: value hasPathCapability is not a member of org.apache.hadoop.fs.FileContext
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:34: not found: value CommonPathCapabilities
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:52: value abort is not a member of org.apache.hadoop.fs.FSDataOutputStream
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:66: value abort is not a member of org.apache.hadoop.fs.FSDataOutputStream
```

Due to the fixed version of HADOOP-15691 being `
3.3.0, 3.2.2, 3.2.3` and the fixed version of HADOOP-16906 being `3.3.1`, so it is definitely not possible to build hadoop-cloud` module using Hadoop 3.1. x. I would like to remove this module to continue my experiment

","1,-1    ",1,-1,0
1519102114,1475305,2023/4/24,v3.2.4,convert to draft to avoid accidental merging,"1,-1    ",1,-1,0
1520218583,1475305,2023/4/24,v3.2.4,"@xkrogen @sunchao @pan3793 Synchronize my experimental results

1. Before building, we need to add the following content to `resource-managers/yarn/pom.xml` refer to https://github.com/apache/spark/pull/33160/files: 

```
    <profile>
      <id>no-shaded-hadoop-client</id>
      <dependencies>
        <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-yarn-api</artifactId>
        </dependency>
        <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-yarn-common</artifactId>
        </dependency>
        <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-yarn-server-web-proxy</artifactId>
        </dependency>
        <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-yarn-client</artifactId>
        </dependency>
        <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
          <scope>test</scope>
        </dependency>
        <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-yarn-server-tests</artifactId>
          <classifier>tests</classifier>
          <scope>test</scope>
        </dependency>
      </dependencies>
    </profile>
```

otherwise, the following compilation error will occurred with `-Pyarn`:

```
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala:30: object MiniYARNCluster is not a member of package org.apache.hadoop.yarn.server
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala:65: not found: type MiniYARNCluster
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala:111: not found: type MiniYARNCluster
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:38: object resourcemanager is not a member of package org.apache.hadoop.yarn.server
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:39: object resourcemanager is not a member of package org.apache.hadoop.yarn.server
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:40: object resourcemanager is not a member of package org.apache.hadoop.yarn.server
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:41: object resourcemanager is not a member of package org.apache.hadoop.yarn.server
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:249: not found: type RMContext
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:251: not found: type RMApp
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:260: not found: type RMApplicationHistoryWriter
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:262: not found: type SystemMetricsPublisher
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:266: not found: type RMAppManager
[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:271: not found: type ClientRMService
```

2. With the above changeï¿½?master can build success with hadoop 3.1.x as following

```
build/mvn clean install -Dhadoop.version=3.1.4 -Dhadoop-client-api.artifact=hadoop-client -Dhadoop-client-runtime.artifact=hadoop-yarn-api -Dhadoop-client-minicluster.artifact=hadoop-client -DskipTests -Pno-shaded-hadoop-client -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive

```

Otherwise, cannot build `yarn` module with hadoop 3.1.x.


3. `hadoop-cloud` can't build with hadoop 3.1.x due to https://github.com/apache/spark/pull/40847#issuecomment-1518931685


**Overall, the current master cannot compile `yarn` and `hadoop-cloud` modules using hadoop 3.1.x without any changes, and all other modules are ok**

","1,-1    ",1,-1,0
1520363712,1475305,2023/4/24,v3.2.4,"More
1. The conclusion using hadoop 3.0.x and hadoop 3.1.x is the same
2. Use hadoop 3.2.x can't build `hadoop-cloud` module too
```
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:34: value ABORTABLE_STREAM is not a member of object org.apache.hadoop.fs.CommonPathCapabilities
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:52: value abort is not a member of org.apache.hadoop.fs.FSDataOutputStream
[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:66: value abort is not a member of org.apache.hadoop.fs.FSDataOutputStream
[ERROR] three errors found
```

3. Currently, only hadoop 3.3.x can build all modules","3,-3    ",3,-3,0
1520953123,506679,2023/4/24,v3.2.4,"Interesting, thanks for the detailed analysis @LuciferYang !

> Use hadoop 3.2.x can't build hadoop-cloud module too

This is Hadoop 3.2.2 ? I remember at some point we started to enable `hadoop-cloud` in Spark release, so I wonder why this didn't cause any error back in the time ..","3,-1    ",3,-1,2
1521071604,1475305,2023/4/24,v3.2.4,"> Interesting, thanks for the detailed analysis @LuciferYang !
> 
> > Use hadoop 3.2.x can't build hadoop-cloud module too
> 
> This is Hadoop 3.2.2 ? I remember at some point we started to enable `hadoop-cloud` in Spark release, so I wonder why this didn't cause any error back in the time ..

I test with hadoop 3.2.4. `AbortableStreamBasedCheckpointFileManager` was introduced in SPARK-40039, and it uses APIs that are only available in Hadoop 3.3.1+([HADOOP-16906](https://issues.apache.org/jira/browse/HADOOP-16906)  `FSDataOutputStream#abort()`)

","1,-1    ",1,-1,0
1536434076,822522,2023/4/24,v3.2.4,Just to be clear are we saying this is OK to merge or there are issues with hadoop-cloud?,"1,-1    ",1,-1,0
1536447477,26535726,2023/4/24,v3.2.4,"> No strong opinion on this, but we should make it clear that this PR is explicitly dropping support for Hadoop 3.0/3.1 and earlier versions of 3.2

@srowen I'm also +1 that we should document clearly the Hadoop client version support strategy

","1,-2    ",1,-2,-1
1536531665,822522,2023/4/24,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1514575012,26535726,2023/4/24,v3.2.4,cc @sunchao @LuciferYang ,"1,-1    ",1,-1,0
1515104782,506679,2023/4/24,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1558856186,1475305,2023/4/24,v3.2.4,"@pan3793 I found an interesting thing, after this one merged, when I run the following commands:

```
build/mvn clean install -DskipTests
build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite
```

there are 4 test failed as following:

```
- read and write *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:458)
  at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:257)
  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:221)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:210)
  ...
- textFile *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)
  at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)
  at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2688)
  at org.apache.spark.sql.Dataset.withResult(Dataset.scala:3128)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2687)
  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$12(ClientE2ETestSuite.scala:169)
  at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
  ...
- write table *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
  at scala.collection.Iterator.toStream(Iterator.scala:1417)
  at scala.collection.Iterator.toStream$(Iterator.scala:1416)
  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)
  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)
  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)
  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:471)
  ...
- write without table or path *** FAILED ***
  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
  at io.grpc.Status.asRuntimeException(Status.java:535)
  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)
  at scala.collection.Iterator.toStream(Iterator.scala:1417)
  at scala.collection.Iterator.toStream$(Iterator.scala:1416)
  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)
  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)
  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)
  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:471)
  ...
```

but  if I revert this one, the the failure will disappear. `CatalogSuite` and `StreamingQuerySuite` also has some problems.Already created [SPARK-43647](https://issues.apache.org/jira/browse/SPARK-43647) to tracking this, Do you have time to investigate togethe?

 ","1,-2    ",1,-2,-1
1558953872,26535726,2023/4/24,v3.2.4,"Tests pass if enable `-Phive`

```
build/mvn clean install -DskipTests -Phive
build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite
```

Seems reasonable","1,-1    ",1,-1,0
1559065577,1475305,2023/4/24,v3.2.4,"> Tests pass if enable `-Phive`
> 
> ```
> build/mvn clean install -DskipTests -Phive
> build/mvn test -pl connector/connect/client/jvm -Dtest=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite
> ```
> 
> Seems reasonable

No, these test can passed without -Phive without this pr","1,-2    ",1,-2,-1
1559067577,1475305,2023/4/24,v3.2.4,"The error stack in server side as follows:

```
java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232)
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:46)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)
	at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)
	at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)
	at scala.collection.TraversableLike.filter(TraversableLike.scala:395)
	at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)
	at scala.collection.AbstractTraversable.filter(Traversable.scala:108)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)
	at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2321)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2091)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handleCommand(SparkConnectStreamHandler.scala:120)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:86)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)
	at org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)
	at org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)
	at org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)
	at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)
	at org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)
	at org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)
	at org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)
	at java.lang.Class.getConstructor0(Class.java:3075)
	at java.lang.Class.newInstance(Class.java:412)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
	... 41 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 46 more
```","1,-2    ",1,-2,-1
1514889480,26535726,2023/4/24,v3.2.4,cc @sunchao @yaooqinn,"2,-2    ",2,-2,0
1515105711,506679,2023/4/24,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1514681056,26535726,2023/4/24,v3.2.4,cc @sunchao @LuciferYang ,"1,-1    ",1,-1,0
1515685347,506679,2023/4/24,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1514686552,3182036,2023/4/25,v3.2.4,cc @viirya ,"1,-1    ",1,-1,0
1515814480,68855,2023/4/25,v3.2.4,Looks good to me.,"2,-1    ",2,-1,1
1515825833,3182036,2023/4/25,v3.2.4,"thanks for the review, merging to master!","1,-1    ",1,-1,0
1514712584,26535726,2023/4/25,v3.2.4,cc @zsxwing @HeartSaVioR @LuciferYang ,"1,-1    ",1,-1,0
1515686346,26535726,2023/4/25,v3.2.4,also cc @sunchao,"1,-1    ",1,-1,0
1516724933,1000778,2023/4/25,v3.2.4,@pan3793 thanks for cleaning up the code! What does `partially` mean in the PR description?,"1,-1    ",1,-1,0
1516739135,26535726,2023/4/25,v3.2.4,"@zsxwing I mean this PR removes the code added in SPARK-19718 for pre-Hadoop 2.8, reserves the code for Hadoop 2.8+","1,-1    ",1,-1,0
1516813349,1000778,2023/4/25,v3.2.4,@pan3793 Could you update the PR description to be more accurate? Thanks!,"2,-1    ",2,-1,1
1517140847,26535726,2023/4/25,v3.2.4,"@zsxwing updated, please take a look again, thanks.","1,-1    ",1,-1,0
1517225396,6477701,2023/4/25,v3.2.4,Merged to master.,"1,-2    ",1,-2,-1
1515459219,1938382,2023/4/25,v3.2.4,"Just curious 

```
Remove this restriction from the client and let the service enforce
length limits.
```
Is the server already enforcing the length limits or you plan to have a follow-up to add that?","1,-1    ",1,-1,0
1515755055,3421,2023/4/25,v3.2.4,There is no server-side validation. The limit to 200 characters was mostly something that was done as a simple protection mechanism. We've looked into different specifications for what could be part of the user agent and the conclusion was to not have a limit at this point.,"1,-1    ",1,-1,0
1517326940,6477701,2023/4/25,v3.2.4,"Merged to master.

(since the last change is basically no-op)","1,-1    ",1,-1,0
1514742707,26535726,2023/4/25,v3.2.4,cc @sunchao @LuciferYang ,"1,-2    ",1,-2,-1
1515613854,26535726,2023/4/25,v3.2.4,"GA passed, kindly ping @sunchao","1,-1    ",1,-1,0
1515681362,506679,2023/4/25,v3.2.4,"Merged to master, thanks!
","1,-1    ",1,-1,0
1514803378,1475305,2023/4/25,v3.2.4,cc @sunchao @pan3793 ,"2,-1    ",2,-1,1
1515685830,506679,2023/4/25,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1515699525,1475305,2023/4/25,v3.2.4,Thanks @sunchao ,"1,-3    ",1,-3,-2
1515827341,7253827,2023/4/25,v3.2.4,"cc @cloud-fan, @maryannxue ","1,-1    ",1,-1,0
1516386920,3182036,2023/4/25,v3.2.4,@peter-toth can you briefly explain the idea of fixing it?,"1,-1    ",1,-1,0
1516659349,7253827,2023/4/25,v3.2.4,"> @peter-toth can you briefly explain the idea of fixing it?

I've updated the PR recently, but the main change is that the CTE accumulator map argument of `buildCTEMap()` changed from 
`mutable.HashMap.empty[Long, (CTERelationDef, Int)]`
to 
`mutable.SortedMap.empty[Long, (CTERelationDef, Int, mutable.Map[Long, Int])]`.
The new `mutable.Map[Long, Int]` part tracks where the references are pointing to from a CTE. (The old `Int` part tracks the ""count of incoming references"".)

Once we have this extended outer map we can correct the ""count of incoming references"" in `cleanCTEMap()`. We just need to iterate the CTEs in reverse order (that's why the outer map is now a `SortedMap`) and if we encounter a CTE whose ""count of incoming references"" is 0 then we decrease the referenced CTE's ""count of incoming references"".

To build the new inner map `buildCTEMap()` has a new `outerCTEId` optional argument.


","1,-2    ",1,-2,-1
1522924425,3182036,2023/4/25,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1522932018,7253827,2023/4/25,v3.2.4,Thanks @cloud-fan!,"1,-2    ",1,-2,-1
1515169251,26535726,2023/4/25,v3.2.4,cc @sunchao ,"1,-2    ",1,-2,-1
1515684865,506679,2023/4/25,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1515270162,68855,2023/4/25,v3.2.4,The PR title is broken.,"1,-1    ",1,-1,0
1515312261,11574708,2023/4/25,v3.2.4,@ueshin Can you please review? Thanks,"1,-1    ",1,-1,0
1515339665,506656,2023/4/25,v3.2.4,Thanks! merging to master.,"1,-1    ",1,-1,0
1515686494,26535726,2023/4/25,v3.2.4,cc @sunchao ,"1,-1    ",1,-1,0
1516560547,26535726,2023/4/25,v3.2.4,also cc @tgravescs ,"1,-1    ",1,-1,0
1516564671,1475305,2023/4/25,v3.2.4,@pan3793 jira id should be [SPARK-43202](https://issues.apache.org/jira/browse/SPARK-43202),"1,-1    ",1,-1,0
1516567176,26535726,2023/4/25,v3.2.4,"> @pan3793 jira id should be [SPARK-43202](https://issues.apache.org/jira/browse/SPARK-43202)

fixed","1,-1    ",1,-1,0
1516876643,506679,2023/4/25,v3.2.4,"Merged to master, thanks all!","1,-1    ",1,-1,0
1530118252,10248890,2023/4/25,v3.2.4,PTAL! @amaliujia @HyukjinKwon Thank you!,"1,-1    ",1,-1,0
1530967756,10248890,2023/4/25,v3.2.4,"Also verified null query names:
```
>>> q = spark.readStream.format(""rate"").load().writeStream.format(""console"").start()
>>> q1 = spark.streams.get(q.id)
>>> q1.name
''
>>> q.name
''
>>> q.name == q1.name
True
```","1,-1    ",1,-1,0
1532065813,47337188,2023/4/25,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1515541943,6477701,2023/4/25,v3.2.4,Thanks for the PR. Let's wait and see.,"1,-1    ",1,-1,0
1515542791,1475305,2023/4/25,v3.2.4,"> Thanks for the PR. Let's wait and see.

ok","1,-1    ",1,-1,0
1515703346,1475305,2023/4/25,v3.2.4,"```
[info] *** 4 TESTS FAILED ***
[error] Failed tests:
[error] 	org.apache.spark.sql.UserDefinedFunctionE2ETestSuite
[error] (connect-client-jvm / Test / test) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 114 s (01:54), completed Apr 20, 2023 4:32:20 AM
```

https://github.com/LuciferYang/spark/actions/runs/4749019479/jobs/8438173582

Rerunning these four cases still failed ....","1,-3    ",1,-3,-2
1515703854,6477701,2023/4/25,v3.2.4,Hmmmm .... ,"1,-1    ",1,-1,0
1515708806,1475305,2023/4/25,v3.2.4,"@HyukjinKwon It seems inevitable to fail, not random... 

local run `build/sbt ""connect-client-jvm/test` 3 times, all four cases have failed.

I'll go have lunch first and  investigate in the afternoon ...

","1,-1    ",1,-1,0
1515746425,6477701,2023/4/25,v3.2.4,Seems like it was because of https://github.com/apache/spark/commit/09a43531d30346bb7c8d213822513dc35c70f82e. Let's close this :-). Thanks for your effort though.,"1,-1    ",1,-1,0
1515752319,1475305,2023/4/25,v3.2.4,"@HyukjinKwon Do you know the maximum memory that GA instances can use? 

I checked the logs and found that with the same memory configuration, logs similar to `In the last 10 seconds, 5.524 (55.8%) were spent in GC` are printed more times in GA than locally

","1,-1    ",1,-1,0
1515762165,6477701,2023/4/25,v3.2.4,Its 7GB / 2cores IIRC (https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources),"1,-2    ",1,-2,-1
1515765515,1475305,2023/4/25,v3.2.4,"hmm... I suggest add more memory. I have seen new `GC overhead limit exceeded.` on GA today. If 7G is available, we can directly make the mima check use 5G

","1,-2    ",1,-2,-1
1515766154,1475305,2023/4/25,v3.2.4,Can you help me to re open this one ? @HyukjinKwon ,"1,-1    ",1,-1,0
1515767671,1475305,2023/4/25,v3.2.4,thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1515775459,6477701,2023/4/25,v3.2.4,"Actually MiMa check seems fine, and doesn;t seem failing.","1,-1    ",1,-1,0
1515776395,1475305,2023/4/25,v3.2.4,"@pan3793 I remember you provided an OOM case this morning. Can you provide a link?

","1,-1    ",1,-1,0
1515777189,26535726,2023/4/25,v3.2.4,"> @pan3793 I remember you provided an OOM case this morning. Can you provide a link?

https://github.com/pan3793/spark/actions/runs/4746374013/jobs/8431906344","1,-1    ",1,-1,0
1515778785,1475305,2023/4/25,v3.2.4,"OK, Let's open this pr for a few days. If the master has  mima check OOM, there is no need to resubmit

","1,-1    ",1,-1,0
1515938627,32387433,2023/4/25,v3.2.4,OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002,"1,-1    ",1,-1,0
1515940157,1475305,2023/4/25,v3.2.4,"> OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002

with 5g?","1,-1    ",1,-1,0
1515944003,32387433,2023/4/25,v3.2.4,"> > OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002
> 
> with 5g?

No, I didn't change anything, so it will be 4G?","1,-1    ",1,-1,0
1515955777,1475305,2023/4/25,v3.2.4,"> > > OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002
> > 
> > 
> > with 5g?
> 
> No, I didn't change anything, so it will be 4G?

4196m now, It seems that 4196m is not stable enough now

","1,-1    ",1,-1,0
1517178002,1475305,2023/4/25,v3.2.4,"Test once more

","1,-1    ",1,-1,0
1517179887,1475305,2023/4/25,v3.2.4,"From the GA task logs, it can be seen that using 5g only 1 ~ 2 `for better performance` related logs during mima check(w/o this pr, it will print 30+ times and still a possibility of `GC overhead limit exceeded`)



","1,-1    ",1,-1,0
1518928754,1475305,2023/4/25,v3.2.4,"> Its 7GB / 2cores IIRC (https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources)

After multiple verifications, I suggest change -Xmx to 5120. From the logs, it will significantly reduce the number of GCs and the amount of time consumed, and there will be no `GC overhead limit exceeded` issue again","1,-1    ",1,-1,0
1519101719,1475305,2023/4/25,v3.2.4,"@HyukjinKwon 

https://github.com/LuciferYang/spark/actions/runs/4776155966/jobs/8491000795

See `GC overhead limit exceeded` again

","1,-1    ",1,-1,0
1519595951,7322292,2023/4/25,v3.2.4,mima test OOM again https://github.com/apache/spark/actions/runs/4783257117/jobs/8503361722,"1,-1    ",1,-1,0
1519597372,7322292,2023/4/25,v3.2.4,merged to master,"1,-2    ",1,-2,-1
1519601176,1475305,2023/4/25,v3.2.4,Let's merge this one to avoid oom :),"1,-1    ",1,-1,0
1519602330,1475305,2023/4/25,v3.2.4,Thanks @zhengruifeng @HyukjinKwon @pan3793 @Hisoka-X ,"1,-1    ",1,-1,0
1515731348,7322292,2023/4/25,v3.2.4,"failure of `UserDefinedFunctionE2ETestSuite ` should be unrelated, but let me take a look","2,-1    ",2,-1,1
1515903570,7322292,2023/4/25,v3.2.4,merged to master,"2,-1    ",2,-1,1
1517079636,6477701,2023/4/25,v3.2.4,cc @haiyangsun-db FYI,"1,-1    ",1,-1,0
1518288506,47337188,2023/4/25,v3.2.4,"After double thoughts, we'd better not touch Pandas UDF to preserve backward compatibility. Let me close the PR and have a new prototype.","1,-1    ",1,-1,0
1516131796,32387433,2023/4/25,v3.2.4,cc @cloud-fan ,"2,-1    ",2,-1,1
1516248434,3182036,2023/4/25,v3.2.4,also cc @jchen5 @allisonwang-db ,"2,-1    ",2,-1,1
1517170830,3182036,2023/4/25,v3.2.4,"> I think your fix will fix the COUNT(*) is null case, but break the false case

Hi @jchen5 , can you help add a test for it in a new PR? The current PR passes all tests which means our test coverage is not good.","1,-1    ",1,-1,0
1517200147,32387433,2023/4/25,v3.2.4,"> > I think your fix will fix the COUNT(*) is null case, but break the false case
> 
> Hi @jchen5 , can you help add a test for it in a new PR? The current PR passes all tests which means our test coverage is not good.

If you don't mind, I can also add the required test cases to my PR. However, after I merged #40811, it seems that the current use case has been able to prove the correctness of this PR.ð","2,-1    ",2,-1,1
1517492697,3182036,2023/4/25,v3.2.4,@Hisoka-X do you mean the bug is already fixed now?,"1,-1    ",1,-1,0
1517521661,32387433,2023/4/25,v3.2.4,"> @Hisoka-X do you mean the bug is already fixed now?

Depends on what is correct results of `select *, (select any_value(false) as result from t1 where t0.a = t1.c) from t0)` ?

> if the subquery is something like select false from ... group by c then it will still actually return null on empty inputs.

The current state is as described by @jchen5  . `select false from ... group by c` will return null on empty inputs, not return `false`. I think this is what @jchen5 worry about. You can check by https://github.com/apache/spark/pull/40865#discussion_r1173210161 .

If you think `select false from ... group by c` in subquery should return `false` on empty inputs, this PR have bug now. If you think it should return `null` on empty inputs, this PR are correct.

","1,-1    ",1,-1,0
1517538486,3182036,2023/4/25,v3.2.4,"Before we make a decision, let's try the query with postgres, oracle, db2, etc.","1,-1    ",1,-1,0
1517543820,32387433,2023/4/25,v3.2.4,"> Before we make a decision, let's try the query with postgres, oracle, db2, etc.

Ok. By the way, I check with postgres, it return `null`
![image](https://user-images.githubusercontent.com/32387433/233600204-e254ca5c-daec-43cb-9490-0bf406d5d844.png)
","1,-1    ",1,-1,0
1517695970,790409,2023/4/25,v3.2.4,"> Depends on what is correct results of select *, (select any_value(false) as result from t1 where t0.a = t1.c) from t0) ?

Yes, this should return null on empty data.

I will try to find another test case to check the potential issue I mentioned. Maybe something like `select *, (select false as result from t1 where t0.a = t1.c limit 1) from t0)`(correct answer for that is also null). (This specific case isn't currently supported though)","1,-1    ",1,-1,0
1518139762,790409,2023/4/25,v3.2.4,"I checked the case of `any_value(false)` in a debugger and it works because resultWithZeroTups is NULL there, so that explains why it works - because there's an aggregation value around false, and not just constant false.

This fix seems to work with the cases I've thought of (results match SQL standard semantics and postgres).

I think this PR is mergeable. I'll plan to add some more test coverage for related cases in a future PR.","1,-1    ",1,-1,0
1518501790,32387433,2023/4/25,v3.2.4,kindly ping @cloud-fan . All CI passed.,"1,-1    ",1,-1,0
1523077396,3182036,2023/4/26,v3.2.4,"@Hisoka-X I think it's clearer to use my example to explain the bug. `NullPropagation` triggers the bug for `(count(1)) is null`, but the root cause is the wrong handling of literals when dealing with the count bug. The bug can be triggered when the scalar subquery has a global aggregate with a constant, which can be produced by `NullPropagation` or by the user query directly.","1,-1    ",1,-1,0
1525125052,3182036,2023/4/26,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1525132518,3182036,2023/4/26,v3.2.4,"It's better to have this fix in 3.4, @Hisoka-X can you open a new PR for 3.4 to backport https://github.com/apache/spark/pull/40946 and this PR together?","1,-1    ",1,-1,0
1525570134,32387433,2023/4/26,v3.2.4,"> It's better to have this fix in 3.4, @Hisoka-X can you open a new PR for 3.4 to backport #40946 and this PR together?

Please check #40977 ","1,-2    ",1,-2,-1
1519482839,7322292,2023/4/26,v3.2.4,the python linter failed,"1,-1    ",1,-1,0
1519852696,7322292,2023/4/26,v3.2.4,merged to master,"2,-1    ",2,-1,1
1515740675,1475305,2023/4/26,v3.2.4,Good catch,"1,-1    ",1,-1,0
1515741631,26535726,2023/4/26,v3.2.4,cc @HyukjinKwon @sunchao ,"1,-1    ",1,-1,0
1516654038,506679,2023/4/26,v3.2.4,"@pan3793 good catch, have you seen any issue caused by this?","2,-1    ",2,-1,1
1516654197,506679,2023/4/26,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1516682131,26535726,2023/4/26,v3.2.4,"> @pan3793 good catch, have you seen any issue caused by this?

Not exactly. Since the current Hive 2.3.9 can talk w/ HMS 2.1+ smoothly (plus [HIVE-25500](https://issues.apache.org/jira/browse/HIVE-25500), the HMS versions expand to 1.2+), it's rare to use a custom Hive client version in our production cases.","1,-1    ",1,-1,0
1517268667,7322292,2023/4/26,v3.2.4,"the failed test should be unrelated, but would you mind re-runing it for double check? ","1,-1    ",1,-1,0
1519237747,7322292,2023/4/26,v3.2.4,merged to master,"1,-1    ",1,-1,0
1519206474,6477701,2023/4/26,v3.2.4,@itholic mind rebasing this please?,"1,-2    ",1,-2,-1
1519406813,7322292,2023/4/26,v3.2.4,merged to master,"1,-1    ",1,-1,0
1515823151,26535726,2023/4/26,v3.2.4,cc @HyukjinKwon @sunchao @LuciferYang ,"1,-1    ",1,-1,0
1516886212,506679,2023/4/26,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1515952938,3182036,2023/4/26,v3.2.4,cc @wangyum ,"1,-1    ",1,-1,0
1517129845,6477701,2023/4/26,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1517130464,6477701,2023/4/26,v3.2.4,Actually ... we need a JIRA since this change is in 3.4.1 but the original change remains in 3.4.0 .. so difficult to set the fixed JRIA version.,"1,-1    ",1,-1,0
1531732218,9700541,2023/4/26,v3.2.4,"Too bad.

To @cloud-fan , as Hyukjin mentioned, we should not revert like this after the official release.

cc @sunchao , too.","1,-2    ",1,-2,-1
1534014167,3182036,2023/4/26,v3.2.4,I've created a new JIRA ticket,"1,-1    ",1,-1,0
1515987900,7322292,2023/4/26,v3.2.4,"actually, `DataFrame.offset` is missing in vanilla pyspark, I will add it in another PR. Since this fix should be also backported to 3.4","1,-1    ",1,-1,0
1517131564,6477701,2023/4/26,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1517098141,7322292,2023/4/26,v3.2.4,cc @HyukjinKwon @xinrong-meng ,"1,-1    ",1,-1,0
1517257868,7322292,2023/4/26,v3.2.4,merged to master,"1,-1    ",1,-1,0
1521283194,7322292,2023/4/26,v3.2.4,"except one run failed due to [something unrelated](https://github.com/apache/spark/runs/12992252712), in all other 10 runs the torch tests passed successfully.","1,-1    ",1,-1,0
1521284621,7322292,2023/4/26,v3.2.4,merged to master,"1,-1    ",1,-1,0
1517215926,8537877,2023/4/26,v3.2.4,GA passed after rerunning failed jobs.,"1,-1    ",1,-1,0
1517216066,8537877,2023/4/26,v3.2.4,cc @HyukjinKwon @cloud-fan ,"1,-1    ",1,-1,0
1518922728,8537877,2023/4/26,v3.2.4,cc @dongjoon-hyun ,"1,-1    ",1,-1,0
1519891829,3182036,2023/4/26,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1516562943,1475305,2023/4/26,v3.2.4,"dup with https://github.com/apache/spark/pull/40860, closed","1,-1    ",1,-1,0
1517177264,1475305,2023/4/26,v3.2.4,cc @dongjoon-hyun @HyukjinKwon ,"1,-1    ",1,-1,0
1522689975,1475305,2023/4/26,v3.2.4,friendly ping @dongjoon-hyun ,"1,-1    ",1,-1,0
1523806413,6477701,2023/4/26,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1523813028,1475305,2023/4/26,v3.2.4,Thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1516266674,47577197,2023/4/26,v3.2.4,"https://github.com/apache/spark/pull/40408 is related to this. 
CC @LuciferYang  ","1,-1    ",1,-1,0
1517673103,47577197,2023/4/26,v3.2.4,"@LuciferYang Thank you ð 
","1,-1    ",1,-1,0
1529608564,47577197,2023/4/26,v3.2.4,@HyukjinKwon will you have a look at this? ,"1,-1    ",1,-1,0
1537086687,47577197,2023/4/26,v3.2.4,"> If you don't mind, could you update the PR description with a micro-benchmark like #35679 ?

#35679 is a PR for ""There is a performance optimization ...""
This PR is only to fix dependents that did have CVE issues. 

I dont think that benchmarks for this PR is relevant. ","1,-1    ",1,-1,0
1537190557,9700541,2023/4/26,v3.2.4,"I believe we need to see that this PR doesn't cause any perf regression, @bjornjorgensen .","1,-1    ",1,-1,0
1537199659,47577197,2023/4/26,v3.2.4,"Oh.. I think you have found this https://github.com/tink-crypto/tink-java/issues/6 
He is using 1.7.0 
https://github.com/ks-yim/tink-bm/blob/960a88d69934b40dcfdd9b2aeb94028c64393a84/build.gradle.kts#LL27C11-L27C11
Witch we are also using https://github.com/apache/spark/pull/37473 ","1,-1    ",1,-1,0
1537501785,47577197,2023/4/26,v3.2.4,"Here https://github.com/bjornjorgensen/GoogleThinkBenchmark is the code that I have used to test it.
With
""com.google.crypto.tink"" % ""tink"" % ""1.6.1""
```
[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...
[info] running (fork) org.openjdk.jmh.Main 
[info] # JMH version: 1.31
[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2
[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java
[info] # VM options: <none>
[info] # Blackhole mode: full + dont-inline hint
[info] # Warmup: 5 iterations, 10 s each
[info] # Measurement: 5 iterations, 10 s each
[info] # Timeout: 10 min per iteration
[info] # Threads: 1 thread, will synchronize iterations
[info] # Benchmark mode: Average time, time/op
[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt
[info] # Run progress: 0.00% complete, ETA 00:08:20
[info] # Fork: 1 of 5
[info] # Warmup Iteration   1: 79207785.709 ns/op
[info] # Warmup Iteration   2: 75605599.346 ns/op
[info] # Warmup Iteration   3: 76214904.538 ns/op
[info] # Warmup Iteration   4: 76989327.792 ns/op
[info] # Warmup Iteration   5: 77099889.177 ns/op
[info] Iteration   1: 75762018.553 ns/op
[info] Iteration   2: 75379888.865 ns/op
[info] Iteration   3: 75498314.789 ns/op
[info] Iteration   4: 75024163.500 ns/op
[info] Iteration   5: 75680816.338 ns/op
[info] # Run progress: 20.00% complete, ETA 00:06:43
[info] # Fork: 2 of 5
[info] # Warmup Iteration   1: 80245617.368 ns/op
[info] # Warmup Iteration   2: 75652339.774 ns/op
[info] # Warmup Iteration   3: 77061164.331 ns/op
[info] # Warmup Iteration   4: 76287806.765 ns/op
[info] # Warmup Iteration   5: 75317346.985 ns/op
[info] Iteration   1: 75686301.880 ns/op
[info] Iteration   2: 75330525.474 ns/op
[info] Iteration   3: 75734112.594 ns/op
[info] Iteration   4: 76288954.348 ns/op
[info] Iteration   5: 76606136.962 ns/op
[info] # Run progress: 40.00% complete, ETA 00:05:02
[info] # Fork: 3 of 5
[info] # Warmup Iteration   1: 80973643.427 ns/op
[info] # Warmup Iteration   2: 77317109.354 ns/op
[info] # Warmup Iteration   3: 76322130.439 ns/op
[info] # Warmup Iteration   4: 76301836.621 ns/op
[info] # Warmup Iteration   5: 77046937.308 ns/op
[info] Iteration   1: 76474108.687 ns/op
[info] Iteration   2: 77131903.554 ns/op
[info] Iteration   3: 76728218.573 ns/op
[info] Iteration   4: 77324718.069 ns/op
[info] Iteration   5: 76930795.244 ns/op
[info] # Run progress: 60.00% complete, ETA 00:03:21
[info] # Fork: 4 of 5
[info] # Warmup Iteration   1: 80497824.184 ns/op
[info] # Warmup Iteration   2: 76670359.000 ns/op
[info] # Warmup Iteration   3: 77410211.954 ns/op
[info] # Warmup Iteration   4: 76880883.221 ns/op
[info] # Warmup Iteration   5: 76903019.008 ns/op
[info] Iteration   1: 76878110.405 ns/op
[info] Iteration   2: 77073021.462 ns/op
[info] Iteration   3: 76818391.687 ns/op
[info] Iteration   4: 76985174.577 ns/op
[info] Iteration   5: 76645837.389 ns/op
[info] # Run progress: 80.00% complete, ETA 00:01:40
[info] # Fork: 5 of 5
[info] # Warmup Iteration   1: 80435216.312 ns/op
[info] # Warmup Iteration   2: 76728526.878 ns/op
[info] # Warmup Iteration   3: 76168514.106 ns/op
[info] # Warmup Iteration   4: 75383839.180 ns/op
[info] # Warmup Iteration   5: 75929712.985 ns/op
[info] Iteration   1: 76427345.038 ns/op
[info] Iteration   2: 76718093.679 ns/op
[info] Iteration   3: 76872120.244 ns/op
[info] Iteration   4: 76085370.545 ns/op
[info] Iteration   5: 76203878.348 ns/op
[info] Result ""com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt"":
[info]   76331532.832 Â±(99.9%) 488639.588 ns/op [Average]
[info]   (min, avg, max) = (75024163.500, 76331532.832, 77324718.069), stdev = 652319.870
[info]   CI (99.9%): [75842893.244, 76820172.420] (assumes normal distribution)
[info] # Run complete. Total time: 00:08:24
[info] REMEMBER: The numbers below are just data. To gain reusable insights, you need to follow up on
[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), design factorial
[info] experiments, perform baseline and negative tests that provide experimental control, make sure
[info] the benchmarking environment is safe on JVM/OS/HW level, ask for reviews from the domain experts.
[info] Do not assume the numbers tell you what you want them to tell.
[info] Benchmark                                  Mode  Cnt         Score        Error  Units
[info] GoogleThinkBenchmark.testAesGcmJceEncrypt  avgt   25  76331532.832 Â± 488639.588  ns/op
[success] Total time: 509 s (08:29), completed May 7, 2023, 7:19:58 PM
```
With
""com.google.crypto.tink"" % ""tink"" % ""1.9.0""
```
[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...
[info] running (fork) org.openjdk.jmh.Main 
[info] # JMH version: 1.31
[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2
[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java
[info] # VM options: <none>
[info] # Blackhole mode: full + dont-inline hint
[info] # Warmup: 5 iterations, 10 s each
[info] # Measurement: 5 iterations, 10 s each
[info] # Timeout: 10 min per iteration
[info] # Threads: 1 thread, will synchronize iterations
[info] # Benchmark mode: Average time, time/op
[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt
[info] # Run progress: 0.00% complete, ETA 00:08:20
[info] # Fork: 1 of 5
[info] # Warmup Iteration   1: 81974464.393 ns/op
[info] # Warmup Iteration   2: 78344967.031 ns/op
[info] # Warmup Iteration   3: 77870422.233 ns/op
[info] # Warmup Iteration   4: 76366142.015 ns/op
[info] # Warmup Iteration   5: 76969131.346 ns/op
[info] Iteration   1: 77587424.279 ns/op
[info] Iteration   2: 77137913.315 ns/op
[info] Iteration   3: 76307498.705 ns/op
[info] Iteration   4: 76561784.282 ns/op
[info] Iteration   5: 77806261.512 ns/op
[info] # Run progress: 20.00% complete, ETA 00:06:42
[info] # Fork: 2 of 5
[info] # Warmup Iteration   1: 80478869.944 ns/op
[info] # Warmup Iteration   2: 77009168.477 ns/op
[info] # Warmup Iteration   3: 77253208.562 ns/op
[info] # Warmup Iteration   4: 77522236.047 ns/op
[info] # Warmup Iteration   5: 77473338.262 ns/op
[info] Iteration   1: 77294234.708 ns/op
[info] Iteration   2: 77547845.969 ns/op
[info] Iteration   3: 77319105.646 ns/op
[info] Iteration   4: 77437665.792 ns/op
[info] Iteration   5: 76279051.841 ns/op
[info] # Run progress: 40.00% complete, ETA 00:05:02
[info] # Fork: 3 of 5
[info] # Warmup Iteration   1: 81151850.952 ns/op
[info] # Warmup Iteration   2: 77319736.377 ns/op
[info] # Warmup Iteration   3: 77127278.746 ns/op
[info] # Warmup Iteration   4: 77566176.907 ns/op
[info] # Warmup Iteration   5: 77195342.962 ns/op
[info] Iteration   1: 77081135.108 ns/op
[info] Iteration   2: 77275550.108 ns/op
[info] Iteration   3: 76869123.466 ns/op
[info] Iteration   4: 77654387.488 ns/op
[info] Iteration   5: 77896922.171 ns/op
[info] # Run progress: 60.00% complete, ETA 00:03:21
[info] # Fork: 4 of 5
[info] # Warmup Iteration   1: 82232457.566 ns/op
[info] # Warmup Iteration   2: 78139230.227 ns/op
[info] # Warmup Iteration   3: 78621229.438 ns/op
[info] # Warmup Iteration   4: 78189807.516 ns/op
[info] # Warmup Iteration   5: 78500250.758 ns/op
[info] Iteration   1: 78417104.656 ns/op
[info] Iteration   2: 77769043.891 ns/op
[info] Iteration   3: 78550508.242 ns/op
[info] Iteration   4: 78590966.453 ns/op
[info] Iteration   5: 78112666.589 ns/op
[info] # Run progress: 80.00% complete, ETA 00:01:40
[info] # Fork: 5 of 5
[info] # Warmup Iteration   1: 80581520.376 ns/op
[info] # Warmup Iteration   2: 77263616.777 ns/op
[info] # Warmup Iteration   3: 76836909.809 ns/op
[info] # Warmup Iteration   4: 77991064.147 ns/op
[info] # Warmup Iteration   5: 76437476.748 ns/op
[info] Iteration   1: 77256809.777 ns/op
[info] Iteration   2: 78291977.688 ns/op
[info] Iteration   3: 77834381.442 ns/op
[info] Iteration   4: 77869096.798 ns/op
[info] Iteration   5: 77068233.792 ns/op
[info] Result ""com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt"":
[info]   77512667.749 Â±(99.9%) 474041.907 ns/op [Average]
[info]   (min, avg, max) = (76279051.841, 77512667.749, 78590966.453), stdev = 632832.384
[info]   CI (99.9%): [77038625.841, 77986709.656] (assumes normal distribution)
[info] # Run complete. Total time: 00:08:23
[info] REMEMBER: The numbers below are just data. To gain reusable insights, you need to follow up on
[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), design factorial
[info] experiments, perform baseline and negative tests that provide experimental control, make sure
[info] the benchmarking environment is safe on JVM/OS/HW level, ask for reviews from the domain experts.
[info] Do not assume the numbers tell you what you want them to tell.
[info] Benchmark                                  Mode  Cnt         Score        Error  Units
[info] GoogleThinkBenchmark.testAesGcmJceEncrypt  avgt   25  77512667.749 Â± 474041.907  ns/op
[success] Total time: 510 s (08:30), completed May 7, 2023, 7:34:13 PM
```","1,-1    ",1,-1,0
1537522027,47577197,2023/4/26,v3.2.4,"Reboot and run with `sbt 'jmh:run -i 10 -wi 10 -f 1 -t 1'`
with
""com.google.crypto.tink"" % ""tink"" % ""1.9.0""
```
[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...
[info] running (fork) org.openjdk.jmh.Main -i 10 -wi 10 -f 1 -t 1
[info] # JMH version: 1.31
[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2
[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java
[info] # VM options: <none>
[info] # Blackhole mode: full + dont-inline hint
[info] # Warmup: 10 iterations, 10 s each
[info] # Measurement: 10 iterations, 10 s each
[info] # Timeout: 10 min per iteration
[info] # Threads: 1 thread, will synchronize iterations
[info] # Benchmark mode: Average time, time/op
[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt
[info] # Run progress: 0.00% complete, ETA 00:03:20
[info] # Fork: 1 of 1
[info] # Warmup Iteration   1: 83028380.132 ns/op
[info] # Warmup Iteration   2: 78802359.000 ns/op
[info] # Warmup Iteration   3: 79210527.969 ns/op
[info] # Warmup Iteration   4: 78695687.141 ns/op
[info] # Warmup Iteration   5: 78938831.701 ns/op
[info] # Warmup Iteration   6: 78575255.695 ns/op
[info] # Warmup Iteration   7: 79112203.724 ns/op
[info] # Warmup Iteration   8: 79323400.079 ns/op
[info] # Warmup Iteration   9: 78537590.867 ns/op
[info] # Warmup Iteration  10: 78413295.234 ns/op
[info] Iteration   1: 77933768.721 ns/op
[info] Iteration   2: 79192640.055 ns/op
[info] Iteration   3: 79182903.512 ns/op
[info] Iteration   4: 79111605.283 ns/op
[info] Iteration   5: 78859743.535 ns/op
[info] Iteration   6: 78318937.016 ns/op
[info] Iteration   7: 77986910.411 ns/op
[info] Iteration   8: 78648235.734 ns/op
[info] Iteration   9: 78609703.445 ns/op
[info] Iteration  10: 79423429.968 ns/op
[info] Result ""com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt"":
[info]   78726787.768 Â±(99.9%) 786981.254 ns/op [Average]
[info]   (min, avg, max) = (77933768.721, 78726787.768, 79423429.968), stdev = 520539.373
[info]   CI (99.9%): [77939806.514, 79513769.022] (assumes normal distribution)
[info] # Run complete. Total time: 00:03:21
[info] REMEMBER: The numbers below are just data. To gain reusable insights, you need to follow up on
[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), design factorial
[info] experiments, perform baseline and negative tests that provide experimental control, make sure
[info] the benchmarking environment is safe on JVM/OS/HW level, ask for reviews from the domain experts.
[info] Do not assume the numbers tell you what you want them to tell.
[info] Benchmark                                  Mode  Cnt         Score        Error  Units
[info] GoogleThinkBenchmark.testAesGcmJceEncrypt  avgt   10  78726787.768 Â± 786981.254  ns/op
```
with 
""com.google.crypto.tink"" % ""tink"" % ""1.6.1""
```
[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...
[info] running (fork) org.openjdk.jmh.Main -i 10 -wi 10 -f 1 -t 1
[info] # JMH version: 1.31
[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2
[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java
[info] # VM options: <none>
[info] # Blackhole mode: full + dont-inline hint
[info] # Warmup: 10 iterations, 10 s each
[info] # Measurement: 10 iterations, 10 s each
[info] # Timeout: 10 min per iteration
[info] # Threads: 1 thread, will synchronize iterations
[info] # Benchmark mode: Average time, time/op
[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt
[info] # Run progress: 0.00% complete, ETA 00:03:20
[info] # Fork: 1 of 1
[info] # Warmup Iteration   1: 83260488.298 ns/op
[info] # Warmup Iteration   2: 77947987.496 ns/op
[info] # Warmup Iteration   3: 78086226.109 ns/op
[info] # Warmup Iteration   4: 76767415.580 ns/op
[info] # Warmup Iteration   5: 76449646.229 ns/op
[info] # Warmup Iteration   6: 78385935.750 ns/op
[info] # Warmup Iteration   7: 77999410.016 ns/op
[info] # Warmup Iteration   8: 76971635.577 ns/op
[info] # Warmup Iteration   9: 76934072.685 ns/op
[info] # Warmup Iteration  10: 77180919.354 ns/op
[info] Iteration   1: 76979372.431 ns/op
[info] Iteration   2: 76777254.260 ns/op
[info] Iteration   3: 76748232.038 ns/op
[info] Iteration   4: 77181769.492 ns/op
[info] Iteration   5: 78160841.664 ns/op
[info] Iteration   6: 77396535.208 ns/op
[info] Iteration   7: 77519909.492 ns/op
[info] Iteration   8: 77570303.233 ns/op
[info] Iteration   9: 77661022.992 ns/op
[info] Iteration  10: 78231386.313 ns/op
[info] Result ""com.googlethinkbenchmark.GoogleThinkBenchmark.testAesGcmJceEncrypt"":
[info]   77422662.712 Â±(99.9%) 782935.090 ns/op [Average]
[info]   (min, avg, max) = (76748232.038, 77422662.712, 78231386.313), stdev = 517863.085
[info]   CI (99.9%): [76639727.623, 78205597.802] (assumes normal distribution)
[info] # Run complete. Total time: 00:03:21
[info] REMEMBER: The numbers below are just data. To gain reusable insights, you need to follow up on
[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), design factorial
[info] experiments, perform baseline and negative tests that provide experimental control, make sure
[info] the benchmarking environment is safe on JVM/OS/HW level, ask for reviews from the domain experts.
[info] Do not assume the numbers tell you what you want them to tell.
[info] Benchmark                                  Mode  Cnt         Score        Error  Units
[info] GoogleThinkBenchmark.testAesGcmJceEncrypt  avgt   10  77422662.712 Â± 782935.090  ns/op
[success] Total time: 207 s (03:27), completed May 7, 2023, 9:13:15 PM
```

Almost the same.. Think 1.9.0 is perhaps a bit slower.","1,-1    ",1,-1,0
1537529238,9700541,2023/4/26,v3.2.4,"Could you put that into the PR description in order to make a commit log, @bjornjorgensen ?","1,-1    ",1,-1,0
1537531873,47577197,2023/4/26,v3.2.4,"ok, I have added 
This have be benchmarks tested 
With
""com.google.crypto.tink"" % ""tink"" % ""1.6.1""
(min, avg, max) = (75024163.500, 76331532.832, 77324718.069), stdev = 652319.870

With
""com.google.crypto.tink"" % ""tink"" % ""1.9.0""
(min, avg, max) = (76279051.841, 77512667.749, 78590966.453), stdev = 632832.384

Almost the same.. Think 1.9.0 is perhaps a bit slower.

Is it ok or? ","1,-1    ",1,-1,0
1537534096,9700541,2023/4/26,v3.2.4,"Merged to master for Apache Spark 3.5.0.
Thank you, @bjornjorgensen and @LuciferYang .","1,-1    ",1,-1,0
1537662748,1475305,2023/4/26,v3.2.4,late LGTM ,"1,-1    ",1,-1,0
1520408175,3182036,2023/4/26,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1517133132,6477701,2023/4/26,v3.2.4,will leave it to @cloud-fan though.,"1,-1    ",1,-1,0
1517166668,3182036,2023/4/26,v3.2.4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1516627922,1475305,2023/4/26,v3.2.4,"Test first, will update pr  description later","1,-1    ",1,-1,0
1516630711,1475305,2023/4/26,v3.2.4,wait https://github.com/apache/spark/pull/40870,"1,-1    ",1,-1,0
1517158750,1475305,2023/4/26,v3.2.4,cc @sunchao @pan3793 ,"1,-1    ",1,-1,0
1517302334,6477701,2023/4/26,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1517315139,1475305,2023/4/26,v3.2.4,Thanks @HyukjinKwon @zhengruifeng @pan3793 ,"1,-1    ",1,-1,0
1517016949,1591700,2023/4/26,v3.2.4,"+CC @attilapiros who last looked into this.
I will circle back to this PR later this week - thanks for detailed jira @yorksity !
Can you please fix the PR description as well ? Thanks.","1,-1    ",1,-1,0
1517241111,38931534,2023/4/26,v3.2.4,"> +CC @attilapiros who last looked into this. I will circle back to this PR later this week - thanks for detailed jira @yorksity ! Can you please fix the PR description as well ? Thanks.

I fixed the PR description,
Next, I will try to fix the pyspark ut error and supplement the UT for this scenario
But it doesn't block the discussion of this issue
","1,-1    ",1,-1,0
1517241722,2017933,2023/4/26,v3.2.4,"This seems to me not a bug but a performance improvement, right?","1,-1    ",1,-1,0
1517242827,2017933,2023/4/26,v3.2.4,If this is a bug please try to reproduce it with a unit test which fails with old code and passes with the new one!,"1,-1    ",1,-1,0
1517245010,38931534,2023/4/26,v3.2.4,"> This seems to me not a bug but a performance improvement, right?

No, this is not an improvement, this is a bug. I introduced the reason for this bug in Jira
https://issues.apache.org/jira/browse/SPARK-43221
","1,-1    ",1,-1,0
1517246050,38931534,2023/4/26,v3.2.4,"> If this is a bug please try to reproduce it with a unit test which fails with old code and passes with the new one!

Yes, I am working on resolving this UT error and adding new UT to protect it","1,-1    ",1,-1,0
1522534258,3514644,2023/4/26,v3.2.4,"@cloud-fan How are docs done? Attach them to the same PR?
The only central place I can see to add this feature is:
https://spark.apache.org/docs/latest/sql-ref-identifier.html

Or we could go into each supported section (e.g. create table) and spell out that identifier() is supported.  ","1,-1    ",1,-1,0
1518150945,79601771,2023/4/27,v3.2.4,FYI the [tests that failed](https://github.com/ryan-johnson-databricks/spark/actions/runs/4765599580/jobs/8471553389) are broken upstream -- they also fail when I run them locally on the version of spark/master this PR is currently based on.,"1,-1    ",1,-1,0
1521356890,3182036,2023/4/27,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1517120083,1938382,2023/4/27,v3.2.4,You probably also need to generate the golden file for `ProtoToParsedPlanTestSuite`. There is instructions documented in that suite.,"1,-1    ",1,-1,0
1518384473,10248890,2023/4/27,v3.2.4,@HyukjinKwon Can you merge this when you get a chance? Thank you!,"1,-1    ",1,-1,0
1520622334,10248890,2023/4/27,v3.2.4,"@amaliujia @HyukjinKwon 
I also changed `ProtoToParsedPlanTestSuite` a little to remove the memory addresses, before the change the test for streaming table would fail with:
```
- streaming_table_API_with_options *** FAILED *** (8 milliseconds)
[info]   Expected and actual plans do not match:
[info]   
[info]   === Expected Plan ===
[info]   SubqueryAlias primary.tempdb.myStreamingTable
[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@752725d9, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@347d8e2a, tempdb.myStreamingTable
[info]   
[info]   
[info]   === Actual Plan ===
[info]   SubqueryAlias primary.tempdb.myStreamingTable
[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@a88a5db, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@2c6b362e, tempdb.myStreamingTable
```
Because the memory address (`InMemoryTable@752725d9`) is different every time it runs. I removed these in the test suite.


And verified that memory addresses doesn't exist in existing explain files:
```
wei.liu:~/oss-spark$ cat connector/connect/common/src/test/resources/query-tests/explain-results/* | grep @
wei.liu:~/oss-spark$ 
```

PTAL, thank you!","1,-1    ",1,-1,0
1521213632,7322292,2023/4/27,v3.2.4,merged to master,"1,-1    ",1,-1,0
1517301080,6477701,2023/4/27,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1517140043,13592258,2023/4/27,v3.2.4,cc @cloud-fan @viirya @RussellSpitzer ,"1,-1    ",1,-1,0
1517948030,13592258,2023/4/27,v3.2.4,Merged to branch-3.3. Thank you all for reviewing!,"1,-1    ",1,-1,0
1519285900,32387433,2023/4/27,v3.2.4,cc @cloud-fan @HyukjinKwon @MaxGekk ,"1,-1    ",1,-1,0
1525117866,3182036,2023/4/27,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1517330940,1591700,2023/4/27,v3.2.4,"Merged to master, thanks for fixing this @pan3793 !
Thanks for review @LuciferYang :-)","1,-1    ",1,-1,0
1517213684,1475305,2023/4/27,v3.2.4,"If this is not consistent with the initial idea of this ticket, please let me know

","1,-1    ",1,-1,0
1517226334,6477701,2023/4/27,v3.2.4,cc @HeartSaVioR ,"1,-1    ",1,-1,0
1517378241,109614351,2023/4/27,v3.2.4,cc @rangadi ,"1,-1    ",1,-1,0
1537695404,1475305,2023/4/27,v3.2.4,rebased and resolved conflicts,"1,-1    ",1,-1,0
1538352094,1475305,2023/4/27,v3.2.4,friendly ping @HyukjinKwon can we merge this one?,"1,-1    ",1,-1,0
1547265202,1317309,2023/4/27,v3.2.4,"I'll leave this to you for self-merging, so that you can test your new permission. Congrats again :)","1,-1    ",1,-1,0
1547269299,1475305,2023/4/27,v3.2.4,"> I'll leave this to you for self-merging, so that you can test your new permission. Congrats again :)

Thanks @HeartSaVioR :)","1,-1    ",1,-1,0
1550888639,1317309,2023/4/27,v3.2.4,I'll just help merging this one as it has been here for multiple weeks and we don't want to require this PR to be rebased anymore.,"1,-1    ",1,-1,0
1550890958,1317309,2023/4/27,v3.2.4,"Thanks @LuciferYang , I merged this to master.","1,-1    ",1,-1,0
1550894768,1475305,2023/4/27,v3.2.4,"Thanks @HeartSaVioR @HyukjinKwon @rangadi ~ 

I have already tested my new permissions in other pr :)

","1,-1    ",1,-1,0
1517218312,26535726,2023/4/27,v3.2.4,"It drops support for building w/ pre Hive 2.3.9, then SPARK-37446 can be reverted.","1,-1    ",1,-1,0
1517500449,47577197,2023/4/27,v3.2.4,CC @srowen,"1,-1    ",1,-1,0
1517997626,822522,2023/4/27,v3.2.4,"Is this possible now that Hadoop 2 support is gone? just checking what the implications of this change are.
Are the Hive.get changes needed, or can we batch those changes with reverting the Hive <2.3.9 support? I also don't know what the implication of that is.","1,-1    ",1,-1,0
1518994829,26535726,2023/4/27,v3.2.4,"@srowen 

> Are the `Hive.get` changes needed

Yes, `Hive.get(conf)` triggers the Hive built-in JSON functions initialization, which requires the Jackson 1.x classes.

@sunchao I suppose Spark does not officially support building against Hive other than 2.3.9, for cases listed in SPARK-37446, it's the vendor's responsibility to port HIVE-21563 into their maintained Hive 2.3.8-[vender-custom-version]","1,-1    ",1,-1,0
1519004897,26535726,2023/4/27,v3.2.4,"@sunchao can we expect a new release(focus on security) for Hive 2.3? Considering Spark master and all maintained branches use Hive 2.3.9, which was reported some CVEs, from thrift, guava, log4j, jackson, etc.

Or, Spark should move forward to a new Hive version. (should take much effort and not sure of benefits other than getting rid of CVEs)","3,-1    ",3,-1,2
1520573329,506679,2023/4/27,v3.2.4,"@pan3793 AFAIK the development efforts in Hive community are only in Hive 3.x/4.x at the moment, and the 2.x branch is barely maintained. I can try to start a conversation in the Hive community to have a new 2.3.10 release and see how it looks like.

From the long term perspective, it'd be better for Spark to move to Hive 3.x/4.x.
","1,-2    ",1,-2,-1
1521012401,822522,2023/4/27,v3.2.4,"OK, am I right that this does not make Spark any _less_ compatible with any version of Hive that is currently supported (>= 2.3.9)? If so then this is fine","1,-1    ",1,-1,0
1521073105,26535726,2023/4/27,v3.2.4,"> OK, am I right that this does not make Spark any _less_ compatible with any version of Hive that is currently supported (>= 2.3.9)? If so then this is fine

Yes.","1,-1    ",1,-1,0
1521831422,822522,2023/4/27,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1523687453,9616802,2023/4/27,v3.2.4,Merging.,"1,-1    ",1,-1,0
1517370683,109614351,2023/4/27,v3.2.4,"Conflict with https://github.com/apache/spark/pull/40892, will close this PR","1,-2    ",1,-2,-1
1522945999,7322292,2023/4/27,v3.2.4,cc @WeichenXu123 @HyukjinKwon @cloud-fan ,"1,-1    ",1,-1,0
1526988838,19235986,2023/4/27,v3.2.4,"> This PR will not add a user-facing API or Parameter or Annotation, instead only a private function attribute will be added

Q: Where is related doc and example usage code for this attribute ? 

Q2: Why it is not a user-facing attribute ? We have 3rd-party library xgboost-spark that uses ""barrier mode"" and we need to make xgboost-spark supports pyspark-connect mode.","1,-2    ",1,-2,-1
1527230215,7322292,2023/4/27,v3.2.4,"> Q: Where is related doc and example usage code for this attribute ?

I will add it.


> Q2: Why it is not a user-facing attribute ? We have 3rd-party library xgboost-spark that uses ""barrier mode"" and we need to make xgboost-spark supports pyspark-connect mode.

we can add doc and test for it, to avoid break it in the future.

But it is kind of `DeveloperApi`, our usage pattern is pretty limited. Moreover, it's likely to lead to weird behavior/failure to end users, due to the underlying `RDDBarrier` limitation.","1,-1    ",1,-1,0
1527643314,19235986,2023/4/27,v3.2.4,"> But it is kind of DeveloperApi, our usage pattern is pretty limited. Moreover, it's likely to lead to weird behavior/failure to end users, due to the underlying RDDBarrier limitation.

Our xgboost users are already aware of the limitations, it is not an issue.
note that currently xgboost library( python package) already uses RDD barrier API,
in future we need to adapt xgboost with spark connect mode, 
this means the SQL side barrier flag should also be an user-facing interface.","1,-1    ",1,-1,0
1539284439,7322292,2023/4/27,v3.2.4,"let me move the functions to sql.pandas.utils, will be less visible then","1,-1    ",1,-1,0
1540629445,47337188,2023/4/27,v3.2.4,"It seems to me that functions under `python/pyspark/sql/pandas/utils.py` are used internally - in the existing Spark source code only.

A developer API ""should"" still be called externally, by developers though,  e.g. `semanticHash`.

So defining a developer API in `utils.py` seems a little unclear to me.
","1,-1    ",1,-1,0
1542107229,19235986,2023/4/27,v3.2.4,"> It seems to me that functions under `python/pyspark/sql/pandas/utils.py` are used internally - in the existing Spark source code only.
> 
> A developer API ""should"" still be called externally, by developers though, e.g. `semanticHash`.
> 
> So defining a developer API in `utils.py` seems a little unclear to me.

Can we move the `barrier` API definition to proper place ?","1,-1    ",1,-1,0
1542989294,7322292,2023/4/27,v3.2.4,"> If we're going to have this standalone API, this should work together with other similar API like groupby().applyInPandas.

I think we won't support other Pandas API `groupby().applyInPandas`:

- it is non-trivial to support due to limitation of `RDDBarrier`;
- the ml side doesn't need them for now;


> It seems to me that functions under python/pyspark/sql/pandas/utils.py are used internally - in the existing Spark source code only.

> A developer API ""should"" still be called externally, by developers though, e.g. semanticHash.

That is a good point, what about keeping `barrier` in `pandas/utils.py` and only used it internally like other helper functions? 

@HyukjinKwon @WeichenXu123 @xinrong-meng ","1,-1    ",1,-1,0
1543373200,6477701,2023/4/27,v3.2.4,Not sure why we want to keep `barrier` as a standalone API if this cannot work together with other similar API. Why don't we just add a param to `mapInPandas` and `mapInArrow`?,"1,-1    ",1,-1,0
1543855959,19235986,2023/4/27,v3.2.4,"> It seems to me that functions under python/pyspark/sql/pandas/utils.py are used internally - in the existing Spark source code only.

This might be an issue, because the 3rd-party project ""xgboost-spark"" is going to use it. Currently ""xgboost-spark"" project already uses barrier RDD API, and in future we want to make it support spark connect.","1,-1    ",1,-1,0
1548778624,7322292,2023/4/27,v3.2.4,"I think we (@HyukjinKwon @WeichenXu123 and I) have reach to agreement that this PR (which introduces an `@barrier` annotation) is no longer, and we will keep current `barrier` parameter in `MapInPandas/Arrow`.

The reason is that barrier UDF is still too specific to `MapInPandas/Arrow`, due to the limitation of underlying `RDDBarrier`:

- can not use it within common dataframe operators like `select`, `withColumn`
- can not use it within other similar pandas function like `applyInPandas`;

I will close it in two days if no more comments.

also cc @mengxr 
","2,-1    ",2,-1,1
1553398850,47337188,2023/4/27,v3.2.4,"+1 for the proposal, thanks @zhengruifeng !

> I think we (@HyukjinKwon @WeichenXu123 and I) have reach to agreement that this PR (which introduces an `@barrier` annotation) is no longer, and we will keep current `barrier` parameter in `MapInPandas/Arrow`.

","1,-1    ",1,-1,0
1517707460,5399861,2023/4/27,v3.2.4,cc @cloud-fan ,"1,-1    ",1,-1,0
1519233962,7322292,2023/4/27,v3.2.4,"```
ERROR: Comparing client jar: /__w/spark/spark/connector/connect/client/jvm/target/scala-2.12/spark-connect-client-jvm-assembly-3.5.0-SNAPSHOT.jar and and sql jar: /__w/spark/spark/sql/core/target/scala-2.12/spark-sql_2.12-3.5.0-SNAPSHOT.jar 
problems: 
method fillValue(java.lang.Object,scala.Option)org.apache.spark.sql.Dataset in class org.apache.spark.sql.DataFrameNaFunctions does not have a correspondent in client version
Exceptions to binary compatibility can be added in 'CheckConnectJvmClientCompatibility#checkMiMaCompatibility'
connect-client-jvm module mima check failed.
Error: Process completed with exit code 1.
```

seems Mima also check `private[sql]`","3,-2    ",3,-2,1
1519471320,7322292,2023/4/27,v3.2.4,"Thanks for the reviews, merged to master","1,-1    ",1,-1,0
1517798535,25019163,2023/4/27,v3.2.4,"The original PR was merged to 3.4, so this bufgix should also go to branch-3.4.","1,-1    ",1,-1,0
1519235641,7322292,2023/4/27,v3.2.4,"Mind filing a JIRA please? If it is a bugfix, I think it deserve a JIRA ticket.","1,-1    ",1,-1,0
1519628355,7322292,2023/4/27,v3.2.4,"merged to master and branch-3.4, thanks","1,-1    ",1,-1,0
1520596794,1938382,2023/4/27,v3.2.4,Thanks for adding the JIRA!,"1,-1    ",1,-1,0
1517850737,26535726,2023/4/27,v3.2.4,@sunchao @LuciferYang ,"1,-2    ",1,-2,-1
1518132719,506679,2023/4/27,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1518017291,1475305,2023/4/27,v3.2.4,cc @pan3793 @sunchao @HyukjinKwon ,"1,-2    ",1,-2,-1
1518441760,6477701,2023/4/27,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1518489585,1475305,2023/4/27,v3.2.4,thanks @HyukjinKwon ,"1,-1    ",1,-1,0
1519025734,7870972,2023/4/28,v3.2.4,"@cloud-fan, tests are passing (and running this time ð). Let me know what you think.","1,-1    ",1,-1,0
1521504998,7870972,2023/4/28,v3.2.4,"@cloud-fan, updated for your suggestions. Tests ran and are green.","1,-1    ",1,-1,0
1521544389,3182036,2023/4/28,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1521689588,7870972,2023/4/28,v3.2.4,Thank you!,"1,-1    ",1,-1,0
1524131444,60895808,2023/4/28,v3.2.4,Closing this and will replace with SPARK-43286 and SPARK-43290.,"1,-1    ",1,-1,0
1518358413,100322362,2023/4/28,v3.2.4,@siying - you might need to enable github actions for the tests to run,"1,-1    ",1,-1,0
1518359441,100322362,2023/4/28,v3.2.4,"@HeartSaVioR - please take a look and merge after builds pass, thx !","1,-1    ",1,-1,0
1520959155,1317309,2023/4/28,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1520997483,1317309,2023/4/28,v3.2.4,@siying Thanks for your first contribution to Apache Spark project. Welcome!,"2,-1    ",2,-1,1
1518400027,10248890,2023/4/28,v3.2.4,@rangadi @pengzhon-db ,"1,-1    ",1,-1,0
1520665915,10248890,2023/4/28,v3.2.4,"Stack trace will be added in this ticket https://issues.apache.org/jira/browse/SPARK-43206, after this PR is merged","1,-1    ",1,-1,0
1521041159,10248890,2023/4/28,v3.2.4,"> CheckConnectJvmClientCompatibility

Ah thanks! Removed it!","1,-1    ",1,-1,0
1522126151,10248890,2023/4/28,v3.2.4,@HyukjinKwon Can you merge this when get a change? Thank you!,"1,-1    ",1,-1,0
1522329072,10248890,2023/4/28,v3.2.4,"The `optional` in command.proto is needed or it throws:
```
[error] /home/wei.liu/oss-spark/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala:210:19: value hasExceptionMessage is not a member of org.apache.spark.connect.proto.StreamingQueryCommandResult.ExceptionResult
[error]     if (exception.hasExceptionMessage) {
[error]                   ^
[error] one error found
```","2,-1    ",2,-1,1
1523376861,1317309,2023/4/28,v3.2.4,"The linter build only failed which linter for scala/python got passed. The failure does not seem to be related.
https://github.com/WweiL/oss-spark/actions/runs/4805404722/jobs/8551810603","3,-2    ",3,-2,1
1523377223,1317309,2023/4/28,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1518441484,6477701,2023/4/28,v3.2.4,Mind filing a JIRA please?,"1,-1    ",1,-1,0
1519352303,6477701,2023/4/28,v3.2.4,Please file a JIRA in ASF JIRA (at here https://issues.apache.org/jira/projects/SPARK/issues). See also https://spark.apache.org/contributing.html,"1,-1    ",1,-1,0
1520825250,109033553,2023/4/28,v3.2.4,"> engine that uses `dir()` to generate autocomplete suggestions (e.g. IPython kernel, Databricks Notebooks) will suggest column names on the completion `df.|`

Sure, create one ASF JIRA: https://issues.apache.org/jira/browse/SPARK-43270","1,-1    ",1,-1,0
1522387106,109033553,2023/4/28,v3.2.4,"Have to mention, this solution is not perfect solution: 

dir won't return private method, so if a column start with an _, it would be ignored in the suggestion","1,-1    ",1,-1,0
1523824585,6477701,2023/4/28,v3.2.4,Looks fine to me.,"1,-1    ",1,-1,0
1526169832,6477701,2023/4/28,v3.2.4,cc @viirya @ueshin @holdenk ,"2,-3    ",2,-3,-1
1526269975,68855,2023/4/28,v3.2.4,Looks okay to me.,"1,-1    ",1,-1,0
1528560625,6477701,2023/4/28,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1530768477,6477701,2023/4/28,v3.2.4,Made a followup to implement this in Spark Connect: https://github.com/apache/spark/pull/41009,"1,-1    ",1,-1,0
1519325403,32387433,2023/4/29,v3.2.4,cc @cloud-fan  ,"1,-1    ",1,-1,0
1532881912,32387433,2023/4/29,v3.2.4,"@cloud-fan @MaxGekk @hvanhovell Hi, PTAL. Thanks!","1,-2    ",1,-2,-1
1535571695,3784871,2023/4/29,v3.2.4,"Hi @holdenk could you please take a look, thanks!","1,-1    ",1,-1,0
1519479597,7322292,2023/4/29,v3.2.4,merged to master,"1,-2    ",1,-2,-1
1518719595,1633312,2023/4/29,v3.2.4,@mridulm Help take a look?,"1,-1    ",1,-1,0
1519231649,6477701,2023/4/29,v3.2.4,"Hey, I think we shouldn't just keep fixing it without knowing the cause. When does this happen?","1,-1    ",1,-1,0
1521237357,1591700,2023/4/29,v3.2.4,"@HyukjinKwon `message` is an optional parameter when creating an `Exception`/`Throwable` and can be `null`.
(The default`Throwable()`, `Throwable(String message)`. etc  can result in `null` message).

This change was identified during review of the earlier PR, and is to harden our codebase against accidental NPE's","3,-1    ",3,-1,2
1524684791,1591700,2023/4/29,v3.2.4,Can you fix the conflict @warrenzhu25 ? We can merge it after that.,"2,-1    ",2,-1,1
1524691247,1633312,2023/4/29,v3.2.4,"> Can you fix the conflict @warrenzhu25 ? We can merge it after that.

Done.","3,-2    ",3,-2,1
1528645742,1591700,2023/4/29,v3.2.4,"Merged to master.
Thanks for fixing this @warrenzhu25 !
Thanks for review @HyukjinKwon :-)","1,-1    ",1,-1,0
1519049541,47577197,2023/4/29,v3.2.4,"To me it seams like we can just add `show_counts` to this function. We already have this max row to calculate on.   

Or we can implement something like this..

```
from collections import Counter
from pyspark.sql.functions import col, count, when

def spark_info(df):
    # Print basic DataFrame information
    print(f""<class '{df.__class__.__module__}.{df.__class__.__name__}'>"")
    print(f""Number of rows: {df.count()}"")
    print(f""Number of columns: {len(df.columns)}"")

    # Print column header for the detailed DataFrame information
    print(""\nColumn"" + "" "" * 110 + ""Non-Null Count"" + "" "" + ""Dtype"")
    print(""-"" * 6, "" "" * 108, ""-"" * 14, ""-"" * 5)

    # Calculate non-null counts for each column
    non_null_counts = df.agg(*[count(when(col(f""`{c}`"").isNotNull(), f""`{c}`"")).alias(c) for c in df.columns]).collect()[0]

    # Initialize a counter to store data type counts
    dtype_counter = Counter()

    # Iterate through the schema fields and print detailed column information
    for i, field in enumerate(df.schema.fields):
        non_null_count = non_null_counts[field.name]
        dtype = field.dataType.simpleString()
        print(f""{field.name:<90} {non_null_count:>30} non-null {dtype}"")

        # Update the data type counter
        dtype_counter[dtype] += 1

    # Print data type summary
    dtypes_summary = "", "".join([f""{dtype}({count})"" for dtype, count in dtype_counter.items()])
    print(f""\ndtypes: {dtypes_summary}"")
 ```

![image](https://user-images.githubusercontent.com/47577197/233838325-b1b7b5ef-b358-4c41-a20c-f841f3484d2c.png)
(...)

![image](https://user-images.githubusercontent.com/47577197/233838368-5599bfe9-2a05-44d6-b583-cd2bbb444127.png)
","1,-1    ",1,-1,0
1519137932,47577197,2023/4/29,v3.2.4,"add Counter to imports 
```
from collections import defaultdict, namedtuple, Counter

def info(
        self,
        verbose: Optional[bool] = None,
        buf: Optional[IO[str]] = None,
        max_cols: Optional[int] = None,
    ) -> None:       
        # To avoid pandas' existing config affects pandas-on-Spark.
        # TODO: should we have corresponding pandas-on-Spark configs?
        #with pd.option_context(
        #    ""display.max_info_columns"", sys.maxsize, ""display.max_info_rows"", sys.maxsize
        #):
        if verbose is None or verbose:
            index_type: Type = type(self.index).__name__
            print(f""<class '{self.__class__.__module__}.{self.__class__.__name__}'>"")
            print(f""{index_type}: {len(self)} entries, {self.index.min()} to {self.index.max()}"")

            # Print column header for the detailed DataFrame information
            print(f""Data columns (total {len(self.columns)} columns):"")
            print(f"" #   Column{' ' * 106}Non-Null Count  Dtype"")
            print(f""---  ------{' ' * 106}--------------  -----"")

        # Calculate non-null counts for each column
        non_null_counts: Dict[str, int] = self.count().to_dict()

        # Initialize a counter to store data type counts
        dtype_counter: Counter = Counter()

        # Iterate through the schema fields and print detailed column information
        for idx, column in enumerate(self.columns):
            dtype: str = str(self[column].dtype)
            non_null_count: int = non_null_counts[column]
            if verbose is None or verbose:
                print(f""{idx:<3} {column:<90} {non_null_count:>30} non-null {dtype}"")

            # Update the data type counter
            dtype_counter[dtype] += 1

        if verbose is None or verbose:
            # Print data type summary
            dtypes_summary: str = "", "".join([f""{dtype}({count})"" for dtype, count in dtype_counter.items()])
            print(f""\ndtypes: {dtypes_summary}"")
        elif not verbose:
            print(f""<class '{self.__class__.__module__}.{self.__class__.__name__}'>"")
            print(f""Index: {len(self)} entries, {self.index.min()} to {self.index.max()}"")
            print(f""Columns: {len(self.columns)} entries, {self.columns[0]} to {self.columns[-1]}"")
            dtypes_summary: str = "", "".join([f""{dtype}({count})"" for dtype, count in dtype_counter.items()])
            print(f""dtypes: {dtypes_summary}"")

```","1,-1    ",1,-1,0
1519205447,6477701,2023/4/30,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1518931775,11972570,2023/4/30,v3.2.4,@cloud-fan Please help to review. Thanks.,"1,-1    ",1,-1,0
1519297848,7322292,2023/5/1,v3.2.4,Do 3.3/3.4/master have the same issue?,"1,-1    ",1,-1,0
1519324450,11972570,2023/5/1,v3.2.4,"> Do 3.3/3.4/master have the same issue?

Spark [3.3](https://github.com/apache/spark/blob/8f0c75cbbab0cb76a30272a157b4f4cc02cab444/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala#L291) have this issue. And [spark 3.4 and main](https://github.com/apache/spark/blob/b70407eb815ac97f5992b6cf961911e878ea5510/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala#L240) doesn't seem to have this issue. Because the [StatFunctions.scala](https://github.com/apache/spark/pull/38346) is reimplemented and doesn't call the rdd.collect() method.","2,-3    ",2,-3,-1
1519347828,6477701,2023/5/1,v3.2.4,"Oh, you're fixing branch-3.2. It reached EOL, and there won't be more releases in 3.2.x.","1,-1    ",1,-1,0
1519349350,6477701,2023/5/1,v3.2.4,I am fine if we can land this to branch-3.3 alone but would need to fix the JIRA's affected version.,"1,-1    ",1,-1,0
1519387403,11972570,2023/5/1,v3.2.4,"> I am fine if we can land this to branch-3.3 alone but would need to fix the JIRA's affected version.

Sure. I have change the version to branch-3.3. Please help to review again. Thanks.","1,-1    ",1,-1,0
1522862616,11972570,2023/5/1,v3.2.4,Thanks for your review. @cloud-fan Can you help to merge?,"1,-1    ",1,-1,0
1523082663,3182036,2023/5/1,v3.2.4,"thanks, merging to 3.3!","1,-1    ",1,-1,0
1519255258,12025282,2023/5/1,v3.2.4,cc @cloud-fan @viirya thank you,"1,-2    ",1,-2,-1
1521346298,7322292,2023/5/1,v3.2.4,merged to master,"1,-1    ",1,-1,0
1519229651,6477701,2023/5/1,v3.2.4,Merged to branch-3.3.,"1,-2    ",1,-2,-1
1521989897,9616802,2023/5/1,v3.2.4,@Knorreman I am not sure how much new things we want to add to the RDD API. The SQL API should be the primary API.,"1,-1    ",1,-1,0
1522172492,1591700,2023/5/1,v3.2.4,"Let us minimize the diff - currently it is fairly large, and it is difficult to reason about what is being changed: most of it is unrelated to the change you are trying to propose.","1,-1    ",1,-1,0
1522262245,43262962,2023/5/1,v3.2.4,"> Let us minimize the diff - currently it is fairly large, and it is difficult to reason about what is being changed: most of it is unrelated to the change you are trying to propose.

@mridulm Yes my bad. I ran the ./dev/scalafmt script before pushing. I guess that introduced more changes than I wanted. ","2,-1    ",2,-1,1
1522268063,43262962,2023/5/1,v3.2.4,"> @Knorreman I am not sure how much new things we want to add to the RDD API. The SQL API should be the primary API.

@hvanhovell Every now and then there are stuff added to RDDs. I think it would be benefitial to RDD users to have this type of join available for some usecases. ","1,-1    ",1,-1,0
1520498405,6235869,2023/5/1,v3.2.4,"@cloud-fan @sunchao @viirya @huaxingao @dongjoon-hyun @gengliangwang, this is a follow-up to PR #40308.","1,-1    ",1,-1,0
1521396482,3182036,2023/5/2,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1521940601,6235869,2023/5/2,v3.2.4,"Thanks for reviewing, @cloud-fan!","1,-1    ",1,-1,0
1520446235,26535726,2023/5/2,v3.2.4,cc @sunchao ,"2,-1    ",2,-1,1
1524462509,26535726,2023/5/2,v3.2.4,kindly ping @sunchao ,"3,-1    ",3,-1,2
1524621007,506679,2023/5/2,v3.2.4,"Oops almost forgot. Merged to master, thanks!","1,-1    ",1,-1,0
1521229465,1591700,2023/5/2,v3.2.4,+CC @otterc ,"2,-1    ",2,-1,1
1519616957,3182036,2023/5/2,v3.2.4,cc @Yikf @sadikovi @gengliangwang ,"1,-1    ",1,-1,0
1520954545,7788766,2023/5/2,v3.2.4,"I think the PR still introduces user-facing changes. 
Also, would it be possible to not make any changes in Cast and do everything in `df.show` method?","1,-1    ",1,-1,0
1521043238,3182036,2023/5/2,v3.2.4,"> would it be possible to not make any changes in Cast and do everything in df.show method?

We can by duplicating the code of `Cast`, but I don't think that's a good idea.","1,-1    ",1,-1,0
1522539250,7788766,2023/5/2,v3.2.4,I suppose it is fine to have changes in Cast. Would it be possible to check the example queries in my comment in https://github.com/apache/spark/pull/40699 and what results they return? Or let me know if this is ready for review and I can check out the PR and try those examples on my machine.,"1,-1    ",1,-1,0
1522635737,7788766,2023/5/2,v3.2.4,Does this PR need https://github.com/apache/spark/pull/40699? I was under the assumption that we had to revert the original patch and have another solution instead.,"1,-2    ",1,-2,-1
1522708607,3182036,2023/5/2,v3.2.4,"Most of the changes in https://github.com/apache/spark/pull/40699 are updating tests, and we still need them as we don't revert the behavior change of `df.show`. The behavior change of Cast is reverted and we can tell it from tests: [sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala](https://github.com/apache/spark/pull/40922/files#diff-7c552f1d8b4654347032ec37209ecc518b82f9b48916c54a18fb4c9bd4aee2d6R787)

It's ready for review now.","1,-2    ",1,-2,-1
1524628328,3182036,2023/5/2,v3.2.4,cc @LuciferYang @AngersZhuuuu @yaooqinn ,"1,-1    ",1,-1,0
1525135989,3182036,2023/5/2,v3.2.4,"thanks for the review, merging to master!","1,-1    ",1,-1,0
1527060339,1475305,2023/5/2,v3.2.4,"+1, late LGTM","1,-1    ",1,-1,0
1521214681,7322292,2023/5/2,v3.2.4,merged to master,"1,-1    ",1,-1,0
1550283049,9616802,2023/5/2,v3.2.4,"@LuciferYang can you update.

Overall this looks good to me. Can you make sure we don't exclude too many cases?","1,-1    ",1,-1,0
1550820753,1475305,2023/5/2,v3.2.4,"```
Error: Exception in thread ""main"" java.lang.IllegalArgumentException: Unsupported class file major version 61
	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:195)
	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:176)
	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:162)
	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:283)
	at org.clapper.classutil.asm.ClassFile$.load(ClassFinderImpl.scala:222)
	at org.clapper.classutil.ClassFinder.classData(ClassFinder.scala:404)
	at org.clapper.classutil.ClassFinder.$anonfun$processOpenZip$2(ClassFinder.scala:359)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator.toStream(Iterator.scala:1417)
	at scala.collection.Iterator.toStream$(Iterator.scala:1416)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
	at scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1417)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
	at scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
	at scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
	at scala.collection.immutable.Stream.filterImpl(Stream.scala:506)
	at scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
	at scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
	at scala.collection.generic.Growable.loop$1(Growable.scala:57)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:61)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:381)
	at scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:329)
	at scala.collection.TraversableLike.to(TraversableLike.scala:786)
	at scala.collection.TraversableLike.to$(TraversableLike.scala:783)
	at scala.collection.AbstractTraversable.to(Traversable.scala:108)
	at scala.collection.TraversableOnce.toSet(TraversableOnce.scala:360)
	at scala.collection.TraversableOnce.toSet$(TraversableOnce.scala:360)
	at scala.collection.AbstractTraversable.toSet(Traversable.scala:108)
	at org.apache.spark.tools.GenerateMIMAIgnore$.getClasses(GenerateMIMAIgnore.scala:[15](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:16)6)
	at org.apache.spark.tools.GenerateMIMAIgnore$.privateWithin(GenerateMIMAIgnore.scala:57)
	at org.apache.spark.tools.GenerateMIMAIgnore$.main(GenerateMIMAIgnore.scala:1[22](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:23))
	at org.apache.spark.tools.GenerateMIMAIgnore.main(GenerateMIMAIgnore.scala)
Error: Process completed with exit code 1.
```

After rebase the code, the `connect-jvm-client-mima-check` will run failed as above, it seems that master branch has a dependency compiled by Java 17, let me investigate first.

","1,-1    ",1,-1,0
1551066383,1475305,2023/5/2,v3.2.4,"> ```
> Error: Exception in thread ""main"" java.lang.IllegalArgumentException: Unsupported class file major version 61
> 	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:195)
> 	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:176)
> 	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:162)
> 	at org.objectweb.asm.ClassReader.<init>(ClassReader.java:283)
> 	at org.clapper.classutil.asm.ClassFile$.load(ClassFinderImpl.scala:222)
> 	at org.clapper.classutil.ClassFinder.classData(ClassFinder.scala:404)
> 	at org.clapper.classutil.ClassFinder.$anonfun$processOpenZip$2(ClassFinder.scala:359)
> 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
> 	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
> 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
> 	at scala.collection.Iterator.toStream(Iterator.scala:1417)
> 	at scala.collection.Iterator.toStream$(Iterator.scala:1416)
> 	at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)
> 	at scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1417)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
> 	at scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
> 	at scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
> 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
> 	at scala.collection.immutable.Stream.filterImpl(Stream.scala:506)
> 	at scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
> 	at scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
> 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
> 	at scala.collection.generic.Growable.loop$1(Growable.scala:57)
> 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:61)
> 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
> 	at scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:381)
> 	at scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:329)
> 	at scala.collection.TraversableLike.to(TraversableLike.scala:786)
> 	at scala.collection.TraversableLike.to$(TraversableLike.scala:783)
> 	at scala.collection.AbstractTraversable.to(Traversable.scala:108)
> 	at scala.collection.TraversableOnce.toSet(TraversableOnce.scala:360)
> 	at scala.collection.TraversableOnce.toSet$(TraversableOnce.scala:360)
> 	at scala.collection.AbstractTraversable.toSet(Traversable.scala:108)
> 	at org.apache.spark.tools.GenerateMIMAIgnore$.getClasses(GenerateMIMAIgnore.scala:[15](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:16)6)
> 	at org.apache.spark.tools.GenerateMIMAIgnore$.privateWithin(GenerateMIMAIgnore.scala:57)
> 	at org.apache.spark.tools.GenerateMIMAIgnore$.main(GenerateMIMAIgnore.scala:1[22](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:23))
> 	at org.apache.spark.tools.GenerateMIMAIgnore.main(GenerateMIMAIgnore.scala)
> Error: Process completed with exit code 1.
> ```
> 
> After rebase the code, the `connect-jvm-client-mima-check` will run failed as above, it seems that master branch has a dependency compiled by Java 17, let me investigate first.

change of [9620361](https://github.com/apache/spark/pull/40925/commits/9620361a677013725befabaa262603f7a450ff32) will fix this issue, I will make a separate pr to upgrade the asm version of tools module, this submitted just to verify that `connect-jvm-client-mima-check` can execute successfully with this one



","1,-1    ",1,-1,0
1554342893,1475305,2023/5/2,v3.2.4,"> Can you make sure we don't exclude too many cases?

Will double check this later","1,-1    ",1,-1,0
1525122988,44108233,2023/5/2,v3.2.4,CI passed. @zhengruifeng Could you take an another look when you find some time?,"1,-1    ",1,-1,0
1525177563,7322292,2023/5/2,v3.2.4,merged to master,"1,-1    ",1,-1,0
1521023572,7322292,2023/5/2,v3.2.4,"to be more consistent with https://github.com/apache/spark/blob/a45affe3c8e7a724aea7dbbc1af08e36001c7540/python/pyspark/sql/column.py#L924-L932 

what about changing to?

```
        if isinstance(length, Column):
            length_expr = length._expr
            start_expr = startPos._expr
        elif isinstance(length, int):
            length_expr = LiteralExpression._from_value(length)
            start_expr = LiteralExpression._from_value(startPos)
        else:
            raise PySparkTypeError(
                error_class=""NOT_COLUMN_OR_INT"",
                message_parameters={""arg_name"": ""length"", ""arg_type"": type(length).__name__},
            )          

```","1,-1    ",1,-1,0
1521055105,44108233,2023/5/2,v3.2.4,"> what about changing to?

@zhengruifeng Nice! Just applied the comment","1,-1    ",1,-1,0
1521111978,7322292,2023/5/2,v3.2.4,"oh, the linter fails","1,-1    ",1,-1,0
1521214668,44108233,2023/5/2,v3.2.4,Fixed!,"1,-1    ",1,-1,0
1521607077,7322292,2023/5/2,v3.2.4,merged to master,"1,-1    ",1,-1,0
1521036530,7322292,2023/5/2,v3.2.4,merged to master,"1,-1    ",1,-1,0
1520312702,1475305,2023/5/2,v3.2.4,"@majdyz Can you enable GA first refer to 
<img width=""1347"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/234031906-ad7fa49e-209b-4369-888a-e81a1299943d.png"">
https://github.com/apache/spark/pull/40929/checks?check_run_id=12977948949","1,-1    ",1,-1,0
1520371824,76959103,2023/5/2,v3.2.4,@LuciferYang Thanks; It's already enabled now. ,"1,-1    ",1,-1,0
1520403075,1938382,2023/5/2,v3.2.4,@cloud-fan @hvanhovell ,"1,-1    ",1,-1,0
1520666866,1938382,2023/5/2,v3.2.4,"This PR fails on 
```
Error:  running /home/runner/work/spark/spark/dev/mima -Phadoop-3 -Pdocker-integration-tests -Pyarn -Pkubernetes -Pspark-ganglia-lgpl -Pmesos -Pconnect -Phive -Phadoop-cloud -Pkinesis-asl -Phive-thriftserver -Pvolcano ; received return code 1
```

but there is no error messages about how to fix. I run this locally which output nothing as well.


Stuck on mima stuff now,","1,-1    ",1,-1,0
1520961265,1938382,2023/5/2,v3.2.4,Trying to exclude the MIMA check for `common-utils`.,"1,-1    ",1,-1,0
1520990871,1938382,2023/5/2,v3.2.4,"ok looks like we also have compatibility checks on Spark-core
```
[error] spark-core: Failed binary compatibility check against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4036)
[error]  * interface org.apache.spark.QueryContext does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.QueryContext"")
[error]  * class org.apache.spark.SparkException does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.SparkException"")
[error]  * object org.apache.spark.SparkException does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.SparkException$"")
[error]  * interface org.apache.spark.SparkThrowable does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.SparkThrowable"")
```

So I choose to exclude these classes.","1,-2    ",1,-2,-1
1521157761,1938382,2023/5/2,v3.2.4,`continuous-integration/appveyor/pr` seems to be stuck.,"1,-1    ",1,-1,0
1522775572,3182036,2023/5/2,v3.2.4,"Hi @LuciferYang , do you have any idea why mima fails for this PR? The error message says nothing. Thanks!","1,-1    ",1,-1,0
1522803587,1475305,2023/5/2,v3.2.4,Looking,"1,-1    ",1,-1,0
1522815901,1475305,2023/5/2,v3.2.4,"@amaliujia @cloud-fan 

https://github.com/apache/spark/blob/b461cdea92ea08ce39bb3c9d733f0af7c56abf8d/project/SparkBuild.scala#L404-L407

should add `commonUtils` to this Seq now due to 3.4.0 no `common-utils` module:

```
[error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0
[error]   Not found
[error]   Not found
[error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom
[error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:344)
[error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:313)
[error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)
[error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:313)
[error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)
[error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:59)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:133)
[error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:146)
[error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:146)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:127)
[error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)
[error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:160)
[error] 	at sbt.Classpaths$.$anonfun$updateTask0$1(Defaults.scala:3687)
[error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)
[error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)
[error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)
[error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)
[error] 	at sbt.Execute.work(Execute.scala:291)
[error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)
[error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)
[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error] 	at java.lang.Thread.run(Thread.java:750)
[error] (oldDeps / update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0
[error]   Not found
[error]   Not found
[error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom
```","1,-1    ",1,-1,0
1522820195,1475305,2023/5/2,v3.2.4,"> @amaliujia @cloud-fan
> 
> https://github.com/apache/spark/blob/b461cdea92ea08ce39bb3c9d733f0af7c56abf8d/project/SparkBuild.scala#L404-L407
> 
> should add `commonUtils` to this Seq now due to 3.4.0 no `common-utils` module:
> 
> ```
> [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0
> [error]   Not found
> [error]   Not found
> [error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml
> [error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom
> [error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:344)
> [error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:313)
> [error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)
> [error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:313)
> [error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)
> [error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:59)
> [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:133)
> [error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)
> [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:146)
> [error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)
> [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:146)
> [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:127)
> [error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)
> [error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:160)
> [error] 	at sbt.Classpaths$.$anonfun$updateTask0$1(Defaults.scala:3687)
> [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)
> [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)
> [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)
> [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)
> [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)
> [error] 	at sbt.Execute.work(Execute.scala:291)
> [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)
> [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
> [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)
> [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
> [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
> [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
> [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
> [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
> [error] 	at java.lang.Thread.run(Thread.java:750)
> [error] (oldDeps / update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0
> [error]   Not found
> [error]   Not found
> [error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml
> [error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom
> ```

with this change and rebase the code, then run the mima check:


```
[warn] multiple main classes detected: run 'show discoveredMainClasses' to see the list
[info] spark-examples: mimaPreviousArtifacts not set, not analyzing binary compatibility
[error] spark-core: Failed binary compatibility check against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4035)
[error]  * interface org.apache.spark.QueryContext does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.QueryContext"")
[error]  * class org.apache.spark.SparkException does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.SparkException"")
[error]  * object org.apache.spark.SparkException does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.SparkException$"")
[error]  * interface org.apache.spark.SparkThrowable does not have a correspondent in current version
[error]    filter with: ProblemFilters.exclude[MissingClassProblem](""org.apache.spark.SparkThrowable"")
[error] java.lang.RuntimeException: Failed binary compatibility check against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4035)
[error] 	at scala.sys.package$.error(package.scala:30)
[error] 	at com.typesafe.tools.mima.plugin.SbtMima$.reportModuleErrors(SbtMima.scala:89)
[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2(MimaPlugin.scala:36)
[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2$adapted(MimaPlugin.scala:26)
[error] 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[error] 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[error] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$1(MimaPlugin.scala:26)
[error] 	at com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$1$adapted(MimaPlugin.scala:25)
[error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)
[error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)
[error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)
[error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)
[error] 	at sbt.Execute.work(Execute.scala:291)
[error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)
[error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)
[error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)
[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
[error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[error] 	at java.lang.Thread.run(Thread.java:750)
[error] (core / mimaReportBinaryIssues) Failed binary compatibility check against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4035)
[error] Total time: 92 s (01:32), completed 2023-4-26 13:46:13

```","1,-1    ",1,-1,0
1522857821,1938382,2023/5/2,v3.2.4,"@LuciferYang thank you!

@cloud-fan then probably it goes back to what I did last time. What do you think?","1,-2    ",1,-2,-1
1522920925,3182036,2023/5/2,v3.2.4,Yea we have to exclude it,"1,-1    ",1,-1,0
1524376979,9616802,2023/5/2,v3.2.4,Merging to master,"1,-2    ",1,-2,-1
1522946251,7253827,2023/5/3,v3.2.4,cc @cloud-fan,"1,-3    ",1,-3,-2
1520673919,47577197,2023/5/3,v3.2.4,CC @pjfanning,"1,-1    ",1,-1,0
1520686760,11783444,2023/5/3,v3.2.4,"if you use jackson 2.14.2 - you can just upgrade snakeyaml to 2.x (there is a fix in jackson 2.14.2 that makes it easier to upgrade snakeyaml dependency)

jackson 2.15.0 has extra changes that you need to worry about - see https://issues.apache.org/jira/browse/SPARK-42854","1,-1    ",1,-1,0
1521835725,822522,2023/5/3,v3.2.4,"Looks good, just resolve the conflict and we can check tests again","1,-1    ",1,-1,0
1521846016,11783444,2023/5/3,v3.2.4,@srowen [CVE-2022-1471](https://nvd.nist.gov/vuln/detail/CVE-2022-1471) is a snakeyaml CVE and you can upgrade snakeyaml explicitly without upgrading jackson. Jackson 2.15 applies limits on the JSON inputs it parses. It is probably not a good idea to use Jackson 2.15 without supporting overrides for these limits. See [SPARK-42854](https://issues.apache.org/jira/browse/SPARK-42854).,"1,-1    ",1,-1,0
1521860271,822522,2023/5/3,v3.2.4,"Ah OK, @bjornjorgensen how about just doing snakeyaml here as an intermediate step?","1,-1    ",1,-1,0
1521927727,11783444,2023/5/3,v3.2.4,Snakeyaml 1.32 introduced an approx 5mb default limit on input yaml. Spark trunk already has this change but it might be worth considering making this limit configurable using spark configs. The snakeyaml class to look for is called LoaderOptions.,"1,-3    ",1,-3,-2
1521933550,1475305,2023/5/3,v3.2.4,"<img width=""1060"" alt=""image"" src=""https://user-images.githubusercontent.com/1475305/234313784-5f6e54f0-5e2b-4725-a1c1-8f62e7cb6d41.png"">

https://github.com/FasterXML/jackson-core/blob/a2c0bdcfb9aae8fca555240e63e57c1d9e6f8079/src/main/java/com/fasterxml/jackson/core/StreamReadConstraints.java#L30-L51

It seems to have added 4 constraints, but `MAX_BIGINT_SCALE_MAGNITUDE ` is not configurable.

May be we should add corresponding configurable items to `org.apache.spark.sql.catalyst.json.JSONOptions` and  inject them in `JSONOptions#buildJsonFactory` by `JsonFactoryBuilder#streamReadConstraints(StreamReadConstraints)`.

This may handle most scenarios, but there are still some places in Spark that call `new ObjectMapper()`, these will using `StreamReadConstraints.defaults()`, but for them, using the default value may be OK(need to analyze it one by one). 

If there is a problem with what I said, please correct me @pjfanning , thanks 




","1,-1    ",1,-1,0
1521970225,1475305,2023/5/3,v3.2.4,"And will these new constraints make JSON parsing slower?

","1,-2    ",1,-2,-1
1521993941,11783444,2023/5/3,v3.2.4,@LuciferYang your analysis summarises the situation well,"1,-1    ",1,-1,0
1522230735,11783444,2023/5/3,v3.2.4,"> And will these new constraints make JSON parsing slower?

there is no evidence of a significant performance drop","2,-1    ",2,-1,1
1522239102,11783444,2023/5/3,v3.2.4,"@bjornjorgensen can you engage with the comments? A few of us have basically asked for this PR to be modified or redone but not to proceed as it is.

* one option is to replace this PR with 1 that upgrades just snakeyaml and leaves jackson alone
* another option is to upgrade jackson but to also introduce new spark configs so a StreamReadConstraints can be created and set on the ObjectMapper based on the config values","2,-1    ",2,-1,1
1522262873,47577197,2023/5/3,v3.2.4,"@pjfanning yes, sorry 
I have marked it as a draft now.  ","1,-1    ",1,-1,0
1522751838,1475305,2023/5/3,v3.2.4,"Please fix the compilation error first @bjornjorgensen thanks

","3,-1    ",3,-1,2
1523073847,1475305,2023/5/3,v3.2.4,"I found `Update jackson-module-scala to 2.15.0 in 2.12.x` in release plan of Scala 2.12.18:

https://github.com/scala/scala/milestone/99","1,-1    ",1,-1,0
1523106145,11783444,2023/5/3,v3.2.4,"> I found `Update jackson-module-scala to 2.15.0 in 2.12.x` in release plan of Scala 2.12.18:
> 
> https://github.com/scala/scala/milestone/99

@LuciferYang Core Scala libs don't have a dependency on Jackson. This upgrade seems to affect a module called `compilerOptionsExporter`. I don't really know what that that is but I doubt whether it has much impact outside the Scala build.","2,-1    ",2,-1,1
1523205372,47577197,2023/5/3,v3.2.4,"anyway this is my ""hello world"" in Scala. 
What is the best way to get this forward? @LuciferYang will you take over? or.. ","1,-1    ",1,-1,0
1525629901,47577197,2023/5/3,v3.2.4,"> Ah OK, @bjornjorgensen how about just doing snakeyaml here as an intermediate step?

Well, upgrading SnakeYAML might work as a temporary solution, but for Spark's long-term benefit, I believe it's best to upgrade Jackson now. This will make future upgrades of Jackson much easier. Additionally, Snyk has opened two PRs to upgrade `com.fasterxml.jackson.dataformat:jackson-dataformat-yaml` to version 2.15.0. By upgrading Jackson now, we'll address these issues and won't receive any further request about this.","1,-1    ",1,-1,0
1525666677,47577197,2023/5/3,v3.2.4,"we can upgrade it to 
```
 private def safeStringToInt(value: String, default: Int): Int = {
    try {
      val intValue = value.toInt
      if (intValue >= 0) intValue else default
    } catch {
      case _: NumberFormatException => default
    }
  }


  private val maxNestingDepth: Int = parameters
    .get(""maxNestingDepth"")
    .map(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_DEPTH))
    .getOrElse(StreamReadConstraints.DEFAULT_MAX_DEPTH)

  private val maxNumLen: Int = parameters
    .get(""maxNumLen"")
    .map(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_NUM_LEN))
    .getOrElse(StreamReadConstraints.DEFAULT_MAX_NUM_LEN)

  private val maxStringLen: Int = parameters
    .get(""maxStringLen"")
    .map(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_STRING_LEN))
    .getOrElse(StreamReadConstraints.DEFAULT_MAX_STRING_LEN)

```

Or chatGPT will have this 

Cache frequently used values to reduce the number of map lookups. For example, in the JSONOptions class, you can create lazy vals for the frequently accessed parameters:
```
  lazy val samplingRatio: Double = parameters.get(SAMPLING_RATIO).map(_.toDouble).getOrElse(1.0)
  lazy val primitivesAsString: Boolean = parameters.get(PRIMITIVES_AS_STRING).map(_.toBoolean).getOrElse(false)
  lazy val prefersDecimal: Boolean = parameters.get(PREFERS_DECIMAL).map(_.toBoolean).getOrElse(false)
 ```
In the safeStringToInt method, you can use scala.util.Try to handle the conversion from String to Int:
```
	private def safeStringToInt(value: String, default: Int): Int = {
		scala.util.Try(value.toInt).filter(_ >= 0).getOrElse(default)
	  }
```	  
Consider using getOrElse with a default value directly instead of map followed by getOrElse for some parameters:
```
  private val maxNestingDepth: Int = parameters
    .getOrElse(""maxNestingDepth"", StreamReadConstraints.DEFAULT_MAX_DEPTH.toString)
    .pipe(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_DEPTH))

  private val maxNumLen: Int = parameters
    .getOrElse(""maxNumLen"", StreamReadConstraints.DEFAULT_MAX_NUM_LEN.toString)
    .pipe(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_NUM_LEN))

  private val maxStringLen: Int = parameters
    .getOrElse(""maxStringLen"", StreamReadConstraints.DEFAULT_MAX_STRING_LEN.toString)
    .pipe(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_STRING_LEN))
```
but it seams to be to mach.. 



","2,-1    ",2,-1,1
1525686850,822522,2023/5/3,v3.2.4,I think it's overkill to put too much processing into this arg parsing,"2,-1    ",2,-1,1
1527567608,822522,2023/5/3,v3.2.4,Merged to master,"2,-2    ",2,-2,0
1527577420,47577197,2023/5/3,v3.2.4,Thank you all so much for your help and support for this update.,"1,-1    ",1,-1,0
1535994835,44700269,2023/5/3,v3.2.4,"Note that this breaks downstream projects that want to read json:

```scala
spark.read.json(""file.json"")
```
```
java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/StreamReadConstraints
	at org.apache.spark.sql.catalyst.json.JSONOptions.buildJsonFactory(JSONOptions.scala:195)
	at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:83)
...
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.core.StreamReadConstraints
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 16 more
```

The reason is that spark-core depends on avro 1.11.1, which pulls in jackson-core 2.12.7:
```
[INFO] +- org.apache.spark:spark-core_2.12:jar:3.5.0-SNAPSHOT:provided
[INFO] |  +- org.apache.avro:avro:jar:1.11.1:provided
[INFO] |  |  \- com.fasterxml.jackson.core:jackson-core:jar:2.12.7:provided
```

Project avro has upgraded to jackson 2.15.0 a few days ago: https://github.com/apache/avro/commit/3b6c6cc43d54ae56b51dacfd6d86d54a0733d57b

I think for this upgrade in Spark to work, the avro dependency of spark-core has to be upgraded to their next release as well.

My project depends on spark-core and spark-sql only.","2,-1    ",2,-1,1
1536025688,47577197,2023/5/3,v3.2.4,"@EnricoMi I read and write JSON files using pyspark. I haven't seen any problems so far. 
 ","1,-1    ",1,-1,0
1536111904,44700269,2023/5/3,v3.2.4,"I doubt `spark-submit`, `spark-shell` or `pyspark` are affected. The Spark sources are also not affected:

```
[INFO] org.apache.spark:spark-core_2.12:jar:3.5.0-SNAPSHOT
[INFO] +- org.apache.avro:avro:jar:1.11.1:compile
[INFO] |  \- com.fasterxml.jackson.core:jackson-core:jar:2.15.0:compile
```

I am talking about Java / Scala projects that depend on `spark-core`, like https://github.com/G-Research/spark-extension.","1,-3    ",1,-3,-2
1536402516,822522,2023/5/3,v3.2.4,"All wrappers would be affected if any are, as all of this goes through the JVM for JSON processing.
Yes, you show that Spark does _not_ pull in older Jackson versions via Avro. So aren't you talking about other projects that may somehow pull in older Jackson, not Spark? I don't quite understand. Spark JSON processing worsk with this change.","1,-3    ",1,-3,-2
1536432927,44700269,2023/5/3,v3.2.4,"Spark pulls in Jackson 2.15.0 for compile scope. Projects depending on Spark do not transitively depend on Jackson 2.15.0.

Spark depends on Avro which depends on Jackson 2.12.7. Projects that depend on Spark transitively depend on Jackson 2.12.7.

Maybe spark-core should depend on Jackson 2.15.0 with runtime scope, rather than compile scope.

The following pom reproduces the issue:
```xml
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
  <modelVersion>4.0.0</modelVersion>
  <groupId>uk.co.gresearch.spark</groupId>
  <artifactId>spark-example_2.13</artifactId>
  <version>1.0.0-SNAPSHOT</version>

  <dependencies>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.13</artifactId>
      <version>3.5.0-SNAPSHOT</version>
      <scope>provided</scope>
    </dependency>
  </dependencies>

  <repositories>
    <!-- required to resolve Spark snapshot versions -->
    <repository>
      <id>apache snapshots</id>
      <name>Apache Snapshots</name>
      <url>https://repository.apache.org/snapshots/</url>
    </repository>
  </repositories>
</project>
```
```bash
mvn dependency:tree
```
```
[INFO] uk.co.gresearch.spark:spark-example_2.13:jar:1.0.0-SNAPSHOT
[INFO] \- org.apache.spark:spark-core_2.13:jar:3.5.0-SNAPSHOT:provided
[INFO]    +- org.scala-lang.modules:scala-parallel-collections_2.13:jar:1.0.4:provided
[INFO]    +- org.apache.avro:avro:jar:1.11.1:provided
[INFO]    |  \- com.fasterxml.jackson.core:jackson-core:jar:2.12.7:provided
```
","1,-1    ",1,-1,0
1536441189,822522,2023/5/3,v3.2.4,"They do transitively depend on Jackson, if it's compile scope. Lots of stuff would never work otherwise.
runtime scope does not work; we could not compile against Jackson code then, and Spark uses Jackson classes.

You are correct that there is a conflict that Maven resolves, and its rules may not give the desired effect. However Spark does ""all it can do"" by directly depending on Jackson. 

The only thing I can think of is to also exclude the jackson dep from Avro explicitly in the Spark POM?","2,-1    ",2,-1,1
1536561751,47577197,2023/5/3,v3.2.4,"> The only thing I can think of is to also exclude the jackson dep from Avro explicitly in the Spark POM?

But then we will have to write a lot of testes to test Avro with  jackson version 2.15.0. 

Cant we open a issue in Avro and ask for a new version? There lasts release is from Aug 5, 2022.   ","1,-1    ",1,-1,0
1536566763,44700269,2023/5/3,v3.2.4,"That was my initial suggested solution. Once Avro releases a new version, and Spark is compatible with that, the problem should be resolved by upgrading to that new Avro version.","3,-1    ",3,-1,2
1536568838,11783444,2023/5/3,v3.2.4,I'm a Jackson committer and 2.15.0 has some big changes in it. A number of changes have already been made based on some bugs and some tweaking of the changes (for a forthcoming v2.15.1 release). I would discourage anyone from making releases that are based on Jackson 2.15.0.,"2,-1    ",2,-1,1
1520774365,6235869,2023/5/3,v3.2.4,cc @gengliangwang @dongjoon-hyun @viirya @huaxingao @sunchao @cloud-fan @HyukjinKwon ,"2,-1    ",2,-1,1
1520982569,6235869,2023/5/3,v3.2.4,The error messages don't seem related.,"1,-1    ",1,-1,0
1520990636,506679,2023/5/3,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1521009283,6235869,2023/5/3,v3.2.4,"Thanks for reviewing, @gengliangwang @sunchao @huaxingao @viirya!","1,-1    ",1,-1,0
1523816341,6477701,2023/5/3,v3.2.4,LGTM2,"1,-1    ",1,-1,0
1521106087,7322292,2023/5/3,v3.2.4,"thanks @LuciferYang for review, merged into master","1,-1    ",1,-1,0
1521207815,502522,2023/5/3,v3.2.4,"cc: @WweiL, @pengzhon-db, @LuciferYang, @grundprinzip, @amaliujia, @juliuszsompolski, @HyukjinKwon 

(Fairly broad CC since it adds caching at the service level and is relevant for future life cycle management for queries)","1,-1    ",1,-1,0
1521805579,1475305,2023/5/3,v3.2.4,"> Solution: This PR adds SparkConnectStreamingQueryCache that does not the following:

does not the following?
","1,-1    ",1,-1,0
1523777137,6477701,2023/5/3,v3.2.4,cc @HeartSaVioR too,"2,-1    ",2,-1,1
1527975880,502522,2023/5/3,v3.2.4,"@amaliujia, could you also take a quick look? I would like to get this merged today. ","2,-2    ",2,-2,0
1528550625,6477701,2023/5/3,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1522820728,7322292,2023/5/3,v3.2.4,merged to master,"1,-1    ",1,-1,0
1522737555,7322292,2023/5/3,v3.2.4,merged to master,"1,-1    ",1,-1,0
1521386648,1475305,2023/5/4,v3.2.4,"Test first, will update pr description laster","1,-1    ",1,-1,0
1522710911,822522,2023/5/4,v3.2.4,Merged to master,"1,-3    ",1,-3,-2
1522745134,1475305,2023/5/4,v3.2.4,Thanks @srowen ,"1,-1    ",1,-1,0
1522630835,7322292,2023/5/4,v3.2.4,merged to master,"1,-1    ",1,-1,0
1536432593,822522,2023/5/4,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1537661915,1475305,2023/5/4,v3.2.4,Thanks @srowen ,"1,-1    ",1,-1,0
1522732825,7322292,2023/5/4,v3.2.4,"after adding `set -ex`, the output will be like:

```
~/spark$ ./dev/protobuf-breaking-changes-check.sh branch-3.4
+ [[ 1 -gt 1 ]]
+++ dirname ./dev/protobuf-breaking-changes-check.sh
++ cd ./dev/..
++ pwd
+ SPARK_HOME=/home/ruifeng.zheng/spark
+ cd /home/ruifeng.zheng/spark
+ BRANCH=master
+ [[ 1 -eq 1 ]]
+ BRANCH=branch-3.4
+ pushd connector/connect/common/src/main
~/spark/connector/connect/common/src/main ~/spark
+ echo 'Start protobuf breaking changes checking against branch-3.4'
Start protobuf breaking changes checking against branch-3.4
+ buf breaking --against https://github.com/apache/spark.git#branch=branch-3.4,subdir=connector/connect/common/src/main
+ echo 'Finsh protobuf breaking changes checking: SUCCESS'
Finsh protobuf breaking changes checking: SUCCESS
+ [[ 0 -ne -0 ]]
```","1,-1    ",1,-1,0
1522813797,7322292,2023/5/4,v3.2.4,cc @HyukjinKwon  @grundprinzip ,"2,-1    ",2,-1,1
1523805734,6477701,2023/5/4,v3.2.4,Thanks @zhengruifeng ,"2,-1    ",2,-1,1
1523805967,6477701,2023/5/4,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1521887284,1145830,2023/5/4,v3.2.4,"Great feature. For example, based on statistics, users can determine whether to merge small files.
Support non-partitioned table statistics?
","2,-1    ",2,-1,1
1521969391,10897625,2023/5/4,v3.2.4,"> Support non-partitioned table statistics?

Yeap, it is supported in this pr.
","1,-1    ",1,-1,0
1546299221,506679,2023/5/4,v3.2.4,"Thanks, merged to master. Test failure unrelated. ","1,-1    ",1,-1,0
1546300263,1475305,2023/5/4,v3.2.4,Thanks @sunchao @mridulm @pan3793 ,"1,-1    ",1,-1,0
1546308609,9700541,2023/5/4,v3.2.4,Thank you all!,"1,-1    ",1,-1,0
1521922867,790409,2023/5/4,v3.2.4,@cloud-fan ,"1,-1    ",1,-1,0
1521940319,1475305,2023/5/4,v3.2.4,"Don't need to regenerate the golden file?

","1,-1    ",1,-1,0
1521982458,790409,2023/5/4,v3.2.4,"No need, test results are the same and the config statements don't get output to the golden file. I ran the test and it passed.","1,-1    ",1,-1,0
1522753302,3182036,2023/5/4,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1530732071,3182036,2023/5/4,v3.2.4,"`FileMetadataStructSuite.metadata struct (json): read partial/all metadata struct fields` fails, @databricks-david-lewis ","1,-1    ",1,-1,0
1534129513,39497924,2023/5/4,v3.2.4,"Oof. This is because of the following:
```
val p1 = new Path(""file:///a"")
val p2 = new Path(""file:/a"")
p1 == p2
p1.toString == p2.toString
p1.toUri == p2.toUri
// but...
p1.toUri.toString != p2.toUri.toString  // ""file:///a"" != ""file:/a""
```

Before any of my changes, we were doing `new Path(uriString).toString` which would ""normalize"" the uri string.
Should we do the same here? Seems silly...
I could also update the  test to explicitly pass in `file:/...` ...

Thoughts @cloud-fan ?","1,-1    ",1,-1,0
1534179161,3182036,2023/5/4,v3.2.4,@databricks-david-lewis let's just update the test,"1,-1    ",1,-1,0
1534989116,39497924,2023/5/4,v3.2.4,Updated @cloud-fan. How do we merge this to the 3.4 branch as well?,"1,-1    ",1,-1,0
1535620328,3182036,2023/5/4,v3.2.4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1522156622,1475305,2023/5/4,v3.2.4,Is there a way to reproduce the failure like GA? I always run successfully using Java 17 locally w/o this pr.,"1,-1    ",1,-1,0
1522175210,21010250,2023/5/4,v3.2.4,"@LuciferYang To repro, I set `JAVA_HOME` to `/usr/lib/jvm/java-17-openjdk-amd64/` and made sure that sbt is picking this settings up (these lines should be in the log `Using /usr/lib/jvm/java-17-openjdk-amd64/ as default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.`)
This was enough to trigger the failure:
<img width=""1124"" alt=""Screenshot 2023-04-25 at 21 42 59"" src=""https://user-images.githubusercontent.com/21010250/234358778-5de172ae-d8fe-4d54-bf2e-8d76dff1555b.png"">
","1,-1    ",1,-1,0
1522177022,1475305,2023/5/4,v3.2.4,"> @LuciferYang To repro, I set `JAVA_HOME` to `/usr/lib/jvm/java-17-openjdk-amd64/` and made sure that sbt is picking this settings up (these lines should be in the log `Using /usr/lib/jvm/java-17-openjdk-amd64/ as default JAVA_HOME. Note, this will be overridden by -java-home if it is set.`) This was enough to trigger the failure: <img alt=""Screenshot 2023-04-25 at 21 42 59"" width=""1124"" src=""https://user-images.githubusercontent.com/21010250/234358778-5de172ae-d8fe-4d54-bf2e-8d76dff1555b.png"">

Let me give it a try ~ thanks ~","1,-1    ",1,-1,0
1522283670,9616802,2023/5/4,v3.2.4,Merging this unblock JDK 17 build.,"1,-1    ",1,-1,0
1525810649,28668597,2023/5/4,v3.2.4,@MaxGekk could you help review this?,"1,-1    ",1,-1,0
1533408418,1580697,2023/5/5,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @amousavigourabi.","1,-1    ",1,-1,0
1533415423,1580697,2023/5/5,v3.2.4,@amousavigourabi Congratulations with your first contribution to Apache Spark!,"2,-1    ",2,-1,1
1522647461,12025282,2023/5/5,v3.2.4,cc @cloud-fan ,"1,-1    ",1,-1,0
1522750138,12025282,2023/5/5,v3.2.4,"@cloud-fan , it happened since https://github.com/apache/spark/pull/32198 and with concurrent writer on.","2,-1    ",2,-1,1
1527067186,12025282,2023/5/5,v3.2.4,@cloud-fan any comments ?,"1,-1    ",1,-1,0
1545099429,9700541,2023/5/5,v3.2.4,"cc @mridulm , too.","1,-2    ",1,-2,-1
1548843215,3182036,2023/5/5,v3.2.4,"thanks, merging to master/3.4!","1,-2    ",1,-2,-1
1532882783,32387433,2023/5/5,v3.2.4,"@cloud-fan @MaxGekk @hvanhovell Hi, PTAL. Thanks!","1,-1    ",1,-1,0
1524348310,7322292,2023/5/5,v3.2.4,also cc @cloud-fan @hvanhovell @ueshin ,"1,-1    ",1,-1,0
1533247038,1580697,2023/5/5,v3.2.4,"Waiting for CI. @liang3zy22 Could you re-trigger GitHub actions:
```
$ git commit --allow-empty -m ""Trigger build""
```","1,-1    ",1,-1,0
1534179289,1580697,2023/5/5,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @liang3zy22.","1,-1    ",1,-1,0
1523683722,9616802,2023/5/5,v3.2.4,@LuciferYang can we just move the RemoteClassLoader to spark/core?,"1,-1    ",1,-1,0
1523822735,1475305,2023/5/5,v3.2.4,"SGTM,  Let GA test it first","1,-1    ",1,-1,0
1523834900,1475305,2023/5/5,v3.2.4,"@hvanhovell Is it better to keep `ExecutorClassLoader` in `org.apache.spark.repl` or move it to other package, like `org.apache.spark.executor`?



","2,-1    ",2,-1,1
1523845845,1475305,2023/5/5,v3.2.4,"will update pr title and description later if all test pass

","3,-1    ",3,-1,2
1523896577,9616802,2023/5/5,v3.2.4,Yeah let's put it in `org.apache.spark.executor`.,"1,-1    ",1,-1,0
1523898344,9616802,2023/5/5,v3.2.4,@LuciferYang can you also get rid of the reflection in Executor.scala that used to be needed?,"3,-1    ",3,-1,2
1524292143,1475305,2023/5/5,v3.2.4,"> @LuciferYang can you also get rid of the reflection in Executor.scala that used to be needed?

I'll look this today, and I think there should be no need to use reflection anymore.

","1,-1    ",1,-1,0
1538384048,9616802,2023/5/5,v3.2.4,@LuciferYang is this one good to go?,"2,-1    ",2,-1,1
1538515358,1475305,2023/5/5,v3.2.4,I think this one is ok,"1,-1    ",1,-1,0
1538524857,9616802,2023/5/5,v3.2.4,Alrighty,"3,-1    ",3,-1,2
1538526427,9616802,2023/5/5,v3.2.4,Merged!,"2,-1    ",2,-1,1
1538528098,1475305,2023/5/5,v3.2.4,Thanks @hvanhovell ~,"2,-1    ",2,-1,1
1523041923,36835301,2023/5/5,v3.2.4,"@MaxGekk hi, could you help to review this?","1,-2    ",1,-2,-1
1525154687,36835301,2023/5/5,v3.2.4,"@MaxGekk Because scala 2.13 mandates handling default case, I replaced this error class with internal error. But `commonNaturalJoinProcessing` is a private method and I can't trigger the default case, so I didn't add any ut. Could you please help me to review this?","1,-1    ",1,-1,0
1525534890,1580697,2023/5/5,v3.2.4,"@JinHelin404 Do you have an account the JIRA: https://issues.apache.org/jira/browse/SPARK-43257. If not, could you leave a request, or leave a comment if you have one.","1,-1    ",1,-1,0
1525552908,36835301,2023/5/5,v3.2.4,"@MaxGekk Yes, and I've just left a comment.","3,-1    ",3,-1,2
1525569182,1580697,2023/5/5,v3.2.4,"+1, LGTM. Merged to master.
Thank you, @JinHelin404 and congratulations with your first contribution to Apache Spark!","1,-1    ",1,-1,0
1525713232,36835301,2023/5/5,v3.2.4,@MaxGekk Thanks!,"1,-1    ",1,-1,0
1547833963,1317309,2023/5/5,v3.2.4,"Could you please file a new JIRA ticket, or add JIRA ticket number as the prefix of PR title? You can see examples https://github.com/apache/spark/pulls.","1,-1    ",1,-1,0
1553740645,94015493,2023/5/5,v3.2.4,@bogao007 Can you add some tests and update PR description ?,"1,-1    ",1,-1,0
1555381692,502522,2023/5/5,v3.2.4,"> Not tested yet, will perform the test when I'm back.

Is this tested yet? Could you update the PR description?","2,-1    ",2,-1,1
1523523485,3182036,2023/5/5,v3.2.4,cc @gengliangwang ,"1,-1    ",1,-1,0
1524530055,3182036,2023/5/5,v3.2.4,"thanks for the review, merging to master/3.4/3.3!","2,-1    ",2,-1,1
1535272732,1475305,2023/5/5,v3.2.4,"Thanks for your review @srowen. I am on vacation and will continue to complete these tasks after May 8th.

________________________________
åä»¶ï¿½? Sean Owen ***@***.***>
åéæ¶ï¿½? 2023ï¿½?ï¿½?ï¿½?22:57:27
æ¶ä»¶ï¿½? apache/spark
æï¿½? yangjie01; Author
ä¸»é¢: Re: [apache/spark] [SPARK-43294][BUILD] Upgrade zstd-jni to 1.5.5-2 (PR #40962)


@srowen approved this pull request.

Looks OK, just add context about what the update does

ï¿½?
Reply to this email directly, view it on GitHub<https://mailshield.baidu.com/check?q=4%2fgpZ9TUEilYD9%2fXl%2f%2fdZ08xybHnY39K5GNnRidojzsXOcQrY4G74%2ftGPWda%2f0bG4rBPHY0ERfSeAWNQe0IWpFuPmjDRr01b0B39Jw%3d%3d>, or unsubscribe<https://mailshield.baidu.com/check?q=TboBl7gBc9ssIw%2fYtfvVtyVmwybmo1xowOHIEtt99hxrTkRQwwlxhTAqm6PKe8JFikz38E%2brBzEwW4wnNg2bfgrGkPmhNQuOrjnb8SXet2ZBamJzhIHTAlf6q8NZ5jsCpAi%2f6Nn5zu0%3d>.
You are receiving this because you authored the thread.Message ID: ***@***.***>
","2,-2    ",2,-2,0
1536436115,822522,2023/5/5,v3.2.4,"The changes are minor:
https://github.com/luben/zstd-jni/compare/v1.5.5-1...v1.5.5-2
I think it's fine.","2,-2    ",2,-2,0
1537662428,1475305,2023/5/5,v3.2.4,"Thanks @dongjoon-hyun @srowen , I am checking the microbenchmark results and updating this pr later

","1,-1    ",1,-1,0
1542520365,9700541,2023/5/5,v3.2.4,"Since the one failure seems to be flaky one, we can ignore it.
```
[info] *** 1 TEST FAILED ***
[error] Failed: Total 3638, Failed 1, Errors 0, Passed 3637, Ignored 10, Canceled 2
[error] Failed tests:
[error] 	org.apache.spark.scheduler.HealthTrackerIntegrationSuite
```","1,-1    ",1,-1,0
1543323741,9700541,2023/5/5,v3.2.4,Please make a clean PR.,"1,-1    ",1,-1,0
1543535713,1475305,2023/5/5,v3.2.4,"> Please make a clean PR.

OK, Let me make a new one.

","1,-1    ",1,-1,0
1543546046,1475305,2023/5/5,v3.2.4,@dongjoon-hyun I open a new one : https://github.com/apache/spark/pull/41135,"1,-1    ",1,-1,0
1526765814,1883812,2023/5/5,v3.2.4,"Fixing test failures in HiveDDLSuite, HiveCatalogedDDLSuite, SQLQueryTestSuite","2,-2    ",2,-2,0
1538927241,1883812,2023/5/5,v3.2.4,Thanks @Hisoka-X for comments. Looking ...,"1,-1    ",1,-1,0
1539311166,7322292,2023/5/5,v3.2.4,"LGTM, merged to master","1,-1    ",1,-1,0
1524332211,7322292,2023/5/5,v3.2.4,merged into master,"1,-1    ",1,-1,0
1523950046,10248890,2023/5/5,v3.2.4,@rangadi @pengzhon-db @amaliujia Can you guys take a look? TY!,"1,-1    ",1,-1,0
1523950470,10248890,2023/5/5,v3.2.4,@allisonwang-db Could you also take a look? Thanks!,"3,-1    ",3,-1,2
1524033540,3676078,2023/5/5,v3.2.4,cc @HyukjinKwon @mengxr @WeichenXu123 ,"1,-1    ",1,-1,0
1524585405,6477701,2023/5/5,v3.2.4,Mind taking a look at https://github.com/apache/spark/pull/40967/checks?check_run_id=13051337101?,"1,-1    ",1,-1,0
1525887988,3676078,2023/5/5,v3.2.4,"Hmm, looks like I might be stuck with [this issue](https://github.com/docker/build-push-action/issues/687), any ideas?  I've configured my ""actions permissions"" to match [this comment](https://github.com/docker/build-push-action/issues/687#issuecomment-1238980158), but I'm still getting this error:
```
Error: buildx failed with: ERROR: failed to solve: failed to push ghcr.io/leewyang/apache-spark-ci-image:master-4815979720: failed commit on ref ""manifest-sha256:80eca005ea656d063e07f9059619043cd701e0c0d17029523fc167e4915405b4"": unexpected status: 403 Forbidden
```","1,-1    ",1,-1,0
1525941689,3676078,2023/5/5,v3.2.4,"Nm, got past this issue... waiting on the rest of the build results (but already failed the [Kubernetes Integration Test](https://github.com/leewyang/spark/actions/runs/4815979720/jobs/8587955821)).","1,-1    ",1,-1,0
1526142286,6477701,2023/5/5,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1526176743,6477701,2023/5/5,v3.2.4,"oops, I missed that the linter failed. Reverting, and reopening the PR.","2,-1    ",2,-1,1
1526819976,7322292,2023/5/5,v3.2.4,merged to master again,"1,-1    ",1,-1,0
1524045898,10248890,2023/5/5,v3.2.4,@rangadi @pengzhon-db @zhenlineo Can you guys take a look? Thank you!,"1,-1    ",1,-1,0
1528421226,10248890,2023/5/5,v3.2.4,@HyukjinKwon Can you merge this? Thanks!,"1,-2    ",1,-2,-1
1536807767,60895808,2023/5/5,v3.2.4,"> @sweisdb Could you remove the error class `AES_SALTED_MAGIC` from `error-classes.json`, and related function `aesInvalidSalt()` + the test ""INVALID_PARAMETER_VALUE.AES_SALTED_MAGIC: AES decrypt failure - invalid salt""

Updated to address this.","1,-1    ",1,-1,0
1537132762,1580697,2023/5/5,v3.2.4,"@sweisdb Could you re-trigger GitHub actions, please. Highly likely the failure of the test org.apache.spark.storage.BlockManagerBasicStrategyReplicationSuite is not related to your changes but just in case let's re-run all tests.","1,-1    ",1,-1,0
1539009810,60895808,2023/5/5,v3.2.4,"@MaxGekk Sure, that worked and it's all green now.","1,-1    ",1,-1,0
1539725033,1580697,2023/5/5,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @sweisdb.","3,-1    ",3,-1,2
1539729811,1580697,2023/5/5,v3.2.4,@sweisdb Congratulations with your first contribution to Apache Spark!,"1,-1    ",1,-1,0
1549417703,1580697,2023/5/5,v3.2.4,"@sweisdb Could you fix the code style issues:
```
Checkstyle checks failed at following occurrences:
Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[56] (sizes) LineLength: Line is longer than 100 characters (found 114).
Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[117] (sizes) LineLength: Line is longer than 100 characters (found 121).
Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[118] (sizes) LineLength: Line is longer than 100 characters (found 102).
Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[121] (sizes) LineLength: Line is longer than 100 characters (found 110).
Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[122] (sizes) LineLength: Line is longer than 100 characters (found 104).
```","1,-1    ",1,-1,0
1550789731,1580697,2023/5/5,v3.2.4,"> This change adds support for optional IV and AAD fields to aes_encrypt and aes_decrypt

@sweisdb Looking at the constructors of the `AesEncrypt` and `AesDecrypt` expressions, they still don't support new parameters. Are you going to update the expressions, correct? ","1,-1    ",1,-1,0
1551795385,60895808,2023/5/6,v3.2.4,@MaxGekk I am planning to doing the user-facing SQL expression changes in a followup to make each change more simple. I want to land this first.,"1,-1    ",1,-1,0
1556111345,1580697,2023/5/6,v3.2.4,"> I want to land this first.

ok. Let's modify PR's title and its description according to your actual changes.","1,-3    ",1,-3,-2
1557535698,60895808,2023/5/6,v3.2.4,I updated the description to clarify that this change is just for `ExpressionImplUtilsSuite` and doesn't expose the user-facing changes in `aes_encrypt` and `aes_decrypt` yet.,"1,-1    ",1,-1,0
1557556926,1580697,2023/5/6,v3.2.4,"> this change is just for ExpressionImplUtilsSuite 
> Adds AES IV and AAD support to ExpressionImplUtilsSuite

@sweisdb Please, replace ExpressionImplUtilsSuite by ExpressionImplUtils. *Suite is the suffix of test suites but you changed not only tests.","1,-1    ",1,-1,0
1557580752,60895808,2023/5/6,v3.2.4,"> > this change is just for ExpressionImplUtilsSuite
> > Adds AES IV and AAD support to ExpressionImplUtilsSuite
> 
> @sweisdb Please, replace ExpressionImplUtilsSuite by ExpressionImplUtils. *Suite is the suffix of test suites but you changed not only tests.

Doh. Done. Thank you.","1,-1    ",1,-1,0
1557847010,1580697,2023/5/6,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @sweisdb.","2,-1    ",2,-1,1
1535638070,3898450,2023/5/6,v3.2.4,"@Ngone51 @attilapiros 
Please help to review, thanks.","1,-1    ",1,-1,0
1537100335,1591700,2023/5/6,v3.2.4,+CC @otterc ,"1,-1    ",1,-1,0
1558452263,3898450,2023/5/6,v3.2.4,"@cloud-fan  @dongjoon-hyun  @HyukjinKwon 

Please help to review, thanks.","2,-1    ",2,-1,1
1530765993,7322292,2023/5/6,v3.2.4,merged to master,"1,-1    ",1,-1,0
1535626787,7322292,2023/5/6,v3.2.4,merged to master,"1,-1    ",1,-1,0
1530768350,7322292,2023/5/6,v3.2.4,merged to master,"2,-1    ",2,-1,1
1525564963,32387433,2023/5/6,v3.2.4,cc @cloud-fan ,"2,-1    ",2,-1,1
1531705235,9700541,2023/5/6,v3.2.4,"I fixed the PR title.
```
- [SPARK-43156][SQL][3.3] Fix COUNT(*) is null bug in correlated scalar subquery
+ [SPARK-43156][SQL][3.4] Fix COUNT(*) is null bug in correlated scalar subquery
```","3,-3    ",3,-3,0
1531709600,9700541,2023/5/6,v3.2.4,cc @sunchao and @parthchandra ,"4,-1    ",4,-1,3
1526845232,44179472,2023/5/6,v3.2.4,"The test failure in GitHub Actions ""track allocated resources by taskId"" seems to be unrelated to this change. ","1,-1    ",1,-1,0
1526857081,44179472,2023/5/6,v3.2.4,"@cloud-fan, @srielau, could you take a look at this?","1,-1    ",1,-1,0
1526917327,3182036,2023/5/6,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1535773899,3182036,2023/5/6,v3.2.4,@MaxGekk sub error class can't have dedicated sql state.,"2,-2    ",2,-2,0
1535827414,1580697,2023/5/6,v3.2.4,"> sub error class can't have dedicated sql state.

ok, but `INTERNAL_ERROR_BROADCAST` and `INTERNAL_ERROR` have the same sql state in the PR. Just in case, can't we override the sql state in sub-classes (are there any principal issues of implementing this?). ","1,-1    ",1,-1,0
1535851229,44179472,2023/5/6,v3.2.4,Another advantage is that having `INTERNAL_ERROR_BROADCAST` will make it possible to have subclasses for the broadcast package. ,"1,-1    ",1,-1,0
1526174238,4190164,2023/5/6,v3.2.4,cc @cloud-fan @hvanhovell This is how keyAs is impl at this moment.,"1,-1    ",1,-1,0
1526254611,100322362,2023/5/6,v3.2.4,@HeartSaVioR - please take a look. Thx,"1,-3    ",1,-3,-2
1529955932,100322362,2023/5/6,v3.2.4,Thanks @HeartSaVioR . @siying and @Kimahriman - could you folks please take a look ? Thx,"1,-1    ",1,-1,0
1530068162,1317309,2023/5/6,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1545902637,79601771,2023/5/6,v3.2.4,"PR description seems a bit vague, given that actual code comments give pretty clear reasoning for why this change is needed?

> Wrap cache in `CodeGenerator` as an example. Feel free to use this in other places where we used Guava cache and don't want fate-sharing behavior.

It's not just an ""example"" -- the code comments detail a specific bad behavior we're trying to avoid with this change.

> This fate sharing behavior might lead to unexpected results in some situation.

Again, we can be specific -- we know it _does_ lead to unexpected results (query canceled during codegen)
","3,-1    ",3,-1,2
1546113299,22358241,2023/5/6,v3.2.4,Hi @ryan-johnson-databricks would you mind triggering the merge for this PR?,"1,-1    ",1,-1,0
1546130844,79601771,2023/5/6,v3.2.4,"> Hi @ryan-johnson-databricks would you mind triggering the merge for this PR?

Sorry, I'm not a spark committer.","1,-1    ",1,-1,0
1548842285,22358241,2023/5/6,v3.2.4,Hi @JoshRosen would you mind merging this PR? Or maybe @cloud-fan could help?,"1,-1    ",1,-1,0
1548847605,50748,2023/5/6,v3.2.4,I've merged this to master (Spark 3.5.0). Thanks @liuzqt!,"1,-1    ",1,-1,0
1526534162,502522,2023/5/6,v3.2.4,"cc: @gengliangwang, @SandishKumarHN ","1,-1    ",1,-1,0
1535538870,1097932,2023/5/6,v3.2.4,"Thanks, merging to master.","1,-1    ",1,-1,0
1535556642,502522,2023/5/6,v3.2.4,Thanks @gengliangwang! ,"3,-1    ",3,-1,2
1530767027,7322292,2023/5/6,v3.2.4,merged to master,"1,-2    ",1,-2,-1
1536946146,6477701,2023/5/6,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1535625853,7322292,2023/5/6,v3.2.4,merged to master,"1,-1    ",1,-1,0
1528338027,506656,2023/5/7,v3.2.4,I'll move the conversion function `PandasConversionMixin._create_converter` to `pyspark/sql/pandas/types.py`.,"1,-3    ",1,-3,-2
1535194866,47337188,2023/5/7,v3.2.4,"Merged to master, thank you!","1,-1    ",1,-1,0
1537036149,3182036,2023/5/7,v3.2.4,seems scala style check failed,"1,-2    ",1,-2,-1
1538063706,3182036,2023/5/7,v3.2.4,there is a test failure in cte.sql,"1,-1    ",1,-1,0
1540663424,114445704,2023/5/7,v3.2.4,@gengliangwang could you help merge it? Thanks,"1,-1    ",1,-1,0
1541167278,3182036,2023/5/7,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1527324475,12025282,2023/5/7,v3.2.4,"cc @cloud-fan @viirya if you have time to review, thank you","1,-1    ",1,-1,0
1527860553,26535726,2023/5/7,v3.2.4,ping @dongjoon-hyun ,"3,-2    ",3,-2,1
1529359585,6477701,2023/5/7,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1530130870,3421,2023/5/7,v3.2.4,"Please update the title of the PR to 

`[SPARK-43332][CONNECT][PYTHON] Make it possible to extend ChannelBuilder for SparkConnectClient`","1,-2    ",1,-2,-1
1530585825,1938382,2023/5/7,v3.2.4,Can I ask when you need a customized channel? ,"2,-1    ",2,-1,1
1530831141,3421,2023/5/7,v3.2.4,You need it you want to pass specialized authentication handlers for GRPC in the form of credentials plugins. In addition it allows you to configure the GRPC channel with additional deployment specific options. ,"1,-1    ",1,-1,0
1531054320,6477701,2023/5/7,v3.2.4,@nfx Mind filing a JIRA please? See also https://spark.apache.org/contributing.html,"1,-1    ",1,-1,0
1532095560,3421,2023/5/7,v3.2.4,@HyukjinKwon @zhengruifeng please review! Thanks!,"1,-1    ",1,-1,0
1532246428,6477701,2023/5/7,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1531517451,26535726,2023/5/7,v3.2.4,cc @dongjoon-hyun and @Yikun ,"1,-2    ",1,-2,-1
1531646232,9700541,2023/5/7,v3.2.4,"Thank you for pinging me, @pan3793 .","1,-1    ",1,-1,0
1533284437,26535726,2023/5/7,v3.2.4,"> Since #41034 is merged, could you rebase this PR to the master branch, @pan3793 ?

Sure, rebased","1,-1    ",1,-1,0
1533285058,9700541,2023/5/7,v3.2.4,Thank you!,"2,-1    ",2,-1,1
1533499271,9700541,2023/5/7,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1527887019,26535726,2023/5/7,v3.2.4,cc @srowen @sunchao @AngersZhuuuu ,"1,-1    ",1,-1,0
1528802953,822522,2023/5/7,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1530130460,506679,2023/5/7,v3.2.4,Late LGTM. Was trying to +1 and merge this but @srowen beat me on it.,"2,-1    ",2,-1,1
1532135184,1097932,2023/5/7,v3.2.4,@dtenedor BTW could you mention the new trait in the PR description?,"1,-1    ",1,-1,0
1532513176,1097932,2023/5/7,v3.2.4,@dtenedor FYI there are test failures in the latest code.,"1,-1    ",1,-1,0
1532513562,1097932,2023/5/7,v3.2.4,cc @cloud-fan since this is DSV2 related.,"1,-1    ",1,-1,0
1533445192,99207096,2023/5/7,v3.2.4,"> @dtenedor FYI there are test failures in the latest code.

@gengliangwang thanks, fixed
","1,-1    ",1,-1,0
1535357400,1097932,2023/5/7,v3.2.4,"Thanks, merging to master/branch-3.4","1,-1    ",1,-1,0
1553467650,9700541,2023/5/7,v3.2.4,Thank you for reverting.,"1,-1    ",1,-1,0
1530538764,6477701,2023/5/7,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1543933328,13965087,2023/5/7,v3.2.4,"@EnricoMi @cloud-fan @dongjoon-hyun can you take a look , thanks ~","1,-1    ",1,-1,0
1543974004,44700269,2023/5/7,v3.2.4,What is the fallout of `committer.setupJob(job)` not being executed in presence of an error?,"1,-1    ",1,-1,0
1543993364,44700269,2023/5/7,v3.2.4,"I think Spark 3.2 is EOL, the final patch release was 3.2.4. a month ago. So this should target branch-3.3.

Note that a similar fix went into master and branch-3.4: #39431","1,-1    ",1,-1,0
1543998529,44700269,2023/5/7,v3.2.4,Is this fixing https://github.com/apache/spark/pull/38358#issuecomment-1455371977?,"1,-1    ",1,-1,0
1544216602,13965087,2023/5/7,v3.2.4,"> Is this fixing [#38358 (comment)](https://github.com/apache/spark/pull/38358#issuecomment-1455371977)?

yes ","1,-1    ",1,-1,0
1544239586,9700541,2023/5/7,v3.2.4,"Hi, @zzzzming95 .
According to Apache Spark versioning policy, Apache Spark 3.2 reached EOL already and 3.2.4 was the last one. As a result, we close all PRs against `branch-3.2`.
- https://spark.apache.org/versioning-policy.html","1,-1    ",1,-1,0
1544250003,13965087,2023/5/7,v3.2.4,"> Hi, @zzzzming95 . According to Apache Spark versioning policy, Apache Spark 3.2 reached EOL already and 3.2.4 was the last one. As a result, we close all PRs against `branch-3.2`.
> 
> * https://spark.apache.org/versioning-policy.html

OK , I see a similar implementation for Spark3.3, and I will submit it to Spark3.3.","1,-1    ",1,-1,0
1544321360,9700541,2023/5/8,v3.2.4,"@zzzzming95 . I'm not sure you are aware of Apache Spark backporting policy. To prevent a future regression, we start to review from `master` branch first. Then, backport it from `master` to `branch-3.4` to `branch-3.3`.
> OK , I see a similar implementation for Spark3.3, and I will submit it to Spark3.3.

","1,-1    ",1,-1,0
1529666648,1317309,2023/5/8,v3.2.4,cc. @cloud-fan @HyukjinKwon @zsxwing @viirya ,"1,-1    ",1,-1,0
1530918000,1317309,2023/5/8,v3.2.4,"Thanks, merging to master!","1,-1    ",1,-1,0
1530671123,6477701,2023/5/8,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1530062704,25019163,2023/5/8,v3.2.4,cc @hvanhovell @HyukjinKwon @grundprinzip ,"1,-1    ",1,-1,0
1532838872,32387433,2023/5/8,v3.2.4,"Hi, the jira ticket should be [SPARK-43331](https://issues.apache.org/jira/browse/SPARK-43331), not [SPARK-43267](https://issues.apache.org/jira/browse/SPARK-43267)","1,-1    ",1,-1,0
1532861488,25019163,2023/5/8,v3.2.4,Thanks @Hisoka-X ! I have no idea how I so randomly mistyped that number.,"1,-1    ",1,-1,0
1533279355,9616802,2023/5/8,v3.2.4,Merging to master thanks! Please address remaining comments in a couple of follow-ups.,"1,-1    ",1,-1,0
1530323184,1938382,2023/5/8,v3.2.4,LGTM,"1,-1    ",1,-1,0
1530405401,9700541,2023/5/8,v3.2.4,"Merged to `master`.
Thank you for the quick fix, @WweiL , @hvanhovell , @amaliujia .","1,-1    ",1,-1,0
1546358692,3514644,2023/5/8,v3.2.4,"@cloud-fan @gengliangwang @dtenedor Aside from my atrocious Scala skills, the code still needs comments. But It think it's ready for a review.
 ","1,-1    ",1,-1,0
1548631061,1097932,2023/5/8,v3.2.4,"@srielau I am rethinking the requirement after reading the related docs (especially [the doc from snowflake](https://docs.snowflake.com/en/sql-reference/identifier-literal))
So how important it is to support all kinds of expressions(e.g string concats) within the `IDENTIFIER()` clause? It would be much easier if we limit the requirement only accepts the following:
* quoted identifier
* session variable

So, instead of 
```
identifierReference
    : IDENTIFIER_KW LEFT_PAREN expression RIGHT_PAREN
    | multipartIdentifier
    ;
```
We can make it
```
identifierReference
    : IDENTIFIER_KW LEFT_PAREN '\'' multipartIdentifier  '\'' RIGHT_PAREN    #singleQuotedIdentifier
    | IDENTIFIER_KW LEFT_PAREN '""' multipartIdentifier  '""' RIGHT_PAREN      #doubleQuotedIdentifier
    | IDENTIFIER_KW LEFT_PAREN '$' multipartIdentifier RIGHT_PAREN           #sessionvariableIdentifier
    | multipartIdentifier                                                    #simpleMultipartIdentifier
    ;
```
And this requires much less changes in the parser and analyzer.
","1,-1    ",1,-1,0
1550353750,3514644,2023/5/8,v3.2.4,"> @srielau I am rethinking the requirement after reading the related docs (especially [the doc from snowflake](https://docs.snowflake.com/en/sql-reference/identifier-literal)) So how important it is to support all kinds of expressions(e.g string concats) within the `IDENTIFIER()` clause? It would be much easier if we limit the requirement only accepts the following:
> 
> * quoted identifier
> * session variable
> 
> So, instead of
> 
> ```
> identifierReference
>     : IDENTIFIER_KW LEFT_PAREN expression RIGHT_PAREN
>     | multipartIdentifier
>     ;
> ```
> 
> We can make it
> 
> ```
> identifierReference
>     : IDENTIFIER_KW LEFT_PAREN '\'' multipartIdentifier  '\'' RIGHT_PAREN    #singleQuotedIdentifier
>     | IDENTIFIER_KW LEFT_PAREN '""' multipartIdentifier  '""' RIGHT_PAREN      #doubleQuotedIdentifier
>     | IDENTIFIER_KW LEFT_PAREN '$' multipartIdentifier RIGHT_PAREN           #sessionvariableIdentifier
>     | multipartIdentifier                                                    #simpleMultipartIdentifier
>     ;
> ```
> 
> And this requires much less changes in the parser and analyzer.
This is similar to what I tried in my previous attempt: https://github.com/apache/spark/pull/40884
We need to support parameter markers as well as ""proper"" (session) variables which would not have a leading '$'.
They would just be identifiers which need to be resolved.
Also we need the ability for users to pre-fix and post-fix. For example a schema-name or a field name.
Without solid support for parameter markers (and variables) we are still open to SQL injection.
I was told that I cannot call eval from within the parser.

  
","1,-1    ",1,-1,0
1553709182,3514644,2023/5/8,v3.2.4,"@gengliangwang @dtenedor @gengliangwang 
It's camera ready now. Please have at it.
","1,-1    ",1,-1,0
1553732854,99207096,2023/5/8,v3.2.4,"@srielau @gengliangwang @cloud-fan 

The general structure of the PR looks OK.

This PR proposes to add new unresolved nodes for several different locations in the parser. This could be useful in the future if we want to add more custom analysis support for these different areas.

Alternatively, we could leave the multipartIdentifier references where they are, and just update its definition instead:

```
multipartIdentifier
    : IDENTIFIER_KW LEFT_PAREN expression RIGHT_PAREN
    | parts+=errorCapturingIdentifier (DOT parts+=errorCapturingIdentifier)*
    ;
```

This would reduce the number of changes to the .g4 file, since the proposed namespaceReference and functionNameReference and relationReference are all the same syntax. Then the PR would be easier to merge into different Spark forks out there. But this approach grants us future flexibility, if we ever anticipate the syntax to diverge for these different cases. I am OK with the proposed approach here.","1,-1    ",1,-1,0
1553790687,3514644,2023/5/8,v3.2.4,"@dtenedor 
There are still 30 multipartIdentifier usages that do NOT support IDENTIFIER() notation.
So we would trade mechanical churn in the grammar for code changes in AstBuilder et al.
","2,-1    ",2,-1,1
1555198846,99207096,2023/5/8,v3.2.4,"> @dtenedor
There are still 30 multipartIdentifier usages that do NOT support IDENTIFIER() notation.
So we would trade mechanical churn in the grammar for code changes in AstBuilder et al.

This is true, we can keep the grammar changes then.","1,-1    ",1,-1,0
1559445596,3514644,2023/5/8,v3.2.4,"@gengliangwang @cloud-fan  Can you take a peek with your error-context hat on?
As you see my adding withOrigin() into the wrapper has focused many of the contexts around the table names.
Sometimes that is quite right, sometimes that is debatable.
Did you add all those createUnresolved*() methods for the context? I suppose I can add them back. ","1,-1    ",1,-1,0
1560160303,1097932,2023/5/8,v3.2.4,"> @gengliangwang @cloud-fan Can you take a peek with your error-context hat on?
As you see my adding withOrigin() into the wrapper has focused many of the contexts around the table names.
Sometimes that is quite right, sometimes that is debatable.


So I am seeing code like 
```
        withIdentClause(relation, ident => {
          InsertIntoStatement(
            UnresolvedRelation(ident),
            partition,
            cols,
            query,
            overwrite = false,
            ifPartitionNotExists)
        })
```
The context `relation` will be applied on the whole `InsertIntoStatement`. I think what you want is on `UnresolvedRelation(ident)` only. 
","1,-1    ",1,-1,0
1560163077,3514644,2023/5/8,v3.2.4,"> > @gengliangwang @cloud-fan Can you take a peek with your error-context hat on?
> > As you see my adding withOrigin() into the wrapper has focused many of the contexts around the table names.
> > Sometimes that is quite right, sometimes that is debatable.
> 
> So I am seeing code like
> 
> ```
>         withIdentClause(relation, ident => {
>           InsertIntoStatement(
>             UnresolvedRelation(ident),
>             partition,
>             cols,
>             query,
>             overwrite = false,
>             ifPartitionNotExists)
>         })
> ```
> 
> The context `relation` will be applied on the whole `InsertIntoStatement`. I think what you want is on `UnresolvedRelation(ident)` only.

Yes.","3,-1    ",3,-1,2
1560166537,1097932,2023/5/8,v3.2.4,"Yeah, so we can have code like
```
val unresolvedRelation = withOrigin(ctx) {
  UnresolvedRelation(ident)
}
 InsertIntoStatement(
            unresolvedRelation,
            partition,
            cols,
            query,
            overwrite = false,
            ifPartitionNotExists)
```","1,-1    ",1,-1,0
1565008904,1097932,2023/5/8,v3.2.4,Thanks for the great work! Merging to master!,"1,-1    ",1,-1,0
1530758059,6477701,2023/5/8,v3.2.4,cc @amaliujia @cloud-fan @hvanhovell FYI,"1,-1    ",1,-1,0
1530964421,6477701,2023/5/8,v3.2.4,Merged to master.,"1,-3    ",1,-3,-2
1530963411,6477701,2023/5/8,v3.2.4,Merged to master.,"1,-3    ",1,-3,-2
1532081510,1097932,2023/5/8,v3.2.4,@MaxGekk Thanks for the reivew. Merging to master/branch-3.4,"3,-1    ",3,-1,2
1530992893,52679095,2023/5/8,v3.2.4,"@srowen @gengliangwang 
Address issue mentioned in https://github.com/apache/spark/pull/36226#issuecomment-1530789474","2,-1    ",2,-1,1
1531983883,52679095,2023/5/8,v3.2.4,"> Is this the only way to fix the issue, @maytasm ?

No. We can also keep the current DataTables version 1.10.25 and modify the jquery.dataTables.1.10.25.min.css file by removing t`!important` i.e. 
`""../images/sort_asc.png"") !important` to `""../images/sort_asc.png"")`","1,-1    ",1,-1,0
1531984471,52679095,2023/5/8,v3.2.4,"> Please remove the following from the PR description, @maytasm .
> 
> > The 1.10.25 version of DataTables, that Spark currently uses, seems vulerable: https://security.snyk.io/package/npm/datatables.net/1.10.25. The vulnerability may not affect Spark but it is safer to update.

Done","1,-1    ",1,-1,0
1533986641,822522,2023/5/8,v3.2.4,@dongjoon-hyun WDYT? I'm neutral on one change vs the other. I suppose modifying the CSS is a smaller change,"2,-1    ",2,-1,1
1534029673,9700541,2023/5/8,v3.2.4,"I prefer the minimal change instead of this PR because there is no easy way for us to guarantee that new version has no regression somewhere. To be honest, actually, it's difficult for me to make it sure.

Anyway, I will not be against this PR too if people are sure about this new version.","1,-1    ",1,-1,0
1535659557,52679095,2023/5/8,v3.2.4,"@srowen @dongjoon-hyun I don't have strong opinion either way too. However, upon thinking about it more, I am leaning toward minimal CSS change without upgrading DataTables library for the following reason:
- DataTables version upgraded to 1.13.2 is only in master branch and has not been part of any release (i.e. 3.3, 3.4) yet. Making it risky
- I am not familiar with DataTables library and have no idea about the changes between the current version and 1.13.2
- The CSS change is very simple, easy to understand, and has a very minimal surface area.  

I will put up a PR for the CSS change but will leave this PR open for a bit longer in case anyone feel strongly for upgrading DataTables to 1.13.2","1,-1    ",1,-1,0
1535811085,52679095,2023/5/8,v3.2.4,"@srowen @dongjoon-hyun 
Here is a simple CSS fix instead: https://github.com/apache/spark/pull/41061","1,-1    ",1,-1,0
1536422306,822522,2023/5/8,v3.2.4,Yeah let's go with your alternative,"1,-1    ",1,-1,0
1530992792,52679095,2023/5/8,v3.2.4,"@srowen @gengliangwang 
Address issue mentioned in https://github.com/apache/spark/pull/36226#issuecomment-1530789474","2,-1    ",2,-1,1
1531534005,822522,2023/5/8,v3.2.4,I agree with back-porting. We just need to get tests to pass. Did the UI seem OK in a quick manual test? just want to verify the arrows work.,"1,-1    ",1,-1,0
1531988614,52679095,2023/5/8,v3.2.4,"> I agree with back-porting. We just need to get tests to pass. Did the UI seem OK in a quick manual test? just want to verify the arrows work.

Do you know why the `On pull request update / Notify test workflow` failed? Looks like all the tests passed on https://github.com/apache/spark/pull/41011","1,-1    ",1,-1,0
1531988820,52679095,2023/5/8,v3.2.4,"> BTW, please remove this from the PR description, @maytasm .
> 
> > The 1.10.25 version of DataTables, that Spark currently uses, seems vulerable: https://security.snyk.io/package/npm/datatables.net/1.10.25. The vulnerability may not affect Spark but it is safer to update.

done","2,-1    ",2,-1,1
1531989351,52679095,2023/5/8,v3.2.4,"> Is this the only way to fix the issue, @maytasm ?

https://github.com/apache/spark/pull/41011#issuecomment-1531983883","2,-1    ",2,-1,1
1535811262,52679095,2023/5/8,v3.2.4,"@srowen @dongjoon-hyun 
Here is a simple CSS fix instead: https://github.com/apache/spark/pull/41060","1,-1    ",1,-1,0
1536422538,822522,2023/5/8,v3.2.4,Yeah let's go with your alternative,"2,-1    ",2,-1,1
1549950169,1938382,2023/5/8,v3.2.4,LGTM,"1,-2    ",1,-2,-1
1550847776,6477701,2023/5/8,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1552174756,506656,2023/5/8,v3.2.4,"Hi, `./bin/pyspark --remote local` shows the following error after this commit.

```py
% ./bin/pyspark --remote local
...
Traceback (most recent call last):
  File ""/.../python/pyspark/shell.py"", line 78, in <module>
    sc = spark.sparkContext
  File ""/.../python/pyspark/sql/connect/session.py"", line 567, in sparkContext
    raise PySparkNotImplementedError(
pyspark.errors.exceptions.base.PySparkNotImplementedError: [NOT_IMPLEMENTED] sparkContext() is not implemented.
```","1,-1    ",1,-1,0
1552230867,6477701,2023/5/8,v3.2.4,creating a followup now,"1,-1    ",1,-1,0
1552233690,6477701,2023/5/8,v3.2.4,https://github.com/apache/spark/pull/41206,"1,-1    ",1,-1,0
1531288054,6477701,2023/5/8,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1532727011,1580697,2023/5/8,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @juliuszsompolski and @grundprinzip @nija-at for review.","1,-1    ",1,-1,0
1537627334,6477701,2023/5/8,v3.2.4,cc @MaxGekk FYI,"2,-1    ",2,-1,1
1549099812,1591700,2023/5/8,v3.2.4,"@thejdeep, can you fix the conflicts please ?","1,-2    ",1,-2,-1
1549102618,1591700,2023/5/8,v3.2.4,"+CC @AngersZhuuuu who last worked on this.
Also +CC @srowen who reviewed the previous changes.","1,-2    ",1,-2,-1
1561383823,1708757,2023/5/8,v3.2.4,"@srowen Thanks for the review! Updated the PR after addressing the comments, can you please take another pass ?","2,-1    ",2,-1,1
1562049449,822522,2023/5/8,v3.2.4,Merged to master,"3,-1    ",3,-1,2
1541941861,1580697,2023/5/8,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @NarekDW.","2,-1    ",2,-1,1
1532448727,9700541,2023/5/8,v3.2.4,"Thank you, @bjornjorgensen and @HyukjinKwon .
Merged to master for Apache Spark 3.5.0.","1,-1    ",1,-1,0
1533310260,47337188,2023/5/8,v3.2.4,Thank you @bjornjorgensen @dongjoon-hyun !,"1,-1    ",1,-1,0
1532065028,12103644,2023/5/8,v3.2.4,cc: @MaxGekk do I need to separate this into multiple PRs (one for each error message)?,"1,-1    ",1,-1,0
1554182276,1580697,2023/5/8,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @imback82.","2,-1    ",2,-1,1
1532656290,6477701,2023/5/8,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1532239242,5334567,2023/5/8,v3.2.4,@HeartSaVioR will you have time to take a look?,"1,-3    ",1,-3,-2
1539068615,5334567,2023/5/8,v3.2.4,"> Nice finding! Looks great in overall - just to see whether we can still read a single file containing special character via escaping it. I'd OK with it if it does not work in any way (Hadoop side bug or something) but better to confirm.

 Thanks for your review and suggestion. I gave it a try ant it looks like ""\["" works. I updated the PR.
","1,-1    ",1,-1,0
1539163892,5334567,2023/5/8,v3.2.4,Not sure why one test ran for 4+ hours and it still didn't succeed or fail.,"1,-1    ",1,-1,0
1539170299,1317309,2023/5/8,v3.2.4,It seems like the actual test run is only 2 hours ago - check out the full log via `view raw logs`.,"1,-1    ",1,-1,0
1539284464,5334567,2023/5/8,v3.2.4,"Failed tests don't seem to be related at all:
```
2023-05-08T23:33:54.8173143Z [0m[[0m[31merror[0m] [0m[0mFailed tests:[0m
2023-05-08T23:33:54.8173854Z [0m[[0m[31merror[0m] [0m[0m	org.apache.spark.sql.connect.client.SparkConnectClientSuite[0m
2023-05-08T23:33:54.8805941Z [0m[[0m[31merror[0m] [0m[0m(connect-client-jvm / Test / [31mtest[0m) sbt.TestsFailedException: Tests unsuccessful[0m
```","1,-1    ",1,-1,0
1539357874,1317309,2023/5/8,v3.2.4,"I see all tests passed; https://github.com/siying/spark/actions/runs/4920444751

Thanks! Merging to master.","1,-1    ",1,-1,0
1532244290,9700541,2023/5/8,v3.2.4,"Thank you, @HyukjinKwon !","1,-1    ",1,-1,0
1532265336,9700541,2023/5/9,v3.2.4,"Thank you, @ueshin .","1,-1    ",1,-1,0
1533095491,9700541,2023/5/9,v3.2.4,Rebased to master since https://github.com/apache/spark/pull/41024 is merged.,"1,-1    ",1,-1,0
1533496469,9700541,2023/5/9,v3.2.4,"All tests passed and there is no change in code during rebasing. Merged to master for Apache Spark 3.5.0.
Thank you again, @HyukjinKwon and @ueshin .","1,-1    ",1,-1,0
1532353812,9700541,2023/5/9,v3.2.4,"Thank you, @HyukjinKwon .","1,-1    ",1,-1,0
1532423123,9700541,2023/5/9,v3.2.4,"Thank you, @Yikun .","1,-1    ",1,-1,0
1532449675,9700541,2023/5/9,v3.2.4,"Could you review once more, @HyukjinKwon and @Yikun . 
I found that we didn't support Python 3.8+ in PyPy3 environment. 
I converted this PR from `INFRA` to `PYTHON` module in both the JIRA and here.","1,-1    ",1,-1,0
1532617421,9700541,2023/5/9,v3.2.4,"Thank you, @HyukjinKwon !","1,-1    ",1,-1,0
1533084625,9700541,2023/5/9,v3.2.4,"Merged to master. Thank you all!
The last commit was disabling a single test case.","1,-1    ",1,-1,0
1532305905,44108233,2023/5/9,v3.2.4,cc @ueshin ,"1,-1    ",1,-1,0
1532806369,6477701,2023/5/9,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1533441031,10248890,2023/5/9,v3.2.4,@HyukjinKwon @HeartSaVioR @xinrong-meng @rangadi @pengzhon-db @amaliujia Can you guys take a look? Thanks!,"1,-1    ",1,-1,0
1550823334,6477701,2023/5/9,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1533132542,6235869,2023/5/9,v3.2.4,cc @dongjoon-hyun @viirya @huaxingao @cloud-fan @sunchao,"1,-1    ",1,-1,0
1533339930,9700541,2023/5/9,v3.2.4,"Thank you for pinging me, @aokolnychyi .","1,-1    ",1,-1,0
1537334840,6235869,2023/5/9,v3.2.4,"Thank you, @dongjoon-hyun @cloud-fan!","1,-2    ",1,-2,-1
1532562199,26535726,2023/5/9,v3.2.4,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1532808982,6477701,2023/5/9,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1534935590,822522,2023/5/9,v3.2.4,"Looks good, if you can resolve the conflicts and tests pass. File a little JIRA and update the title if you please too https://spark.apache.org/contributing.html","1,-1    ",1,-1,0
1533016085,26535726,2023/5/9,v3.2.4,cc @dongjoon-hyun @Yikun ,"1,-1    ",1,-1,0
1533089837,9700541,2023/5/9,v3.2.4,"Thank you, @pan3793 . ","1,-2    ",1,-2,-1
1533268482,9700541,2023/5/9,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1537459679,32387433,2023/5/9,v3.2.4,"This change is not backward compatible, I'm not sure if this change is necessary, if necessary, also need parameters to control whether to open.","1,-1    ",1,-1,0
1537472631,5399861,2023/5/9,v3.2.4,@stijndehaes Do you have the error message if do not quote date?,"1,-1    ",1,-1,0
1538219912,5019818,2023/5/9,v3.2.4,"@wangyum @Hisoka-X 

There is more info in the issue on JIRA: https://issues.apache.org/jira/browse/SPARK-43357

So basically when integrating we glue we are getting issues:

```
org.apache.hadoop.hive.metastore.api.InvalidObjectException: Unsupported expression '2023 - 05 - 03' (Service: AWSGlue; Status Code: 400; Error Code: InvalidInputException; Request ID: beed68c6-b228-442e-8783-52c25b9d2243; Proxy: null)
```

Now this issue might be on the side of glue, but glue is supposed to be hive compatible. So I think this should also work with a hive catalog. However I do not have access to one, and would net to se one up.
So maybe someone with more experience with that could help me to test compatibility.","1,-1    ",1,-1,0
1539779576,3182036,2023/5/9,v3.2.4,@wangyum does all hive versions support it?,"1,-1    ",1,-1,0
1539821190,5399861,2023/5/9,v3.2.4,"> @wangyum does all hive versions support it?

Hive 0.13.0 and later support it: https://issues.apache.org/jira/browse/HIVE-5679.","1,-1    ",1,-1,0
1540087266,3182036,2023/5/9,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1533402592,14280154,2023/5/9,v3.2.4,"> Thanks. Please file a JIRA and update the PR title, @hiboyang .
> 
> cc @HyukjinKwon , @cloud-fan , @hvanhovell , @LuciferYang , @grundprinzip

Thanks for the suggestion! Added JIRA.","1,-1    ",1,-1,0
1533705017,14280154,2023/5/9,v3.2.4,"> This is awesome! Thanks for starting the work. I think the next step would be to have a quick discussion in a readme or the PR on the rough design of the objects and methods so that we get an idea of what might work or not.

Thanks @grundprinzip for the feedback :) Updated PR description and README just now!
","1,-1    ",1,-1,0
1533984760,14280154,2023/5/9,v3.2.4,Need to add unit test / integration test.,"1,-1    ",1,-1,0
1537337003,3421,2023/5/9,v3.2.4,"If you don't mind, my first suggestion is that we make a quick prototype of the build system so that we don't have to check in the generated code. 

I can help with that as well. ","1,-1    ",1,-1,0
1537339507,14280154,2023/5/9,v3.2.4,"> If you don't mind, my first suggestion is that we make a quick prototype of the build system so that we don't have to check in the generated code.
> 
> I can help with that as well.

Yes, one thing to consider is how Spark Connect Go application could reference those code as a Go library. Please go ahead with the prototype, thanks for helping here!
","1,-1    ",1,-1,0
1537416130,3421,2023/5/9,v3.2.4,"I created a draft PR that contains the necessary build code and moves the files around to fit the approach, feel free to just use the code in your PR.

https://github.com/apache/spark/pull/41080","1,-1    ",1,-1,0
1537477043,14280154,2023/5/9,v3.2.4,"> I created a draft PR that contains the necessary build code and moves the files around to fit the approach, feel free to just use the code in your PR.
> 
> #41080

This is great, thanks @grundprinzip! I copied code from your PR to this PR. Those `make` commands are very convenient!

By the way, we need to discuss where to publish those generated Go proto files. People will need to reference those files as Go module/package, when they write Spark Connect Go application. Any thoughts on this?
","1,-1    ",1,-1,0
1537503907,3421,2023/5/9,v3.2.4,"
> By the way, we need to discuss where to publish those generated Go proto files. People will need to reference those files as Go module/package, when they write Spark Connect Go application. Any thoughts on this?

I think we probably need to figure out how `go get` will work  as well. Unfortunately, the release distribution will require us to check-in the generated code. 



","1,-1    ",1,-1,0
1537505103,3421,2023/5/10,v3.2.4,"It would be great if you could just merge the change from my PR (just add the remote tree to your branch and merge the changes) and then push to your PR again. Right now, the Makefile is semi broken because you didn't mv the examples from examples to `cmd`.

I took the project layout from: https://github.com/golang-standards/project-layout

Now, for the package name we will have to refactor this a bit to make go get work. I think it should become: `https://github.com/apache/spark/connector/connect/client/go`

We can use a versioning scheme similar to what arrow does.","1,-1    ",1,-1,0
1537507895,3421,2023/5/10,v3.2.4,"@hvanhovell @HyukjinKwon @cloud-fan @gatorsmile How do you propose we make progress here?

I'm worried that we will accumulate a large number of changes in particular with the generated code that needs to be checked in and changed to make the prototype working. In the worst case it will make it harder to review.

What would be a good skeleton to submit?","1,-1    ",1,-1,0
1537797356,3182036,2023/5/10,v3.2.4,"Do we have a dev guideline for this go client? And will Github Action run tests for it? I think we can merge this PR once the infra is ready, the API coverage can be improved later.","1,-1    ",1,-1,0
1537823312,3421,2023/5/10,v3.2.4,"I can help run the tests and build as part of the CI that shouldn't be too hard when the make build runs locally. 

@hiboyang let me know when make and make fulltest pass locally and then we can try to get a first version merged before we add a lot more coverage. ","1,-1    ",1,-1,0
1538707652,14280154,2023/5/10,v3.2.4,"> I can help run the tests and build as part of the CI that shouldn't be too hard when the make build runs locally.
> 
> @hiboyang let me know when make and make fulltest pass locally and then we can try to get a first version merged before we add a lot more coverage.

Yes, thanks @grundprinzip for helping CI here!

`make fulltest` is passing now. Need to run `make internal/generated.out` first to generate Go proto files.

```
make internal/generated.out

make fulltest
>> TEST, ""coverage""
     ?   	github.com/apache/spark/go/v_3_4/examples/spark-connect-example-raw-grpc-client	[no test files]
     ?   	github.com/apache/spark/go/v_3_4/examples/spark-connect-example-spark-session	[no test files]
     ?   	github.com/apache/spark/go/v_3_4/internal/generated	[no test files]
     ok  	github.com/apache/spark/go/v_3_4/spark/sql	0.358s	coverage: 22.6% of statements
```
","1,-1    ",1,-1,0
1538720811,14280154,2023/5/10,v3.2.4,"> It would be great if you could just merge the change from my PR (just add the remote tree to your branch and merge the changes) and then push to your PR again. Right now, the Makefile is semi broken because you didn't mv the examples from examples to `cmd`.
> 
> I took the project layout from: https://github.com/golang-standards/project-layout
> 
> Now, for the package name we will have to refactor this a bit to make go get work. I think it should become: `https://github.com/apache/spark/connector/connect/client/go`
> 
> We can use a versioning scheme similar to what arrow does.

Got it, I am not super familiar with git commands to merge between different PRs, sorry for breaking here. I think now Makefile is good.

I renamed to `https://github.com/apache/spark/connector/connect/client/go` in the code.

","1,-1    ",1,-1,0
1538727584,3421,2023/5/10,v3.2.4,"@hiboyang I will need some time to integrate a build into the CI (github worklows). I hope I can get it done quickly
","1,-1    ",1,-1,0
1542142103,6477701,2023/5/10,v3.2.4,Build link :https://github.com/hiboyang/spark/actions/runs/4933558573/jobs/8817627400,"1,-1    ",1,-1,0
1543522121,3421,2023/5/10,v3.2.4,"@hiboyang I've updated the github workflows file to build the Go tests as well. Please pick the changes from my test PR https://github.com/apache/spark/pull/41117

* .github/workflows/build_and_test.yml

In addition, please make sure to pick the changes on the `.gitignore` and please remove the `internal/generated.out` file from the git history it's needed to trigger the code generation.","1,-1    ",1,-1,0
1544807716,14280154,2023/5/10,v3.2.4,Thanks @grundprinzip for adding github workflow! I merged your whole PR branch to my PR just now.,"1,-1    ",1,-1,0
1546009193,14280154,2023/5/10,v3.2.4,"Hi @grundprinzip, I see some error like following in PR check (https://github.com/hiboyang/spark/actions/runs/4955561542/jobs/8865088163). I already run `./dev/connect-gen-protos.sh`, but still see this error. Do you have any suggestion on how to deal with this?

```
Different files: ['base_pb2.pyi', 'catalog_pb2.pyi', 'commands_pb2.pyi', 'common_pb2.pyi', 'example_plugins_pb2.pyi', 'expressions_pb2.pyi', 'relations_pb2.pyi', 'types_pb2.pyi']
Generated files for pyspark-connect are out of sync! If you have touched files under connector/connect/common/src/main/protobuf/, please run ./dev/connect-gen-protos.sh. If you haven't touched any file above, please rebase your PR against main branch.
Error: Process completed with exit code 255.
```","1,-1    ",1,-1,0
1535156188,10248890,2023/5/10,v3.2.4,"@HyukjinKwon This is relatively small but changes something, can you take a look?
Also CC @rangadi @pengzhon-db ","1,-1    ",1,-1,0
1537601913,6477701,2023/5/10,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1536437302,822522,2023/5/10,v3.2.4,What is this?,"1,-1    ",1,-1,0
1536562171,104398294,2023/5/10,v3.2.4,"@srowen fixed description, didn't expect you to be so quick","1,-1    ",1,-1,0
1545194170,1580697,2023/5/10,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @vitaliili-db.","1,-1    ",1,-1,0
1533860277,10248890,2023/5/10,v3.2.4,@rangadi @pengzhon-db ,"1,-1    ",1,-1,0
1543167284,9616802,2023/5/10,v3.2.4,@WweiL can you add `StreamingManager` to the `CheckConnectJvmClientCompatibility` test?,"3,-3    ",3,-3,0
1550483880,6477701,2023/5/10,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1536601129,30429546,2023/5/10,v3.2.4,@cloud-fan Can you help me merge this? Thx!,"1,-1    ",1,-1,0
1537614211,6477701,2023/5/10,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1535567226,6477701,2023/5/10,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1533931491,100322362,2023/5/10,v3.2.4,@HeartSaVioR @siying - please take a look. Thanks,"1,-1    ",1,-1,0
1537616320,6477701,2023/5/10,v3.2.4,cc @HeartSaVioR and @xuanyuanking FYI,"1,-1    ",1,-1,0
1537621497,1317309,2023/5/10,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1538109470,8326978,2023/5/10,v3.2.4,thanks @dongjoon-hyun ,"1,-1    ",1,-1,0
1535565733,66282705,2023/5/10,v3.2.4,cc @HyukjinKwon ,"2,-1    ",2,-1,1
1535605065,7322292,2023/5/10,v3.2.4,merged to master,"1,-1    ",1,-1,0
1534437431,35164941,2023/5/10,v3.2.4,"I think I added old error class back when resolving merge conflict. So create this PR to delete it.
@MaxGekk , please help review it.","1,-1    ",1,-1,0
1534785306,1580697,2023/5/10,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @liang3zy22.","1,-1    ",1,-1,0
1535702981,3182036,2023/5/10,v3.2.4,"Can we mention it in the PR description that this is not a real perf issue, but just make the plan clearer about subquery reuse?","1,-1    ",1,-1,0
1537036880,3182036,2023/5/10,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1537066795,12025282,2023/5/10,v3.2.4,thank you @cloud-fan @peter-toth @dongjoon-hyun ,"1,-1    ",1,-1,0
1535609762,12025282,2023/5/10,v3.2.4,"The CI has passed with
https://github.com/ulysses-you/spark/actions/runs/4882406778","1,-1    ",1,-1,0
1537066682,12025282,2023/5/10,v3.2.4,thank you @dongjoon-hyun @HyukjinKwon @juliuszsompolski ,"1,-1    ",1,-1,0
1537119128,1580697,2023/5/10,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @bozhang2820.","1,-1    ",1,-1,0
1535559116,822522,2023/5/10,v3.2.4,Merged to master/3.4,"1,-1    ",1,-1,0
1535282609,50450311,2023/5/10,v3.2.4,@JoshRosen and @dongjoon-hyun Can you please check the changes?,"1,-2    ",1,-2,-1
1535324629,9700541,2023/5/10,v3.2.4,Thank u for pinging me. Will take a look tonight.,"1,-1    ",1,-1,0
1536393665,50450311,2023/5/10,v3.2.4,"> Thank u for pinging me. Will take a look tonight.

Thanks @dongjoon-hyun 
I am not sure I can find the reason behind the build errors though
```
#35 ERROR: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4886449337: unexpected status: 403 Forbidden

#36 [auth] amahussein/apache-spark-ci-image:pull,push apache/spark/apache-spark-github-action-image-cache:pull token for ghcr.io
#36 DONE 0.0s
------
 > exporting to image:
------
ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4886449337: unexpected status: 403 Forbidden
Error: buildx failed with: ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4886449337: unexpected status: 403 Forbidden
```","1,-1    ",1,-1,0
1536741822,4563792,2023/5/10,v3.2.4,"@dongjoon-hyun  do you know about what might be missing for permissions for his build error here? 

 (ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4897157804: unexpected status: 403 Forbidden)

I haven't seen that before.","1,-2    ",1,-2,-1
1536757870,9700541,2023/5/10,v3.2.4,"To @tgravescs , I also hit that issue before when I re-forked Spark repository. Like 403 error means, it was a permission issue in GitHub setting. But, I forgot the details. At that time, @Yikun helped me because he did setup the pipeline in the community~
","1,-1    ",1,-1,0
1536881780,9700541,2023/5/10,v3.2.4,"Welcome to the Apache Spark community, @amahussein .
I added you to the Apache Spark contributor group and assigned SPARK-43340 to you.","1,-2    ",1,-2,-1
1536958447,50748,2023/5/10,v3.2.4,LGTM as well. Thanks for fixing this!,"1,-1    ",1,-1,0
1536985087,1736354,2023/5/10,v3.2.4,"> ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4897157804: unexpected status: 403 Forbidden

There are two possible issue we met before:
1. Github Action flaky issue: https://github.com/orgs/community/discussions/32184
2. Permisssion seeting issue. By default, write permission already included to your GITHUB_TOKEN, but if you set it manually (see also [1], [2]), it will failed, current CI will first build infra and push to your ghcr so write permission is required.
[1] https://github.blog/changelog/2021-04-20-github-actions-control-permissions-for-github_token/
[2] https://github.com/users/amahussein/packages/container/apache-spark-ci-image/settings

>  Like 403 error means, it was a permission issue in GitHub setting. But, I forgot the details. At that time, @Yikun helped me because he did setup the pipeline in the community~

Yes, we do some configure on https://github.com/apache/spark/pull/37745#issuecomment-1234063012 , but after that I found it was a flaky issue of github aciton.","1,-1    ",1,-1,0
1536438357,822522,2023/5/10,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1536553825,9700541,2023/5/10,v3.2.4,"Thank you, @srowen !","1,-1    ",1,-1,0
1539001947,1097932,2023/5/10,v3.2.4,"> Interval types can be read as date or timestamp types that would lead to wildly different results


@zeruibao could you update the PR description about what's the behavior will be?","1,-1    ",1,-1,0
1539002193,1097932,2023/5/10,v3.2.4,"> Interval types can be read as date or timestamp types that would lead to wildly different results


@zeruibao could you update the PR description about what the behavior will be?","1,-1    ",1,-1,0
1562196017,1097932,2023/5/10,v3.2.4,"@zeruibao shall we only block the cases mentioned in the PR description?
```
 prevent reading interval types as date or timestamp types to avoid getting corrupt dates as well as reading decimal types with incorrect precision.
```","2,-2    ",2,-2,0
1563212026,125398515,2023/5/10,v3.2.4,"Yeah, ok. I can only block 
- read DayTimeIntervalType as TimestampType
- read DayTimeIntervalType as TimestampNTZType
- read DayTimeIntervalType as DateType
- read YearMonthIntervalType as TimestampType
- read YearMonthIntervalType as TimestampNTZType
- read YearMonthIntervalType as DateType

Does it look good to you now? @gengliangwang ","2,-2    ",2,-2,0
1563250388,1097932,2023/5/10,v3.2.4,Yes,"1,-1    ",1,-1,0
1563648846,125398515,2023/5/11,v3.2.4,"> > Interval types can be read as date or timestamp types that would lead to wildly different results
> 
> @zeruibao could you update the PR description about what the behavior will be?

Sure, just add it.","1,-1    ",1,-1,0
1563712405,1097932,2023/5/11,v3.2.4,@zeruibao please also update `sql-migration-guide.md` and mention this behavior change.,"1,-1    ",1,-1,0
1564208278,1591700,2023/5/11,v3.2.4,+CC @shardulm94 ,"1,-3    ",1,-3,-2
1540567922,47337188,2023/5/11,v3.2.4,cc @ueshin ,"1,-1    ",1,-1,0
1542746395,47337188,2023/5/11,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1542747111,47337188,2023/5/11,v3.2.4,"Please feel free to leave comments if any, I'll adjust them in a follow-up.","1,-1    ",1,-1,0
1535934798,3182036,2023/5/11,v3.2.4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1536965828,7322292,2023/5/11,v3.2.4,cc @HyukjinKwon ,"1,-3    ",1,-3,-2
1537293368,6477701,2023/5/11,v3.2.4,Merged to master.,"1,-3    ",1,-3,-2
1535725584,1580697,2023/5/11,v3.2.4,"@JinHelin404 Sorry, this is a duplicate of https://github.com/apache/spark/pull/41020. cc @imback82 ","1,-1    ",1,-1,0
1535954461,36835301,2023/5/11,v3.2.4,"@MaxGekk Ok, i will close this pr.","1,-1    ",1,-1,0
1535961391,1580697,2023/5/11,v3.2.4,@JinHelin404 Thank you.,"1,-1    ",1,-1,0
1535678084,44011673,2023/5/11,v3.2.4,@dongjoon-hyun @cometta would be good if you have time to review this PR,"1,-1    ",1,-1,0
1536741675,9700541,2023/5/11,v3.2.4,"I wrote a clean-revert PR with your authorship. Reverting PR should follow this style instead of claiming a new contribution like `Support ...`.
- https://github.com/apache/spark/pull/41069","1,-1    ",1,-1,0
1536963450,44011673,2023/5/11,v3.2.4,Replaced this with https://github.com/apache/spark/pull/41069,"1,-1    ",1,-1,0
1535945085,8486025,2023/5/11,v3.2.4,ping @cloud-fan ,"1,-1    ",1,-1,0
1537062982,1580697,2023/5/11,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @liang3zy22.","1,-1    ",1,-1,0
1537063079,35164941,2023/5/11,v3.2.4,"Don't know why CI ""continuous-integration/appveyor/pr"" is triggered. Checked other PR, this CI test all not triggered.","2,-1    ",2,-1,1
1536662415,822522,2023/5/11,v3.2.4,Merged to 3.3,"2,-1    ",2,-1,1
1536423360,822522,2023/5/11,v3.2.4,Merged to 3.4,"1,-1    ",1,-1,0
1535897335,26535726,2023/5/11,v3.2.4,@dtenedor @gengliangwang ,"2,-1    ",2,-1,1
1538085206,26535726,2023/5/11,v3.2.4,kindly ping @cloud-fan ~,"1,-1    ",1,-1,0
1558897018,26535726,2023/5/11,v3.2.4,Close as not required,"1,-1    ",1,-1,0
1537617380,6477701,2023/5/11,v3.2.4,cc @AngersZhuuuu ,"1,-1    ",1,-1,0
1537882461,46485123,2023/5/11,v3.2.4,"First, pls write a jira and assign this pr to jira ticket
These code is used to format sql from LogicalPlan, seems copied from Hive, ok to remove.","1,-1    ",1,-1,0
1537909147,32387433,2023/5/11,v3.2.4,"> First, pls write a jira and assign this pr to jira ticket These code is used to format sql from LogicalPlan, seems copied from Hive, ok to remove.

Ok, I created [SPARK-43405](https://issues.apache.org/jira/browse/SPARK-43405)","2,-1    ",2,-1,1
1539294578,32387433,2023/5/11,v3.2.4,"> 1. Could you revise the PR title and description, @Hisoka-X ?

Done

> 2. Do you happen to know when these methods become useless?

It unused when https://github.com/apache/spark/pull/16869 .
","1,-1    ",1,-1,0
1541342373,9700541,2023/5/11,v3.2.4,"Thank you for the info, @Hisoka-X .

cc @jiangxb1987 and @hvanhovell from #16869","1,-1    ",1,-1,0
1542308979,9616802,2023/5/11,v3.2.4,"@dongjoon-hyun that is looonnnnggg time ago :)

Let me take a look.","1,-1    ",1,-1,0
1542309745,9616802,2023/5/11,v3.2.4,Merging.,"1,-1    ",1,-1,0
1542436037,9700541,2023/5/11,v3.2.4,"Thank you, @Hisoka-X and @hvanhovell . :) ","1,-1    ",1,-1,0
1536143460,51110188,2023/5/11,v3.2.4,cc @cloud-fan ,"2,-2    ",2,-2,0
1541193866,51110188,2023/5/11,v3.2.4,Thank you @dongjoon-hyun @HyukjinKwon ~,"1,-2    ",1,-2,-1
1536145255,51110188,2023/5/11,v3.2.4,cc @cloud-fan ,"3,-1    ",3,-1,2
1537618129,6477701,2023/5/11,v3.2.4,cc @sadikovi FYI,"2,-4    ",2,-4,-2
1562196105,51110188,2023/5/11,v3.2.4,"Thanks @dongjoon-hyun for your point : )

The intent of this PR is to change the way `df.show` represents the map data type, which is currently different from some mainstream databases. We also want `df.show` and `spark-sql `CLI to be as consistent as possible, since they are both spark CLI. So we think the representation after PR might be a better representation.

Adding configuration is also a good idea.","1,-1    ",1,-1,0
1538282346,51110188,2023/5/11,v3.2.4,"Kindly ping @cloud-fan , Could you please take a look if you find a time :-)","3,-4    ",3,-4,-1
1537619340,6477701,2023/5/11,v3.2.4,Mind filing a JIRA? See also https://spark.apache.org/contributing.html,"1,-1    ",1,-1,0
1547064602,1080762,2023/5/11,v3.2.4,"Filed: 
https://issues.apache.org/jira/browse/SPARK-43496
","1,-1    ",1,-1,0
1537096523,1580697,2023/5/11,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @eogren.","1,-1    ",1,-1,0
1537096866,1580697,2023/5/11,v3.2.4,@eogren Congratulations with your first contribution to Apache Spark!,"1,-3    ",1,-3,-2
1536742215,9700541,2023/5/11,v3.2.4,cc @dcoliversun This will be merged with your authorship.,"1,-1    ",1,-1,0
1536744873,9700541,2023/5/11,v3.2.4,"After merging this clean-revert, you may want to make a new and proper test PR for https://github.com/apache/spark/pull/41057/files#r1186469992 if you want.","1,-1    ",1,-1,0
1536962452,44011673,2023/5/11,v3.2.4,@dongjoon-hyun Thanks for your PR. I'm +1,"2,-2    ",2,-2,0
1537192694,9700541,2023/5/11,v3.2.4,"Thank you, @dcoliversun .","2,-2    ",2,-2,0
1537192736,9700541,2023/5/11,v3.2.4,"Could you review this PR, @viirya ?","1,-1    ",1,-1,0
1537218037,68855,2023/5/11,v3.2.4,May I ask why this is reverted?,"1,-1    ",1,-1,0
1537250757,9700541,2023/5/11,v3.2.4,"Sorry for missing context, @viirya .

SPARK-43329 and SPARK-43342 were filed as a regression at Apache Spark 3.4.0 and those issues have the error message, `IllegalArgumentException: PVC ClaimName: a1pvc should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors`.

I linked all the reported issues to the original JIRA which caused this regression.
- https://issues.apache.org/jira/browse/SPARK-39006 Show a directional error message for PVC Dynamic Allocation Failure

![Screenshot 2023-05-06 at 5 13 51 PM](https://user-images.githubusercontent.com/9700541/236651391-dcb09a64-3384-4dea-8bc9-de5a886b19a3.png)



And, the following was the reverting PR from the author of SPARK-39006, @dcoliversun.
- https://github.com/apache/spark/pull/41057


In short, while adding a log message, we missed a corner case which one NFS-backed PVC is mounted by multiple executors. So, we decide to revert SPARK-39006 because it doesn't give actual benefit (except logging).","1,-1    ",1,-1,0
1537259529,68855,2023/5/11,v3.2.4,Thank you @dongjoon-hyun ,"1,-1    ",1,-1,0
1537261947,9700541,2023/5/11,v3.2.4,Thank you!,"1,-1    ",1,-1,0
1537262163,9700541,2023/5/11,v3.2.4,Merged to master/3.4.,"2,-2    ",2,-2,0
1536830449,26535726,2023/5/11,v3.2.4,cc @cloud-fan @sunchao @beliefer ,"1,-1    ",1,-1,0
1539748021,3182036,2023/5/11,v3.2.4,"thanks, merging to master!","1,-1    ",1,-1,0
1536935143,1633312,2023/5/11,v3.2.4,@dongjoon-hyun @mridulm help take a look?,"3,-1    ",3,-1,2
1537104070,1591700,2023/5/11,v3.2.4,+CC @otterc,"1,-1    ",1,-1,0
1537251921,9700541,2023/5/11,v3.2.4,Thank you for the fix.,"2,-1    ",2,-1,1
1545903094,1633312,2023/5/11,v3.2.4,@dongjoon-hyun any more comments on this?,"1,-1    ",1,-1,0
1537619872,6477701,2023/5/11,v3.2.4,cc @gengliangwang FYI,"2,-1    ",2,-1,1
1537117725,26535726,2023/5/11,v3.2.4,cc @srowen ,"1,-1    ",1,-1,0
1537117991,822522,2023/5/11,v3.2.4,It's probably OK (re-run the tests?) ,"2,-1    ",2,-1,1
1537118290,26535726,2023/5/11,v3.2.4,"GA passed, the `continuous-integration/appveyor/pr` randomly appears, and it [never succeeded](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/history), where does the appveyor come from?","2,-1    ",2,-1,1
1537441995,822522,2023/5/11,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1537083034,26535726,2023/5/12,v3.2.4,cc @srowen @LuciferYang ,"1,-1    ",1,-1,0
1537144110,822522,2023/5/12,v3.2.4,Actually there is another tar command in release-build.sh ; do we need this change there too?,"1,-1    ",1,-1,0
1537153689,26535726,2023/5/12,v3.2.4,We don't need to apply this change to `release-build.sh` because it's only called inside the `spark-rm` container during the release procedure. Unless you want it to work on macOS too.,"2,-1    ",2,-1,1
1537155770,822522,2023/5/12,v3.2.4,Merged to master/3.4/3.3/3.2,"1,-2    ",1,-2,-1
1537192564,9700541,2023/5/12,v3.2.4,"To @srowen , we released Apache Spark 3.2.4 as EOL release and `branch-3.2` is not considered maintained now.  It would be great if we don't touch EOL branches because it will cause confusion to the contributors.","1,-1    ",1,-1,0
1537199393,822522,2023/5/12,v3.2.4,"Yeah it won't matter, likely. On the off chance we make one more release, this is a low-risk 'fix' to the release process","2,-1    ",2,-1,1
1537245401,1002986,2023/5/12,v3.2.4,cc @rangadi @HyukjinKwon @pang-wu,"2,-1    ",2,-1,1
1539511840,6477701,2023/5/12,v3.2.4,@rangadi is it good to go?,"1,-1    ",1,-1,0
1540649572,502522,2023/5/12,v3.2.4,"@HyukjinKwon Yep, this looks good to go.","1,-1    ",1,-1,0
1541197793,1938382,2023/5/12,v3.2.4,Just wondering why this is relevant to Spark Connect (seeing `[CONNECT]` in the PR title)?,"1,-1    ",1,-1,0
1541267600,6477701,2023/5/12,v3.2.4,Merged to master.,"2,-3    ",2,-3,-1
1541425683,1002986,2023/5/12,v3.2.4,"> Just wondering why this is relevant to Spark Connect (seeing `[CONNECT]` in the PR title)?

ah sorry about that. i think i confused that this is under the `connector/` folder with `CONNECT` (which im guessing now refers to Spark Connect?) which i saw in some other PR titles. In the future i'll add only `PROTOBUF` ð ","1,-1    ",1,-1,0
1537271924,1633312,2023/5/12,v3.2.4,@dongjoon-hyun @mridulm Help take a look?,"1,-1    ",1,-1,0
1537293952,1633312,2023/5/12,v3.2.4,"> Thank you for pinging me. Could you improve this proposal more, @warrenzhu25 ?
> 
> * This PR claims that data migration can hurt performance, but the code is applied in all cases including the case data migration (shuffle/rdd) is disabled. Moreover, in general, when the migration data size is small, the claim is invalid.
> * In the same way, I can imagine this PR introduces another regression in terms of resource utilizations. For example, Spark Thrift Server can decommission all executors at the same time, but this configuration (< 0.1) may hurt the speed of scaling down.
> * It would be great if we can have a clear documentation about the relationship between this and other decommission configs.

`maxRatio` has default value 1, so it's same as current behavior. Users has full control of this ratio based on their judgement of storage migration size and impact.","1,-1    ",1,-1,0
1537541657,9700541,2023/5/12,v3.2.4,"No, I'm not saying the default value. The default value should be 1.
> maxRatio has default value 1, so it's same as current behavior. 

When the new configuration `maxRatio` itself is controversial in terms of the benefits, it's just a dark-magic to confuse the users to put into the mud holes. Why do we need to add this dark-magic configuration, @warrenzhu25 ?
> Users has full control of this ratio based on their judgement of storage migration size and impact.","1,-1    ",1,-1,0
1537561161,1633312,2023/5/12,v3.2.4,"> No, I'm not saying the default value. The default value should be 1.
> 
> > maxRatio has default value 1, so it's same as current behavior.
> 
> When the new configuration `maxRatio` itself is controversial in terms of the benefits, it's just a dark-magic to confuse the users to put into the mud holes. Why do we need to add this dark-magic configuration, @warrenzhu25 ?
> 
> > Users has full control of this ratio based on their judgement of storage migration size and impact.

I agree this config is not a perfect solution to solve this issue of migration competing with shuffle fetch. But it provides a feasible workaround to limit the impact of data migration. In our prod, using `maxRatio` of 0.3 could have similar perf as without shuffle migration while `maxRatio` of 0.5 or large has significant perf regression.","1,-1    ",1,-1,0
1537295989,1633312,2023/5/12,v3.2.4,@dongjoon-hyun helpt take a look?,"1,-1    ",1,-1,0
1539247388,9700541,2023/5/12,v3.2.4,"Merged to master for Apache Spark 3.5.0.
Thank you, @warrenzhu25 and @pan3793 ","1,-1    ",1,-1,0
1537302656,32387433,2023/5/12,v3.2.4,cc @gengliangwang @HyukjinKwon @srowen ,"1,-1    ",1,-1,0
1537441000,822522,2023/5/12,v3.2.4,"Can you fill out the JIRA, and add a little bit of explanation here?
It seems like you just avoid trying to extract more fields if basic parsing fails?","1,-1    ",1,-1,0
1537447586,32387433,2023/5/12,v3.2.4,"> Can you fill out the JIRA, and add a little bit of explanation here? 

Done

> It seems like you just avoid trying to extract more fields if basic parsing fails?

Another formatter parsing method is used to prevent the formatter from throwing exceptions, which is similar to https://github.com/apache/spark/pull/36562 .","2,-3    ",2,-3,-1
1537595353,6477701,2023/5/12,v3.2.4,cc @sadikovi too,"1,-1    ",1,-1,0
1541990704,1580697,2023/5/12,v3.2.4,"The same question as for https://github.com/apache/spark/pull/41091:

_Do the benchmarks `CSVBenchmark` and `JsonBenchmark` show any improvements? Could you regenerate the results `JsonBenchmark.*.txt` and `CSVBenchmark.*.txt`, please._","1,-1    ",1,-1,0
1545800041,1580697,2023/5/12,v3.2.4,"@Hisoka-X Could you highlight in PR description how much does it become faster. Please, put some numbers to the section `Why are the changes needed?`.","1,-1    ",1,-1,0
1545815468,32387433,2023/5/12,v3.2.4,"> @Hisoka-X Could you highlight in PR description how much does it become faster. Please, put some numbers to the section `Why are the changes needed?`.

Done","1,-1    ",1,-1,0
1546657983,32387433,2023/5/12,v3.2.4,"@MaxGekk I adjusted the code by your suggestion. By the way, maybe we should use spotless to avoid problem like this.","1,-1    ",1,-1,0
1546828727,1580697,2023/5/12,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @Hisoka-X and @srowen @HyukjinKwon for review.","1,-1    ",1,-1,0
1546829555,32387433,2023/5/12,v3.2.4,Thanks @MaxGekk for your patience and @srowen @HyukjinKwon.,"1,-1    ",1,-1,0
1537417081,26535726,2023/5/12,v3.2.4,@HyukjinKwon ,"1,-1    ",1,-1,0
1537509158,1633312,2023/5/12,v3.2.4,@dongjoon-hyun help take a look?,"1,-1    ",1,-1,0
1537557668,1633312,2023/5/13,v3.2.4,"> The removed code seems to be originated from Apache Spark 3.0.0. Could you give a correct `Affected Versions` to [SPARK-43398](https://issues.apache.org/jira/browse/SPARK-43398) if you think this is a bug, @warrenzhu25 ?
> 
> ![Screenshot 2023-05-07 at 1 22 51 PM](https://user-images.githubusercontent.com/9700541/236700961-c5886940-1c55-40ff-9555-6980447f751d.png)

Done.","1,-1    ",1,-1,0
1537523100,1633312,2023/5/13,v3.2.4,@dongjoon-hyun @mridulm Help take a look?,"1,-1    ",1,-1,0
1545904179,1633312,2023/5/13,v3.2.4,@dongjoon-hyun any comments on this?,"1,-1    ",1,-1,0
1546807210,1591700,2023/5/13,v3.2.4,How are you observing recoverable fetch failures ?,"1,-1    ",1,-1,0
1548009051,1633312,2023/5/13,v3.2.4,"> How are you observing recoverable fetch failures ?

I have seen 2 cases when target executor has busy shuffle fetch and upload due to shuffle migration:
1. All Netty request handler threads are exhausted, it'll throw `TimeoutException` when creating connection to target executor took longer than `connectTimeout`.
2. `IdleStateHandler` will close connection when fetch request took longer than `requestTimeout` to receive response.

Both cases are recoverable.","1,-1    ",1,-1,0
1550571606,1591700,2023/5/13,v3.2.4,"These looks like things which can be handled by appropriate configuration tuning ?
The PR itself requires a bit more work if that is not a feasible direction (efficient cleanup, handling corner cases, etc).","1,-1    ",1,-1,0
1552141121,1633312,2023/5/13,v3.2.4,"> These looks like things which can be handled by appropriate configuration tuning ? The PR itself requires a bit more work if that is not a feasible direction (efficient cleanup, handling corner cases, etc).

Case 2 can't be handled by existing config, there'll be other similar recoverable cases. Generally speaking, I think unregister all map output when fetch failed is too aggressive. So it's better to have config to control or disable such behavior. If the executor is really dead, the map output will be unregistered 
when removing executor, if executor is just experiencing temporary and recoverable hiccup, then unregister is too expensive.","1,-1    ",1,-1,0
1553434747,1591700,2023/5/14,v3.2.4,"> Case 2 can't be handled by existing config, there'll be other similar recoverable cases

Did you try increasing idle timeout ?
The behavior is specific to the environment application is running in - where executors are unable to respond to shuffle requests for more than 2 minutes: this looks more like a tuning or deployment issue.

> Generally speaking, I think unregister all map output when fetch failed is too aggressive.

As described, this is a case of not appropriately configuring spark for the load/cluster characteristics.
For example, in our internal env, the network timeout is set to a significantly higher value than the default 120s due to a variety of factors - the default 2mins would result in failures (including this specific shuffle issue mentioned).

This proposed change would complicate the way we reason about when shuffle data is lost - and I am not inclined towards it if it is something that can be mitigated with appropriate tuning.","2,-1    ",2,-1,1
1537597639,6477701,2023/5/14,v3.2.4,"@hauntsaninja mind creating a JIRA, and keeping the PR description template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)? See also https://spark.apache.org/contributing.html","1,-1    ",1,-1,0
1537603053,12621235,2023/5/14,v3.2.4,"Thanks, I updated the PR to keep the template. Do I need to have a JIRA? The issue here is trivial / I don't have an account / my previous PR to Spark didn't need a JIRA (https://github.com/apache/spark/pull/29264)","1,-1    ",1,-1,0
1537624567,6477701,2023/5/14,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1537695395,6477701,2023/5/14,v3.2.4,cc @dongjoon-hyun ,"1,-1    ",1,-1,0
1537845455,9700541,2023/5/14,v3.2.4,"Could you re-trigger the failed linter tests? It looks irrelevant.
```
python/pyspark/broadcast.py:106: error: Overloaded function implementation does not accept all possible arguments of signature 3  [misc]
Found 1 error in 1 file (checked 511 source files)
```","1,-1    ",1,-1,0
1537867055,6477701,2023/5/14,v3.2.4,^ That should be fixed by https://github.com/apache/spark/pull/41086.,"1,-1    ",1,-1,0
1537867869,6477701,2023/5/14,v3.2.4,But let me retrigger anyway .. seems another test got stuck ..,"1,-1    ",1,-1,0
1538186256,6477701,2023/5/14,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1538639562,9700541,2023/5/15,v3.2.4,Thank you!,"1,-1    ",1,-1,0
1537649515,6477701,2023/5/15,v3.2.4,Not sure why this fails from a cursory look. MyPy version looks the same. Should be caused by either Python version or dependency of MyPy.,"3,-1    ",3,-1,2
1537668218,6477701,2023/5/15,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1537799363,12025282,2023/5/15,v3.2.4,cc @cloud-fan @viirya thank you,"1,-1    ",1,-1,0
1549359117,12025282,2023/5/15,v3.2.4,@cloud-fan @viirya @wangyum  do you have other thought ? ,"1,-1    ",1,-1,0
1537792038,100322362,2023/5/15,v3.2.4,@HeartSaVioR - please take a look. Thx,"1,-1    ",1,-1,0
1539189818,1317309,2023/5/15,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1542339003,32387433,2023/5/15,v3.2.4,"> Do the benchmarks `CSVBenchmark` and `JsonBenchmark` show any improvements? Could you regenerate the results `JsonBenchmark.*.txt` and `CSVBenchmark.*.txt`, please.

I'm doing benchmarks, but I found a problem, like @gengliangwang said in https://github.com/apache/spark/pull/36562#issuecomment-1127600354 .The benchmarks no case for `the string inputs are not valid timestamps`. The speed up only work when `string input are not valid timestamps`. I'm worry about the benchmarks can't prove anything. Can I create a PR for add benchmarks for `type inference when string input are not valid timestamps`","1,-1    ",1,-1,0
1542426882,1580697,2023/5/15,v3.2.4,"> Can I create a PR for add benchmarks for type inference when string input are not valid timestamps

Yep. Let's do that.","1,-1    ",1,-1,0
1546829442,1580697,2023/5/15,v3.2.4,"@Hisoka-X Please, resolve the conflicts and rebase on the recent master.","1,-1    ",1,-1,0
1546830139,32387433,2023/5/15,v3.2.4,"> @Hisoka-X Please, resolve the conflicts and rebase on the recent master.

Ok, I will add benchmarks for this too. Please wait. Thanks!","1,-1    ",1,-1,0
1549612885,1580697,2023/5/15,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @Hisoka-X.","1,-1    ",1,-1,0
1549617800,32387433,2023/5/15,v3.2.4,Thanks @MaxGekk ,"1,-1    ",1,-1,0
1538531237,1475305,2023/5/15,v3.2.4,cc @sunchao FYI,"1,-1    ",1,-1,0
1538737387,506679,2023/5/15,v3.2.4,"Merged to master, thanks @clownxc !","1,-1    ",1,-1,0
1538772623,9700541,2023/5/15,v3.2.4,"Welcome to the Apache Spark community, @clownxc .
@sunchao added you to the Apache Spark contributor group and assigned SPARK-43410 to you.","1,-1    ",1,-1,0
1539075096,790409,2023/5/15,v3.2.4,@cloud-fan @allisonwang-db ,"1,-1    ",1,-1,0
1548842243,3182036,2023/5/15,v3.2.4,"thanks, merging to master!","1,-2    ",1,-2,-1
1550014858,9700541,2023/5/15,v3.2.4,"Thank you, @jchen5 and @cloud-fan .","1,-1    ",1,-1,0
1539083069,10248890,2023/5/15,v3.2.4,@rangadi @pengzhon-db ,"1,-1    ",1,-1,0
1540757753,9616802,2023/5/15,v3.2.4,Merging,"1,-1    ",1,-1,0
1539561948,1317309,2023/5/15,v3.2.4,Thanks! Merging to master.,"1,-1    ",1,-1,0
1539589395,55518381,2023/5/15,v3.2.4,@HeartSaVioR ,"1,-1    ",1,-1,0
1542642125,100322362,2023/5/15,v3.2.4,"@chaoqin-li1123 - is the test failure related to the change ?

```
 - Handle unsupported input of message type *** FAILED *** (12 milliseconds)
 ```","1,-1    ",1,-1,0
1548781495,55518381,2023/5/15,v3.2.4,I have addressed most pending comments. Could you take a look? @HeartSaVioR ,"1,-1    ",1,-1,0
1553989670,55518381,2023/5/15,v3.2.4,"Hi @HyukjinKwon, the tests I add in this pr cause the sql test ci to timeout, is it possible to move streaming tests to a separate test runnner?

`2023-05-18T21:02:41.4826861Z ##[error]The runner has received a shutdown signal. This can happen when the runner service is stopped, or a manually started runner is canceled.
2023-05-18T21:02:41.6228000Z Session terminated, killing shell...
2023-05-18T21:02:41.7441894Z ##[error]The operation was canceled.`","1,-1    ",1,-1,0
1563874048,55518381,2023/5/15,v3.2.4,"I can verify that all the streaming query integration tests we override inherits StateStoreMetricsTest, which use a shared spark session. And they also use SQL conf to override state format version, so they should also respect the changelog checkpointing config we set.
RocksDBStateStoreIntegrationSuite  extends StreamTest, which have a shared spark session and use sql conf.
I do find that RocksDBStateStore suite ignore the sqlconf, have fixed that.
","1,-1    ",1,-1,0
1541632079,6477701,2023/5/15,v3.2.4,"> If a bucket scan has no interesting partition or contains shuffle exchange, then we would disable it

qq: what should we disable?","1,-1    ",1,-1,0
1541832545,12025282,2023/5/15,v3.2.4,"@HyukjinKwon sorry for the confusion. It means bucket scan, the functionality of rule `DisableUnnecessaryBucketedScan` is auto disable bucket scan when the bucket scan is unnecessary, and this pr takes table cache into account. I have replaced it in pr description.","1,-2    ",1,-2,-1
1539982555,6477701,2023/5/15,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1541384644,8326978,2023/5/15,v3.2.4,cc @cloud-fan ,"1,-1    ",1,-1,0
1543362119,8326978,2023/5/15,v3.2.4,thanks @dongjoon-hyun and @cloud-fan ,"1,-1    ",1,-1,0
1540632431,1134248,2023/5/15,v3.2.4,"@MaxGekk Thanks for the pointer, I wasn't sure where to add the test. I've added one (and that one fails if I revert the change).","1,-1    ",1,-1,0
1541182188,5399861,2023/5/15,v3.2.4,@Fokko Rename PR title from `[SPARK-43425][CORE]` to `[SPARK-43425][SQL]`?,"1,-1    ",1,-1,0
1541337853,9700541,2023/5/15,v3.2.4,"Thank you, @Fokko , @MaxGekk , @wangyum , @aokolnychyi .

Also, cc @cloud-fan too.

If the JIRA issue is categorized as `Bug`, we may want to bring this to branch-3.4.","1,-1    ",1,-1,0
1541447791,1134248,2023/5/15,v3.2.4,Thanks @dongjoon-hyun for reviewing. Thanks @MaxGekk and @aokolnychyi for the review. I've changed it to a bug in Jira and I'll create a backport in a second.,"1,-2    ",1,-2,-1
1539978061,1825975,2023/5/15,v3.2.4,cc @ryan-johnson-databricks @cloud-fan ,"1,-1    ",1,-1,0
1541542885,1825975,2023/5/15,v3.2.4,@cloud-fan tests passed after I fixed scalastyle,"2,-1    ",2,-1,1
1541554276,6477701,2023/5/15,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1540504315,88070094,2023/5/15,v3.2.4,cc @srowen,"1,-1    ",1,-1,0
1541270960,88070094,2023/5/15,v3.2.4,cc @LuciferYang ,"2,-1    ",2,-1,1
1541571622,88070094,2023/5/15,v3.2.4,"Here are some key code segments:

1. Old SparkUI  goes into `com.google.common.cache.LocalCache#removalNotificationQueue` when it is removed from `appCache`.

**com.google.common.cache.LocalCache.Segment**
```java
    @GuardedBy(""Segment.this"")
    @Nullable
    ReferenceEntry<K, V> removeValueFromChain(ReferenceEntry<K, V> first,
        ReferenceEntry<K, V> entry, @Nullable K key, int hash, ValueReference<K, V> valueReference,
        RemovalCause cause) {
      // Put removed value into removalNotificationQueue
      enqueueNotification(key, hash, valueReference, cause);
      writeQueue.remove(entry);
      accessQueue.remove(entry);

      if (valueReference.isLoading()) {
        valueReference.notifyNewValue(null);
        return first;
      } else {
        return removeEntryFromChain(first, entry);
      }
    }
```

2. Execution of `removalListener#onRemoval` is not guarded by `appCache`'s lock

**com.google.common.cache.LocalCache.Segment**
```java
    V lockedGetOrLoad(K key, int hash, CacheLoader<? super K, V> loader)
        throws ExecutionException {
      ReferenceEntry<K, V> e;
      ValueReference<K, V> valueReference = null;
      LoadingValueReference<K, V> loadingValueReference = null;
      boolean createNewEntry = true;

      lock();
      try {
         ...
      } finally {
        unlock();
        // `removalListener#onRemoval` executes inside postWriteCleanup()
        postWriteCleanup();
      }
      ...
    }
```

3. New SparkUI is detached unexpectedly during detaching of old SparkUI

**FsHistoryProvider**
```scala
  override def onUIDetached(appId: String, attemptId: Option[String], ui: SparkUI): Unit = {
    val uiOption = synchronized {
      // After new SparkUI is loaded, `activeUIs` contains new SparkUI
      activeUIs.remove((appId, attemptId))
    }
    uiOption.foreach { loadedUI =>
      loadedUI.lock.writeLock().lock()
      try {
        loadedUI.ui.store.close()
      } finally {
        loadedUI.lock.writeLock().unlock()
      }

      diskManager.foreach { dm =>
        // If the UI is not valid, delete its files from disk, if any. This relies on the fact that
        // ApplicationCache will never call this method concurrently with getAppUI() for the same
        // appId / attemptId.
        dm.release(appId, attemptId, delete = !loadedUI.valid)
      }
    }
  }
```","3,-3    ",3,-3,0
1541576100,88070094,2023/5/15,v3.2.4,@srowen @LuciferYang Added some code segments to help to understand this issue.,"1,-2    ",1,-2,-1
1542317728,162090,2023/5/15,v3.2.4,"don't remember the cache stuff, sorry. I know we have a problem in hadoop where cache lookup can trigger >1 fs creation and the same time, and if that is slow then at best: needless work, at worst: conflict and sometimes failures. so we use a semaphore to limit the #of threads which can create a new FS at the same time (HADOOP-17313). spark/tez workers and cloud stores doing network IO in initialize() are the troublespot here, FWIW.","2,-2    ",2,-2,0
1543275831,88070094,2023/5/15,v3.2.4,"> don't remember the cache stuff, sorry. I know we have a problem in hadoop where cache lookup can trigger >1 fs creation and the same time, and if that is slow then at best: needless work, at worst: conflict and sometimes failures. so we use a semaphore to limit the #of threads which can create a new FS at the same time (HADOOP-17313). spark/tez workers and cloud stores doing network IO in initialize() are the troublespot here, FWIW.

@steveloughran Thanks for your inputs.

In SparkHistoryServer, SparkUIs are tracked by <appId, attemptId> in many places:
1. `ApplicationCache#appCache`
2. `FsHistoryProvider#activeUIs`
3. `HistoryServerDiskManager#active`
4. Disk-based KVStore backend local path: appStoreDir/\<appId>_\<attemptId>/
5. Disk-based KVStore backend local file lock: appStoreDir/\<appId>_\<attemptId>/LOCK

All the above places assumes there is only one SparkUI loaded for one <appId, attemptId> pair.

Guava LoadingCache can ensure this when SparkUIs are guarded by its locks.
But SparkUI's detaching is not guarded by LoadingCache's lock.  

This PR is aimed to ensure this from SparkUI's loading to detaching.","1,-2    ",1,-2,-1
1544115380,88070094,2023/5/15,v3.2.4,"@LuciferYang Exception ""java.lang.IllegalStateException: DB is closed."" can be reproduced in test now.

<img width=""1138"" alt=""image"" src=""https://github.com/apache/spark/assets/88070094/cea0d752-4323-40ef-b4d1-5ec457fc267e"">
","1,-1    ",1,-1,0
1544930839,1591700,2023/5/15,v3.2.4,+CC @thejdeep ,"2,-2    ",2,-2,0
1547067573,6477701,2023/5/15,v3.2.4,cc @gengliangwang and @sarutak too FYI,"2,-1    ",2,-1,1
1542015280,1580697,2023/5/15,v3.2.4,"The GA [Run / Build modules: pyspark-sql, pyspark-mllib, pyspark-resource](https://github.com/MaxGekk/spark/actions/runs/4934033324/jobs/8818623610#logs) frozen in the run https://github.com/MaxGekk/spark/runs/13365446543 but it passed in https://github.com/MaxGekk/spark/runs/13370236086","1,-2    ",1,-2,-1
1542017477,1580697,2023/5/15,v3.2.4,"Merging to master. Thank you, @HyukjinKwon @vicennial @amaliujia for review.","1,-1    ",1,-1,0
1540562071,1002986,2023/5/15,v3.2.4,"cc @rangadi i've made a draft implementation here but just wanted to get your thoughts quickly on:

1. does the problem / solution make sense?
2. should we make this behavior the default or should we add an option to turn it on? 

thanks ð ","1,-1    ",1,-1,0
1543602788,1002986,2023/5/15,v3.2.4,also cc @HyukjinKwon as you reviewed https://github.com/apache/spark/pull/31921 and have reviewed many proto changes too ð ,"1,-1    ",1,-1,0
1547235141,502522,2023/5/15,v3.2.4,"Where is the information loss or overflow? Java code generated by Protobuf for a uint32 field also returns an `int`, not `long`.  ","1,-1    ",1,-1,0
1556292297,1002986,2023/5/15,v3.2.4,"> Where is the information loss or overflow? Java code generated by Protobuf for a uint32 field also returns an `int`, not `long`.

sorry i didn't get a chance to reply to this until now. There is no information loss, technically, as uint32 is 4 bytes and uint64 is 8 bytes, same as int and long respectively. However, there is overflow in the representation.

Here's an example:

Consider a protobuf message like:
```
syntax = ""proto3"";

message Test {
  uint64 val = 1;
}
```

Generate a protobuf with a value above 2^63. I did this in python with a small script like:

```
import test_pb2

s = test_pb2.Test()
s.val = 9223372036854775809 # 2**63 + 1
serialized = s.SerializeToString()
print(serialized)
```

This generates the binary representation:

```
b'\x08\x81\x80\x80\x80\x80\x80\x80\x80\x80\x01'
```

Then, deserialize this using `from_protobuf`. I did this in a notebook so its easier to see, but could reproduce in a scala test as well:

<img width=""597"" alt=""image"" src=""https://github.com/apache/spark/assets/1002986/a6c58c19-b9d3-44d4-8c2a-605991d3d5de"">


This is exactly what we'd expect when you take a 64 bit number with the highest bit as `1` and then try to interpret it as a signed number (long), it becomes negative.

So this PR proposes some changes to the deserialization behavior. However, I don't know if its right to change the default or have an option to allow unpacking as a larger type.

","1,-1    ",1,-1,0
1557649439,502522,2023/5/15,v3.2.4,">  So this PR proposes some changes to the deserialization behavior. However, I don't know if its right to change the default or have an option to allow unpacking as a larger type.

What if you have a UDF that converts this to BigDecimal? Will you get the value back?
I guess that is the intention behind why protobuf-java casts unsiged to signed in its Java methods. 
I think it simpler to go this way. Given these kinds of issues, I guess it is not a good practice to use unsiged in protobuf. It can be intepreted correctly at application level when they are infact used this way.","2,-1    ",2,-1,1
1557723788,1002986,2023/5/15,v3.2.4,"> What if you have a UDF that converts this to BigDecimal? Will you get the value back? I guess that is the intention behind why protobuf-java casts unsiged to signed in its Java methods. I think it simpler to go this way. 

Yeah, there is no information loss so you can get the right value the way I did in this PR (Integer.toUnsignedLong, Long.toUnsignedString). I think, though, it's useful if the `spark-protobuf` library can do this; the burden of taking a struct and trying to do this transformation is cumbersome, in my opinion.

However, one additional piece of information is that **for unsigned types in parquet, the default behavior is to represent them in larger types**. I put this in the PR description but see this ticket https://issues.apache.org/jira/browse/SPARK-34817 implemented in this PR: https://github.com/apache/spark/pull/31921. Or the existing code today https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala#L243-L247 which shows that **by default** parquet unsigned values are actually expanded to larger types in spark.

So, since this same problem/solution exists in another storage format, I think its useful to implement this behavior here as well. I also think that it actually _does_ make sense to do it by default, as parquet already does this. However, i'm open also to doing this transformation behind an option so that no existing usages are broken. Mainly, I want to just make sure we do  what is the most correct and broadly consistent thing to do (and i'm not really sure exactly what that is, and would love some other inputs). cc @HyukjinKwon as well here since you reviewed the original PR doing this for parquet!

","1,-1    ",1,-1,0
1540611367,1938382,2023/5/15,v3.2.4,@hvanhovell @cloud-fan ,"2,-2    ",2,-2,0
1541174344,9616802,2023/5/15,v3.2.4,Merging.,"1,-2    ",1,-2,-1
1543987640,1475305,2023/5/15,v3.2.4,"A late question: Should `error/error-classes.json` and `SparkThrowableHelper` be moved together to the `common-utils` module?

","1,-1    ",1,-1,0
1542650564,9616802,2023/5/15,v3.2.4,Merging.,"3,-1    ",3,-1,2
1547474099,32387433,2023/5/15,v3.2.4,cc @MaxGekk @cloud-fan @hvanhovell ,"1,-1    ",1,-1,0
1558435904,51392682,2023/5/15,v3.2.4,"Hi @Hisoka-X 

Good day. I would like to know whether this pull request to for spark sql query with delta lake delta table v2.
I simply encounter this issue with spark 3.2 (delta lake 1.2) and spark 3.3 (delta lake 2.2), which I think this pull request could help resolve this issue.

![image](https://github.com/apache/spark/assets/51392682/591ada27-5525-402e-995d-14f5c3c3f11f)

![image](https://github.com/apache/spark/assets/51392682/0908e173-325e-4057-be6b-c9ae5e990f8f)

I saw this ANALYZE TABLE command is originated from spark : https://spark.apache.org/docs/3.3.1/sql-ref-syntax-aux-analyze-table.html#analyze-table. But delta lake documentation do not mention ANALYZE TABLE usage https://docs.delta.io/2.2.0/delta-intro.html.

Since this pull request will merge to master branch and want to know **whether it will backfill to spark 3.2, spark 3.3 also**. 

Thank you

Best regards,
Jerry","1,-2    ",1,-2,-1
1558455448,32387433,2023/5/15,v3.2.4,"> I would like to know whether this pull request to for spark sql query with delta lake delta table v2.

This PR suite for all DataSource V2 which support `SupportsReportStatistics` interface.

> Since this pull request will merge to master branch and want to know whether it will backfill to spark 3.2, spark 3.3 also.

I'm not sure for this, seem like only merge into master.  @HyukjinKwon Can you help to answer this question? Thanks.","1,-1    ",1,-1,0
1566359887,51392682,2023/5/15,v3.2.4,"Hi @HyukjinKwon ,

I would like to know this feature will be backfilled to spark 3.2, spark 3.3 also? It is important for us to review our OSS spark version.

Best regards,
Jerry Lin","1,-1    ",1,-1,0
1541450600,1134248,2023/5/15,v3.2.4,cc @dongjoon-hyun ,"1,-1    ",1,-1,0
1541550132,6477701,2023/5/15,v3.2.4,cc @gengliangwang ,"1,-1    ",1,-1,0
1541629891,1134248,2023/5/16,v3.2.4,@HyukjinKwon thanks for updating the title ðð» ,"1,-1    ",1,-1,0
1542502605,9700541,2023/5/16,v3.2.4,"Also, cc @MaxGekk ","1,-1    ",1,-1,0
1542983238,6477701,2023/5/16,v3.2.4,Merged to branch-3.4.,"1,-1    ",1,-1,0
1542991219,9700541,2023/5/16,v3.2.4,"Thank you, @Fokko , @MaxGekk , @gengliangwang , @HyukjinKwon !","1,-1    ",1,-1,0
1541546681,7322292,2023/5/16,v3.2.4,"@HyukjinKwon @rangadi I am going to disable this test for now, also create https://issues.apache.org/jira/browse/SPARK-43435 to track this issue.","2,-1    ",2,-1,1
1542105531,6477701,2023/5/16,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1545210505,3626747,2023/5/16,v3.2.4,"Hi, @cloud-fan @ulysses-you @wangyum  could you help to review this PR ? Thanks ","1,-1    ",1,-1,0
1547146972,6477701,2023/5/16,v3.2.4,cc @rednaxelafx ,"1,-1    ",1,-1,0
1547147273,6477701,2023/5/16,v3.2.4,and @peter-toth ,"1,-1    ",1,-1,0
1554063667,3626747,2023/5/16,v3.2.4,"Hi, @rednaxelafx @peter-toth could you help to review this PR ? Thanks","1,-1    ",1,-1,0
1554266843,7253827,2023/5/16,v3.2.4,"> Hi, @rednaxelafx @peter-toth could you help to review this PR ? Thanks

Hi @wankunde, thanks for pinging me. I can take a look at this PR sometime next week or the week after...","1,-1    ",1,-1,0
1543763941,1580697,2023/5/16,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @bozhang2820.","1,-1    ",1,-1,0
1542472612,1475305,2023/5/16,v3.2.4,will update result of `StateStoreBasicOperationsBenchmark` later,"2,-1    ",2,-1,1
1545057975,1475305,2023/5/16,v3.2.4,"When I tried this upgrade, I found that the result of 'StateStoreBasicOperationsBenchmark' was unexpected. For check if it was a new version(8.1.1.1) issue, I also ran the 'StateStoreBasicOperationsBenchmark' on the master branch(with 8.0.0), and there were significant differences between the test result and the previous records and 'StateStoreBasicOperationsBenchmark' also run timeoutï¼more than 6hours, It should have been completed in 3 hours beforeï¿½?

https://github.com/LuciferYang/spark/actions/runs/4949396450/jobs/8856766625

```
[success] Total time: 791 s (13:11), completed May 11, 2023 7:33:07 PM
23/05/11 19:33:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Running org.apache.spark.sql.execution.benchmark.StateStoreBasicOperationsBenchmark:
23/05/11 19:33:15 WARN SparkContext: The JAR file:/home/runner/work/spark/spark/core/target/scala-2.12/spark-core_2.12-3.5.0-SNAPSHOT-tests.jar at spark://localhost:44659/jars/spark-core_2.12-3.5.0-SNAPSHOT-tests.jar has been added already. Overwriting of added jar is not supported in the current version.
Running benchmark: putting 10000 rows (10000 rows to overwrite - rate 100)
  Running case: In-memory
  Stopped after 10000 iterations, 82351 ms
  Running case: RocksDB (trackTotalNumberOfRows: true)
  Stopped after 10000 iterations, 599839 ms
  Running case: RocksDB (trackTotalNumberOfRows: false)
  Stopped after 10000 iterations, 210482 ms

OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (10000 rows to overwrite - rate 100):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
---------------------------------------------------------------------------------------------------------------------------------------
In-memory                                                            7              8           1          1.4         739.9       1.0X
RocksDB (trackTotalNumberOfRows: true)                              58             60           1          0.2        5828.2       0.1X
RocksDB (trackTotalNumberOfRows: false)                             20             21           0          0.5        2033.4       0.4X

OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (7500 rows to overwrite - rate 75):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
In-memory                                                          8              9           1          1.3         772.9       1.0X
RocksDB (trackTotalNumberOfRows: true)                            56             58           1          0.2        5567.7       0.1X
RocksDB (trackTotalNumberOfRows: false)                           21             23           4          0.5        2126.2       0.4X

OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (5000 rows to overwrite - rate 50):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
In-memory                                                          8              9           1          1.3         760.7       1.0X
RocksDB (trackTotalNumberOfRows: true)                            51             53           1          0.2        5089.3       0.1X
RocksDB (trackTotalNumberOfRows: false)                           21             22           1          0.5        2133.7       0.4X


OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (2500 rows to overwrite - rate 25):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
In-memory                                                          7              8           1          1.3         747.6       1.0X
RocksDB (trackTotalNumberOfRows: true)                            46             47           1          0.2        4603.3       0.2X
RocksDB (trackTotalNumberOfRows: false)                           21             22           1          0.5        2141.8       0.3X


OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (1000 rows to overwrite - rate 10):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
In-memory                                                          7              8           1          1.4         732.3       1.0X
RocksDB (trackTotalNumberOfRows: true)                            43             44           1          0.2        4283.7       0.2X
RocksDB (trackTotalNumberOfRows: false)                           21             22           1          0.5        2132.0       0.3X


OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (500 rows to overwrite - rate 5):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-----------------------------------------------------------------------------------------------------------------------------------
In-memory                                                        7              8           1          1.4         732.3       1.0X
RocksDB (trackTotalNumberOfRows: true)                          42             43           1          0.2        4169.4       0.2X
RocksDB (trackTotalNumberOfRows: false)                         21             22           1          0.5        2124.4       0.3X


OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
putting 10000 rows (0 rows to overwrite - rate 0):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
---------------------------------------------------------------------------------------------------------------------------------
In-memory                                                      7              8           1          1.4         727.9       1.0X
RocksDB (trackTotalNumberOfRows: true)                        40             42           1          0.2        4038.6       0.2X
RocksDB (trackTotalNumberOfRows: false)                       21             22           1          0.5        2129.9       0.3X

.......

OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure
Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz
evicting 1000 rows (maxTimestampToEvictInMillis: 999) from 10000 rows:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-----------------------------------------------------------------------------------------------------------------------------------------------------
In-memory                                                                          5              5           0          2.2         458.3       1.0X
RocksDB (trackTotalNumberOfRows: true)                                             9              9           0          1.1         871.2       0.5X
RocksDB (trackTotalNumberOfRows: false)                                            5              6           0          1.9         518.3       0.9X

Running benchmark: evicting 500 rows (maxTimestampToEvictInMillis: 499) from 10000 rows
  Running case: In-memory
  Stopped after 10000 iterations, 50617 ms
  Running case: RocksDB (trackTotalNumberOfRows: true)
Error: The operation was canceled.
```

I will try to investigate this issue, cc @dongjoon-hyun @HeartSaVioR FYI


EDIT:  The `RocksDB (trackTotalNumberOfRows: false) ` scene looks twice as slow as before","3,-1    ",3,-1,2
1545091809,9700541,2023/5/16,v3.2.4,"Thank you for your investigation, @LuciferYang !","2,-1    ",2,-1,1
1545192031,1317309,2023/5/16,v3.2.4,"That's because we no longer use writebatch which has been problematic on memory usage. We should have probably run the benchmark and updated the result...

The overall performance won't be significantly reduced as we pay the cost in each operation without writebatch (which leads slowdown in benchmark), whereas we are going to pay the cost at once in commit phase when we use writebatch & flush in commit phase. (We benchmarked by ourselves.)

cc. @anishshri-db Would you mind adding more context here? We probably need to update the benchmark, with reduce of the number of operations.","1,-3    ",1,-3,-2
1545207874,1475305,2023/5/16,v3.2.4,"If this is expected, optimizing benchmark testing should be more reasonable, as current benchmark testing will fail due to GA timeout

","1,-1    ",1,-1,0
1547231543,100322362,2023/5/16,v3.2.4,"@LuciferYang - As @HeartSaVioR mentioned, this is part of our broader effort to reduce memory usage for rocksdb. As part of that change, we don't use writeBatch any more and have also moved to disable the write ahead log. As part of this, puts/deletes will be written to memtables and gets will also be served from memtables/block cache which might lead to some difference in perf for those options. We can cover up for most of that since the db write during commit is now eliminated and we can also do periodic flushing with a related upcoming change. That overall portion, I believe is not covered by the b/mark test that we have today.

Could you let me know which b/mark test will fail and how we run that ?

@HeartSaVioR - do you proposing changing the b/mark itself ? or rerunning and updating the results page ?","3,-1    ",3,-1,2
1547239087,1475305,2023/5/16,v3.2.4,"@anishshri-db I need to update the results of `StateStoreBasicOperationsBenchmark` when upgrade rocksdbjni, we can run `StateStoreBasicOperationsBenchmark` with GA as following:

<img width=""355"" alt=""image"" src=""https://github.com/apache/spark/assets/1475305/b5ce0ad4-b3e8-48a1-bc52-3db3b0dfa5ad"">
 
It did not fail and was killed by GA due to timeout. Previously, `StateStoreBasicOperationsBenchmark` was executed for 3 hours(The last time I updated the rocksdbjni version), but now it has been executed for more than 6 hours and has not been completed yet.
","2,-1    ",2,-1,1
1547242180,100322362,2023/5/16,v3.2.4,@LuciferYang - ok cool thanks. I'll update the b/mark and try running this workflow.,"2,-1    ",2,-1,1
1547242903,1475305,2023/5/16,v3.2.4,Thanks @anishshri-db ,"1,-1    ",1,-1,0
1552036061,9700541,2023/5/16,v3.2.4,"Merged to master for Apache Spark 3.5.0. Thank you, @LuciferYang and all!","2,-1    ",2,-1,1
1542499515,1938382,2023/5/16,v3.2.4,@hvanhovell @cloud-fan ,"3,-1    ",3,-1,2
1547146047,6477701,2023/5/16,v3.2.4,Merged to master.,"1,-2    ",1,-2,-1
1543359349,9700541,2023/5/16,v3.2.4,"Welcome to the Apache Spark community, @TQJADE .
I added you to the Apache Spark contributor group and assigned SPARK-43441 to you.","1,-1    ",1,-1,0
1547145558,6477701,2023/5/16,v3.2.4,Merged to master.,"3,-2    ",3,-2,1
1543228247,51110188,2023/5/16,v3.2.4,"cc @rednaxelafx @dongjoon-hyun , Could you take a look if you find a time, thanks : )","1,-1    ",1,-1,0
1543322191,9700541,2023/5/16,v3.2.4,Thank you! Merged to master.,"1,-1    ",1,-1,0
1543676556,7322292,2023/5/16,v3.2.4,"It seems that `test_ops_on_diff_frames_*` is much slower atop Connect

```
Starting test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/e0c7d560-dd29-408d-9f7e-834cd2cc683a/python3.9__pyspark.pandas.tests.test_ops_on_diff_frames__j1jfu3kt.log)
Finished test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames (177s)
Starting test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/bff4fd9b-cf0b-4847-bc19-08114b9b106c/python3.9__pyspark.pandas.tests.test_ops_on_diff_frames_groupby__0fb3fdq5.log)
Finished test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_groupby (116s)
Starting test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/89a994de-9f04-4ed8-aa77-16d95b79956e/python3.9__pyspark.pandas.tests.test_ops_on_diff_frames_slow__zjjj9b5a.log)
Finished test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_slow (163s)
```

```
Starting test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/80844664-a5e0-4685-804e-8eff938c6681/python3.9__pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames__mdlsorlh.log)
Finished test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames (1191s)
Starting test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/bb5e908e-9185-46ac-826f-a2055f247f43/python3.9__pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby__50hudnbv.log)
Finished test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby (443s) ... 8 tests were skipped
Starting test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/a20e748b-8b2d-493c-b926-ade588f0cb7a/python3.9__pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_slow__c78c3kwt.log)
Finished test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_slow (911s) ... 7 tests were skipped
```

cc @itholic @HyukjinKwon ","1,-1    ",1,-1,0
1544366612,47337188,2023/5/16,v3.2.4,"> It seems that `test_ops_on_diff_frames_*` is much slower atop Connect

Interesting.. May I ask if CI jobs run those Connect tests on a single node?","1,-1    ",1,-1,0
1547061233,7322292,2023/5/16,v3.2.4,"> > It seems that `test_ops_on_diff_frames_*` is much slower atop Connect
> 
> Interesting.. May I ask if CI jobs run those Connect tests on a single node?

yes, all tests are on single node. I am afraid there maybe some performance issue in pandas on spark on connect","3,-1    ",3,-1,2
1547153412,7322292,2023/5/16,v3.2.4,"In the [latest run](https://github.com/zhengruifeng/spark/actions/runs/4975618218/jobs/8902974576):

the `pyspark-pandas-connnect` took 1h 25m 21s, and `pyspark-pandas-slow-connnect` took 1h 45m 22s, so I think the split itself should be fine.

but still see :

```
Starting test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/bcf91e64-da92-456c-a65f-2acbb5a57c5e/python3.9__pyspark.pandas.tests.test_ops_on_diff_frames__vhzam369.log)
Finished test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames (183s)
Starting test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/21464771-1bb4-4e2e-8a76-cc5424bc3eee/python3.9__pyspark.pandas.tests.test_ops_on_diff_frames_groupby__3jskgr8q.log)
Finished test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_groupby (121s)
Starting test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/4ec97d5c-ff3a-474f-aaa4-a3e08b503667/python3.9__pyspark.pandas.tests.test_ops_on_diff_frames_slow__bfsd6ob8.log)
Finished test(python3.9): pyspark.pandas.tests.test_ops_on_diff_frames_slow (167s)
```


```
Starting test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/7d09fc80-b92b-4872-ae33-62f61aa4d2d9/python3.9__pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames__3d49xqjs.log)
Finished test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames (1018s)
Starting test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/6542f9a4-39fb-4605-9e7b-6d9a3e7b5a63/python3.9__pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby__swoyoc14.log)
Finished test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_groupby (350s) ... 8 tests were skipped
Starting test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/7880eb8c-9299-4e1c-af32-daa48aa10715/python3.9__pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_slow__y24aawsj.log)
Finished test(python3.9): pyspark.pandas.tests.connect.test_parity_ops_on_diff_frames_slow (732s) ... 7 tests were skipped
```


I file https://issues.apache.org/jira/browse/SPARK-43501 to track the tests' performance, would you mind taking a look? @itholic ","1,-1    ",1,-1,0
1548298138,9700541,2023/5/16,v3.2.4,"Thank you so much, @zhengruifeng , @HyukjinKwon , @xinrong-meng , @itholic !","2,-1    ",2,-1,1
1548749738,7322292,2023/5/16,v3.2.4,"thank you all for the reviews, @dongjoon-hyun @HyukjinKwon @xinrong-meng @itholic ","1,-1    ",1,-1,0
1543395522,9700541,2023/5/16,v3.2.4,"I verified manually. Merged to master.
```
$ build/sbt ""catalyst/testOnly org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite""
...
[info] ExpressionImplUtilsSuite:
[info] - AesDecrypt Only (43 milliseconds)
[info] - AesEncrypt and AesDecrypt (3 milliseconds)
[info] Run completed in 993 milliseconds.
[info] Total number of tests run: 2
[info] Suites: completed 1, aborted 0
[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[success] Total time: 17 s, completed May 10, 2023, 11:10:40 PM
```","1,-1    ",1,-1,0
1543425929,10248890,2023/5/16,v3.2.4,"This code rn is messy because it's based on unmerged PR: https://github.com/apache/spark/pull/41026

Only need to look at the part I commented","1,-1    ",1,-1,0
1544324125,4190164,2023/5/16,v3.2.4,"@WweiL I can run your test succesfully with SBT commands:
```
./build/sbt package -Phive -Pconnect
sbt ""testOnly org.apache.spark.sql.streaming.StreamingQuerySuite""
```

You got the `ClassNotFound` exception probably because you were trying to run via the shell?
It would work with Ammonite shell. If you define the user defined writer via the shell, it should also work.","1,-1    ",1,-1,0
1544474453,4190164,2023/5/16,v3.2.4,"If you hit `stream classdesc serialVersionUID = -2719662620125650908, local class serialVersionUID = 6534627183855972490`

It means the client has a version and server has another, when trying to mapping them java failed to match the UID.
The solution is to let the client use the server version and move the server side version into a `sql-util` package.","1,-1    ",1,-1,0
1564985008,10248890,2023/5/16,v3.2.4,"Getting a scala test related error, the code works fine in REPL
```
 - foreach Row *** FAILED *** (15 milliseconds)
[info]   java.io.NotSerializableException: org.scalatest.Engine
[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
[info]   at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
[info]   at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
[info]   at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
[info]   at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
[info]   at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
[info]   at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
[info]   at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
[info]   at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
[info]   at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
[info]   at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
[info]   at org.apache.spark.util.Utils$.serialize(Utils.scala:126)
[info]   at org.apache.spark.sql.streaming.DataStreamWriter.foreach(DataStreamWriter.scala:226)
[info]   at org.apache.spark.sql.streaming.StreamingQuerySuite.$anonfun$new$12(StreamingQuerySuite.scala:204)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.sql.streaming.StreamingQuerySuite.org$scalatest$BeforeAndAfterAll$$super$run(StreamingQuerySuite.scala:35)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.sql.streaming.StreamingQuerySuite.run(StreamingQuerySuite.scala:35)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750)
```","1,-2    ",1,-2,-1
1543589047,9700541,2023/5/16,v3.2.4,"Thank you, @HyukjinKwon .
All test passed. Merged to master.","2,-1    ",2,-1,1
1543401743,32387433,2023/5/16,v3.2.4,cc @MaxGekk ,"1,-1    ",1,-1,0
1543750186,1580697,2023/5/16,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @Hisoka-X.","1,-3    ",1,-3,-2
1543754244,32387433,2023/5/16,v3.2.4,Thanks @MaxGekk ,"1,-1    ",1,-1,0
1543523838,9700541,2023/5/16,v3.2.4,"Thank you, @HyukjinKwon . 
AppVeyor passed. Merged to master.","1,-1    ",1,-1,0
1543578997,26535726,2023/5/16,v3.2.4,cc @dongjoon-hyun @sunchao @steveloughran ,"2,-1    ",2,-1,1
1543609365,162090,2023/5/16,v3.2.4,"we emptied the jar but left the stub artifact there so that things which did explicitly pull it in wouldn't start breaking.

Now that spark is 3.3.5+ only most of the hadoop-cloud-storage dependencies can be reworked down to just
* import hadoop-cloud-storage
* cut alliyun-sdk if you don't want it (the 3.3.5 version isn't breaking s3 any more, FWIW)
* add google gcs","1,-2    ",1,-2,-1
1543610018,26535726,2023/5/16,v3.2.4,@steveloughran does Hadoop 3.3.5 guarantee compatibility w/ previous versions? e.g. is it OK to use Hadoop 3.3.5 client to access Hadoop 3.3.0~3.3.4 server?,"1,-2    ",1,-2,-1
1543614246,162090,2023/5/16,v3.2.4,"SPARK-42537 covers the full cleanup.

w.r.t this patch: LGTM. ","2,-3    ",2,-3,-1
1543619460,9700541,2023/5/16,v3.2.4,Merged to master for Apache Spark 3.5.0.,"1,-1    ",1,-1,0
1543772258,162090,2023/5/16,v3.2.4,">  is it OK to use Hadoop 3.3.5 client to access Hadoop 3.3.0~3.3.4 server?

should be. IPC is all based on protobuf and we try not to remove things to avoid breaking existing code. HDFS compatibility across major versions is something which mattersd a lot, I believe webhdfs has the strongest guarantees.

what does break, guaranteed, is mixing hadoop libraries from different versions on the classpath. Avoid that. on and for cloudstuff openssl/wildfly is a source of extreme brittleness, even though when it works it's often faster than JVM ssl

","1,-1    ",1,-1,0
1543784029,26535726,2023/5/16,v3.2.4,"Got it, thanks @steveloughran ","1,-1    ",1,-1,0
1543671645,9700541,2023/5/16,v3.2.4,"Thank you, @HyukjinKwon , @wangyum , @zhengruifeng , @LuciferYang .
Every jobs are triggered successfully and running.
Since this is irrelevant to the test results, I'll merge to master for Apache Spark 3.5.0.","1,-3    ",1,-3,-2
1543545493,1475305,2023/5/16,v3.2.4,"cc @dongjoon-hyun @srowen FYI
","1,-1    ",1,-1,0
1543553685,1475305,2023/5/16,v3.2.4,Thanks @dongjoon-hyun @srowen ,"1,-1    ",1,-1,0
1543671132,26535726,2023/5/16,v3.2.4,cc @dongjoon-hyun @Yikun ,"3,-1    ",3,-1,2
1543685724,26535726,2023/5/16,v3.2.4,"> How many PRs do you want to make

No overall estimation, I have been working on this area recently, and would like to propose change when I find something has room for improvement.","2,-1    ",2,-1,1
1543725377,9700541,2023/5/16,v3.2.4,"Then, let's hold on this PR for a while because we don't need to take any risk.","1,-1    ",1,-1,0
1544188704,3421,2023/5/16,v3.2.4,Please change the subject: `[SPARK-43457][CONNECT][PYTHON]`,"1,-1    ",1,-1,0
1549237396,6477701,2023/5/16,v3.2.4,Merged to master.,"1,-3    ",1,-3,-2
1543806584,6757692,2023/5/16,v3.2.4,gental ping @dongjoon-hyun  @pan3793 ,"1,-1    ",1,-1,0
1543812345,26535726,2023/5/17,v3.2.4,"Please follow the https://spark.apache.org/contributing.html to

> Go to âActionsï¿½?tab on your forked repository and enable âBuild and testï¿½?and âReport test resultsï¿½?workflows","1,-1    ",1,-1,0
1545522908,6757692,2023/5/17,v3.2.4,Gentle ping @dongjoon-hyun ,"1,-1    ",1,-1,0
1547168365,6757692,2023/5/17,v3.2.4,gentle ping @holdenk  ,"1,-1    ",1,-1,0
1550972313,6757692,2023/5/17,v3.2.4,"thanks for the comments, I will check it.","1,-1    ",1,-1,0
1546607319,1580697,2023/5/17,v3.2.4,Waiting for CI.,"2,-1    ",2,-1,1
1547714691,44179472,2023/5/17,v3.2.4,"CI test `single listener, check trigger events are generated correctly` failed, which should be irrelevant to this change?","1,-1    ",1,-1,0
1549633131,1580697,2023/5/17,v3.2.4,"@bozhang2820 Could you rebase on the recent master and re-trigger GAs, please.","1,-1    ",1,-1,0
1549971413,1580697,2023/5/17,v3.2.4,"Looking at the two last commit, seems like just flaky tests.

Merging to master. Thank you, @bozhang2820.","2,-1    ",2,-1,1
1544256933,1475305,2023/5/17,v3.2.4,"I found some downloading log when run `dev/make-distribution.sh --tgz` with this pr

<img width=""1481"" alt=""image"" src=""https://github.com/apache/spark/assets/1475305/7d12d1a0-fb62-41ee-a7a8-63efc5fac0ee"">

<img width=""1535"" alt=""image"" src=""https://github.com/apache/spark/assets/1475305/5bc7ac2e-7f43-4a79-b770-637b66a2846c"">


is this expected","1,-1    ",1,-1,0
1544292470,1475305,2023/5/17,v3.2.4,"@wangyum I think we should not skip building the test jar. I test the following commands with this pr:

```
build/mvn versions:set -DnewVersion=3.5.0.1
dev/make-distribution.sh --tgz 
```

then the build will failed as follows:

```
[INFO] -----------------< org.apache.spark:spark-sketch_2.12 >-----------------
[INFO] Building Spark Project Sketch 3.5.0.1                             [3/30]
[INFO]   from common/sketch/pom.xml
[INFO] --------------------------------[ jar ]---------------------------------
Downloading from gcs-maven-central-mirror: https://maven-central.storage-download.googleapis.com/maven2/org/apache/spark/spark-tags_2.12/3.5.0.1/spark-tags_2.12-3.5.0.1-tests.jar
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/spark/spark-tags_2.12/3.5.0.1/spark-tags_2.12-3.5.0.1-tests.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.5.0.1:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  3.746 s]
[INFO] Spark Project Tags ................................. SUCCESS [  4.028 s]
[INFO] Spark Project Sketch ............................... FAILURE [01:16 min]
[INFO] Spark Project Local DB ............................. SKIPPED
[INFO] Spark Project Networking ........................... SKIPPED
[INFO] Spark Project Shuffle Streaming Service ............ SKIPPED
[INFO] Spark Project Unsafe ............................... SKIPPED
[INFO] Spark Project Common Utils ......................... SKIPPED
[INFO] Spark Project Launcher ............................. SKIPPED
[INFO] Spark Project Core ................................. SKIPPED
[INFO] Spark Project ML Local Library ..................... SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] Spark Project Connect Common ....................... SKIPPED
[INFO] Spark Project Connect Server ....................... SKIPPED
[INFO] Spark Project Connect Client ....................... SKIPPED
[INFO] Spark Protobuf ..................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  01:24 min
[INFO] Finished at: 2023-05-12T00:15:34+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project spark-sketch_2.12: Could not resolve dependencies for project org.apache.spark:spark-sketch_2.12:jar:3.5.0.1: Could not transfer artifact org.apache.spark:spark-tags_2.12:jar:tests:3.5.0.1 from/to central (https://repo.maven.apache.org/maven2): transfer failed for https://repo.maven.apache.org/maven2/org/apache/spark/spark-tags_2.12/3.5.0.1/spark-tags_2.12-3.5.0.1-tests.jar: Connect to repo.maven.apache.org:443 [repo.maven.apache.org/151.101.76.215, repo.maven.apache.org/2a04:4e42:12:0:0:0:0:215] failed: No route to host (connect failed) -> [Help 1]
```

the test jar `spark-tags_2.12-3.5.0.1-tests.jar` cannot be downloaded

","1,-1    ",1,-1,0
1544896492,5399861,2023/5/17,v3.2.4,Thank you @LuciferYang. It's not expected.,"1,-1    ",1,-1,0
1548013401,5399861,2023/5/17,v3.2.4,cc @dongjoon-hyun ,"1,-1    ",1,-1,0
1550638623,1475305,2023/5/17,v3.2.4,Thanks @wangyum @dongjoon-hyun @pan3793 . Merged to master.,"1,-1    ",1,-1,0
1544213148,3182036,2023/5/17,v3.2.4,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1550769163,8326978,2023/5/17,v3.2.4,"thanks, merged to master","1,-1    ",1,-1,0
1547632273,1580697,2023/5/17,v3.2.4,"I do believe the failed test suite `HealthTrackerIntegrationSuite` is not related to my changes. Though, I have checked it locally.

Merging to master. Thank you, @HyukjinKwon and @cloud-fan for review. ","1,-1    ",1,-1,0
1549107922,1591700,2023/5/17,v3.2.4,"These are part of spark env tab, right ? Why do we need to log them ?
... oh wait, you want to log this in each node as well ?

Agree with @HyukjinKwon, we should do it when driver/executor starts up.","1,-1    ",1,-1,0
1553393559,47337188,2023/5/17,v3.2.4,@ueshin @HyukjinKwon @zhengruifeng would you please review?,"1,-1    ",1,-1,0
1555302733,47337188,2023/5/17,v3.2.4,"Merged to master, thank you!","3,-1    ",3,-1,2
1555303500,47337188,2023/5/17,v3.2.4,"Please free to leave comments if any, I'll adjust them in follow-ups.","1,-1    ",1,-1,0
1545089488,9700541,2023/5/17,v3.2.4,"Merged to master. Thank you, @ueshin and @allisonwang-db .","1,-1    ",1,-1,0
1548403863,47337188,2023/5/17,v3.2.4,Does that refactoring still conform to UNSUPPORTED_DATA_TYPE_FOR_ARROW_VERSION?,"1,-1    ",1,-1,0
1548429951,506656,2023/5/17,v3.2.4,"@xinrong-meng 

> Does that refactoring still conform to UNSUPPORTED_DATA_TYPE_FOR_ARROW_VERSION?

This PR doesn't change anything related to pyarrow version.","1,-1    ",1,-1,0
1548613143,47337188,2023/5/17,v3.2.4,"Sorry I meant `UNSUPPORTED_DATA_TYPE_FOR_ARROW_CONVERSION`. Do we have plans to remove the constraints?
@ueshin ","2,-1    ",2,-1,1
1548627514,47337188,2023/5/17,v3.2.4,"Specifically, nested StructType, and MapType with keys/values in StructType/TimestampType?","1,-1    ",1,-1,0
1548658278,506656,2023/5/17,v3.2.4,"> Do we have plans to remove the constraints?

I'm not sure if it's planned, but now we can remove the constraints with a bit more work.","3,-1    ",3,-1,2
1548862046,6477701,2023/5/17,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1561810427,4534389,2023/5/17,v3.2.4,"This looks great, thanks for doing it @ueshin !","3,-1    ",3,-1,2
1544992096,1317309,2023/5/17,v3.2.4,cc. @zsxwing @viirya @HyukjinKwon Please take a look. Thanks!,"1,-1    ",1,-1,0
1548872475,6477701,2023/5/17,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1547096505,8486025,2023/5/17,v3.2.4,ping @cloud-fan ,"1,-1    ",1,-1,0
1547787139,3182036,2023/5/17,v3.2.4,"thanks, merging to master/3.4!","3,-1    ",3,-1,2
1548874149,8486025,2023/5/17,v3.2.4,@cloud-fan @HyukjinKwon Thank you for all!,"3,-1    ",3,-1,2
1545245681,26535726,2023/5/17,v3.2.4,cc @srowen @sunchao ,"1,-1    ",1,-1,0
1545265733,26535726,2023/5/17,v3.2.4,"There are other unexpected jars that were wrongly included in the assembly jar, would like to address them in separate PRs, since they were caused by different tickets. But I'm fine to fix all issues in one PR if the reviewers prefer.","3,-1    ",3,-1,2
1545981248,506679,2023/5/17,v3.2.4,"Merged to master, thanks!","2,-1    ",2,-1,1
1546043097,26535726,2023/5/17,v3.2.4,cc @dongjoon-hyun @srowen @sunchao @LuciferYang ,"1,-1    ",1,-1,0
1546298148,1475305,2023/5/17,v3.2.4,"https://github.com/apache/spark/blob/46332d985e52f76887c139f67d1471b82e85d5ca/connector/kafka-0-10-assembly/pom.xml#L62-L66

This one should be `2.5.0` before, I think we should manually specify its version to keep the behavior unchanged ?

","1,-1    ",1,-1,0
1546311720,1475305,2023/5/17,v3.2.4,"> Thank you for doing this with nice analysis. Ya, definitely, this was the goal. BTW, could you send an email to dev@spark because this is an important removal of dependency, @pan3793 ?

+1, Agree","1,-1    ",1,-1,0
1546587050,26535726,2023/5/17,v3.2.4,"@dongjoon-hyun @LuciferYang FYI, I have sent a mail to the mailing list
https://lists.apache.org/thread/xzrov348c3dq0d8jwcxf4j7fk7lb3r92","1,-1    ",1,-1,0
1546587564,26535726,2023/5/17,v3.2.4,"> https://github.com/apache/spark/blob/46332d985e52f76887c139f67d1471b82e85d5ca/connector/kafka-0-10-assembly/pom.xml#L62-L66
> 
> This one should be `2.5.0` before, I think we should manually specify its version to keep the behavior unchanged ?

@LuciferYang The version here does not matter. What's matter is the `<scope>provided</scope>`, which is used to exclude deps from the assembly jar.","2,-1    ",2,-1,1
1546897239,822522,2023/5/17,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1547138889,6477701,2023/5/17,v3.2.4,@zzzzming95 do you know in which PR fixed the issue in master branch?,"2,-2    ",2,-2,0
1556092507,13965087,2023/5/17,v3.2.4,"> do you know in which PR fixed the issue in master branch?
@HyukjinKwon 


https://github.com/apache/spark/pull/36637

It looks like this issue introduces `eagerlyExecuteCommands` so that AQE execute() is triggered before `InsertIntoHadoopFsRelationCommand`.","1,-1    ",1,-1,0
1550794232,1580697,2023/5/17,v3.2.4,"@johanl-db Are you working on the PR? Could you, please, address the comments above.","1,-1    ",1,-1,0
1557081376,112876214,2023/5/17,v3.2.4,"> @johanl-db Are you working on the PR? Could you, please, address the comments above.

I was away the past few days, I'm picking this up now. I addressed your comments, please take another look.","1,-1    ",1,-1,0
1557963875,1580697,2023/5/17,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @johanl-db.","1,-1    ",1,-1,0
1547138179,6477701,2023/5/17,v3.2.4,cc @gengliangwang FYI,"1,-1    ",1,-1,0
1554096090,32387433,2023/5/17,v3.2.4,cc @cloud-fan ,"1,-3    ",1,-3,-2
1547090980,6477701,2023/5/17,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1547047859,1317309,2023/5/17,v3.2.4,Thanks! Merging to master.,"1,-3    ",1,-3,-2
1547089829,6477701,2023/5/17,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1547089134,6477701,2023/5/17,v3.2.4,Merged to master.,"1,-3    ",1,-3,-2
1548618044,47337188,2023/5/17,v3.2.4,"Late LGTM, thank you!","1,-1    ",1,-1,0
1546538472,1580697,2023/5/17,v3.2.4,@panbingkun Isn't this a duplicate of https://github.com/apache/spark/pull/41155?,"1,-1    ",1,-1,0
1546540944,15246973,2023/5/17,v3.2.4,"> @panbingkun Isn't this a duplicate of #41155?

It seems like it's duplicated. Should I turn it off?","1,-1    ",1,-1,0
1546606467,1580697,2023/5/17,v3.2.4,"> It seems like it's duplicated. Should I turn it off?

Please, keep it alive so far.","1,-1    ",1,-1,0
1557967085,1580697,2023/5/17,v3.2.4,"@panbingkun Could you close this PR, please.","1,-1    ",1,-1,0
1558107678,15246973,2023/5/17,v3.2.4,"> @panbingkun Could you close this PR, please.

OK","1,-1    ",1,-1,0
1549567123,132866841,2023/5/17,v3.2.4,"@cloud-fan @wzhfy , please help review this pr, thanks. ","2,-2    ",2,-2,0
1551234335,132866841,2023/5/17,v3.2.4,gentle ping @cloud-fan ,"1,-1    ",1,-1,0
1552291841,10878553,2023/5/17,v3.2.4,"I also think that the different results between 0 in ('00') and 0 = '00' are confusing, and seems hive already fixed this problem.
Could you also take a look? @cloud-fan @MaxGekk ","1,-1    ",1,-1,0
1552411237,3182036,2023/5/17,v3.2.4,"I think this is indeed an issue, but it seems a bit weird to special-case the 1-element-in-list case. Thoughts? @gengliangwang @srielau ","1,-1    ",1,-1,0
1552504318,3182036,2023/5/17,v3.2.4,"BTW can we also check the behavior in other databases like mysql, postgres, oracle, etc.?","1,-1    ",1,-1,0
1552572954,132866841,2023/5/17,v3.2.4,"quickly check behavior in mysql, and `in ('00')` has same query result with `= '00'` . @cloud-fan 
![image](https://github.com/apache/spark/assets/132866841/715bf24b-f700-4110-bc10-27c15a27fc0d)
and also postgres behavior is consistent.
![image](https://github.com/apache/spark/assets/132866841/826134c9-8c3c-431f-9f83-1f895fdfcee7)


","1,-1    ",1,-1,0
1546777312,104112126,2023/5/17,v3.2.4,@cloud-fan Could you review the changes?,"1,-1    ",1,-1,0
1547111382,1475305,2023/5/17,v3.2.4,cc @sunchao @mridulm @pan3793 FYI,"1,-1    ",1,-1,0
1548109195,506679,2023/5/17,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1549015308,1475305,2023/5/17,v3.2.4,Thanks @sunchao @dongjoon-hyun @pan3793 ,"1,-1    ",1,-1,0
1546898548,1475305,2023/5/17,v3.2.4,Will update bench result and pr description later,"1,-1    ",1,-1,0
1547889645,822522,2023/5/18,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1549018817,1475305,2023/5/18,v3.2.4,Thanks @srowen ,"1,-1    ",1,-1,0
1547132182,6477701,2023/5/18,v3.2.4,"quick question, do you have any reference of this expression in other DBMSes?","1,-1    ",1,-1,0
1547154637,32387433,2023/5/18,v3.2.4,"> quick question, do you have any reference of this expression in other DBMSes?

Just presto. https://prestodb.io/docs/current/functions/json.html#json_array_get","1,-1    ",1,-1,0
1547821409,1475305,2023/5/18,v3.2.4,"<img width=""1007"" alt=""image"" src=""https://github.com/apache/spark/assets/1475305/342d7c37-95cb-4760-9829-5cdc4a4ebfc9"">

It seems that presto does not recommend using this function, and it may be removed in future presto version, so is it really necessary for Spark to support it?

","1,-2    ",1,-2,-1
1547829158,32387433,2023/5/18,v3.2.4,"> <img alt=""image"" width=""1007"" src=""https://user-images.githubusercontent.com/1475305/238357206-342d7c37-95cb-4760-9829-5cdc4a4ebfc9.png"">
> It seems that presto does not recommend using this function, and it may be removed in future presto version, so is it really necessary for Spark to support it?

The reason as presto say, the semantics of this function are broken. If we will not have the same issue, I think we can support it. ","2,-1    ",2,-1,1
1547844263,32387433,2023/5/18,v3.2.4,"By the way, does Spark have any way to get json array value use specified index. This is a useful function, if we can achieve this requirement without this function, this function maybe unnecessary.","2,-1    ",2,-1,1
1547916306,807537,2023/5/18,v3.2.4,"> By the way, does Spark have any way to get json array value use specified index. This is a useful function, if we can achieve this requirement without this function, this function maybe unnecessary.

I believe you cloud use get_json_object to archive the same result for `json_array_get`
```
scala> spark.sql(""""""select get_json_object('[{""a"":123},{""b"":""hello""}]', ""$[0]"");"""""").show
+------------------------------------------------+
|get_json_object([{""a"":123},{""b"":""hello""}], $[0])|
+------------------------------------------------+
|                                       {""a"":123}|
+------------------------------------------------+


scala> spark.sql(""""""select get_json_object('[{""a"":123},{""b"":""hello""}]', ""$[1]"");"""""").show
+------------------------------------------------+
|get_json_object([{""a"":123},{""b"":""hello""}], $[1])|
+------------------------------------------------+
|                                   {""b"":""hello""}|
+------------------------------------------------+
```","3,-1    ",3,-1,2
1547959397,32387433,2023/5/18,v3.2.4,"Seem like this function are unnecessary, I close the PR. Thanks @HyukjinKwon @LuciferYang @advancedxy ","1,-1    ",1,-1,0
1547442342,7322292,2023/5/18,v3.2.4,"thanks @HyukjinKwon for reviews, merged to master","1,-1    ",1,-1,0
1547370686,807537,2023/5/18,v3.2.4,The CI should pass. Would you mind to take a look at this @cloud-fan @vanzin?  Thanks.,"1,-1    ",1,-1,0
1549554217,807537,2023/5/18,v3.2.4,gently ping @cloud-fan @vanzin.,"1,-1    ",1,-1,0
1560607516,807537,2023/5/18,v3.2.4,@LuciferYang would you mind to take a look at this? ,"1,-1    ",1,-1,0
1547342279,15246973,2023/5/18,v3.2.4,cc @MaxGekk ,"1,-1    ",1,-1,0
1547471468,1475305,2023/5/18,v3.2.4,"Maybe we should keep connect client synchronized with this change, or at least add an `exclude` entry in `CheckConnectJvmClientCompatibility`","1,-2    ",1,-2,-1
1547530123,15246973,2023/5/18,v3.2.4,"> Maybe we should keep connect client synchronized with this change, or at least add an `exclude` entry in `CheckConnectJvmClientCompatibility`

I will add `exclude` entry in `CheckConnectJvmClientCompatibility`.
I will implements it at `connect` later.
","1,-1    ",1,-1,0
1547697865,1475305,2023/5/18,v3.2.4,"@panbingkun run `ProtoToParsedPlanTestSuite:` with this pr, `function_levenshtein` failed as follows:
```
[info] - function_levenshtein *** FAILED *** (4 milliseconds)
[info]   Expected and actual plans do not match:
[info]   
[info]   === Expected Plan ===
[info]   Project [levenshtein(g#0, bob) AS levenshtein(g, bob)#0]
[info]   +- LocalRelation <empty>, [id#0L, a#0, b#0, d#0, e#0, f#0, g#0]
[info]   
[info]   
[info]   === Actual Plan ===
[info]   Project [levenshtein(g#0, bob, None) AS levenshtein(g, bob)#0]
[info]   +- LocalRelation <empty>, [id#0L, a#0, b#0, d#0, e#0, f#0, g#0] (ProtoToParsedPlanTestSuite.scala:177)
```

We should re-generate the golden files.","2,-2    ",2,-2,0
1548233629,122326661,2023/5/18,v3.2.4,"I am wondering whether it is better to follow PostgreSQL's semantics:

> If the actual distance is less than or equal to max_d, then levenshtein_less_equal returns the correct distance; otherwise it returns some value greater than max_d.

or to follow `org.apache.commons.text.similarity.LevenshteinDistance.limitedCompare`'s semantics to return -1 when the distance is greater than the threshold (the current code).

I think the former is probably better: the optimizer can safely convert `levenshtein(s1, s2) < c` into `levenshtein(s1, s2, c) < c`, which I believe should be a quite common use case of `levenshtein`.

---

Update: never mind, I just realized that `levenshtein(s1, s2) < c` can also be converted into `levenshtein(s1, s2, c) != -1`  in the current semantics.","1,-1    ",1,-1,0
1550573293,1475305,2023/5/18,v3.2.4,also cc @wangyum @beliefer FYI,"1,-1    ",1,-1,0
1559850268,1580697,2023/5/18,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun.","1,-1    ",1,-1,0
1548731573,822522,2023/5/18,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1547728854,15246973,2023/5/18,v3.2.4,cc @LuciferYang ,"1,-2    ",1,-2,-1
1547794345,1475305,2023/5/18,v3.2.4,"Are there any other similar cases

","1,-1    ",1,-1,0
1547798463,15246973,2023/5/18,v3.2.4,"> Are there any other similar cases

No further cases have been found so far.","3,-1    ",3,-1,2
1548930116,15246973,2023/5/18,v3.2.4,cc @srielau @MaxGekk ,"1,-1    ",1,-1,0
1550154022,9700541,2023/5/18,v3.2.4,Merged to master for Apache Spark 3.5.0.,"1,-1    ",1,-1,0
1548830799,1191767,2023/5/18,v3.2.4,cc @tgravescs @jerryshao Please help review.,"3,-1    ",3,-1,2
1548929007,1191767,2023/5/18,v3.2.4,retest please,"1,-1    ",1,-1,0
1550575495,1591700,2023/5/18,v3.2.4,+CC @zhouyejoe ,"1,-1    ",1,-1,0
1553426997,4563792,2023/5/18,v3.2.4,"Have you only seen this once?  Do you have any repro case or tried to make a test to simulate?

I'm a bit unclear exactly how this happens, from taking a quick look, both those functions that add and remove are synchronized.  Am I missing how these can be called and actually run at the same time?

You are essentially implying that the completed container is called after we launched the executor but before we could add it containerIdToExecutorIdAndResourceProfileId.  I do see from your logs, all that happened with the same second.  Maybe not crucial but do you know why that container was completed so quickly?

As from my comment, I don't see you cleaning up this new datastructure, I would much rather look at either preventing this from running or perhaps having another launching state that you would check before removing.  But would like to understand above first.","1,-1    ",1,-1,0
1553978308,1191767,2023/5/18,v3.2.4,@tgravescs we have seen it quite often when YARN queues were full and the containers were **immediately preempted after launch**.  I've updated the PR keeping track of containers launching executors which can be cleaned up.,"1,-1    ",1,-1,0
1560490935,1191767,2023/5/18,v3.2.4,@tgravescs could you take another look?,"1,-1    ",1,-1,0
1561159748,4563792,2023/5/18,v3.2.4,+1 looks good,"1,-1    ",1,-1,0
1549024817,100322362,2023/5/18,v3.2.4,"@HeartSaVioR @LuciferYang - pls take a look. Thx

Runs now complete ~40-50 mins.","1,-1    ",1,-1,0
1549941757,9700541,2023/5/18,v3.2.4,"Thank you, @anishshri-db , @LuciferYang , @HeartSaVioR . Merged to master for Apache Spark 3.5.0.","1,-1    ",1,-1,0
1558518780,7322292,2023/5/18,v3.2.4,I think you may need to add `torcheval ` to https://github.com/apache/spark/blob/f55fdca10b1d9df2d8126cde07a26f89a75ae1d2/dev/infra/Dockerfile#L74,"1,-1    ",1,-1,0
1548791272,6477701,2023/5/18,v3.2.4,cc @sadikovi and @ueshin for a look.,"3,-1    ",3,-1,2
1548867900,6477701,2023/5/18,v3.2.4,Merged to master and branch-3.4.,"1,-1    ",1,-1,0
1552963918,26535726,2023/5/18,v3.2.4,cc @wangyum ,"1,-1    ",1,-1,0
1553712610,5399861,2023/5/18,v3.2.4,@pan3793 Could you update the pr description?,"1,-1    ",1,-1,0
1553912457,9700541,2023/5/18,v3.2.4,+1 for @wangyum 's comment.,"1,-1    ",1,-1,0
1553917876,26535726,2023/5/18,v3.2.4,"@wangyum @dongjoon-hyun, the PR description is updated, please take a look again.","1,-1    ",1,-1,0
1553948802,9700541,2023/5/18,v3.2.4,Thank you for updating. Merged to master for Apache Spark 3.5.0.,"2,-1    ",2,-1,1
1548903322,15246973,2023/5/18,v3.2.4,friendly cc @MaxGekk ,"1,-1    ",1,-1,0
1549343866,1580697,2023/5/18,v3.2.4,"I have checked the last commit locally.

Merging to master. Thank you, @panbingkun.","1,-1    ",1,-1,0
1549567125,1475305,2023/5/18,v3.2.4,late LGTM,"1,-1    ",1,-1,0
1548979353,7322292,2023/5/18,v3.2.4,merged to master,"1,-1    ",1,-1,0
1549019299,26535726,2023/5/18,v3.2.4,"I encountered the same issues w/ Spark 3.3.1. This sounds like a regression (I suppose it works before SPARK-25815, I don't have experience w/ running such an old version of Spark on K8s).

The key point is that the executor needs to download artifacts during the bootstrap phase, so the assumption in SPARK-25815 is not always true.

> adding the Hadoop config to the executor pods: this is not needed
> since the Spark driver will serialize the Hadoop config and send
> it to executors when running tasks.

Given the executor use `SparkHadoopUtil.get.newConfiguration(conf)` to construct Hadoop conf,  we can put the related hdfs/s3 configurations into `spark-defaults.conf` w/ `spark.hadoop.` prefix as a workaround.

https://github.com/apache/spark/blob/0df4c01b7c4d4476fe0de9dccb3425cc1295fc85/core/src/main/scala/org/apache/spark/executor/Executor.scala#L1006-L1012

This PR definitely fixes some use cases, @turboFei would you mind updating ""Does this PR introduce any user-facing change?""","1,-1    ",1,-1,0
1549024117,6757692,2023/5/18,v3.2.4,"> would you mind updating ""Does this PR introduce any user-facing change?""

updated","1,-1    ",1,-1,0
1549820782,6757692,2023/5/18,v3.2.4,"seems the k8s integration testing is stuck, will check this pr in our dev hadoop cluster tomorrow.","1,-1    ",1,-1,0
1550791604,6757692,2023/5/18,v3.2.4,"thanks for the comments, I will check it","2,-2    ",2,-2,0
1550797386,807537,2023/5/18,v3.2.4,"> Thank you for making a PR, @turboFei .
> 
> However, this PR might cause a outage because the number of configMap is controlled by quota.
> 
> ```
> $ kubectl describe quota | grep configmaps
> count/configmaps                                                  4     150
> ```
> 
> To avoid the production outage, this should be under a new configuration with `false` by default at least.

150 is a bit small for serious production usage, we may add this note in the `running_on_k8s.md`  documentation.

And BTW, this PR doesn't create new ConfigMaps, it either uses a user pre-set config map (no creation) or just reuse the config map created by driver which is created if necessary. ","1,-1    ",1,-1,0
1550976280,6757692,2023/5/18,v3.2.4,"> this PR doesn't create new ConfigMaps, it either uses a user pre-set config map (no creation) or just reuse the config map created by driver which is created if necessary.

yes, this PR doesn't create new ConfigMap.","2,-1    ",2,-1,1
1550983146,9700541,2023/5/18,v3.2.4,"Oh, got it. Thank you for correcting me.","1,-1    ",1,-1,0
1551118696,6757692,2023/5/18,v3.2.4,"the UT has passed, gentle ping @dongjoon-hyun ","1,-1    ",1,-1,0
1551222246,6757692,2023/5/18,v3.2.4,"thanks @advancedxy
added more UT to check `additionalPodSystemProperties` and executor pod.","2,-1    ",2,-1,1
1552365072,6757692,2023/5/18,v3.2.4,gentle ping @dongjoon-hyun would you like to review again? thanks,"1,-1    ",1,-1,0
1554062154,8326978,2023/5/18,v3.2.4,"The Hadoop configurations can be propagated after https://github.com/apache/spark/pull/27735. And putting and locating extra configuration files in SPARK_HOME/conf is also a suggested way from our docs, so is this step necessary?

Alternatively, if both exist, what is the precedence between them? Is it idempotent?","1,-1    ",1,-1,0
1556452364,6757692,2023/5/18,v3.2.4,"> And putting and locating extra configuration files in SPARK_HOME/conf is also a suggested way from our docs, so is this step necessary?

I think it is necessary.

Hadoop and spark are different components, it is better to maintain them separately.

In our company, we have conf version for hadoop conf, so we do not put hadoop config files under SPARK_HOME/conf, we use soft link to manage the hadoop conf.


> Alternatively, if both exist, what is the precedence between them? Is it idempotent?

In this pr, it just mounts the hadoop config map in the executor side(mounts HADOOP_CONF_DIR env) and the hadoop conf mounted is absolute same with that in driver pod.

As shown below, the SPARK_CONF_DIR has higher precedence. I think it is idempotent.
https://github.com/apache/spark/blob/e096bce604ed6fab37713437ac0d985673910537/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L68-L76

","1,-1    ",1,-1,0
1558479516,6757692,2023/5/18,v3.2.4,gentle ping @yaooqinn @dongjoon-hyun ,"1,-1    ",1,-1,0
1558489805,8326978,2023/5/18,v3.2.4,"> Hadoop and spark are different components, it is better to maintain them separately.

I do not fully agree. In the early days, Hadoop may be special. We have a specific code path to read HADOOP_CONF_DIR. But now Hadoop is an option, as we have other options for storage and scheduling, especially on the cloud or Kubernetes.

Maybe we shall treat it like hive configurations or other third-party components to reduce the maintenance burden and complexity of the deployment.","1,-1    ",1,-1,0
1558506309,6757692,2023/5/19,v3.2.4,"> Maybe we shall treat it like hive configurations or other third-party components to reduce the maintenance burden and complexity of the deployment.

I believe different companies treat hadoop conf differently.

For ebay, we add conf version for hadoop conf, because it is used by
- public  hadoop client nodes
- private hadoop client nodes
- hadoop service nodes(nn, rm, hms, kyuubi)
- hadoop slave nodes(nm, dn)

and between different conf versions, there might be incompatibilities. 

<img width=""974"" alt=""image"" src=""https://github.com/apache/spark/assets/6757692/83d503bf-bbfb-4ede-80e9-120ad8a24b38"">

and we have an RESTful service to download the hdaoop conf and we use soft link to manage them locally.

Recently, we are making spark migration, from spark3 + hadoop2 to spark3 + hadoop3.

For hadoop2 and hadoop3, the hadoop confs are even different.

So to manage the hadoop conf well and due to the current situation,  in ebay, we do not want to put the hadoop conf files and spark conf files together.


> treat it like hive configurations or other third-party components to reduce the maintenance burden and complexity of the deployment.

yes, I agree, it makes it easy.","1,-1    ",1,-1,0
1562256974,38865949,2023/5/19,v3.2.4,"I can also help with a use case for this, usually the submission client is on a single environment (Lets say we have it on cloud), and with spark on k8s, we can easily run jobs in different envs like in private Cloud Clusters being submitted from public Cloud. Where we would need diff properties to be passed for the submission client as well as for drivers and executors. This is also a use case where mounting the hadoopConfMap in executors would help in making the task easy to maintain the configs. ","2,-1    ",2,-1,1
1562455824,6757692,2023/5/19,v3.2.4,"> I can also help with a use case for this, usually the submission client is on a single environment (Lets say we have it on cloud), and with spark on k8s, we can easily run jobs in different envs like in private Cloud Clusters being submitted from public Cloud. Where we would need diff properties to be passed for the submission client as well as for drivers and executors. This is also a use case where mounting the hadoopConfMap in executors would help in making the task easy to maintain the configs.


Yes, I think this pr is general for hadoop conf use case, and it does not create more resource because it just use the existing config map.

@yaooqinn @dongjoon-hyun could you help to take another look? Appreciated for your help.
","1,-1    ",1,-1,0
1549094936,26535726,2023/5/19,v3.2.4,"Please use text instead of pictures to cite in the PR description, in case of users may want to search commit history for the issue caused by third-party dependencies","2,-1    ",2,-1,1
1549148024,15246973,2023/5/19,v3.2.4,"> Please use text instead of pictures to cite in the PR description, in case of users may want to search commit history for the issue caused by third-party dependencies

Ok, fix it.","1,-1    ",1,-1,0
1549882311,822522,2023/5/19,v3.2.4,Looks OK if tests can pass,"1,-1    ",1,-1,0
1550025030,9700541,2023/5/19,v3.2.4,"Thank you, @panbingkun , @pan3793 , @srowen . Merged to master.","1,-1    ",1,-1,0
1549688820,1475305,2023/5/19,v3.2.4,Merged to master.,"2,-1    ",2,-1,1
1549313503,15246973,2023/5/19,v3.2.4,I have checked all the codes of the `connect` module. (not include UT's code),"1,-1    ",1,-1,0
1549396581,6477701,2023/5/19,v3.2.4,Can we fix https://github.com/apache/spark/blob/master/scalastyle-config.xml to enforce this?,"2,-1    ",2,-1,1
1550941249,15246973,2023/5/19,v3.2.4,"Through the debug plugin scalastyle, I found that the logic of the ImportOrderChecker rule is as follows, for example, in the `ConnectRepl` file, the import order of reference classes is as followsï¿½?
<img width=""541"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/1ba23a98-9f88-4b67-a41a-0862d590eaf0"">

Firstly, when it iterates through the first import - `import ammonite.compiler.CodeClassWrapper`, it believes that there are no imports related to group `group.java` and `group.scala` at this time. The logic enters the check of the third group (`group.3rdParty`), and the rule we configured at this time only does not allow starting with `org.apache.spark.`. This condition is met, but it is not what we want.

By adjusting the regular matching rules of `group.3rdParty` group, the above situation can be avoided. We do not allow starting with `javax?.`, `scala.`, or `org.apache.spark.`.

PS: The logic of `org.scalastyle.scalariform.ImportOrderChecker` is as follows:
<img width=""588"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/0b47f217-19c0-4288-8537-c266731ec868"">
https://github.com/scalastyle/scalastyle/blob/ec14399543d2d5ccf93c3713aa5df21793844791/src/main/scala/org/scalastyle/scalariform/ImportsChecker.scala#L238-L276



","1,-1    ",1,-1,0
1550945373,15246973,2023/5/19,v3.2.4,friendly ping @HyukjinKwon @dongjoon-hyun @srowen @LuciferYang ,"1,-1    ",1,-1,0
1551107058,6477701,2023/5/19,v3.2.4,"@LuciferYang would you mind trying to merge this into `master` branch by `./dev/merge_spark_pr.py` when the tests pass? It would require you to set several environment variables like `JIRA_USERNAME`, and set the `remote` (please also read `./dev/merge_spark_pr.py` script). You should also install `pip install jira` (https://pypi.org/project/jira/) before running that script.

For example, this is my remote:

```
git remote -v
...
apache	https://github.com/apache/spark.git (fetch)
apache	https://github.com/apache/spark.git (push)
apache-github	https://github.com/apache/spark.git (fetch)
apache-github	https://github.com/apache/spark.git (push)
...
origin	https://github.com/HyukjinKwon/spark.git (fetch)
origin	https://github.com/HyukjinKwon/spark.git (push)
...
upstream	https://github.com/apache/spark.git (fetch)
upstream	https://github.com/apache/spark.git (push)
...
```","1,-1    ",1,-1,0
1551116687,1475305,2023/5/19,v3.2.4,"> @LuciferYang would you mind trying to merge this into `master` branch by `./dev/merge_spark_pr.py` when the tests pass? It would require you to set several environment variables like `JIRA_USERNAME`, and set the `remote` (please also read `./dev/merge_spark_pr.py` script). You should also install `pip install jira` (https://pypi.org/project/jira/) before running that script.
> 
> For example, this is my remote:
> 
> ```
> git remote -v
> ...
> apache	https://github.com/apache/spark.git (fetch)
> apache	https://github.com/apache/spark.git (push)
> apache-github	https://github.com/apache/spark.git (fetch)
> apache-github	https://github.com/apache/spark.git (push)
> ...
> origin	https://github.com/HyukjinKwon/spark.git (fetch)
> origin	https://github.com/HyukjinKwon/spark.git (push)
> ...
> upstream	https://github.com/apache/spark.git (fetch)
> upstream	https://github.com/apache/spark.git (push)
> ...
> ```

OK","1,-1    ",1,-1,0
1551123626,15246973,2023/5/19,v3.2.4,I hope to be qualified to do this one day in the future. Let me continue to strive for it! Congratulations again to @LuciferYang! haha,"1,-1    ",1,-1,0
1551130139,1475305,2023/5/19,v3.2.4,"> I hope to be qualified to do this one day in the future. Let me continue to strive for it! Congratulations again to @LuciferYang! haha

Thanks @panbingkun :)","1,-1    ",1,-1,0
1551415754,1475305,2023/5/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1558618352,15246973,2023/5/19,v3.2.4,"@HyukjinKwon @dongjoon-hyun @LuciferYang 
I have found issues similar to the above in the Java code. Do we need to make restrictions on this? Because the checkStyle plugin uses a similar configuration.
https://checkstyle.sourceforge.io/config_imports.html#ImportOrder
https://github.com/apache/spark/blob/eeab2e701330f7bc24e9b09ce48925c2c3265aa8/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java#L18-L36
","1,-1    ",1,-1,0
1558624517,1475305,2023/5/19,v3.2.4,"In fact, Spark does not perform checks on the import order of Java code

","1,-1    ",1,-1,0
1558630916,1475305,2023/5/19,v3.2.4,"@panbingkun Checking the import order of Java code involves a lot of code changes, which may cause conflicts with previous versions. I think this is not worth because it is just `import order`.

If we could define a rule that checks the import group without checking the order within the group, it might be easier to accept

","2,-2    ",2,-2,0
1558650701,15246973,2023/5/19,v3.2.4,"> If we could define a rule that checks the import group without checking the order within the group, it might be easier to accept

@LuciferYang 
CheckStyle supports this feature
<img width=""1141"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/3b2b0830-13fd-449b-ad87-1e5352d7117d"">
","1,-1    ",1,-1,0
1558665695,1475305,2023/5/19,v3.2.4,"Oh, I think you can give it a try","2,-1    ",2,-1,1
1549387264,15246973,2023/5/19,v3.2.4,cc @srowen ,"1,-1    ",1,-1,0
1550037989,9700541,2023/5/19,v3.2.4,"Could you re-trigger the failed pipeline, @panbingkun ?","1,-1    ",1,-1,0
1550518666,6477701,2023/5/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1550560512,1475305,2023/5/19,v3.2.4,late LGTM,"1,-1    ",1,-1,0
1549682379,7322292,2023/5/19,v3.2.4,this PR should be backported to branch-3.4,"1,-1    ",1,-1,0
1549683955,7322292,2023/5/19,v3.2.4,cc @HyukjinKwon ,"1,-1    ",1,-1,0
1550485109,6477701,2023/5/19,v3.2.4,Merged to master and branch-3.4.,"2,-1    ",2,-1,1
1550582947,32387433,2023/5/19,v3.2.4,cc @cloud-fan @sadikovi,"1,-1    ",1,-1,0
1551347103,3182036,2023/5/19,v3.2.4,do you know which commit broke this?,"2,-3    ",2,-3,-1
1551370991,32387433,2023/5/19,v3.2.4,"> do you know which commit broke this?

#37965","1,-1    ",1,-1,0
1552522063,3182036,2023/5/19,v3.2.4,"thanks, merging to master/3.4!","1,-1    ",1,-1,0
1552548713,32387433,2023/5/19,v3.2.4,Thanks @cloud-fan @sadikovi ,"1,-1    ",1,-1,0
1553612750,502522,2023/5/19,v3.2.4,@justaparth could you take a look at the comments? Would like to merge asap.,"1,-1    ",1,-1,0
1553751208,1002986,2023/5/19,v3.2.4,@rangadi just updated to respond to your comments! ,"1,-2    ",1,-2,-1
1554081799,502522,2023/5/19,v3.2.4,Thank you!,"1,-1    ",1,-1,0
1554108357,1002986,2023/5/19,v3.2.4,cc @HyukjinKwon  would you mind taking a look and merging this one? thanks ð ,"1,-1    ",1,-1,0
1554162004,7322292,2023/5/19,v3.2.4,merged to master,"1,-1    ",1,-1,0
1552735182,1475305,2023/5/19,v3.2.4,"@pengzhon-db Are you still interested in this? Do you have time to resolve the conflict?

","1,-1    ",1,-1,0
1553707533,94015493,2023/5/19,v3.2.4,@LuciferYang This is a POC PR. I've converted it to Draft for now. Will look into it later.,"1,-1    ",1,-1,0
1550510110,6477701,2023/5/19,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1550511351,47337188,2023/5/19,v3.2.4,"LGTM, thanks!","1,-1    ",1,-1,0
1550514716,7322292,2023/5/19,v3.2.4,late LGTMï¿½?,"1,-2    ",1,-2,-1
1553328363,99207096,2023/5/19,v3.2.4,"Note @MaxGekk @gengliangwang it says the GitHub actions CI failed, but actually this was just an unrelated PySpark flake and they actually passed :)","1,-1    ",1,-1,0
1556009814,99207096,2023/5/19,v3.2.4,Update on this: I thought of a simpler way to break apart this work into smaller pieces so we can make gradual improvements. Let me make a commit and then I can ping this thread again.,"1,-1    ",1,-1,0
1557836990,99207096,2023/5/19,v3.2.4,"This is done, I have deferred the constant folding logic to the analyzer. This is ready for another look.","1,-1    ",1,-1,0
1550478215,502522,2023/5/19,v3.2.4,"cc: @SandishKumarHN, @justaparth, @gengliangwang","1,-1    ",1,-1,0
1551260315,807537,2023/5/19,v3.2.4,"I believe one main problem of carrying the byte buffer is that it's serialized and deserialized when scheduling tasks. 

When the `FileDescritptorSet` size is large enough or many protobuf functions are used, task size would be larger and cause some scheduling overhead. It would be much lightweighter to just carrying the file path name.

To address that problem, we would normally broadcast the byte buffer, however that may not work well with spark connect?

Do you think it's necessary to give users the option to pass by descriptor file a.k.a the current behavior ?","1,-1    ",1,-1,0
1552305443,502522,2023/5/19,v3.2.4,@advancedxy broadcast is an interesting idea. Lets continue the discussion in a code comment here: https://github.com/apache/spark/pull/41192#discussion_r1197264386,"1,-1    ",1,-1,0
1552404914,502522,2023/5/19,v3.2.4,@gengliangwang PTAL when you get chance. ,"2,-1    ",2,-1,1
1553438582,502522,2023/5/19,v3.2.4,@SandishKumarHN Please approve if this looks good. Would like to merge this soon if there are no outstanding issues.,"1,-1    ",1,-1,0
1556869768,1002986,2023/5/19,v3.2.4,"thanks for this change, definitely nice to read the descriptor once and pass it around to avoid drift if the descriptor file changes over time ð ","2,-1    ",2,-1,1
1563840198,502522,2023/5/19,v3.2.4,"@LuciferYang do you need any more changes for this? I fixed a test that fails only github CI. 
I would like to get this merged asap.","1,-1    ",1,-1,0
1563840708,502522,2023/5/19,v3.2.4,"@LuciferYang please let me know if you need any more changes. If none, I can ask someone else to take a look and merge as well.","1,-1    ",1,-1,0
1564149233,1475305,2023/5/19,v3.2.4,"@rangadi 

<img width=""1150"" alt=""image"" src=""https://github.com/apache/spark/assets/1475305/70d708cf-a019-4ff6-a22f-4d7fea72aa02"">
","1,-1    ",1,-1,0
1564151004,1475305,2023/5/19,v3.2.4,"Let me take a look again

","1,-1    ",1,-1,0
1564211233,1475305,2023/5/19,v3.2.4,"@rangadi Please fix the compilation error first. For the rest, I only have this one question: https://github.com/apache/spark/pull/41192#discussion_r1206557463

","1,-1    ",1,-1,0
1565463249,502522,2023/5/19,v3.2.4,@LuciferYang all the test pass could you merge? I have multiple PRs waiting for this. ,"2,-1    ",2,-1,1
1565493031,1475305,2023/5/19,v3.2.4,Merged to master. Thanks @rangadi ,"1,-1    ",1,-1,0
1550524919,506656,2023/5/19,v3.2.4,@xinrong-meng Could you triage and take a look at the tests still disabled?,"1,-1    ",1,-1,0
1550545050,47337188,2023/5/19,v3.2.4,Thank you @ueshin !,"2,-1    ",2,-1,1
1550610593,7322292,2023/5/19,v3.2.4,merged to master,"1,-1    ",1,-1,0
1550822915,1475305,2023/5/19,v3.2.4,late LGTM,"3,-1    ",3,-1,2
1551422222,822522,2023/5/19,v3.2.4,"Can you explain this more - your steps to reproduce require you to remove files from the build, and that causes the problem? but the JARs were there before you removed them. How does removing the scope change the result?","1,-1    ",1,-1,0
1551463081,5399861,2023/5/19,v3.2.4,Thank you @srowen. I have updated the PR description.,"1,-1    ",1,-1,0
1551701267,822522,2023/5/19,v3.2.4,"It seems weird that log4j 2 config works, if you add log4j 1.x. Maybe so, just trying to figure out if this is really what's going on and if we have to let log4j 1.x back in? because then we have both log4j in the distribution","2,-1    ",2,-1,1
1552245794,3536454,2023/5/20,v3.2.4,"Maybe similar reason I made https://github.com/apache/spark/pull/37694 a while ago? Basically Spark logging setup assumes log4j2, but with hadoop provided you get 1.x from Hadoop. So you get weird logging behavior. We just have both providers in the class path and slf seems to pick the 2.x one","2,-2    ",2,-2,0
1554550499,822522,2023/5/20,v3.2.4,Seems reasonable then. Let's just get the tests to run again.,"1,-1    ",1,-1,0
1555080586,3536454,2023/5/20,v3.2.4,"Actually hit a new issue related to this after finally being able to test out Spark 3.4 from the Delta release. Because of the bump to slf4j 2, it seems `log4j-slf4j2-impl` doesn't get recognized, so it always falls back to the log4j/reload4j 1.x for logging. I had to also include slf4j-api 2.0.6 to get Spark's built-in log handling to work correctly (i.e. `SparkContext.setLogLevel`)","1,-1    ",1,-1,0
1555428658,5399861,2023/5/20,v3.2.4,@Kimahriman Do you have a way to reproduce?,"1,-1    ",1,-1,0
1555905091,3536454,2023/5/20,v3.2.4,"Nevermind it looks like including `log4j-1.2-api` actually fixes the issue, I wasn't including that before. In fact, just including `log4j-1.2-api` fixes the issue I made for https://issues.apache.org/jira/browse/SPARK-40246. That might not fix custom log4j2 configs, but at least makes `SparkContext.setLogLevel` work with a hadoop-provided build","2,-1    ",2,-1,1
1556043789,822522,2023/5/20,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1551100694,6477701,2023/5/20,v3.2.4,Merged to master.,"3,-4    ",3,-4,-1
1551115142,1475305,2023/5/20,v3.2.4,"cc @dongjoon-hyun @HyukjinKwon @srowen @hvanhovell FYI

I want to use `GenerateMIMAIgnore` to generate default exclude filters file base on master branch to solve SPARK-43246 for connect module and encountered this problem. 

In the long run, we may also encounter this issue when we need run dev/mima with `previousSparkVersion = 3.5.0`
 ","1,-1    ",1,-1,0
1551419275,822522,2023/5/20,v3.2.4,To be clear this particular issue is not related to MiMa right? this ASM change fixes Jackson + Java bytecode version issues? they are related?,"3,-1    ",3,-1,2
1551467738,1475305,2023/5/21,v3.2.4,"Yes, this is not related to MiMa check itself.

1. I hope to reuse `GenerateMIMAIgnore` base on master to generate `.generated-mima-class-excludes` and `.generated-mima-member-excludes` during  `dev/connect-jvm-client-mima-check` check process(https://github.com/apache/spark/pull/40925), then I encountered this problem.

2. In the long run, when we need to performing mima check between Spark 3.6.0 and Spark 3.5.0, we will also need to solve this problem if jackson-core always with Java 11/19 code(due to the `OLD_DEPS_CLASSPATH` will also contain jackson-core 2.15.0+), but this can indeed be fixed when things happen



I have give an independent pr to solve this issue. If this is not appropriate, I can directly place this change in https://github.com/apache/spark/pull/40925 for workaround


","1,-1    ",1,-1,0
1551484882,1475305,2023/5/21,v3.2.4,And I think the key issue solved by this PR is that the classpath processed by GenerateMIMAIgnore cannot contain Java 17 compiled code now due to the ASM version is too low,"2,-1    ",2,-1,1
1551518733,1475305,2023/5/21,v3.2.4,Updated the pr description. ,"2,-1    ",2,-1,1
1551703450,822522,2023/5/22,v3.2.4,"Oh shoot, I was looking at the wrong PR - I'm not sure that tests passed before I merged. Let me watch the result.","2,-1    ",2,-1,1
1551729535,9700541,2023/5/22,v3.2.4,"No worry, @srowen ~ I'll monitor together.","1,-1    ",1,-1,0
1551749676,1475305,2023/5/22,v3.2.4,"If there are any issues, please revert and I will resubmit one :)

","1,-1    ",1,-1,0
1551953229,822522,2023/5/22,v3.2.4,"OK yeah it was fine, false alarm. Oops.","1,-2    ",1,-2,-1
1552371620,1475305,2023/5/22,v3.2.4,Thanks @srowen @dongjoon-hyun ~,"3,-2    ",3,-2,1
1551416331,822522,2023/5/22,v3.2.4,See https://spark.apache.org/contributing.html and please fix up this PR. Needs more explanation too,"1,-1    ",1,-1,0
1561648001,3080594,2023/5/22,v3.2.4,@dongjoon-hyun could you please review this. Thanks!,"1,-1    ",1,-1,0
1554099340,15246973,2023/5/22,v3.2.4,This is done.,"1,-2    ",1,-2,-1
1555176941,1580697,2023/5/22,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun.","2,-1    ",2,-1,1
1551615074,9700541,2023/5/22,v3.2.4,cc @pralabhkumar and @holdenk from #37417,"1,-1    ",1,-1,0
1551827663,59893,2023/5/22,v3.2.4,+1 looks reasonable module the existing suggestions (clean up the logging + tighten the test). Thanks for making this PR :) ,"1,-1    ",1,-1,0
1552336382,16147255,2023/5/22,v3.2.4,LGTM . ,"1,-1    ",1,-1,0
1558478053,6757692,2023/5/22,v3.2.4,"gentle ping @dongjoon-hyun  @holdenk @pralabhkumar @pan3793 for the latest change, thanks a lot.","1,-1    ",1,-1,0
1551806149,790409,2023/5/22,v3.2.4,"Thanks for comments, updated","1,-1    ",1,-1,0
1552472727,9700541,2023/5/22,v3.2.4,Merged to master~,"1,-1    ",1,-1,0
1551726374,17288675,2023/5/22,v3.2.4,"@bersprockets here are the changes to handle non-foldable input args, based on our conversation in https://github.com/apache/spark/pull/40615. cc @dtenedor @mkaravel ","1,-1    ",1,-1,0
1551745541,99207096,2023/5/22,v3.2.4,@RyanBerti thanks for the update!,"1,-1    ",1,-1,0
1551953337,17288675,2023/5/22,v3.2.4,"@dtenedor I just pushed a commit that tries to generalize the foldable check, as I'm seeing duplicate code in the datasketches functions as well as others (see ApproxCountDistinctForIntervals, ApproximatePercentile, CountMinSketchAgg, etc). I'm open to modifying the trait as needed, or reverting the commit and implementing your suggestion.","1,-1    ",1,-1,0
1551964588,99207096,2023/5/22,v3.2.4,The new trait looks good. In the future we can think about reusing it.,"1,-1    ",1,-1,0
1559903605,17288675,2023/5/22,v3.2.4,@HyukjinKwon could you help us merge this one?,"1,-2    ",1,-2,-1
1553162305,1580697,2023/5/22,v3.2.4,"@cloud-fan Could you review this PR, please.","1,-1    ",1,-1,0
1553436137,1580697,2023/5/22,v3.2.4,"Merging to master. Thank you, @cloud-fan for review.","1,-1    ",1,-1,0
1554129823,1580697,2023/5/22,v3.2.4,"@ericm-db Could you allow GitHub actions in your fork and re-trigger GAs, please.","1,-1    ",1,-1,0
1555324959,132308037,2023/5/22,v3.2.4,"Thanks for the review! I've made the changes, and I think it's ready to merge now @MaxGekk @HeartSaVioR ","2,-1    ",2,-1,1
1555791735,1580697,2023/5/22,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @ericm-db.","1,-1    ",1,-1,0
1555800355,1580697,2023/5/22,v3.2.4,"@ericm-db Congratulations with your first contribution to Apache Spark!
","1,-1    ",1,-1,0
1552363238,3421,2023/5/22,v3.2.4,Thanks!,"1,-1    ",1,-1,0
1552430701,6477701,2023/5/22,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1552339005,6477701,2023/5/22,v3.2.4,Merged to branch-3.4.,"1,-2    ",1,-2,-1
1552340648,6477701,2023/5/22,v3.2.4,Merged to branch-3.4.,"2,-2    ",2,-2,0
1552334558,1475305,2023/5/22,v3.2.4,cc @attilapiros @viirya @sunchao @pan3793 FYI,"1,-1    ",1,-1,0
1552341297,15246973,2023/5/22,v3.2.4,"<img width=""893"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/6da74b5d-4e71-440e-bb47-d17ba7f7de1e"">
","2,-1    ",2,-1,1
1552365417,6477701,2023/5/22,v3.2.4,Hmm .. does that mean Hadoop 3.2.0 won't work with this?,"1,-1    ",1,-1,0
1552366068,6477701,2023/5/22,v3.2.4,"Hm, we currently build Spark w/ Hadoop 3.3.0 by default it might be fine but I would also ask some more looks e.g., @srowen @mridulm @tgravescs @dongjoon-hyun ","1,-1    ",1,-1,0
1552385793,506679,2023/5/22,v3.2.4,"Spark only works with Hadoop 3.3.1+ at the moment, as we discovered in https://github.com/apache/spark/pull/40847. We can potentially make it work with Hadoop 3.2.2+ if there's a workaround for https://issues.apache.org/jira/browse/SPARK-40039 which uses Hadoop API that only exist in Hadoop 3.3.1+. It definitely won't work with Hadoop 3.2.0 due to some shaded client related issues.","1,-1    ",1,-1,0
1552395877,6477701,2023/5/22,v3.2.4,Thanks for clarification. Lgtm2,"1,-1    ",1,-1,0
1552489145,15246973,2023/5/22,v3.2.4,"1.https://issues.apache.org/jira/browse/HADOOP-14067
<img width=""816"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/3e59dc12-75de-457e-bd19-2fc75294f0ce"">

2.https://issues.apache.org/jira/browse/SPARK-32256
<img width=""652"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/efe72ef7-8e95-4cf1-b1ae-3d8358798ec5"">
","1,-1    ",1,-1,0
1552689097,9700541,2023/5/22,v3.2.4,"Merged to master. Thank you, @panbingkun .","1,-1    ",1,-1,0
1563757051,44108233,2023/5/22,v3.2.4,"@HyukjinKwon @dongjoon-hyun CI passed.
Also cc @zhengruifeng @ueshin @xinrong-meng FYI.","1,-1    ",1,-1,0
1552886306,8486025,2023/5/22,v3.2.4,ping @dongjoon-hyun @Yikun ,"3,-1    ",3,-1,2
1554271802,8486025,2023/5/22,v3.2.4,"> Do you think you can make this new test environment variable works for both Maven and SBT, @beliefer ?

AFAIK, `SparkBuilder` only used for SBT.","2,-1    ",2,-1,1
1558461486,8486025,2023/5/22,v3.2.4,@dongjoon-hyun Could you take a look again ? cc @Yikun ,"1,-1    ",1,-1,0
1558857649,9700541,2023/5/22,v3.2.4,"> > Do you think you can make this new test environment variable works for both Maven and SBT, @beliefer ?
> 
> AFAIK, `SparkBuilder` only used for SBT.

What I asked was to support `HEAP_SIZE` in Maven, too.","1,-1    ",1,-1,0
1558932202,8486025,2023/5/22,v3.2.4,"@dongjoon-hyun I think no need to support `HEAP_SIZE` in Maven, just like the `METASPACE_SIZE` .","1,-1    ",1,-1,0
1559104006,9700541,2023/5/22,v3.2.4,Got it.,"1,-1    ",1,-1,0
1560355666,8486025,2023/5/22,v3.2.4,cc @viirya @srowen ,"1,-1    ",1,-1,0
1561593418,822522,2023/5/22,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1562137379,8486025,2023/5/22,v3.2.4,@srowen @dongjoon-hyun @viirya Thank you!,"1,-1    ",1,-1,0
1553995933,5399861,2023/5/22,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1554019805,8326978,2023/5/22,v3.2.4,thanks @wangyum ,"1,-1    ",1,-1,0
1552962354,3182036,2023/5/23,v3.2.4,cc @Yikf @dongjoon-hyun @HyukjinKwon ,"1,-1    ",1,-1,0
1553938639,9700541,2023/5/23,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1552936070,26535726,2023/5/23,v3.2.4,cc @LuciferYang ,"1,-1    ",1,-1,0
1553807658,6477701,2023/5/23,v3.2.4,"@panbingkun how did you find them?

FWIW, I thought we added a compilation feature to disallow unused variables IIRC, @LuciferYang .","1,-1    ",1,-1,0
1553870280,15246973,2023/5/23,v3.2.4,"> @panbingkun how did you find them?
> 
> FWIW, I thought we added a compilation feature to disallow unused variables IIRC, @LuciferYang .

@HyukjinKwon 

1.When I review the HadoopRDD code, I noticed some inspection prompts, eg:
<img width=""808"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/4dc7c9b9-a549-4e32-a0cf-679a236f13b1"">

2.So I wonder if there are other cases in the core module, then I export `unused declaration` rule to single file, name as `unused_declaration.xml`
<img width=""443"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/be2d73d6-a900-4a33-9927-91f9e191703c"">

3.execute command:
""/Users/panbingkun/Library/Application Support/JetBrains/Toolbox/apps/IDEA-C/ch-0/231.9011.34/IntelliJ IDEA CE.app/Contents/bin/inspect.sh"" /Users/panbingkun/Developer/spark/spark-community /Users/panbingkun/Developer/spark/spark-community/unused_declaration.xml /Users/panbingkun/Developer/spark/spark-community/unused_declaration_results -v2 -d /Users/panbingkun/Developer/spark/spark-community/core/src

4.The declared unused variable results in all `core` modules will be saved in the `unused_declaration_results` file.

5.Open it by IDE.
<img width=""695"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/74096512-4f09-4fab-8678-97e6a59cd85c"">

6.Some inspect results may be inaccurate and ultimately require manual reconfirmation.

","1,-1    ",1,-1,0
1553924881,1475305,2023/5/23,v3.2.4,"> IIRC

Currently, we have only added compilation checks on `-Ywarn-unused:imports` for Scala 2.12 (the behavior of Scala 2.13 is slightly different). 

https://github.com/apache/spark/blob/411bcd2f18b2275466edc752c8cdcdfcaab1cb9c/pom.xml#L2864

For the current version of Scala 2.12, we can further try to add more checks, such as `patvars`, `privates`, and `locals`. For codes that cannot be changed, we can use `@nowarn` to suppress them.","1,-3    ",1,-3,-2
1563920852,1580697,2023/5/23,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun and @LuciferYang @HyukjinKwon for review.","1,-1    ",1,-1,0
1553692887,5399861,2023/5/23,v3.2.4,"I tested it through Java 20.
Before: https://github.com/wangyum/spark/actions/runs/5015734585/jobs/8991701615
After: https://github.com/wangyum/spark/actions/runs/5015807076/jobs/8992120361","1,-3    ",1,-3,-2
1553728429,9700541,2023/5/23,v3.2.4,"If you don't mind, let's hold on this PR because Java 20 is not our real target.","1,-1    ",1,-1,0
1553807202,5399861,2023/5/23,v3.2.4,I'd like to find a failed test case to add to https://github.com/Homebrew/homebrew-core/pull/131189.,"1,-1    ",1,-1,0
1553857614,9700541,2023/5/23,v3.2.4,"Well, `cyclonedx-maven-plugin` is not a test case. Also, it's used at release process, not a runtime.
> I'd like to find a failed test case to add to https://github.com/Homebrew/homebrew-core/pull/131189.","2,-1    ",2,-1,1
1553861234,9700541,2023/5/23,v3.2.4,"Maybe, you want to hide `cyclonedx-maven-plugin` under a profile?","1,-1    ",1,-1,0
1553862383,5399861,2023/5/23,v3.2.4,"2.7.9 also contains other bug fixes and improvements:
https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.9
https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.8
https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.7","1,-1    ",1,-1,0
1553903460,9700541,2023/5/23,v3.2.4,"Do you think Apache Spark's SBOM artifacts are affected by one of them?
> 2.7.9 also contains other bug fixes and improvements:","1,-1    ",1,-1,0
1553945799,5399861,2023/5/23,v3.2.4,OK. so ,"1,-1    ",1,-1,0
1553947929,9700541,2023/5/23,v3.2.4,Thank you for closing. We can reopen this when `master` branch is ready for `Java 21` and we are able to verify this patch.,"1,-1    ",1,-1,0
1553540066,1580697,2023/5/23,v3.2.4,"@dongjoon-hyun @cloud-fan Could you review this backport, please.","1,-1    ",1,-1,0
1553726468,9700541,2023/5/23,v3.2.4,Merged to branch-3.4 for Apache Spark 3.4.1.,"1,-1    ",1,-1,0
1553845924,9700541,2023/5/23,v3.2.4,Merged to branch-3.3.,"1,-1    ",1,-1,0
1553804886,9700541,2023/5/23,v3.2.4,Thank you!,"1,-1    ",1,-1,0
1553926407,9700541,2023/5/23,v3.2.4,"Thank you, @HyukjinKwon !
K8s (Unit and Integration) tests passed at the first commit and dependency test passed at the second commit already.
Let me merge this.","1,-1    ",1,-1,0
1553915757,15246973,2023/5/23,v3.2.4,"> Oh, could you take a look at the build error? This looks like a breaking change.
> 
> ```
> [error] /home/runner/work/spark/spark/project/SparkBuild.scala:34:21: object sbt is not a member of package com.typesafe
> [error] import com.typesafe.sbt.pom.{PomBuild, SbtPomKeys}
> ```

Ok, let me check it.","1,-1    ",1,-1,0
1553928960,9700541,2023/5/23,v3.2.4,Thank you for update!,"1,-1    ",1,-1,0
1554000199,9700541,2023/5/23,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1553937512,4690923,2023/5/23,v3.2.4,@mridulm @shuwang21 @zhouyejoe Please review,"1,-1    ",1,-1,0
1553952178,4690923,2023/5/23,v3.2.4,"> I'm wondering if this can unblock [SPARK-36744](https://issues.apache.org/jira/browse/SPARK-36744) (Support IO encryption for push-based shuffle). If not, do you happen to know what are the remaining blockers?

This doesn't unblock [SPARK-36744](https://issues.apache.org/jira/browse/SPARK-36744). To be able to read merged data, clients only know the map outputs belonging to that merged blocks. If the map outputs are encrypted, the client will need to know the size as well to decrypt it which is a protocol change. 

Here, the server supports encryption but the application which is doing push-based shuffle doesn't have encryption enabled. I will add this to the summary.","1,-1    ",1,-1,0
1554001410,9700541,2023/5/23,v3.2.4,"Thank you for sharing the rich context, @otterc .","1,-1    ",1,-1,0
1554049148,8847816,2023/5/23,v3.2.4,"LGTM. Thanks for your efforts!

Do you think when `spark.network.crypto.saslFallback=true` and L95 from `AuthRpcHandler.java`. 
```
saslHandler = new SaslRpcHandler(conf, channel, null, secretKeyHolder);
```
which will set `delegate=null`. Will this cause potential NPE? 

","1,-1    ",1,-1,0
1554906573,4690923,2023/5/23,v3.2.4,"> Do you think when `spark.network.crypto.saslFallback=true` and L95 from `AuthRpcHandler.java`.
> 
> ```
> saslHandler = new SaslRpcHandler(conf, channel, null, secretKeyHolder);
> ```
> 
> which will set `delegate=null`. Will this cause potential NPE?

No, with this modification, we obtain the reference to the `MergedBlockMetaReqHandler` from the `delegate (ExternalBlockHandler)` that the `AuthRpcHandler` is initialized with. The `saslHandler` instance within the `AuthRpcHandler` is specifically created for fallback to SASL, and we should not have used it to retrieve the MergedBlockMetaReqHandler. It is insignificant whether the saslHandler is initialized with a null delegate or not.","1,-1    ",1,-1,0
1555014603,8847816,2023/5/23,v3.2.4,"> > Do you think when `spark.network.crypto.saslFallback=true` and L95 from `AuthRpcHandler.java`.
> > ```
> > saslHandler = new SaslRpcHandler(conf, channel, null, secretKeyHolder);
> > ```
> > 
> > 
> >     
> >       
> >     
> > 
> >       
> >     
> > 
> >     
> >   
> > which will set `delegate=null`. Will this cause potential NPE?
> 
> No, with this modification, we obtain the reference to the `MergedBlockMetaReqHandler` from the `delegate (ExternalBlockHandler)` that the `AuthRpcHandler` is initialized with. The `saslHandler` instance within the `AuthRpcHandler` is specifically created for fallback to SASL, and we should not have used it to retrieve the MergedBlockMetaReqHandler. It is insignificant whether the saslHandler is initialized with a null delegate or not.

I see. Make sense to me. Thanks!","1,-1    ",1,-1,0
1557929717,4690923,2023/5/23,v3.2.4,@dongjoon-hyun @mridulm @tgravescs @Ngone51 Could you please help review,"1,-1    ",1,-1,0
1558024638,4690923,2023/5/23,v3.2.4,Thank you @dongjoon-hyun and @shuwang21!,"1,-1    ",1,-1,0
1554205847,1475305,2023/5/23,v3.2.4,"cc @zhenlineo I remember you mentioned a bug in mima 1.1.1: `the MiMa will not be able to check the class methods if the object is marked private`, so Spark have been using 1.1.0 before, do you have time to help confirm if 1.1.2 has fixed this issue?


","1,-1    ",1,-1,0
1554208491,9700541,2023/5/23,v3.2.4,"Oh, +1 for @LuciferYang 's comment.","1,-1    ",1,-1,0
1554804591,4190164,2023/5/23,v3.2.4,"I checked locally. MiMa 1.1.2 can find errors about missing private classes e.g. `private[sql] object Dataset`
```
object org.apache.spark.sql.Dataset does not have a correspondent in client version
```
So it is good to upgrade to 1.1.2!","1,-1    ",1,-1,0
1555032827,9700541,2023/5/23,v3.2.4,"Thank you, @panbingkun , @LuciferYang , @zhenlineo !
Merged to master.","1,-1    ",1,-1,0
1556417112,1475305,2023/5/23,v3.2.4,late LGTM,"1,-1    ",1,-1,0
1554067227,1475305,2023/5/23,v3.2.4,"Expected NPE occurred:

```
Warning: Unable to serialize throwable of type io.grpc.StatusRuntimeException for TestFailed(Ordinal(0, 271),INTERNAL: Job aborted due to stage failure: Task 0 in stage 150.0 failed 1 times, most recent failure: Lost task 0.0 in stage 150.0 (TID 316) (localhost executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:53)
	at org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.serialize(TypedAggregateExpression.scala:267)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:620)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:280)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:107)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:117)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.sca...,UserDefinedFunctionE2ETestSuite,org.apache.spark.sql.UserDefinedFunctionE2ETestSuite,Some(org.apache.spark.sql.UserDefinedFunctionE2ETestSuite),Dataset reduce,Dataset reduce,Vector(),Vector(),Some(io.grpc.StatusRuntimeException: INTERNAL: Job aborted due to stage failure: Task 0 in stage 150.0 failed 1 times, most recent failure: Lost task 0.0 in stage 150.0 (TID 316) (localhost executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:53)
	at org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.serialize(TypedAggregateExpression.scala:267)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:620)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:280)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:107)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:117)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.sca...),Some(290),Some(IndentedText(- Dataset reduce,Dataset reduce,0)),Some(SeeStackDepthException),Some(org.apache.spark.sql.UserDefinedFunctionE2ETestSuite),None,pool-1-thread-1-ScalaTest-running-UserDefinedFunctionE2ETestSuite,1684472246665), setting it as NotSerializableWrapperException.
[info] - Dataset reduce *** FAILED *** (290 milliseconds)
[info]   io.grpc.StatusRuntimeException: INTERNAL: Job aborted due to stage failure: Task 0 in stage 150.0 failed 1 times, most recent failure: Lost task 0.0 in stage 150.0 (TID 316) (localhost executor driver): java.lang.NullPointerException
[info] 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:53)
[info] 	at org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.serialize(TypedAggregateExpression.scala:267)
[info] 	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:620)
[info] 	at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:280)
[info] 	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:107)
[info] 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:117)
[info] 	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)
[info] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
[info] 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
[info] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[info] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[info] 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info] 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[info] 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[info] 	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
[info] 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
[info] 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
[info] 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[info] 	at org.apache.spark.scheduler.Task.run(Task.sca...
[info]   at io.grpc.Status.asRuntimeException(Status.java:535)
[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)
[info]   at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)
[info]   at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)
[info]   at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)
[info]   at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2744)
[info]   at org.apache.spark.sql.Dataset.withResult(Dataset.scala:3184)
[info]   at org.apache.spark.sql.Dataset.collect(Dataset.scala:2743)
[info]   at org.apache.spark.sql.Dataset.reduce(Dataset.scala:1292)
[info]   at org.apache.spark.sql.UserDefinedFunctionE2ETestSuite.$anonfun$new$34(UserDefinedFunctionE2ETestSuite.scala:212)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
[info]   at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
[info]   at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
[info]   at org.scalatest.Suite.run(Suite.scala:1114)
[info]   at org.scalatest.Suite.run$(Suite.scala:1096)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.sql.UserDefinedFunctionE2ETestSuite.org$scalatest$BeforeAndAfterAll$$super$run(UserDefinedFunctionE2ETestSuite.scala:35)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.sql.UserDefinedFunctionE2ETestSuite.run(UserDefinedFunctionE2ETestSuite.scala:35)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:750)
```","1,-1    ",1,-1,0
1555915926,822522,2023/5/23,v3.2.4,Merged to master,"2,-1    ",2,-1,1
1557130514,47577197,2023/5/23,v3.2.4,"Using ./build/mvn with openjdk 17 I get this error with scala-maven-plugin.version 4.8.1 
```
#14 238.4 [INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---
#14 238.4 [INFO] Compiler bridge file: /root/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__61.0-1.8.0_20221110T195421.jar
#14 238.4 [INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)
#14 238.4 [INFO] compiling 602 Scala sources and 101 Java sources to /tmp/spark/core/target/scala-2.12/classes ...
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: value sun
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: value sun
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: value Signal
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: value Signal
Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: value Signal
#14 238.4 [ERROR] 19 errors found
#14 238.4 [INFO] ------------------------------------------------------------------------
#14 238.4 [INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:
#14 238.4 [INFO] 
#14 238.4 [INFO] Spark Project Parent POM ........................... SUCCESS [ 34.963 s]
#14 238.4 [INFO] Spark Project Tags ................................. SUCCESS [ 23.148 s]
#14 238.4 [INFO] Spark Project Sketch ............................... SUCCESS [ 12.645 s]
#14 238.4 [INFO] Spark Project Local DB ............................. SUCCESS [ 17.202 s]
#14 238.4 [INFO] Spark Project Networking ........................... SUCCESS [ 20.815 s]
#14 238.4 [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 17.392 s]
#14 238.4 [INFO] Spark Project Unsafe ............................... SUCCESS [ 16.924 s]
#14 238.4 [INFO] Spark Project Common Utils ......................... SUCCESS [ 11.795 s]
#14 238.4 [INFO] Spark Project Launcher ............................. SUCCESS [ 13.309 s]
#14 238.4 [INFO] Spark Project Core ................................. FAILURE [ 40.546 s]
#14 238.4 [INFO] Spark Project ML Local Library ..................... SKIPPED
#14 238.4 [INFO] Spark Project GraphX ............................... SKIPPED
#14 238.4 [INFO] Spark Project Streaming ............................ SKIPPED
#14 238.4 [INFO] Spark Project Catalyst ............................. SKIPPED
#14 238.4 [INFO] Spark Project SQL .................................. SKIPPED
#14 238.4 [INFO] Spark Project ML Library ........................... SKIPPED
#14 238.4 [INFO] Spark Project Tools ................................ SKIPPED
#14 238.4 [INFO] Spark Project Hive ................................. SKIPPED
#14 238.4 [INFO] Spark Project REPL ................................. SKIPPED
#14 238.4 [INFO] Spark Project Assembly ............................. SKIPPED
#14 238.4 [INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
#14 238.4 [INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
#14 238.4 [INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
#14 238.4 [INFO] Spark Project Examples ............................. SKIPPED
#14 238.4 [INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
#14 238.4 [INFO] Spark Avro ......................................... SKIPPED
#14 238.4 [INFO] Spark Project Connect Common ....................... SKIPPED
#14 238.4 [INFO] Spark Protobuf ..................................... SKIPPED
#14 238.4 [INFO] Spark Project Connect Server ....................... SKIPPED
#14 238.4 [INFO] Spark Project Connect Client ....................... SKIPPED
#14 238.4 [INFO] ------------------------------------------------------------------------
#14 238.4 [INFO] BUILD FAILURE
#14 238.4 [INFO] ------------------------------------------------------------------------
#14 238.4 [INFO] Total time:  03:29 min
#14 238.4 [INFO] Finished at: 2023-05-22T10:27:14Z
#14 238.4 [INFO] ------------------------------------------------------------------------
#14 238.4 [ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile (scala-compile-first) on project spark-core_2.12: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile failed: org.apache.commons.exec.ExecuteException: Process exited with an error: 255 (Exit value: 255) -> [Help 1]
#14 238.4 [ERROR] 
#14 238.4 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
#14 238.4 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
#14 238.4 [ERROR] 
#14 238.4 [ERROR] For more information about the errors and possible solutions, please read the following articles:
#14 238.4 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
```

When I change scala-maven-plugin.version to 4.8.0 it works","1,-2    ",1,-2,-1
1557246884,47577197,2023/5/23,v3.2.4,"@panbingkun dident you upgrade scala-maven-plugin from 4.8.0 to 4.8.1 in https://github.com/apache/spark/pull/40442
and @LuciferYang did have to revere it in https://github.com/apache/spark/commit/1530e8dc44d7d963791ede03d54b1c2ad5e91c99 
CC @srowen ","1,-1    ",1,-1,0
1557264678,822522,2023/5/23,v3.2.4,"Let's revert that change and leave a note that this plugin cannot be updated further. It seems to be missing the JDK 'internal' classes, maybe for good reasons but I don't see why","1,-1    ",1,-1,0
1553998773,9700541,2023/5/23,v3.2.4,"Could you review this please, @HyukjinKwon?","1,-1    ",1,-1,0
1554045519,9700541,2023/5/23,v3.2.4,"Could you review this, @LuciferYang ?","1,-1    ",1,-1,0
1554052891,9700541,2023/5/23,v3.2.4,"Thank you, @LuciferYang ! ð ","1,-2    ",1,-2,-1
1554075901,9700541,2023/5/23,v3.2.4,Merged to master/3.4/3.3,"1,-2    ",1,-2,-1
1558484910,6477701,2023/5/23,v3.2.4,cc @wangyum @cloud-fan ,"1,-1    ",1,-1,0
1558529341,3182036,2023/5/23,v3.2.4,"I feel it's better to always respect the user-specified num slice parameter. If the num slice is not specified, I agree that we can make it not larger than the num elements.","2,-4    ",2,-4,-2
1559702584,1475305,2023/5/23,v3.2.4,"> I feel it's better to always respect the user-specified num slice parameter. If the num slice is not specified, I agree that we can make it not larger than the num elements.

This may require adding a status to record whether it is a user-specified slice or use default? Sounds like it will increase the complexity of the code. I don't think it's worth because this just a minor case, maybe keep it as it is is better
","1,-1    ",1,-1,0
1554050789,15246973,2023/5/23,v3.2.4,"+1, LGTM","1,-1    ",1,-1,0
1554275933,9700541,2023/5/23,v3.2.4,Thank you all! Merged to master for Apache Spark 3.5.0. All tests passed.,"1,-1    ",1,-1,0
1554277884,1475305,2023/5/23,v3.2.4,Thanks @dongjoon-hyun @yaooqinn @panbingkun ,"1,-2    ",1,-2,-1
1554094150,9700541,2023/5/23,v3.2.4,"Could you review this PR, @LuciferYang ?","1,-1    ",1,-1,0
1554166101,9700541,2023/5/23,v3.2.4,"Thank you so much, @LuciferYang !","1,-1    ",1,-1,0
1554293573,9700541,2023/5/23,v3.2.4,Merged to master/3.4,"1,-1    ",1,-1,0
1554814355,39398911,2023/5/23,v3.2.4,This was small but much needed as it confuses developers. Thanks @dongjoon-hyun . ,"1,-1    ",1,-1,0
1555078951,9700541,2023/5/23,v3.2.4,"Thank you, @anigos .","1,-1    ",1,-1,0
1554318417,1475305,2023/5/23,v3.2.4,cc @HyukjinKwon FYI,"1,-1    ",1,-1,0
1558510210,1475305,2023/5/23,v3.2.4,Thanks @HyukjinKwon ~ Merged to master,"1,-1    ",1,-1,0
1554761464,9700541,2023/5/23,v3.2.4,"Thank you again, @LuciferYang ! Merged to branch-3.3.","1,-1    ",1,-1,0
1558509237,1475305,2023/5/23,v3.2.4,Thanks @HyukjinKwon ~ Merged to master.,"1,-1    ",1,-1,0
1557969773,1580697,2023/5/23,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun.","1,-1    ",1,-1,0
1559281646,1134248,2023/5/23,v3.2.4,cc @dongjoon-hyun Any thoughts on this small change?,"1,-1    ",1,-1,0
1560964066,1134248,2023/5/23,v3.2.4,"This would actually allow:

```scala
  def createSerializerForLocalDateTime(inputObject: Expression): Expression = {
    StaticInvoke(
      DateTimeUtils.getClass,
      TimestampNTZType,
      ""localDateTimeToMicros"",
      inputObject :: Nil,
      returnNullable = false)
  }
```

to change to:

```scala
  def createSerializerForLocalDateTime(inputObject: Expression): Expression = {
    StaticInvoke(
      DateTimeUtils.getClass,
      TimestampNTZType,
      ""anyToMicros"",
      inputObject :: Nil,
      returnNullable = false)
  }
```

This would also make some code on the Iceberg side simpler:

```java
        // This if/else can be removed once https://github.com/apache/spark/pull/41238 is in
        if (ts.shouldAdjustToUTC()) {
          // if spark.sql.datetime.java8API.enabled is set to true, java.time.Instant
          // for Spark SQL TIMESTAMP type is returned otherwise java.sql.Timestamp is returned.
          return DateTimeUtils.anyToMicros(object);
        } else {
          LocalDateTime ldt = (LocalDateTime) object;
          return DateTimeUtils.localDateTimeToMicros(ldt);
        }
```

https://github.com/apache/iceberg/blob/b2e99b5b35df8ced7366ad32e2c4fdf26ab3c331/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java#L75-L86","1,-1    ",1,-1,0
1566321334,6477701,2023/5/23,v3.2.4,@Fokko no biggie but would be great to fill the PR description.,"1,-1    ",1,-1,0
1561769197,47337188,2023/5/23,v3.2.4,nit: the TODO [here](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/types.py#LL393C5-L393C74) seems to be removable,"1,-1    ",1,-1,0
1561786519,506656,2023/5/23,v3.2.4,"> nit: the TODO [here](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/types.py?rgh-link-date=2023-05-24T18%3A50%3A58Z#LL393C5-L393C74) seems to be removable

I don't think so. The function only supports top level timestamp type. I'm not sure we will support the nested timestamp types in the function, though.","1,-2    ",1,-2,-1
1562176512,7322292,2023/5/23,v3.2.4,merged to master,"1,-1    ",1,-1,0
1557839597,1580697,2023/5/23,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun.","1,-2    ",1,-2,-1
1555864892,8486025,2023/5/23,v3.2.4,ping @MaxGekk ,"3,-1    ",3,-1,2
1556632758,1580697,2023/5/23,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @beliefer.","1,-1    ",1,-1,0
1556688113,8486025,2023/5/23,v3.2.4,@MaxGekk Thank you!,"2,-4    ",2,-4,-2
1555915864,822522,2023/5/23,v3.2.4,Merged to master,"1,-1    ",1,-1,0
1556565071,6477701,2023/5/23,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1556424038,9700541,2023/5/23,v3.2.4,"Could you review this PR, @zhengruifeng ?","3,-1    ",3,-1,2
1556435985,9700541,2023/5/23,v3.2.4,"Thank you so much, @zhengruifeng ! 
Merged to master.","1,-1    ",1,-1,0
1556423040,7322292,2023/5/23,v3.2.4,"> Hi, @zhengruifeng . Does it mean we need to add a lower-bound instead?
> 
> >

I guess we don't need a lower-bound:
1, we don't use a lower-bound in [CI](https://github.com/apache/spark/blob/master/dev/infra/Dockerfile#L68);
2, the `matplotlib` is only used in Pandas API on Spark, and `pandas` already has a [lower-bound](https://github.com/pandas-dev/pandas/blob/main/requirements-dev.txt#L29) `matplotlib>=3.6.1` for `matplotlib`.","1,-1    ",1,-1,0
1556423778,9700541,2023/5/23,v3.2.4,"Then, Pefect! Thank you!","1,-1    ",1,-1,0
1556531491,7322292,2023/5/23,v3.2.4,"@dongjoon-hyun thank you for the reviews. 
merged to master","1,-1    ",1,-1,0
1556440769,1475305,2023/5/23,v3.2.4,"Test first, will update pr description later

","1,-1    ",1,-1,0
1556804312,1475305,2023/5/23,v3.2.4,Thanks @HyukjinKwon ~ Merged to master.,"1,-1    ",1,-1,0
1557568235,1475305,2023/5/23,v3.2.4,Thanks @dongjoon-hyun ,"1,-1    ",1,-1,0
1556542346,6477701,2023/5/23,v3.2.4,@zhengruifeng @MaxGekk @vicennial @hvanhovell PTAL,"1,-1    ",1,-1,0
1557007436,7322292,2023/5/23,v3.2.4,we would document the migration `sc.addArchive` to `session.addArchive` somewhere,"1,-1    ",1,-1,0
1558479839,6477701,2023/5/23,v3.2.4,Merged to master.,"1,-2    ",1,-2,-1
1558480343,6477701,2023/5/23,v3.2.4,"> we would document the migration sc.addArchive to session.addArchive somewhere

I will document them in the next PR","1,-1    ",1,-1,0
1558411256,32387433,2023/5/23,v3.2.4,cc @cloud-fan @MaxGekk @hvanhovell ,"1,-1    ",1,-1,0
1556936279,8486025,2023/5/23,v3.2.4,"> 

Got it.","1,-1    ",1,-1,0
1558922212,1580697,2023/5/24,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @beliefer.","1,-2    ",1,-2,-1
1558971346,8486025,2023/5/24,v3.2.4,@MaxGekk Thank you!,"1,-1    ",1,-1,0
1560256094,6477701,2023/5/24,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1558217797,44108233,2023/5/24,v3.2.4,"cc @HyukjinKwon @zhengruifeng @ueshin @xinrong-meng 
Could you please take a look at this when you have time? This is essentially a foundational task for achieving 100% test coverage of the pandas API on Spark, built on top of Spark Connect.","1,-1    ",1,-1,0
1558322402,7322292,2023/5/24,v3.2.4,merged to master,"1,-1    ",1,-1,0
1557062046,15246973,2023/5/24,v3.2.4,"eg: https://github.com/apache/spark/pull/41236/files/dcda3fb8673f639d410ad6e3f2d7e6b5286208ae#diff-d41e24da75af19647fadd76ad0b63ecb22b08c0004b07091e4603a30ec0fe013R5630

<img width=""788"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/49d7a7e8-140f-41ac-985f-8213bad69ad1"">
","1,-1    ",1,-1,0
1557062324,15246973,2023/5/24,v3.2.4,cc @MaxGekk ,"1,-1    ",1,-1,0
1557275855,15246973,2023/5/24,v3.2.4,"> > which does not comply with code specifications
> 
> Which specifications do you mean BTW. Please, point them out in the PR description.

1.I think it's a universal underlying rule, eg: 
python: https://www.flake8rules.com/rules/W391.html

2.Why?
https://stackoverflow.com/questions/2287967/why-is-it-recommended-to-have-empty-line-in-the-end-of-a-source-file

3.All Spark source codes follow this rule

4.BTW, Perhaps we need to clarify this rule in `https://github.com/databricks/scala-style-guide` or other docs? @MaxGekk ","2,-1    ",2,-1,1
1558702596,1580697,2023/5/24,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun.","2,-1    ",2,-1,1
1557104415,807537,2023/5/24,v3.2.4,"Hi @dongjoon-hyun and @ScrapCodes, would you mind to take a look at this pr, and let's discuss and sync on the same pages before I'm going any further.

I wrote an email to the dev list but got no replies. Thought I should go ahead and create this PR first.","2,-1    ",2,-1,1
1560406438,807537,2023/5/24,v3.2.4,Gently ping @dongjoon-hyun ,"1,-1    ",1,-1,0
1566439071,807537,2023/5/24,v3.2.4,Gently ping @dongjoon-hyun and @ScrapCodes .,"1,-1    ",1,-1,0
1557150938,7322292,2023/5/24,v3.2.4,cc @HyukjinKwon @ueshin @xinrong-meng @itholic ,"3,-1    ",3,-1,2
1560645895,7322292,2023/5/24,v3.2.4,"I wanna think again about this, so convert to draft for now","2,-2    ",2,-2,0
1565064654,7322292,2023/5/24,v3.2.4,closing this in favor of https://github.com/apache/spark/pull/41330,"2,-1    ",2,-1,1
1557265725,32387433,2023/5/24,v3.2.4,cc @xinrong-meng @hvanhovell  ,"1,-1    ",1,-1,0
1557604999,47337188,2023/5/24,v3.2.4,"LGTM, thanks for improving the error handling!","1,-1    ",1,-1,0
1557650037,47337188,2023/5/24,v3.2.4,"Merged to master, thanks!","1,-1    ",1,-1,0
1558256203,32387433,2023/5/24,v3.2.4,Thanks! @xinrong-meng ,"2,-1    ",2,-1,1
1557345334,25019163,2023/5/24,v3.2.4,@hvanhovell @LuciferYang ,"1,-3    ",1,-3,-2
1557491106,25019163,2023/5/24,v3.2.4,"Hold with merging, I may make some more small changes to print better errors here, as I try to reproduce and debug https://github.com/apache/spark/pull/41005#discussion_r1200551487","1,-1    ",1,-1,0
1558855488,25019163,2023/5/24,v3.2.4,"@dongjoon-hyun yes, it's ready as is.
I was trying to debug https://github.com/apache/spark/pull/41005#discussion_r1200924867 and maybe do something to fix it, but I'm stumped there. It seems to be some manifestation of https://issues.apache.org/jira/browse/SPARK-43227 and may need a separate investigation, and is orthogonal of query interrupt.
It also isn't failing in CI, so seems specific to some environment...","1,-2    ",1,-2,-1
1558862230,1475305,2023/5/24,v3.2.4,"> It also isn't failing in CI, so seems specific to some environment...

GA passed because it uses sbt for testing, and the connect server fork by sbt will with classpath of connect client, which will masks some issues","2,-1    ",2,-1,1
1558865464,25019163,2023/5/24,v3.2.4,"> GA passed because it uses sbt for testing, and the connect server fork by sbt will with classpath of connect client, which will masks some issues

I see.
But it's still interesting that the same test copied to `UserDefinedFunctionE2ETestSuite`, which should have the same classpath, works...","1,-1    ",1,-1,0
1558865927,9700541,2023/5/24,v3.2.4,"~Ur? Do you mean that this PR causes failures?~ Sorry, I overlooked the comment, @LuciferYang .","1,-1    ",1,-1,0
1558870447,9700541,2023/5/24,v3.2.4,"For the Maven failure issue, shall we file a dedicated JIRA issue, @LuciferYang ?
- https://github.com/apache/spark/pull/41005#discussion_r1200551487","1,-1    ",1,-1,0
1558878769,25019163,2023/5/24,v3.2.4,I will create a JIRA issue.,"2,-2    ",2,-2,0
1558889073,25019163,2023/5/24,v3.2.4,Created https://issues.apache.org/jira/browse/SPARK-43744,"1,-1    ",1,-1,0
1558895557,1475305,2023/5/24,v3.2.4,"> Created https://issues.apache.org/jira/browse/SPARK-43744

Actually I have created https://issues.apache.org/jira/browse/SPARK-43648 yesterday :)","1,-2    ",1,-2,-1
1558900074,9700541,2023/5/24,v3.2.4,"Thanks. Then, let's track that in SPARK-43648 instead of here.
And, sorry again for missing your comment, @LuciferYang .","1,-1    ",1,-1,0
1558935637,1475305,2023/5/24,v3.2.4,@dongjoon-hyun No problem :),"1,-2    ",1,-2,-1
1557461778,47577197,2023/5/24,v3.2.4,"@panbingkun  @LuciferYang  @srowen
Do we need a new JIRA? ","1,-1    ",1,-1,0
1557467060,1475305,2023/5/24,v3.2.4,"I tend to create a new jira and add some comments for this

","1,-1    ",1,-1,0
1558288308,15246973,2023/5/24,v3.2.4,late LGTM,"1,-1    ",1,-1,0
1558435015,1097932,2023/5/24,v3.2.4,"LGTM, pending on tests.
Awesome refactoring!","1,-2    ",1,-2,-1
1566303092,9700541,2023/5/24,v3.2.4,Merged to master for Apache Spark 3.5.0.,"1,-1    ",1,-1,0
1558129679,7788766,2023/5/24,v3.2.4,cc @dongjoon-hyun @sunchao @gengliangwang,"1,-1    ",1,-1,0
1558131316,9700541,2023/5/24,v3.2.4,"Thank you for pinging me, @sadikovi .","2,-1    ",2,-1,1
1560335616,7788766,2023/5/24,v3.2.4,@dongjoon-hyun Could you take another look when you have time? Thank you.,"1,-1    ",1,-1,0
1562219052,5334567,2023/5/24,v3.2.4,"Move the knob from SQLConf to AvroOptions, after the discussion with @sadikovi and @rangadi ","2,-1    ",2,-1,1
1563349116,5334567,2023/5/24,v3.2.4,@dongjoon-hyun I addressed the comments and the CI appears to pass now. Can you help take a look? ,"1,-1    ",1,-1,0
1564205552,1591700,2023/5/24,v3.2.4,+CC @shardulm94 ,"1,-1    ",1,-1,0
1564666771,5334567,2023/5/24,v3.2.4,@dongjoon-hyun the tests are all pass now. Can you help take a look?,"1,-1    ",1,-1,0
1559691393,1475305,2023/5/24,v3.2.4,also cc @cloud-fan ,"1,-1    ",1,-1,0
1559723307,1475305,2023/5/24,v3.2.4,"What is the value that allows `numSlices` to be greater than `numElements`? I haven't figured it out yet

","1,-1    ",1,-1,0
1559751742,1938382,2023/5/24,v3.2.4,"@LuciferYang https://github.com/apache/spark/blob/5f325ec917ced819a19911b472ebf7eb52010203/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala#L317

It is a parameter.","1,-1    ",1,-1,0
1559773238,1475305,2023/5/24,v3.2.4,"Thanks @amaliujia , I know this","1,-1    ",1,-1,0
1562835078,1475305,2023/5/24,v3.2.4,friendly ping @MaxGekk Do you have time to take a look at this one? Thanks ~,"1,-2    ",1,-2,-1
1558381761,790409,2023/5/24,v3.2.4,"Looks good, thanks!

CC @cloud-fan ","1,-2    ",1,-2,-1
1562514221,3182036,2023/5/24,v3.2.4,"thanks, merging to master!","1,-3    ",1,-3,-2
1558173108,9700541,2023/5/24,v3.2.4,"WDTY about this PR, @sarutak and @gengliangwang ?","1,-1    ",1,-1,0
1558502802,9700541,2023/5/24,v3.2.4,"Thank you, @HyukjinKwon and @sarutak .","1,-1    ",1,-1,0
1558537215,9700541,2023/5/24,v3.2.4,"Thank you all! All tests passed. Merged to master/3.4/3.3.

![Screenshot 2023-05-22 at 10 16 27 PM](https://github.com/apache/spark/assets/9700541/4d95cb45-05ce-40ea-8177-bcaa4125c3a1)
","1,-3    ",1,-3,-2
1558496978,6477701,2023/5/24,v3.2.4,cc @rednaxelafx @cloud-fan FYI,"1,-3    ",1,-3,-2
1558520783,3182036,2023/5/24,v3.2.4,late LGTM,"1,-1    ",1,-1,0
1560626691,7322292,2023/5/24,v3.2.4,merged to master,"1,-2    ",1,-2,-1
1558316208,15246973,2023/5/24,v3.2.4,Let me append the results of ZStandardBenchmark.,"1,-2    ",1,-2,-1
1558432546,15246973,2023/5/24,v3.2.4,cc @dongjoon-hyun  @srowen @LuciferYang FYI,"1,-1    ",1,-1,0
1559080139,9700541,2023/5/24,v3.2.4,"Merged to master. Thank you, @panbingkun and all.","1,-1    ",1,-1,0
1558563405,15246973,2023/5/24,v3.2.4,"> Hi, @panbingkun . Could you elaborate a little more about what was the release notes and patches we are going to test?
> 
> > Routine upgrade.

This is done.
Thanksï¿½?
1560256554,6477701,2023-05-23,Merged to master.
1558404693,1475305,2023-05-23,Please don't upgrade `commons-crypto`","1,-1    ",1,-1,0
,,2023/5/24,v3.2.4,,"1,-1    ",1,-1,0
"https://github.com/apache/spark/pull/40082#issuecomment-1443129166""",,2023/5/24,v3.2.4,,"3,-2    ",3,-2,1
1558415899,15246973,2023/5/25,v3.2.4,"> #40082 (comment)

Ok, Let me revert it.","3,-2    ",3,-2,1
1559034415,15246973,2023/5/25,v3.2.4,"> Could you check the avro failures and re-trigger the failed pipeline?
> 
> ```
> 
> [info] *** 2 TESTS FAILED ***
> 
> [error] Failed: Total 294, Failed 2, Errors 0, Passed 292, Ignored 2
> 
> [error] Failed tests:
> 
> [error] 	org.apache.spark.sql.avro.AvroV1Suite
> 
> [error] 	org.apache.spark.sql.avro.AvroV2Suite
> 
> ```

OKï¼let me check it","3,-3    ",3,-3,0
1559131778,15246973,2023/5/25,v3.2.4,"- 2.11.0 FileUtils.touch, Call stack as follows
1.FileUtils.touch
2.FileUtils.setLastModified
3.File.setLastModified(timeMillis)
4.FileSystem.setLastModifiedTime(this, time)
5.UnixFileSystem.setLastModifiedTime --> **Found that the file does not exist, throw FileNotFoundException**
<img width=""517"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/120783ee-6606-414f-9f78-9fe9326807c6"">

- 2.12.0 FileUtils.touch, Call stack as follows
1.FileUtils.touch
2.PathUtils.touch
3.java.nio.file.Files.createFile
4.java.nio.file.Files.newByteChannel
5.sun.nio.fs.UnixFileSystemProvider.newByteChannel
6.sun.nio.fs.UnixChannelFactory.newFileChannel
7.sun.nio.fs.UnixChannelFactory.open
8.UnixNativeDispatcher.open
9.(Native)UnixNativeDispatcher.open0 -> **throw UnixException**
10.UnixException.rethrowAsIOException
10.UnixException.translateToIOException --> **Convert to java.nio.file.NoSuchFileException throw it**
<img width=""545"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/1d81d5e5-08a0-4d98-8349-5a81b74b9737"">
<img width=""768"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/63216355-6d4a-4670-bbdc-aefe7d1366e2"">","1,-1    ",1,-1,0
1559690210,9700541,2023/5/25,v3.2.4,"1. Please put the analysis to the PR description.
2. +1 for `NoSuchFileException` because it's better than a general `IOException`.","1,-1    ",1,-1,0
1560334734,15246973,2023/5/25,v3.2.4,"> 1. Please put the analysis to the PR description.
> 2. +1 for `NoSuchFileException` because it's better than a general `IOException`.

This is done.","3,-1    ",3,-1,2
1560355918,15246973,2023/5/25,v3.2.4,"@dongjoon-hyun @HyukjinKwon @LuciferYang 
In fact, there is a problem with this UT. Its original intention was to test if there is no file with `.avro` extensions in the directory, and the read should fail. However, this UT triggered the error as `FileUtils.touch` instead of `spark.read.format(""avro"").load(dir.toString)`. 

After this PR is completed, I will submit a new PR to fix this issue.

The root cause for the failure of this case is that the parent directory was not created. When FileUtils.touch is called in version 1.11.0, it just throws `java.io.FileNotFoundException`, which covers the error.
","1,-1    ",1,-1,0
1560456111,15246973,2023/5/25,v3.2.4,"> I merged #41289 . Please rebase this PR to the master branch.

This is done.","1,-1    ",1,-1,0
1560461083,1475305,2023/5/25,v3.2.4,"```
2.Note: The error exception of the FileUtils.touch method has been changed from java.io.FileNotFoundException to java.nio.file.NoSuchFileException, changes from PR: [Add PathUtils.touch(Path)](https://github.com/apache/commons-io/commit/fd7c8182d2117d01f43ccc9fe939105f834ba672). The analysis process is as follows:
```

still need this part in description? @panbingkun ","1,-1    ",1,-1,0
1560471893,9700541,2023/5/25,v3.2.4,"Right, @LuciferYang . We can remove that from this PR description.","1,-1    ",1,-1,0
1560478875,15246973,2023/5/25,v3.2.4,"> Right, @LuciferYang . We can remove that from this PR description.

Ok, Let me update it.","2,-1    ",2,-1,1
1561324911,9700541,2023/5/25,v3.2.4,"Thank you, @panbingkun and all! Merged to master.","1,-1    ",1,-1,0
1558890037,7322292,2023/5/25,v3.2.4,merged to master,"1,-1    ",1,-1,0
1558887876,7322292,2023/5/25,v3.2.4,thank you @dongjoon-hyun @HyukjinKwon ,"1,-1    ",1,-1,0
1558660368,9700541,2023/5/25,v3.2.4,Merged to master/3.4/3.3.,"1,-1    ",1,-1,0
1558739452,15246973,2023/5/25,v3.2.4,"./build/mvn -Pkinesis-asl -Pmesos -Pkubernetes -Pyarn -Phive -Phive-thriftserver -am checkstyle:checkstyle
<img width=""1106"" alt=""image"" src=""https://github.com/apache/spark/assets/15246973/d458c5a2-47ac-4b45-a9e6-455a3d8e181a"">

","1,-1    ",1,-1,0
1559872701,1475305,2023/5/25,v3.2.4,"@panbingkun why we need check `dev/sbt-checkstyle`, it is not using Maven, we should check `build/mvn checkstyle: checkstyle -P...`

","1,-1    ",1,-1,0
1560183622,15246973,2023/5/25,v3.2.4,"> @panbingkun why we need check `dev/sbt-checkstyle`, it is not using Maven, we should check `build/mvn checkstyle: checkstyle -P...`

Sorry, I updated the wrong information. I thought it was another PR.
Updated!","1,-1    ",1,-1,0
1560361052,1475305,2023/5/25,v3.2.4,"Please update the pr description, it still `Manual testing by: sh dev/sbt-checkstyle
`","1,-1    ",1,-1,0
1560379869,15246973,2023/5/25,v3.2.4,"> Please update the pr description, it still `Manual testing by: sh dev/sbt-checkstyle `

Updated","2,-1    ",2,-1,1
1560615787,1475305,2023/5/25,v3.2.4,Merged to master. Thanks @HyukjinKwon and @panbingkun ~,"1,-1    ",1,-1,0
1559025027,9700541,2023/5/25,v3.2.4,"According to the `Affected Version` of JIRA, I merged to master only. But, I'm also +1 for backporting this if you want.","1,-3    ",1,-3,-2
1560937976,1580697,2023/5/25,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @bozhang2820.","1,-1    ",1,-1,0
1559420546,6477701,2023/5/25,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1562182627,8486025,2023/5/25,v3.2.4,@MaxGekk The failure tests seems unrelated to this PR.,"1,-1    ",1,-1,0
1563244806,1580697,2023/5/25,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @beliefer.","1,-2    ",1,-2,-1
1563719304,8486025,2023/5/25,v3.2.4,@MaxGekk Thank you!,"1,-1    ",1,-1,0
1559459748,26535726,2023/5/25,v3.2.4,cc @cloud-fan ,"1,-1    ",1,-1,0
1563767824,8326978,2023/5/25,v3.2.4,"thanks, merged to master(3.5.0), 3.4.1 and 3.3.3","1,-1    ",1,-1,0
1560843908,47577197,2023/5/25,v3.2.4,"How is this ""update to secure version of fasterxml"" ? 

""this is to fix the vulnerability sonatype-2022-6438"" 
that was fixed in 2.15.0 witch we updated in https://github.com/apache/spark/commit/a4a274c4e4f709765e7a8c687347816d8951a681 


This is the change log for 2.15.1 https://github.com/FasterXML/jackson-databind/blob/77789abaecd2e42a3765af5231e252ee62578b18/release-notes/VERSION-2.x#LL17C21-L17C21","1,-1    ",1,-1,0
1560902138,25390316,2023/5/25,v3.2.4,Ok my scans were giving a spurious result sorry,"1,-1    ",1,-1,0
1560905608,47577197,2023/5/25,v3.2.4,@ronandoolan2 you can update it to 2.15.1 if you will?,"1,-1    ",1,-1,0
1560942273,25390316,2023/5/25,v3.2.4,ok sure,"1,-1    ",1,-1,0
1560953148,47577197,2023/5/25,v3.2.4,"we need a JiRA for this 
and you must run `./dev/test-dependencies.sh --replace-manifest` to update the rest of pom.xml file. ","1,-1    ",1,-1,0
1561145840,47577197,2023/5/25,v3.2.4,"This is how I would have done it. 
Feel free to change what you want to change :)

Change tittle to something like [SPARK-XXXX][BUILD] Upgrade FasterXML jackson to 2.15.1


What changes were proposed in this pull request?
Upgrade FasterXML jackson from 2.15.0 to 2.15.1

Why are the changes needed?
New version that fix some bugs see release-notes https://github.com/FasterXML/jackson-databind/blob/77789abaecd2e42a3765af5231e252ee62578b18/release-notes/VERSION-2.x#LL17C21-L17C21 for more info.

Does this PR introduce any user-facing change?
No.

How was this patch tested?
Pass GA ","1,-1    ",1,-1,0
1561237520,25390316,2023/5/25,v3.2.4,@bjornjorgensen do you have the link to Jira? I assume I need to create a ticket with the number SPARK-43774,"1,-1    ",1,-1,0
1561304752,1475305,2023/5/25,v3.2.4,"Let's update the release notes to https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1

And if upgrading to this version has benefits for Spark, it can also be listed in the PR description. @ronandoolan2 

","1,-1    ",1,-1,0
1561353843,47577197,2023/5/25,v3.2.4,"@ronandoolan2 you need to have a JIRA account https://selfserve.apache.org/jira-account.html
for this one you can use https://issues.apache.org/jira/browse/SPARK-43774 witch is what you are using.","1,-1    ",1,-1,0
1562332786,25390316,2023/5/25,v3.2.4,"> Let's update the release notes to https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1
> 
> And if upgrading to this version has benefits for Spark, it can also be listed in the PR description. @ronandoolan2

Where are the release notes that I need to update, I couldn't find them in the repo","1,-1    ",1,-1,0
1562491042,47577197,2023/5/26,v3.2.4,"@ronandoolan2 It looks like @LuciferYang  have changed it. 

""New version that fix some bugs, the full release-notes as follows:

https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1"" ","1,-1    ",1,-1,0
1562498723,25390316,2023/5/26,v3.2.4,"> @ronandoolan2 It looks like @LuciferYang have changed it.
> 
> ""New version that fix some bugs, the full release-notes as follows:
> 
> https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1""

Ok great thanks, do you know when the next version of pyspark will be released to pypi?","2,-1    ",2,-1,1
1562500359,47577197,2023/5/26,v3.2.4,"| July 16th 2023 | Code freeze. Release branch cut.|
| Late July 2023 | QA period. Focus on bug fixes, tests, stability and docs. Generally, no new features merged.|
| August 2023    | Release candidates (RC), voting, etc. until final release passes


https://github.com/apache/spark-website/commit/18ca078b23f826c24bed32df1dc89854a91cb580","1,-1    ",1,-1,0
1562506696,1475305,2023/5/26,v3.2.4,also cc @srowen FYI,"1,-1    ",1,-1,0
1562822384,1475305,2023/5/26,v3.2.4,Merged to master. Thanks @ronandoolan2 @srowen @bjornjorgensen  ~,"1,-1    ",1,-1,0
1562842044,47577197,2023/5/26,v3.2.4,"Welcome, @ronandoolan2 

We are thrilled to have you join the Apache Spark community as a new contributor. It's always exciting to see fresh minds and ideas. We're confident that your skills and expertise will be an asset to our project, enhancing our collaborative efforts to improve Apache Spark.

Just as a reminder, the Apache Spark community values open communication, respect, and inclusiveness. Don't hesitate to ask questions, share your thoughts, or request help when needed. Whether it's through bug fixes, adding new features, or improving documentation, every contribution is highly appreciated.

We hope that you'll find this experience rewarding. Together, we can make Apache Spark even better. Again, welcome aboard, @ronandoolan2","3,-2    ",3,-2,1
1559736722,1475305,2023/5/26,v3.2.4,"Test first, will add some comments and update pr description later

 ","1,-1    ",1,-1,0
1559866005,1475305,2023/5/26,v3.2.4,"Thanks @dongjoon-hyun This is a problem that was exposed after https://github.com/apache/spark/pull/40848 was merged, I will explain why this change was made tomorrow, and we can discuss whether there is a better solution. 

It's too late in my time zone, I'm going to bed. Good night ~

","2,-1    ",2,-1,1
1561333854,1475305,2023/5/26,v3.2.4,"Sorry, needs to be postponed, some works that have to be completed first ...","2,-1    ",2,-1,1
1562927992,1475305,2023/5/26,v3.2.4,"@dongjoon-hyun I have updated the PR description, is it clear? Do you have any better suggestions?

Of course, there are other ways to work around even without making any code changes:

Maven: manual clean hive module before test

```
build/mvn clean install -DskipTests
build/mvn clean -pl sql/hive
build/mvn test -pl connector/connect/client/jvm
```

SBT: test `connect-client-jvm` module always with `clean`

`build/sbt ""connect-client-jvm/test""`","1,-1    ",1,-1,0
1562932798,1475305,2023/5/26,v3.2.4,"Another possible way is to find a way to remove `$sparkHome/sql/hive/target/scala-2.12/classes/` from SimpleSparkConnectService classpath with test without `-Phive` profile, but this requires further investigation.

","3,-2    ",3,-2,1
1563637692,9700541,2023/5/26,v3.2.4,Feel free to merge~,"1,-1    ",1,-1,0
1563697307,1475305,2023/5/26,v3.2.4,Thanks @dongjoon-hyun ~,"1,-1    ",1,-1,0
1563698446,1475305,2023/5/26,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1560166746,4190164,2023/5/26,v3.2.4,"It would be really nice to have https://github.com/apache/spark/pull/40925 to filter package private methods. Looking at MiMa issues. It seems they have been trying to solve the same issue. e.g. https://github.com/lightbend/mima/issues/53 But when using the MiMa check in our tests, we still have to skip the private[sql] classes manually. ","1,-2    ",1,-2,-1
1560384416,9700541,2023/5/26,v3.2.4,"Thank you, @sunchao and all. Merged to master/3.4.

Most tests (except dependency test) passed in the first commit. And, second commit passed dependency test too.
![Screenshot 2023-05-23 at 7 49 03 PM](https://github.com/apache/spark/assets/9700541/ccbee96f-497f-4414-8708-ce628e5add2e)
","2,-1    ",2,-1,1
1560437247,9700541,2023/5/26,v3.2.4,"Sorry folks. I missed that `branch-3.4` has Hadoop 2 profile still. I made a follow-up for branch-3.4.
- https://github.com/apache/spark/pull/41291","1,-1    ",1,-1,0
1560459471,7322292,2023/5/26,v3.2.4,"@ueshin thanks for the fix, merged to master/branch-3.4","3,-2    ",3,-2,1
1560343612,21131848,2023/5/26,v3.2.4,"Is there some setting needed to materialize the issue? Using the queries you added as tests (in `scalar-subquery-predicate.sql` and `scalar-subquery-select.sql`), I am getting your expected results on the master branch:
```
spark-sql (default)> select version();
3.5.0 5f325ec917ced819a19911b472ebf7eb52010203
Time taken: 2.69 seconds, Fetched 1 row(s)
spark-sql (default)> select *
                   > from range(1, 3) t1
                   > where (select sum(c) from (
                   >         select t2.id * t2.id c
                   >         from range (1, 2) t2 where t1.id = t2.id
                   >         group by t2.id
                   >        )
                   > ) is not null;
1
Time taken: 1.141 seconds, Fetched 1 row(s)
spark-sql (default)> select *
                   > from
                   > (
                   >  select t1.id c1, (
                   >                     select sum(c)
                   >                     from (
                   >                       select t2.id * t2.id c
                   >                       from range (1, 2) t2 where t1.id = t2.id
                   >                       group by t2.id
                   >                     )
                   >                    ) c2
                   >  from range (1, 3) t1
                   > ) t
                   > where t.c2 is not null;
1	1
Time taken: 0.404 seconds, Fetched 1 row(s)
spark-sql (default)> 
```","1,-1    ",1,-1,0
1561664367,2280399,2023/5/26,v3.2.4,"> Is there some setting needed to materialize the issue? Using the queries you added as tests (in `scalar-subquery-predicate.sql` and `scalar-subquery-select.sql`), I am getting your expected results on the master branch:
> 
> ```
> spark-sql (default)> select version();
> 3.5.0 5f325ec917ced819a19911b472ebf7eb52010203
> Time taken: 2.69 seconds, Fetched 1 row(s)
> spark-sql (default)> select *
>                    > from range(1, 3) t1
>                    > where (select sum(c) from (
>                    >         select t2.id * t2.id c
>                    >         from range (1, 2) t2 where t1.id = t2.id
>                    >         group by t2.id
>                    >        )
>                    > ) is not null;
> 1
> Time taken: 1.141 seconds, Fetched 1 row(s)
> spark-sql (default)> select *
>                    > from
>                    > (
>                    >  select t1.id c1, (
>                    >                     select sum(c)
>                    >                     from (
>                    >                       select t2.id * t2.id c
>                    >                       from range (1, 2) t2 where t1.id = t2.id
>                    >                       group by t2.id
>                    >                     )
>                    >                    ) c2
>                    >  from range (1, 3) t1
>                    > ) t
>                    > where t.c2 is not null;
> 1	1
> Time taken: 0.404 seconds, Fetched 1 row(s)
> spark-sql (default)> 
> ```

Great catch! I used the wrong query tests, updated them now. In both cases the results on the master branch are wrong:
```
select * from (
 select t1.id c1, (
  select t2.id c from range (1, 2) t2
  where t1.id = t2.id  ) c2
 from range (1, 3) t1 ) t
where t.c2 is not null
-- !query schema
struct<c1:bigint,c2:bigint>
-- !query output
1	1
2	NULL

```
and 

```
-- !query
select *
from range(1, 3) t1
where (select t2.id c
       from range (1, 2) t2 where t1.id = t2.id
      ) is not null
-- !query schema
struct<id:bigint>
-- !query output
1
2
```

-- in both queries the second row should not be present.","1,-1    ",1,-1,0
1561680423,384788,2023/5/26,v3.2.4,@gengliangwang  @cloud-fan ,"1,-1    ",1,-1,0
1561760662,21131848,2023/5/26,v3.2.4,Makes more sense now. Can you update the Jira as well?,"1,-1    ",1,-1,0
1561843655,2280399,2023/5/26,v3.2.4,Updated Jira ticket too,"2,-1    ",2,-1,1
1563005932,3182036,2023/5/26,v3.2.4,"LGTM, can we fix the conflicts?","2,-1    ",2,-1,1
1563084087,2280399,2023/5/26,v3.2.4,Conflicts are resolved.,"1,-1    ",1,-1,0
1560475074,6477701,2023/5/26,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1560369407,15246973,2023/5/26,v3.2.4,cc @dongjoon-hyun @HyukjinKwon @LuciferYang,"1,-2    ",1,-2,-1
1560399798,15246973,2023/5/27,v3.2.4,"> withTempDir

Yep, Perhaps using `withTempDir` code here is more concise,
Let me do it?
Updated.","1,-2    ",1,-2,-1
1564785345,1580697,2023/5/27,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @beliefer.","1,-1    ",1,-1,0
1565099133,8486025,2023/5/27,v3.2.4,@MaxGekk Thank you!,"2,-1    ",2,-1,1
1560437419,9700541,2023/5/28,v3.2.4,"Could you review this, @sunchao and @viirya ?","1,-2    ",1,-2,-1
1560446536,9700541,2023/5/28,v3.2.4,"Could you review this, @HyukjinKwon and @wangyum ?","1,-2    ",1,-2,-1
1560447159,9700541,2023/5/29,v3.2.4,"Thank you so much, @wangyum .","1,-1    ",1,-1,0
1560449712,9700541,2023/5/29,v3.2.4,"I verified this manually. Merged to branch-3.4.
```
$ dev/test-dependencies.sh --replace-manifest
```","1,-1    ",1,-1,0
1560567476,68855,2023/5/29,v3.2.4,lgtm,"1,-1    ",1,-1,0
1561333212,9700541, but not a cpu ,v3.2.4,Thank you!,"1,-1    ",1,-1,0
1562708894,6477701, we can always default it to `1`.,v3.2.4,Merged to master.,"1,-1    ",1,-1,0
1560575635,15246973,,v3.2.4,cc @hvanhovell @zhengruifeng ,"1,-1    ",1,-1,0
1562099061,15246973,,v3.2.4,"> Thanks for doing this! Are you also taking care of python?

I want to try it. ð","1,-1    ",1,-1,0
1562131002,15246973,,v3.2.4,"The implementation of `Python` & `connect Python` is: https://github.com/apache/spark/pull/41296,
After it, I will continue to work on above.
@hvanhovell ","1,-1    ",1,-1,0
1562520041,1475305,,v3.2.4,"@panbingkun 
Before submitting your change, please make sure to format your code using the following command:
./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm
Error: Process completed with exit code 1.
","1,-1    ",1,-1,0
1562520371,1475305,,v3.2.4,is this one ready to go? @panbingkun ,"1,-1    ",1,-1,0
1562533084,15246973,,v3.2.4,"> ./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm

OK","1,-1    ",1,-1,0
1562536921,15246973,,v3.2.4,"> is this one ready to go

Yes","1,-2    ",1,-2,-1
1563703184,7322292,,v3.2.4,merged to master,"1,-1    ",1,-1,0
1561327387,1475305,,v3.2.4,Merged to master. Thanks @dongjoon-hyun and @panbingkun ,"1,-1    ",1,-1,0
1561322732,1475305,,v3.2.4,cc @zhenlineo FYI ,"1,-1    ",1,-1,0
1562131685,15246973,,v3.2.4,Waiting for https://github.com/apache/spark/pull/41293,"1,-1    ",1,-1,0
1563703633,7322292,,v3.2.4,@panbingkun you would need `dev/reformat-python` to fix python linter issue,"1,-1    ",1,-1,0
1563719262,15246973,,v3.2.4,"> @panbingkun you would need `dev/reformat-python` to fix python linter issue

Ok, let me try. Thanks!","1,-1    ",1,-1,0
1563729956,15246973,,v3.2.4,"> @panbingkun you would need `dev/reformat-python` to fix python linter issue

This is done.","1,-1    ",1,-1,0
1565765182,7322292,,v3.2.4,merged to master,"1,-1    ",1,-1,0
1561032331,15246973,,v3.2.4,"cc @MaxGekk 
Sorry, The previous PR was damaged by me, this is a new one.
","1,-1    ",1,-1,0
1561371825,1580697,,v3.2.4,"BTW, if `identifier` can be `NOSCAN` or nothing, we should specify this in rules:
```
    | ANALYZE TABLE multipartIdentifier partitionSpec? COMPUTE STATISTICS
        (NOSCAN | FOR COLUMNS identifierSeq | FOR ALL COLUMNS)?    #analyze
    | ANALYZE TABLES ((FROM | IN) multipartIdentifier)? COMPUTE STATISTICS
        (NOSCAN)?                                                  #analyzeTables
```","1,-1    ",1,-1,0
1562444812,1580697,,v3.2.4,"+1, LGTM. Merging to master.
Thank you, @panbingkun.","1,-1    ",1,-1,0
1561771192,1938382,,v3.2.4,@cloud-fan ,"1,-1    ",1,-1,0
1562120730,1938382,,v3.2.4,I needed to re-generate the golden files.,"1,-1    ",1,-1,0
