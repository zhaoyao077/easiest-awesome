Merged to master and branch-3.4.
Merging to master/3.4
merged into master/3.4
merged into master/3.4.  Thank you @ulysses-you
cc @HyukjinKwon @ueshin
close this one in favor of https://github.com/apache/spark/pull/40006
late 【LGTM】
cc @JoshRosen  @liuzqt
We're closing this PR because it hasn't been updated in a while. This isn't a judgement on the merit of the PR in any way. It's just a way of keeping the PR queue manageable.<br>If you'd like to revive this PR, please reopen it and ask a committer to remove the Stale tag!
@HeartSaVioR , @HyukjinKwon , @SandishKumarHN PTAL. I would like to get this fix asap to make it into 3.4.x in OSS and 12.2 RC cut in DBR.
cc: @gengliangwang
【Thanks】, merging to master/3.4
`DataFrame.drop` in PySpark seems problematic when there are duplicated column names, let me address it first.
@zhengruifeng what is the current problem with drop?
> @zhengruifeng what is the current problem with drop?<br><br>existing implement in Python Client follows the behavior in PySpark that always convert column name to column, then it has this problem https://issues.apache.org/jira/browse/SPARK-42444<br>
cc @hvanhovell @HyukjinKwon @xinrong-meng @amaliujia
cc @HyukjinKwon @xinrong-meng would you mind taking a look at this one?
cc @srowen, @dongjoon-hyun, @gengliangwang, @wangyum
cc @gengliangwang FYI
I did a quick 【test】 and the UI looks ok. Also I tried git grep and all the old version `1.10.25` is gone.<br>@peter-toth 【Thanks】 for the work. Merging to master.
> 【LGTM】 if the UI works<br><br>I played a bit with the UI while making this PR and it looked ok to me.<br><br>Actually, I'm not sure that anything from our 【customization】s in `core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css` is still needed after this version update, but I removed only the up/dpwn arrows related part that caused issues...<br>E.g. the executor \"show ... entries\" box (https://github.com/apache/spark/pull/36226#issuecomment-1100810575) looked ok without this last fix: https://github.com/apache/spark/pull/36226/files#diff-e96c8dc10974b7da0c5dd4f675702462124b66c4bf2f7a0682f2ec1482a2eee3
> I did a quick 【test】 and the UI looks ok. Also I tried git grep and all the old version `1.10.25` is gone. @peter-toth 【Thanks】 for the work. Merging to master.<br><br>【Thanks】 everyone for the 【review】!
cc @zhengruifeng IIRC there were some open questions to support storage level in Connect?
@HyukjinKwon can you please 【check】 if it looks Ok?
@HyukjinKwon @amaliujia can you please 【review】? <br>If you think implementation of StorageLevel is not right I will close and can open a new one for the param only (when there's alternative implementation is 【available】). The problem is you cant just add something to PySpark without touching Connect. I guess it's intentional.
Closing as no traction
Sorry for late responses. We should better have this 【feature】 parity. But I need to 【check】 w/ the concern about storage level (raised internally). I will ping the guy to comment here.
【Thanks】 @HyukjinKwon
Hi @khalidmammadov, now that `Catalog` in Scala client including protobuf definition has been implemented at #40438, do you want to continue working on this?<br>Otherwise, I can take this over.<br>【Thanks】.
Hi, thanks for letting me know. I will look at it<br><br>On Mon, 3 Apr 2023, 21:15 Takuya UESHIN, ***@***.***> wrote:<br><br>> Hi @khalidmammadov <https://github.com/khalidmammadov>, now that Catalog<br>> in Scala client including protobuf definition has been implemented, do you<br>> want to continue working on this?<br>> Otherwise, I can take this over.<br>> 【Thanks】.<br>><br>> —<br>> Reply to this email directly, view it on GitHub<br>> <https://github.com/apache/spark/pull/40015#issuecomment-1494921787>, or<br>> unsubscribe<br>> <https://github.com/notifications/unsubscribe-auth/ACYJ3NHUQOOWBY4SFT7D3XDW7MVVDANCNFSM6AAAAAAU3TDHCU><br>> .<br>> You are receiving this because you were mentioned.Message ID:<br>> ***@***.***><br>><br>
The build was failing due to https://github.com/apache/spark/pull/40674 and now 【<font color=blue>fix】ed】 by https://github.com/apache/spark/pull/40681<br><br>Rebased.
【Thanks】! merging to master.
【Thanks】 @ueshin!
cc @cloud-fan
thanks, merging to master/3.4! (`multiTransform` is a to-be-released developer API)
【Thanks】 @cloud-fan for the 【review】!
Maybe @gengliangwang ?
cc @zhenlineo
For the 【review】ers. This is a mostly mechanical PR; the size is large but the complexity is low. All implemented 【documentation】 and function signatures were copies from Dataset.
merging this!
Sorry about getting back and forth about the schema inference. I am trying to tell a good story about the timestamp without time zone. This should be the final version before 3.4.0 release.
@cloud-fan
Let's do this kind of 【clean】ing when we touch the codes around here next time.
Merged to master.
@zhenlineo can you make the PR description a bit more descriptive?
Oh and can you add the ticket to the title?
So it is ok to not have e2e 【test】s for read API (similarly for the write side)?
【LGTM】<br><br>Looks like you only need `./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm` to fix the style.
merging.
FYI, bunch of Scala 2.13 【test】s fail (https://github.com/apache/spark/actions/runs/4187249065/jobs/7256849374#step:9:19944) should be 【<font color=blue>fix】ed】 before cutting RC cc @xinrong-meng
Hi, @hvanhovell .<br>Since we are close to RC1 cut, could you 【test】 Scala 2.13 before merging your PR, please?<br>If there is the best person to do that, it's you, @hvanhovell . 【:)】
Thank you @HyukjinKwon @dongjoon-hyun
No, it was broken since Spark 3.3.0.
> cc @HyukjinKwon and @viirya Note that I don't think this is a blocker for 3.3.2 RC1 vote.<br><br>Hmm, I think you will back port it to branch-3.3? But we don't need to cut RC2 for it? Do you mean that?
Yes, I'll land this after your release, @viirya .
Thank you, @viirya . Let me open this until Tomorrow. 【:)】
Also, cc @xinrong-meng
Thank you, @viirya and @HyukjinKwon .<br><br>For the record, Apache Spark 3.3.2 RC1 vote passed. This patch will be delivered via Spark 3.4.0/3.3.3.<br>- https://lists.apache.org/thread/krnpf0tv1jdxy6dlssho1cn7ckfjwk2m
Attaching screenshots of the updated pages.<br><br><img width=\"1160\" alt=\"getting_started-install rst\" src=\"https://user-images.githubusercontent.com/112507318/218973521-6206edc3-abc5-481b-8c90-183fe0d82b7b.png\"><br><br><img width=\"1160\" alt=\"getting_started-quickstart_df ipynb\" src=\"https://user-images.githubusercontent.com/112507318/218973623-1d9e4e02-0b4a-42fa-aa87-b5dda72e64bc.png\"><br><br><img width=\"1160\" alt=\"reference-pyspark streaming rst\" src=\"https://user-images.githubusercontent.com/112507318/218973697-be6f786b-7c90-4bfb-a86f-1b870987a2b7.png\"><br><br><img width=\"1160\" alt=\"user_guide-index rst\" src=\"https://user-images.githubusercontent.com/112507318/218973758-e2d9b81a-63f9-43b0-8f72-1fb96f594ed7.png\"><br>
Late 【LGTM】, thank you @allanf-db !
@haoyanzhang It is better to create a separate branch per every PR:<br><img width=\"700\" alt=\"Screenshot 2023-02-16 at 12 54 49\" src=\"https://user-images.githubusercontent.com/1580697/219332108-0c909f7d-907c-46c3-b290-b70029675ea1.png\"><br><br>see \"Pull request\" at https://spark.apache.org/contributing.html
@haoyanzhang Do you have an account at OSS JIRA: https://issues.apache.org/jira ? If not, I'll create it for you.
> @haoyanzhang It is better to create a separate branch per every PR: <img alt=\"Screenshot 2023-02-16 at 12 54 49\" width=\"700\" src=\"https://user-images.githubusercontent.com/1580697/219332108-0c909f7d-907c-46c3-b290-b70029675ea1.png\"><br>> <br>> see \"Pull request\" at https://spark.apache.org/contributing.html<br><br>got it,  will do it next time
> @haoyanzhang Do you have an account at OSS JIRA: https://issues.apache.org/jira ? If not, I'll create it for you.<br><br>I have one, thanks
+1, 【LGTM】. Merging to master.<br>Thank you, @haoyanzhang and @HyukjinKwon for 【review】.
@haoyanzhang Congratulations with the first 【contribution】 to Apache Spark!
cc @HyukjinKwon
Thank you, @yaooqinn , @HyukjinKwon , @LuciferYang .
So now we can 【upgrade】 guava?
Not yet, @bjornjorgensen ~ 【:)】 Please hold on any significant 【c<font color=blue>hang】es】 until Apache Spark 3.4 is released. We still need to backport many bug fixes during Apache Spark 3.4 RC period. Here I just removed the broken CI which no one will take care.
I remember Guava 【upgrade】 is also blocked by Hive .. IIRC ..
cc @olaky
【Thanks】, this is a helpful clarification
@cloud-fan @olaky Added more clarifications, let me know WDYT! 【Thanks】
【Thanks】! updated
cc @cloud-fan Fixed the style, and passed all 【check】s, thanks!
thanks, merging to master/3.4!
cc @juliuszsompolski @cloud-fan FYI
kindly ping @HyukjinKwon @HyukjinKwon
thanks, merged to master and 3.4
【Thanks】 for fixing this. Merged to master and branch-3.4.
cc @zhenlineo we discussed this today
【Thanks】 for looking into this. We should definitely fix the leak. I would probably do the same:<br>When we close the session, we go through all buffers and close them. The Result and 【clean】er also holds some resources.
We can also just 【clean】-up the results. Part of the problem is that the E2E 【test】 does not close results. I am not sure if there are similar issues on the server side.
Will submit a PR shortly.
@hvanhovell What about this? https://github.com/apache/spark/compare/master...zhenlineo:spark:allocator-memleak?expand=1 when a session is closed, we force to close all buffers.
Let me close this one, thanks @hvanhovell @zhenlineo
【Thanks】 @wangyum @dongjoon-hyun
@srielau could you take a look, pls?
Also, I'd like to share some 【performance】 measurements from my local machine, using JMH:<br>code example:<br>```java<br>...<br>    @Benchmark<br>    public void convert_branch_master(Blackhole bh) {<br>        // Convert to unsigned<br>        for (int i = -10_000; i < 10_000; i++) {<br>            UTF8String convert = NumberConverter<br>                    .convert(UTF8String.fromString(String.【value】Of(i)).getBytes(), 10, 16);<br>            bh.consume(convert);<br>        }<br><br>        // Convert to signed<br>        for (int i = -10_000; i < 10_000; i++) {<br>            UTF8String convert = NumberConverter<br>                    .convert(UTF8String.fromString(String.【value】Of(i)).getBytes(), 10, -16);<br>            bh.consume(convert);<br>        }<br>    }<br>...<br><br>// the same code for SPARK-42399 branch<br>```<br><br>With Java 8, current PR even 【speed】s up the 【performance】:<br>```java<br># JMH version: 1.36<br># VM version: JDK 1.8.0_362, OpenJDK 64-Bit Server VM, 25.362-b00<br># VM invoker: /usr/local/Cellar/openjdk@8/1.8.0+362/libexec/openjdk.jdk/Contents/Home/jre/bin/java<br># Blackhole mode: full + dont-inline hint (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)<br># Warmup: 5 iterations, 10 s each<br># Measurement: 5 iterations, 10 s each<br># Timeout: 10 min per iteration<br># Threads: 1 thread, will synchronize iterations<br># Benchmark mode: Average time, time/op<br><br><br><br>Benchmark                                           Mode  Cnt   Score   Error  Units<br>NumberConverterBenchmark.convert_branch_master      avgt   10  30.458 ± 1.638  ms/op<br>NumberConverterBenchmark.convert_branch_spark42399  avgt   10  22.857 ± 0.421  ms/op<br><br>```<br><br>With Java 11, implementation from master branch works more then 2 times 【fast】er than the same implementation with Java 8!<br>But there is not a big gap in 【performance】 difference between master branch implementation and implementation from current branch. <br>```java<br># JMH version: 1.36<br># VM version: JDK 11.0.16.1, OpenJDK 64-Bit Server VM, 11.0.16.1+0<br># VM invoker: /usr/local/Cellar/openjdk@11/11.0.16.1/libexec/openjdk.jdk/Contents/Home/bin/java<br># Blackhole mode: full + dont-inline hint (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)<br># Warmup: 5 iterations, 10 s each<br># Measurement: 5 iterations, 10 s each<br># Timeout: 10 min per iteration<br># Threads: 1 thread, will synchronize iterations<br># Benchmark mode: Average time, time/op<br><br>Benchmark                                           Mode  Cnt   Score   Error  Units<br>NumberConverterBenchmark.convert_branch_master      avgt   10  14.453 ± 1.082  ms/op<br>NumberConverterBenchmark.convert_branch_spark42399  avgt   10  17.956 ± 0.489  ms/op<br><br>```<br><br>With Java 17, implementation from master branch works more then 3 times 【fast】er than the same implementation with Java 8 and ~ 2 times 【fast】er then the same implementation with Java 11!<br>And here is a significant difference between master branch implementation and current branch (master branch is ~2x times 【fast】er...). <br>```java<br># JMH version: 1.36<br># VM version: JDK 17.0.4.1, OpenJDK 64-Bit Server VM, 17.0.4.1+1<br># VM invoker: /usr/local/Cellar/openjdk@17/17.0.4.1_1/libexec/openjdk.jdk/Contents/Home/bin/java<br># Blackhole mode: compiler (auto-detected, use -Djmh.blackhole.autoDetect=false to disable)<br># Warmup: 5 iterations, 10 s each<br># Measurement: 5 iterations, 10 s each<br># Timeout: 10 min per iteration<br># Threads: 1 thread, will synchronize iterations<br># Benchmark mode: Average time, time/op<br><br>Benchmark                                           Mode  Cnt   Score   Error  Units<br>NumberConverterBenchmark.convert_branch_master      avgt   10   8.410 ± 0.161  ms/op<br>NumberConverterBenchmark.convert_branch_spark42399  avgt   10  18.162 ± 0.036  ms/op<br>```
@hvanhovell
cc @sadikovi @HyukjinKwon
We discussed before and decided to keep prefersDate similar to a decimal config cc @HyukjinKwon
Hmm, I take a quick 【check】, do you mean the json option `prefersDecimal`?  It is inconsistent with other JSON data source options either https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala#L240 ... Users can make mistakes since they are used to data source options without `s` on the verb.<br><br>Shall we support `preferDecimal` and `prefersDecimal` in JSON, and use `prefer` in the new options of Spark 3.4?
im fine w/ that
I am curious why did we not do it from the beginning?
> I am curious why did we not do it from the beginning?<br><br>I didn't join the 【discussion】, but let's make the naming consistent before it is too late. There is only one exception `prefersDecimal` now.<br><br>
CC @HyukjinKwon @zhengruifeng
Attaching screenshots of the updated Migration Guides page and the new version 【upgrade】 sub-page.<br><br><img width=\"1340\" alt=\"pyspark_migration_guides_doc_page\" src=\"https://user-images.githubusercontent.com/112507318/219209282-27bea6e8-beb5-4336-a6c1-7e91a011a225.png\"><br><br><img width=\"1340\" alt=\"pyspark_migration_guide_【upgrade】_doc_page\" src=\"https://user-images.githubusercontent.com/112507318/219209350-bdbf18e4-e77b-414e-a19d-8e431fb2a60a.png\"><br>
Yeah, let's close. I don't think this is an issue.
Thank you, @HyukjinKwon and @rithwik-db .
cc @zhenlineo @LuciferYang
https://github.com/apache/spark/blob/7ee8a32077b09cb847b6ac41cdc5067cf7bd83e9/connector/connect/client/jvm/src/【test】/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala#L155-L159<br><br>should we remove this `try catch`? This may cover up some problems, but this may be another pr<br><br>
According to your comment, is it okay for me to merge, @LuciferYang ?
> According to your comment, is it okay for me to merge, @LuciferYang ?<br><br>ok to merge ~
【Thanks】, @LuciferYang . Merged to master/3.4.<br>Thank you, @hvanhovell , @HyukjinKwon , @LuciferYang .
thanks for the 【review】, merging to master/3.4!
Interestingly, it passed locally while GitHub Action jobs keep failing.<br>```<br>$ build/sbt \"sql/【test】Only *.OrcSourceV1Suite -- -z SPARK-11412\"<br>...<br>[info] All 【test】s passed.<br>[【success】] Total time: 23 s, completed Feb 20, 2023, 12:54:28 PM<br>```
Since this happens in GitHub Action currently, I made a WIP PR for further 【investigation】. If it's valid, I'll convert it to the official PR separately from this PR.<br>- https://github.com/apache/spark/pull/40095
I closed my PR because the failure seems to happen more earlier than this commit.
Note for the 【review】er. I still want to add a couple of 【test】s for duplicate functions.
@dongjoon-hyun I will fix those today. I do think we should have a 【discussion】 about this. Currently we have both maven and scala-2.13 that are not 【test】ed during CI. That seems wrong if both are apparently supported. The mental overhead of 【test】ing these manually is very high.
As for blocking other 【community】 members severely, the same applies to the lack of 【test】ing of scala-2.13.
Fix for 2.13 has merged. I am going to hold this off until https://github.com/apache/spark/pull/40056 is merged.
merging
thanks all
Some info I searched about it: https://docs.docker.com/build/at【test】ations/slsa-provenance/.
Thank you. The affected versions are the future releases (3.4.0/3.3.3/3.2.4) instead of the released version.
K8s Integration Tests passed. Merged to master/3.4/3.3/3.2.
cc @wangyum @srowen FYI
Could you 【test】 against hadoop-2?
> Could you 【test】 against hadoop-2?<br><br>Do same 【check】 with -Phadoop-2<br><br>**Maven** <br><br>```<br>build/mvn 【clean】<br>build/mvn 【clean】 install -DskipTestes -pl resource-managers/yarn -am -Pyarn -Phadoop-2<br>build/mvn -D【test】=none -DwildcardSuites=org.apache.spark.deploy.yarn.YarnClusterSuite -pl resource-managers/yarn 【test】 -Pyarn -Phadoop-2<br>build/mvn 【test】 -pl resource-managers/yarn -Pyarn -D【test】.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest  -Phadoop-2<br>```<br><br>**SBT**<br><br>```<br>build/sbt 【clean】 yarn/【test】 -Pyarn -D【test】.exclude.tags=org.apache.spark.tags.ExtendedLevelDBTest -Phadoop-2<br>```<br><br><br>All 【test】s passed.<br><br>
Merged to master
Merged to master. 【Thanks】 @LuciferYang
Thank you, @LuciferYang and @huaxingao !
【Thanks】 @huaxingao @dongjoon-hyun
@nija-at can you file a ticket in the JIRA (or ask me to do it), and add that to the title?
Can you also do this for the scala client?
Actually. Please open a JIRA and add JIRA id to the PR title along with module names (example: `[SPARK-xxxx][CONNECT][PYTHON]) accept user_agent in spark connect's connection string`)
@hvanhovell <br><br>> can you file a ticket in the JIRA (or ask me to do it), and add that to the title?<br><br>Done.<br><br>> Can you also do this for the scala client?<br><br>I'll do that as a separate change<br>
@dongjoon-hyun this time I have 【test】ed 2.13.
Merging.
Thank you so much, @hvanhovell and @cloud-fan !
For the 【review】s, I have manually 【test】ed this with 2.13.
Alright merging this.
> For the 【review】s, I have manually 【test】ed this with 2.13.<br><br>Thank you so much, @hvanhovell !
Merged to master/3.4 for Apache Spark 3.4.0. Thank you, @huaxingao and @viirya .<br><br>cc @MaxGekk since he filed SPARK-39859 originally.
【Thanks】 @dongjoon-hyun @viirya
cc @dongjoon-hyun @HyukjinKwon
Merged to master for Apache Spark 3.5. Thank you, @sadikovi and @HyukjinKwon .
cc @viirya
Thank you, @viirya !
The 【test】s passed in the GitHub Action. Merged to master<br>![Screenshot 2023-02-17 at 1 08 13 AM](https://user-images.githubusercontent.com/9700541/219601614-e84bf609-e8fd-4a0b-80b8-0afa5f41b457.png)<br><br>Merged to master/3.4.
It seems more complicated than I thought, I think we can simplify it in this way<br><br>In client:<br>```<br>    def sql(self, sqlQuery: str, args: Optional[Dict[str, str]] = None) -> \"DataFrame\":<br>        df = DataFrame.withPlan(SQL(sqlQuery, args), self)<br>        print(df.schema)   <- eagerly analyze the plan<br>        return df<br>```<br><br>In connect planner:<br>```<br>  private def transformSql(sql: proto.SQL): LogicalPlan = {<br>    // scalastyle:off println<br>    println(s\"invoke transformSql $sql\")<br>    session<br>      .sql(sql.getQuery, sql.getArgsMap.asScala.toMap)    <- directly invoke the spark session api<br>      .logicalPlan<br>  }<br>```<br><br><br>bin/pyspark --remote \"local[*]\"<br>```<br>>>> spark.sql(\"set spark.sql.adaptive.enabled=false\")<br>invoke transformSql query: \"set spark.sql.adaptive.enabled=false\"<br><br>StructType([StructField('key', StringType(), False), StructField('【value】', StringType(), False)])<br>DataFrame[key: string, 【value】: string]<br>```
@cloud-fan @boneanxs could you please take a look if you find a time?
@Yikf can you provide a 【test】 case, or at least the error stacktrace you hit in your environment?
@cloud-fan This case is the error that Apache kyuubi encountered when upgrading from spark 3.3.1 to 3.3.2, can see this [link](https://github.com/apache/kyuubi/actions/runs/4192366930/jobs/7268919556#step:6:2611) to find the error stacktrace.
updated, verified w/ kyuubi on spark 3.3.2 and all 【test】s passed<br>```<br>build/mvn 【clean】 【test】 -pl :kyuubi-spark-connector-hive_2.12 -am -Pspark-3.3 -Dspark.version=3.3.2<br>```
kindly ping @cloud-fan , @boneanxs Any suggestions?
I have not followed the 【c<font color=blue>hang】es】 in this part of the code too much in a while - but this 【specific】 PR will result in a different `jobId` each time the class is deserialized - I would expect that to cause issues with output formats ?
@mridulm 【Thanks】 your 【review】, this is a nice question for me, `JobId` maybe is different when each time the class is deserialized.<br><br>How about this idea that `SparkHadoopWriterUtils.createJobTrackerID` to generate an ID for a job tracker, and the job tracker is 【unique】, JobId is constructed using a 【unique】 tackerid when the class is deserialized.
@Yikf Agree - we only specify two parts for the `JobID` - the `String jtIdentifier` and `int id`.<br>We can persist those in the class - and make jobId a `transient lazy val` which recreates it each time.<br><br>Something like this instead:<br><br>```<br><br>  private[this] val jobIdParts: (String, Int) = {<br>    val id = SparkHadoopWriterUtils.createJobID(new Date, 0)<br>    (id.getJtIdentifier, id.getId)<br>  }<br><br>  @transient private lazy val jobId = new JobID(jobIdParts._1, jobIdParts._2)<br>```<br><br>Thoughts ?<br>
@mridulm Nice suggestion, and we can simplify to as follow since `int id` is 【unique】.<br><br>```scala<br>  private[this] val jobTrackerID = SparkHadoopWriterUtils.createJobTrackerID(new Date)<br>  @transient private lazy val jobId = new JobID(jobTrackerID, 0)<br>```
Hi, @cloud-fan . SPARK-41448 landed to master/3.3/3.2 and this is merge this to master/3.4 only. I'm wondering if we are planning backporting to branch-3.3 and 3.2.<br>- https://github.com/apache/spark/pull/38980#issuecomment-1346229161<br>
Also, cc @sunchao
@Yikf can you help to open a backport PR for 3.2/3.3? 【Thanks】!
> @Yikf can you help to open a backport PR for 3.2/3.3? 【Thanks】!<br><br>Sure
cc @dongjoon-hyun
I make another one build with maven 3.8.7 + cyclonedx-maven-plugin  2.7.4 https://github.com/LuciferYang/spark/actions/runs/4205904014/jobs/7298678641<br><br><img width=\"1074\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/219719321-dc1e6aa3-1a21-4e93-92ce-60cee921493b.png\"><br>
I mean in our GitHub Action repo. We are using CycloneDX 2.7.3, aren't we?<br><br>> I make another one build with maven 3.8.7 + cyclonedx-maven-plugin 2.7.4 https://github.com/LuciferYang/spark/actions/runs/4205904014/jobs/7298678641
Yes, we use CycloneDX 2.7.3. So I should not explain that 2.7.4 has such issue in the pr description, because it does not affect Spark now, am I right?
Please let me explain my intention more:<br><br>1. First of all, I want to update maven to 3.9.0(keep use CycloneDX 2.7.3), then I found the following error:<br><br>```<br>[ERROR] An error occurred attempting to read POM<br>org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42) <br>    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)<br>    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)<br>    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)<br>    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)<br>    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)<br>    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)<br>    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)<br>    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)<br>    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)<br>    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)<br>    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)<br>    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)<br>    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)<br>    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)<br>    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)<br>    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)<br>    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)<br>    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)<br>    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke (Method.java:498)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)<br>```<br><br>I think We should see similar errors here: https://github.com/LuciferYang/spark/actions/runs/4206035140/jobs/7299042843 later<br><br>2. then I want to 【test】 maven 3.9.0 + CycloneDX 2.7.4 couple of days ago, but there an error same as  `maven 3.8.7 + cyclonedx-maven-plugin 2.7.4`,  I think we should see it here: https://github.com/LuciferYang/spark/runs/11424487074 later<br><br>3. then I 【test】 maven 3.9.0 + CycloneDX 2.7.5 today, there is no above issues(we can 【check】 https://github.com/LuciferYang/spark/runs/11424568023 later).<br><br>So If I want to 【upgrade】 Spark to use maven 3.9.0, I must 【upgrade】 cyclonedx-maven-plugin to 2.7.5, I should 【upgrade】 them in one or two pr? <br>
I'm trying to assess the issue. So, those combination issue is not the AS-IS Apache Spark issue in both master/branch-3.4, right?<br><br>FYI, Cyclone plugin 2.7.4 issue is a known one. When I started SBOM works, 2.7.4 was the las【test】 but was unusable across multiple ASF projects. That was the main reason I chose 2.7.3 instead of the la【test】 at that time. I'm not quite sure if 2.7.5 is 【stable】 enough.<br><br>Anyway, we can apply this PR on `master` branch for Apache Spark 3.5.0 only separately from the Maven issue. Maven is also another big issues always.
Yeah, Spark 3.4.0 does not need this pr.<br><br>
If you don't mind, please allow me one or two days. I'll 【check】 this during weekend~ Thank you for your patience always.
@dongjoon-hyun found a new issue related to 2.7.5: https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/284<br><br><br>
Got it. Thank you for informing.
I think we should wait for 2.7.6 or higher to 【test】 【usability】, then we can reuse this jira. I will close this pr first, thanks @dongjoon-hyun
+1 for your decision, @LuciferYang . Thank you for letting me know before I started my work~ 【:)】
I'm hitting this when trying to build hadoop having updated maven via homebrew so as to get spark to work.  joy.
This PR is superseded by https://github.com/apache/spark/pull/40726 .
I think we should better file a JIRA.
Discussed this with @grundprinzip. Apparently the retries are kept so long intentionally. The fix here needs to occur differently. Closing this PR for now.
cc @HyukjinKwon mind taking a look when you find some time?
【Thanks】, @HyukjinKwon !
Late 【LGTM】
wait https://github.com/apache/spark/pull/40065
Need to wait for a 【stable】 CycloneDX version, close this first<br>
Addressed comments. @LuciferYang <br>And gentle ping @wangyum @sunchao: could you also take a look?
Add a conf `spark.sql.hive.dropPartitionByName.enabled` and two 【test】s. cc: @sunchao
Merged to master, thanks!
Thank youso much, @wecharyu, @sunchao and all!
I need to update golden files in this PR.
When you have some time, could you 【review】 this, @viirya ? I want to merge this to proceed the further 【investigation】s.
Thank you so much always for your help, @viirya !<br>Merged to master for Apache Spark 3.5.
Some 【test】s always fail with <br>```<br>112 did not equal 104 Invalid stopIndex of a query context. Actual:SQLQueryContext(Some(1),Some(15),Some(15),Some(112),Some(select id from hive.`/home/runner/work/oss-spark/oss-spark/target/tmp/spark-9ff647b3-c1b5-449d-ae54-19a7f3baff9d`),None,None)<br>```<br>Length of string \"select id from hive.`/home/runner/work/oss-spark/oss-spark/target/tmp/spark-9ff647b3-c1b5-449d-ae54-19a7f3baff9d`\" is 113, so the last index is 112.<br><br>I guess that's because my folder name is `oss-spark`. The two `oss-` contains 8 chars, that's equal to 112 - 104.<br><br>https://github.com/WweiL/oss-spark/actions/runs/4235566927/jobs/7359363624<br><br>So I changed the [hard-coded length here](https://github.com/apache/spark/blob/b36d1484c1a090a33d9add056730128b9ba5729f/sql/hive/src/【test】/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala#L1410) to a variable-length one.<br><br>@MaxGekk @srielau @itholic I found that this is related to the PR (https://github.com/apache/spark/pull/39977) you pushed / 【review】ed. Can you guys also take a look? 【Thanks】!
【Thanks】 for catching out, @WweiL !
@cloud-fan Can you merge this to master when you get a chance? Thank you!
thanks, merging to master!
Was this merged only to Spark 3.5 (master branch)? The JIRA ticket is not properly marked for fix version as well as status, and we need to make it clear to determine the version range to apply SPARK-42572.
> Was this merged only to Spark 3.5 (master branch)? The JIRA ticket is not properly marked for fix version as well as status, and we need to make it clear to determine the version range to apply [SPARK-42572](https://issues.apache.org/jira/browse/SPARK-42572).<br><br>Yes this was only merged to master. I've updated the version in SPARK-42572. BTW is there a way to quickly decide what's the version of the current master branch? I thought it was 3.4...
Sorry I seem to look at different JIRA ticket. SPARK-42484 contains the 【<font color=blue>fix】ed】 version, 3.5.0. That said, SPARK-42572 only needs to be applied to master branch.
The kub 【test】 is not related to this PR, I believe:<br>```<br>[info] org.apache.spark.deploy.k8s.integration【test】.KubernetesSuite *** ABORTED *** (27 minutes, 35 seconds)<br>[info]   io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/api/v1/namespaces. Message: object is being deleted: namespaces \"spark-e7de0ffd81044f09afb2693a0e227a43\" already exists. Received status: <br>```
+1, 【LGTM】. Merging to master/3.4.<br>Thank you, @gengliangwang.
Move `SaveMode` to catalyst module is a break change, need add `ProblemFilters` to `MimaExcludes`
> Move `SaveMode` to catalyst module is a break change, need add `ProblemFilters` to `MimaExcludes`<br><br>I think we shouldn't make such breaking change?
> > Move `SaveMode` to catalyst module is a break change, need add `ProblemFilters` to `MimaExcludes`<br>> <br>> I think we shouldn't make such breaking change?<br><br>best to avoid
Depends on https://github.com/apache/spark/pull/40109 to get the correct error message for `create`.
Sorry late 【LGTM】
Before we try to use multiple task schedulers, why not fix the 【parallel】ism issues in the existing one. Moving from a locking everywhere approach, to an event loop should yield quite a throughput 【improvement】.
@hvanhovell it won't be enough, we still need to scale communication with executors
@yaooqinn you did the last 【upgrade】 on this one in https://github.com/apache/spark/pull/38733 will you have a look at this one?
@dongjoon-hyun yes, that's right.<br>I think we can have this small update now to master and branch 3.4. When 3.4 is released, we can try to update to a newer version.
Got it. Thank you for the clarification. Sounds safe.
Oh, I missed that you wrote `branch-3.4` explicitly. Are you sure, @bjornjorgensen ?<br>> I think we can have this small update now to master and branch 3.4. When 3.4 is released, we can try to update to a newer version.<br><br>If you want to claim this PR as a blocker, you need to do that properly. In general, you had better do two things at least.<br>- Ping, @xinrong-meng , in order to inform her the existence of the blocker.<br>- Update SPARK-42486 properly. Currently, the JIRA Affected Version is quite opposite from your comment (or intention) here.<br><br>![Screenshot 2023-02-19 at 5 15 05 PM](https://user-images.githubusercontent.com/9700541/219988088-7bc0c346-4ef3-41ce-91cf-0f83b57868f5.png)<br>
I did 【upgrade】 the JIRA ticket now to include 3.4.0. <br>This is not a blocker. <br>I think this is more a nice too have 【upgrade】 for 3.4.0.
I'm not sure new 3.6.4 Zookeepr is urgent or safe in branch-3.4. According to the Maven Central, 3.6.4 is the least adopted (unverified) release among the recent versions. Do we have a 【test】 case to support that we need this?<br><br>![Screenshot 2023-02-20 at 3 30 59 PM](https://user-images.githubusercontent.com/9700541/220212850-0ae350c0-bf7d-4b9d-a844-7780bd6362e6.png)<br><br><br>
Let's probably don't add it to branch-3.4 ...
Since branch-3.4 CI is under fix so I may have to re-create the tag later. Please let me know if we shall wait for this PR or not.
IMO, we don't need to wait for this PR, @xinrong-meng .
【Thanks】 @dongjoon-hyun!
Ok, thank you, @dongjoon-hyun
cc: @gengliangwang, @HeartSaVioR, @SandishKumarHN
@rangadi with this change, we can conclude that spark-protobuf supports proto2 and proto3 versions right?
@dongjoon-hyun, done. Remove 3.4 in the 【summary】. <br><br>> @rangadi with this change, we can conclude that spark-protobuf supports proto2 and proto3 versions right?<br><br>Yes. And we always supported Proto2. <br>
Thank you for updates, @rangadi .<br><br>cc @hvanhovell
@gengliangwang could you take a quick look and merge in to master and 3.4.<br>Currently, it looks like you are the only one who can merge Protobuf patches. Please suggest someone else also for next time. <br>
@rangadi can you update the PR description to follow the PR template?
> @rangadi can you update the PR description to follow the PR template?<br><br>@gengliangwang update the 【summary】 to follow the template. Sorry about that. <br>Also added a new 【test】. PTAL.
cc @srowen
【Thanks】 @srowen @dongjoon-hyun
【test】 first
【Thanks】 @dongjoon-hyun @srowen
BTW, I have one question, @LuciferYang . We explicitly exclude `jna` from `commons-crypto`. Do you know what are we losing from the enumerated items in the `commons-crypto` releasenotes?<br><br>https://github.com/apache/spark/blob/5fc44dabe5084fb784f064afe691951a3c270793/pom.xml#L2679-L2689
From the release notes, I think nothing is losing.<br><br>`OpenSsl20XNativeJna`,  which is not mentioned in release notes, is a new support in version 1.2.0(Because jna is excluded, it cannot be used in Spark).<br><br>
Oh, this seems to break Apple Silicon environment, @LuciferYang . I hit the failure on Macbook environment.<br>```<br>$ build/sbt \"sql/【test】Only *.RowQueueSuite\"<br>...<br>[info] RowQueueSuite:<br>[info] - in-memory queue (33 milliseconds)<br>[info] - disk queue (encryption = off) (28 milliseconds)<br>[info] org.apache.spark.sql.execution.python.RowQueueSuite *** ABORTED *** (125 milliseconds)<br>[info]   java.lang.ExceptionInInitializerError:<br>[info]   at java.base/java.lang.Class.forName0(Native Method)<br>[info]   at java.base/java.lang.Class.forName(Class.java:398)<br>[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)<br>[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)<br>[info]   at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)<br>[info]   at org.apache.spark.【security】.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)<br>[info]   at org.apache.spark.【security】.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)<br>[info]   at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)<br>[info]   at scala.Option.map(Option.scala:230)<br>[info]   at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)<br>[info]   at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)<br>[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)<br>[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)<br>[info]   at org.apache.spark.【security】.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)<br>[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[info]   at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[info]   at org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)<br>[info]   at org.scala【test】.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)<br>[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>[info]   at scala.collection.immutable.List.foreach(List.scala:431)<br>[info]   at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[info]   at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[info]   at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.Suite.run(Suite.scala:1114)<br>[info]   at org.scala【test】.Suite.run$(Suite.scala:1096)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[info]   at org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[info]   at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[info]   at java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>[info]   at java.base/java.lang.Thread.run(Thread.java:829)<br>[info]   Cause: java.lang.IllegalStateException: java.【security】.GeneralSecurityException: Native library is not loaded<br>[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:67)<br>[info]   at java.base/java.lang.Class.forName0(Native Method)<br>[info]   at java.base/java.lang.Class.forName(Class.java:398)<br>[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)<br>[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)<br>[info]   at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)<br>[info]   at org.apache.spark.【security】.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)<br>[info]   at org.apache.spark.【security】.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)<br>[info]   at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)<br>[info]   at scala.Option.map(Option.scala:230)<br>[info]   at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)<br>[info]   at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)<br>[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)<br>[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)<br>[info]   at org.apache.spark.【security】.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)<br>[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[info]   at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[info]   at org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)<br>[info]   at org.scala【test】.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)<br>[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>[info]   at scala.collection.immutable.List.foreach(List.scala:431)<br>[info]   at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[info]   at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[info]   at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.Suite.run(Suite.scala:1114)<br>[info]   at org.scala【test】.Suite.run$(Suite.scala:1096)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[info]   at org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[info]   at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[info]   at java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>[info]   at java.base/java.lang.Thread.run(Thread.java:829)<br>[info]   Cause: java.【security】.GeneralSecurityException: Native library is not loaded<br>[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.【check】Native(OpenSslCryptoRandom.java:79)<br>[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:65)<br>[info]   at java.base/java.lang.Class.forName0(Native Method)<br>[info]   at java.base/java.lang.Class.forName(Class.java:398)<br>[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)<br>[info]   at org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)<br>[info]   at org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)<br>[info]   at org.apache.spark.【security】.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)<br>[info]   at org.apache.spark.【security】.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)<br>[info]   at org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)<br>[info]   at scala.Option.map(Option.scala:230)<br>[info]   at org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)<br>[info]   at org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)<br>[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)<br>[info]   at org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)<br>[info]   at org.apache.spark.【security】.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)<br>[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[info]   at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[info]   at org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)<br>[info]   at org.scala【test】.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)<br>[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>[info]   at scala.collection.immutable.List.foreach(List.scala:431)<br>[info]   at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[info]   at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[info]   at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.Suite.run(Suite.scala:1114)<br>[info]   at org.scala【test】.Suite.run$(Suite.scala:1096)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[info]   at org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)<br>[info]   at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[info]   at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[info]   at java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>[info]   at java.base/java.lang.Thread.run(Thread.java:829)<br>[error] Uncaught exception when running org.apache.spark.sql.execution.python.RowQueueSuite: java.lang.ExceptionInInitializerError<br>[error] sbt.ForkMain$ForkError: java.lang.ExceptionInInitializerError: null<br>[error] \tat java.base/java.lang.Class.forName0(Native Method)<br>[error] \tat java.base/java.lang.Class.forName(Class.java:398)<br>[error] \tat org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)<br>[error] \tat org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)<br>[error] \tat org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)<br>[error] \tat org.apache.spark.【security】.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)<br>[error] \tat org.apache.spark.【security】.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)<br>[error] \tat org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)<br>[error] \tat scala.Option.map(Option.scala:230)<br>[error] \tat org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)<br>[error] \tat org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)<br>[error] \tat org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)<br>[error] \tat org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)<br>[error] \tat org.apache.spark.【security】.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)<br>[error] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[error] \tat org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[error] \tat org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[error] \tat org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[error] \tat org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[error] \tat org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[error] \tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[error] \tat org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[error] \tat org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)<br>[error] \tat org.scala【test】.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227<br>[error] \tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[error] \tat org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413<br>[error] \tat scala.collection.immutable.List.foreach(List.scala:431)<br>[error] \tat org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[error] \tat org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[error] \tat org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[error] \tat org.scala【test】.Suite.run(Suite.scala:1114)<br>[error] \tat org.scala【test】.Suite.run$(Suite.scala:1096)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[error] \tat org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[error] \tat org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[error] \tat org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[error] \tat org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[error] \tat sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[error] \tat java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>[error] \tat java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>[error] \tat java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>[error] \tat java.base/java.lang.Thread.run(Thread.java:829)<br>[error] Caused by: sbt.ForkMain$ForkError: java.lang.IllegalStateException: java.【security】.GeneralSecurityException: Native library is not loaded<br>[error] \tat org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:67)<br>[error] \tat java.base/java.lang.Class.forName0(Native Method)<br>[error] \tat java.base/java.lang.Class.forName(Class.java:398)<br>[error] \tat org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)<br>[error] \tat org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)<br>[error] \tat org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)<br>[error] \tat org.apache.spark.【security】.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)<br>[error] \tat org.apache.spark.【security】.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)<br>[error] \tat org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)<br>[error] \tat scala.Option.map(Option.scala:230)<br>[error] \tat org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)<br>[error] \tat org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)<br>[error] \tat org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)<br>[error] \tat org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)<br>[error] \tat org.apache.spark.【security】.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)<br>[error] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[error] \tat org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[error] \tat org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[error] \tat org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[error] \tat org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[error] \tat org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[error] \tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[error] \tat org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[error] \tat org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)<br>[error] \tat org.scala【test】.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227<br>[error] \tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[error] \tat org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413<br>[error] \tat scala.collection.immutable.List.foreach(List.scala:431)<br>[error] \tat org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[error] \tat org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[error] \tat org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[error] \tat org.scala【test】.Suite.run(Suite.scala:1114)<br>[error] \tat org.scala【test】.Suite.run$(Suite.scala:1096)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[error] \tat org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[error] \tat org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[error] \tat org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[error] \tat org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[error] \tat sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[error] \tat java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>[error] \tat java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>[error] \tat java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>[error] \tat java.base/java.lang.Thread.run(Thread.java:829)<br>[error] Caused by: sbt.ForkMain$ForkError: java.【security】.GeneralSecurityException: Native library is not loaded<br>[error] \tat org.apache.commons.crypto.random.OpenSslCryptoRandom.【check】Native(OpenSslCryptoRandom.java:79)<br>[error] \tat org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:65)<br>[error] \tat java.base/java.lang.Class.forName0(Native Method)<br>[error] \tat java.base/java.lang.Class.forName(Class.java:398)<br>[error] \tat org.apache.commons.crypto.utils.ReflectionUtils.getClassByNameOrNull(ReflectionUtils.java:93)<br>[error] \tat org.apache.commons.crypto.utils.ReflectionUtils.getClassByName(ReflectionUtils.java:64)<br>[error] \tat org.apache.commons.crypto.random.CryptoRandomFactory.getCryptoRandom(CryptoRandomFactory.java:189)<br>[error] \tat org.apache.spark.【security】.CryptoStreamUtils$.createInitializationVector(CryptoStreamUtils.scala:138)<br>[error] \tat org.apache.spark.【security】.CryptoStreamUtils$.createCryptoOutputStream(CryptoStreamUtils.scala:56)<br>[error] \tat org.apache.spark.serializer.SerializerManager.$anonfun$wrapForEncryption$3(SerializerManager.scala:151)<br>[error] \tat scala.Option.map(Option.scala:230)<br>[error] \tat org.apache.spark.serializer.SerializerManager.wrapForEncryption(SerializerManager.scala:151)<br>[error] \tat org.apache.spark.sql.execution.python.DiskRowQueue.<init>(RowQueue.scala:119)<br>[error] \tat org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2(RowQueueSuite.scala:72)<br>[error] \tat org.apache.spark.sql.execution.python.RowQueueSuite.$anonfun$new$2$adapted(RowQueueSuite.scala:68)<br>[error] \tat org.apache.spark.【security】.EncryptionFunSuite.$anonfun$encryptionTest$2(EncryptionFunSuite.scala:32)<br>[error] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[error] \tat org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[error] \tat org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[error] \tat org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[error] \tat org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[error] \tat org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[error] \tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:207)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[error] \tat org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[error] \tat org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)<br>[error] \tat org.scala【test】.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227<br>[error] \tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[error] \tat org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413<br>[error] \tat scala.collection.immutable.List.foreach(List.scala:431)<br>[error] \tat org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[error] \tat org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[error] \tat org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[error] \tat org.scala【test】.Suite.run(Suite.scala:1114)<br>[error] \tat org.scala【test】.Suite.run$(Suite.scala:1096)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[error] \tat org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[error] \tat org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[error] \tat org.apache.spark.SparkFunSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[error] \tat org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[error] \tat org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:66)<br>[error] \tat org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[error] \tat org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[error] \tat sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[error] \tat java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>[error] \tat java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)<br>[error] \tat java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)<br>[error] \tat java.base/java.lang.Thread.run(Thread.java:829)<br>[info] Run completed in 1 second, 175 milliseconds.<br>[info] Total number of 【test】s run: 2<br>[info] Suites: completed 0, aborted 1<br>[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0<br>[info] *** 1 SUITE ABORTED ***<br>[error] Error during 【test】s:<br>[error] \torg.apache.spark.sql.execution.python.RowQueueSuite<br>[error] (sql / Test / 【test】Only) sbt.TestsFailedException: Tests un【success】ful<br>[error] Total time: 176 s (02:56), completed Feb 24, 2023, 12:30:57 AM<br>```
In short, it's `Native library is not loaded` error. Could you verify from your side, @LuciferYang ?<br>```<br>java.lang.IllegalStateException: java.【security】.GeneralSecurityException: Native library is not loaded<br>[info]   at org.apache.commons.crypto.random.OpenSslCryptoRandom.<clinit>(OpenSslCryptoRandom.java:67)<br>```
【Thanks】 @dongjoon-hyun, it's my bad, I hit same issue, let me revert this one, I will remember 【check】 this part next time.<br><br>
Well, let me revert directly~
Since it's on master only, it's easier.
ok, thanks @dongjoon-hyun
It's reverted via https://github.com/apache/spark/commit/ca22c41eb30c6ff4588cf4e91252fe93ae946236 .
FYI, I also confimed that the revert commit recovers the following streaming module 【test】, too.<br>```<br>org.apache.spark.streaming.ReceivedBlockHandlerWithEncryptionSuite<br>```
On these, can you give any info about why the 【upgrade】 is useful or important?
Updated pr description, listing bug fix and possibly useful 【improvement】s
+1 to raise a 【discussion】 thread in mailing list
Absolutely! Will start a thread @dongjoon-hyun.
Thank you, @allanf-db and @HyukjinKwon .
BTW, I changed the affected version of this JIRA from 3.4.0 to 3.5.0.<br>I don't think Apache Spark 3.4 needs this in any cases. To simply put, it's too late.
Attaching screenshot illustrating the change.<br>Current page on the left - Scala code example on the first tab.<br>New page on the right - Python code example on the first tab.<br><br><img width=\"1706\" alt=\"making_python_first_tab\" src=\"https://user-images.githubusercontent.com/112507318/219992905-c4c079d8-ae3f-4462-b41e-2de38d83e6d8.png\"><br>
We should discuss this on the list, but I'm +1 on making Python first (in fact I suggested to Allan to do this). There are way more users of Spark in Python than Scala today, and people who want to see another language can just click that other tab. (In fact, if we want to make this better, we could even remember the language preference in a cookie and flip all tabs at the same time when you click one). Even our homepage at https://spark.apache.org has used Python for its example for a long time.
(FWIW I'm OK with showing Python first too)
Of course, I'm also open for 【discussion】. IIRC, there are several issues like 【feature】 parity because Scala is still our first development language.
Per the 【discussion】 in https://lists.apache.org/thread/1p8s09ysrh4jqsfd47qdtrl7rm4rrs05, I will merge this in with few days if there's no objection.
Thank you all for initiating the official 【discussion】. I also agree with the decision to merge this for Apache Spark 3.5.0.
cc @gengliangwang
I forgot to update it. TBH since these are verbatim copies of the existing API, I think there is a case to be made to use the original version instead the version of when the client was built.
Or we can remove all `@since *` and just add one `@since 3.4.0` at the top level of the connect. And then, let the new (master branch only) APIs to have `@since 3.5.0` in the next release.
Or just grip and replace `@since *` to `@since 3.4.0` ...
When we add new languages like Python/R, did you use the same versions of Scala API, @hvanhovell ? They were also a wrappers on top of Scala API.<br><br>> I forgot to update it. TBH since these are verbatim copies of the existing API, I think there is a case to be made to use the original version instead the version of when the client was built.<br><br>
Personally, I think it is a little strange to change `@since `. For example,  the following function is changed to `@since 3.4.0`, but `deprecated` since 2.1.0 ....<br><br>https://github.com/apache/spark/blob/1688a8768fb34060548f8790e77f645027f65db2/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/functions.scala#L221-L226
That's a good question. Maybe, what about avoiding this propagation of deprecated methods, @hvanhovell , @HyukjinKwon , @LuciferYang ? Since we already deprecated these, it sounds more consistent with what Apache Spark has been claiming (not to use this APIs in new use cases).
+1, Agree `avoiding this propagation of deprecated methods`
I have updated the since tags.<br><br>A couple of things I would like to highlight. The scala clients tries to be as compatible as possible (both source and binary) with the exiting API. That is why this is different that API that is built on top of the scala API, that was new, this however is not, it should be or at least aspires to be the same API. Hence my argument that you could also use the original since tags. Anyway I am fine with updating them, like I did in other PRs, I just missed it in this case.<br><br>As for the removal of deprecated methods. Again we want to be as compatible as possible, so I want to given users access to them. We aspire that in the best case a user can migrate their code to connect without any issues (preferably no recompilation). I don't think we should ever remove these methods, they are poorly named that is all. They do not inhibit us in any way to keep evolving Spark since they are just aliases. I have kept out a few deprecated methods in the current connect Dataset, but I do intent to bring them back once we can support them.<br><br>
Merged to master, branch-3.4 and branch-3.3.
Based on https://github.com/apache/parquet-mr/pull/982#issuecomment-1376750703, I guess that the Parquet 【community】 may think it's not a critical issue, but in my case, it's critical.
banch-3.3/3.2 use parquet 1.12.2, if this fix is accepted, would you mind submitting pr for these two branches? @pan3793 <br><br>
> banch-3.3/3.2 use parquet 1.12.2, if this fix is accepted, would you mind submitting pr for these two branches? @pan3793<br><br>sure.
I verified this patch by scanning a large parquet/zstd table and updated the PR description.
Thank you all. And, feel free to merge to land this to the release branches, @sunchao .<br><br>cc @cloud-fan , @HyukjinKwon , @mridulm , @tgravescs
【Thanks】! merged to master/branch-3.3/branch-3.2
@sunchao also need merge to branch-3.4 ...
Yes, it's in branch-3.4 as well
This seems duplicating a lot of the existing quickstart.<br>Should we maybe just mention in the original quickstart, and have a separate quickstart for connect?<br>We should demonstrate the idea of being separate client and server Ideally.
note for myself, I 【check】ed this PR by https://mybinder.org/v2/gh/itholic/spark.git/SPARK-42475?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_connect.ipynb
Can we replace  \"SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\" <br>with just spark.stop() ?<br><br><br>
> Can we replace \"SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\"<br>> with just spark.stop() ?<br><br>This was actually my suggestion to explicitly show the diff between SparkSession.builder.master vs SparkSession.builder.remote ;-).
> This was actually my suggestion to explicitly show the diff between SparkSession.builder.master vs SparkSession.builder.remote ;-).<br><br>Yes, and additionally I considered the case where the user does not use Spark through the `pyspark` shell command or does not give the name `spark` to the Spark session.<br><br>【Thanks】 for the suggestion!
TiDB also supports these 【optimization】s:<br><img width=\"857\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5399861/221442994-5b350e5d-76b0-4cbe-a0b8-fb3c39bb2cd3.png\"><br><br>
I think we should ignore the `plan_id` when comparing the plans, since it depends on the order of invocation and is not 【stable】.
Unfortunately, this seems to be unable to mitigate it.
Sorry I may be out of context. Do we happen to have an 【overview】 of the situation of 3.4 CI? I removed the RC tag just now. Let me know if there is something I could help with.
Hi, @xinrong-meng . The branch-3.4 CI is still broken and I made the second PR to mitigate it.<br>- https://github.com/apache/spark/pull/40101
@zhengruifeng <br><br>Would you update the PR description to attach the list of classes that are moved to mllib-common ?
@zhengruifeng If the PR is ready for 【review】, would you remove the WIP and draft mark on PR ?
> This PR also copies following 【test】suites to spark-mllib-common:<br>> 1, org.apache.spark.ml.attribute.*<br>> 2, org.apache.spark.ml.linalg.* except:<br>> <br>> 【test】(\"JavaTypeInference with VectorUDT\") in VectorUDTSuite due to cyclical dependency;<br>> 3, org.apache.spark.ml.param.* except:<br>> <br>> 【test】(\"Filtering ParamMap\") in ParamsSuite due to cyclical dependency;<br><br>This is bad for code maintenance. Can we move these 【test】s to a \"ml-common-【test】\" jar, and in ml-client / ml-server 【test】s they both run 【test】s in \"ml-common-【test】\" jar ?
> > This PR also copies following 【test】suites to spark-mllib-common:<br>> > 1, org.apache.spark.ml.attribute.*<br>> > 2, org.apache.spark.ml.linalg.* except:<br>> > 【test】(\"JavaTypeInference with VectorUDT\") in VectorUDTSuite due to cyclical dependency;<br>> > 3, org.apache.spark.ml.param.* except:<br>> > 【test】(\"Filtering ParamMap\") in ParamsSuite due to cyclical dependency;<br>> <br>> This is bad for code maintenance. Can we move these 【test】s to a \"ml-common-【test】\" jar, and in ml-client / ml-server 【test】s they both run 【test】s in \"ml-common-【test】\" jar ?<br><br>let me take a look
convert to draft since we will focus on `pyspark.ml` first
@zhengruifeng Is it ready for another pass 【review】 ?
@WeichenXu123  I think it is ready for 【review】
For the 2 exceptions:<br>> 2, org.apache.spark.ml.linalg.* except VectorUDTSuite due to cyclical dependency; (it copies the VectorUDTSuite except 【test】(\"JavaTypeInference with VectorUDT\"))<br>> 3, org.apache.spark.ml.param.* except ParamsSuite due to cyclical dependency; (it copies the ParamsSuite except 【test】(\"Filtering ParamMap\"))<br><br><br>You can move \"JavaTypeInference with VectorUDT\" out of VectorUDTSuite, and move \"Filtering ParamMap\" out of ParamsSuite, and then can move remaining 【test】 code without duplication.
> For the 2 exceptions:<br>> <br>> > 2, org.apache.spark.ml.linalg.* except VectorUDTSuite due to cyclical dependency; (it copies the VectorUDTSuite except 【test】(\"JavaTypeInference with VectorUDT\"))<br>> > 3, org.apache.spark.ml.param.* except ParamsSuite due to cyclical dependency; (it copies the ParamsSuite except 【test】(\"Filtering ParamMap\"))<br>> <br>> You can move \"JavaTypeInference with VectorUDT\" out of VectorUDTSuite, and move \"Filtering ParamMap\" out of ParamsSuite, and then can move remaining 【test】 code without duplication.<br><br>good point, done
cc @hvanhovell @grundprinzip @srowen @HyukjinKwon would you mind taking a look?
cc @viirya @cloud-fan , thank you
Adding 【test】 cases<br><br>
cc @zhengruifeng
I'm not sure whether we can use `child.maxRowsPerPartition` in a global sort, for example, the partition sizes maybe [5, 5, 5] in child, but after global sort the distribution maybe [3, 3, 3, 3, 3]<br><br>cc @cloud-fan
Thank you @zhengruifeng, makes sense.
Hi, @xinrong-meng , @HyukjinKwon , @cloud-fan <br>I decided to simplify the 【test】 case to be more 【robust】 in terms of the order of merged schema.<br>This will unblock Apache Spark 3.4.0 RC1.
or catch `TestFailedException` then 【check】 another set of parameters ?<br><br>
【LGTM】. 【Thanks】 @dongjoon-hyun !
【Thanks】 all, merged to master and branch-3.4
Thank you, @xinrong-meng , @HyukjinKwon , @LuciferYang .
Merged to master, branch-3.4, branch-3.3 and branch-3.2.
@cloud-fan Could you 【review】 this PR, please.
Merging to master/3.4/3.3. Thank you, @cloud-fan for 【review】.
May I get a 【review】, please? @zhengruifeng @HyukjinKwon <br><br>Also cc @grundprinzip @ueshin
Merged to master and branch-3.4, thanks all!
cc @HyukjinKwon @hvanhovell @dongjoon-hyun FYI
@LuciferYang what else are you working on? I am preparing a PR to add most of the remaining functions, it would be a pity if we duplicate our efforts.
@hvanhovell The original plan is `Collection functions`. However, I have encountered some problems in `LambdaFunction` that I haven't solved it yet.  So you can submit new prs freely. If possible, please let me help to 【review】, thanks ~<br><br><br>
@LuciferYang let me take a look at the collections functions.
Merged to master/3.4. 【Thanks】 for doing this!
@cloud-fan @techaddict
Good work, late 【LGTM】
The 【test】s fail with the error:<br><br>```<br>pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:<br>\tstatus = StatusCode.UNAVAILABLE<br>\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:44883: Failed to connect to remote host: Connection refused\"<br>\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:44883: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-02-22T00:04:47.405869565+00:00\"}\"<br>><br>```<br><br>They pass in my local env, though.<br><br>cc @HyukjinKwon @zhengruifeng
let me take a quick look
okay .. finally found out why .. let me create a PR ..
retriggered 【test】s
【LGTM】
merging this
https://github.com/apache/spark/pull/40159 will replace this one.
This is a real case from production: `SELECT ... LIMIT 5000`<br>Before this PR | After this PR<br>-- | --<br><img src=\"https://user-images.githubusercontent.com/5399861/224493524-a0aac742-025f-4e1b-9551-82d07f32bce0.jpg\" width=\"400\" height=\"750\"> | <img src=\"https://user-images.githubusercontent.com/5399861/224493280-09d487db-be57-4a80-8051-1c1ab8bf3109.jpg\" width=\"400\" height=\"750\"><br>
Date | No. of queries 【optimized】 by this patch | No. of total queries<br>-- | -- | --<br>2023/4/5 | 62 | 167608<br>2023/4/4 | 139 | 203393<br>2023/4/3 | 62 | 191147<br>2023/4/2 | 14 | 133519<br>2023/4/1 | 10 | 175728<br><br>
cc @cloud-fan @wangyum
@zml1206 Could you update the PR title to `[SPARK-42525][SQL] Collapse ...`?
the change 【LGTM】 but the PR title is a bit confusing. How is it related to subquery?
> the change 【LGTM】 but the PR title is a bit confusing. How is it related to subquery?<br><br>subquery is one of the cases where the qualifiers are different，it's really confusing, is there anything else I can do to modify it
`... with semantically-same partition/order`
I guess you may need to `Go to “Actions” tab on your forked repository and enable “Build and 【test】” and “Report 【test】 results” workflows`<br><br>https://spark.apache.org/contributing.html
> Did that
Eh, this does not explain the issue at all. Please do so.
I have enabled the workflows on the branch. Is there something else that I need to do?
Sean not sure which issue you were referring to. I updated the why the 【c<font color=blue>hang】es】 are needed section of the pull request to mirror what Zheng had already put in his pull request.
This is about SPARK-41391? it also doesn't contain a 【simple】 description of what you're reporting, just code snippets. I can work it out, but this could be explained in just a few sentences
Please fix the PR description too https://spark.apache.org/contributing.html
Sean  I tried to correct the two things pointed  out by you. Let me know if that works
Looks better. Title should start with `[SPARK-41391]` to link it. Please include the description in the title; there is nothing there now
Not sure how my 【check】ins are causing javadoc genration error
It's the `[[Star]]` in the scaladoc you added. Just don't make it a reference
Is there anything else that I need to do for the fix to be accepted?
@cloud-fan or @HyukjinKwon do you have an opinion?
Not sure why the suggested 【c<font color=blue>hang】es】 made the build fail in the <br>catalyst,hive-thriftserver module  and<br>sql-other 【test】 module.<br>2023-03-01T22:23:36.6700903Z Error instrumenting class:org.apache.spark.sql.execution.streaming.state.SchemaHelper$SchemaV2Reader2023-03-01T22:23:36.8662344Z Error instrumenting class:org.apache.spark.sql.v2.avro.AvroScan<br>2023-03-01T22:23:36.8712474Z Error instrumenting class:org.apache.spark.api.python.DoubleArrayWritable
@cloud-fan  always using unresolvedAlias seems to be causing the sql-other module to fail. Will be reverting to the original fix of creating unresolvedAlias only for \"*\" or distinct.
Any comments. Apparently having all expr as unresolvedAlias is not working.
> Apparently having all expr as unresolvedAlias is not working.<br><br>Can you share the 【test】 failures? Maybe we just need to update the 【test】s with the different alias name.
[7_Run  Build modules sql - other 【test】s.txt](https://github.com/apache/spark/files/10923393/7_Run.Build.modules.sql.-.other.【test】s.txt)<br>
I think the 【test】 is easy to fix. It wants to 【test】 the aggregate function result, but not the generated alias, so we just change the 【test】ing query to add alias explicitly.<br>```<br>val avgDF = intervalData.select(<br>      avg($\"year-month\").as(\"a1\"),<br>      avg($\"year\").as(\"a2\"),<br>      ...<br>```
> I think the 【test】 is easy to fix. It wants to 【test】 the aggregate function result, but not the generated alias, so we just change the 【test】ing query to add alias explicitly.<br>> <br>> ```<br>> val avgDF = intervalData.select(<br>>       avg($\"year-month\").as(\"a1\"),<br>>       avg($\"year\").as(\"a2\"),<br>>       ...<br>> ```<br><br>Couple of questions<br><br>1.  Is it required and documented that we should add  alias with the aggregate functions? If that is not a requirement then fixing the 【test】 case is potentially  covering an issue.<br>2. The Thread leaks reported in the sql-other 【test】s in not just from DataFrameAggregateSuite, but from multiple other suites<br><br>023-03-03T04:05:16.9822203Z 04:05:16.978 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 393.0 failed 1 times; aborting job<br>2023-03-03T04:05:16.9866693Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-30668: use legacy timestamp parser in to_timestamp (154 milliseconds)\u001B[0m\u001B[0m<br>2023-03-03T04:05:17.0464670Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-30752: convert time zones on a daylight saving day (62 milliseconds)\u001B[0m\u001B[0m<br>2023-03-03T04:05:17.1930942Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-30766: date_trunc of old timestamps to hours and days (142 milliseconds)\u001B[0m\u001B[0m<br>2023-03-03T04:05:17.3358608Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-30793: truncate timestamps before the epoch to seconds and minutes (146 milliseconds)\u001B[0m\u001B[0m<br>2023-03-03T04:05:17.3824844Z 04:05:17.382 WARN org.apache.spark.sql.DateFunctionsSuite: <br>2023-03-03T04:05:17.3845065Z <br>2023-03-03T04:05:17.3846873Z ===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.DateFunctionsSuite,
The auto-generated alias name is fr【agile】 and we are trying to 【improve】 it at https://github.com/apache/spark/pull/40126<br><br>Can you give some examples of how the new update 【c<font color=blue>hang】es】 the alias name? If it's not reasonable, we should keep the previous code.
> The auto-generated alias name is fr【agile】 and we are trying to 【improve】 it at #40126<br>> <br>> Can you give some examples of how the new update 【c<font color=blue>hang】es】 the alias name? If it's not reasonable, we should keep the previous code.<br><br>I am attaching a file showing some failures when all the aggregate expressions were made UnresolvedAlias. My la【test】 【check】in where I only make those aggregate expressions that have \"*\" as UnresolvedAlias works. The build went through.So it is essentially the unresolvedstar() that is being produced by the toPrettySQL  for the agg expr with star that the Analyzer is not able to resolve. <br>[sqlOtherTests.txt](https://github.com/apache/spark/files/10972570/sqlOtherTests.txt)<br>
Can anyone tell me how I am getting this single quote in count expression. Attaching the picture. This can potentially cause problems down the lane where tree nodes are compared in the transformDownWithPruning where the two nodes are not same because of this single quote<br><img width=\"1495\" alt=\"Screen Shot 2023-03-15 at 9 34 51 AM\" src=\"https://user-images.githubusercontent.com/13139216/225378952-74ba895b-2c36-407a-ab1c-7ad46b469ae7.png\"><br>
The single quote indicates that the expression is unresolved, I think it doesn't matter here.
Perhaps the following would be better solution. Instead of looking for star any  UnresolvedFunction should have UnresolvedAlias.  Any comments?<br><br>  private[this] def alias(expr: Expression): NamedExpression = expr match {<br>    case expr: NamedExpression => expr<br>    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =><br>      UnresolvedAlias(a, Some(Column.generateAlias))<br>    case expr: Expression =><br>       if (expr.isInstanceOf[UnresolvedFunction]) {<br>        UnresolvedAlias(expr, None)<br>      } else {<br>         Alias(expr, toPrettySQL(expr))()<br>      }<br>  }
> any UnresolvedFunction should have UnresolvedAlias.<br><br>SGTM. Or more aggressively, any expression should have `UnresolvedAlias`, and update failed 【test】s.
Right. This is 【simple】 1 file fix with addition of 【test】 case versus the other one which may involve number of files.
Please see if this fix can be pulled.
cc @gengliangwang, sorry I missed two more 【test】s.
Hello @dongjoon-hyun @holdenk , please help 【review】 this PR, given that there is a lot of user feedback on JIRA about this issue
Overall I like it, but I would want to see a unit 【test】 (does not need to go all the way to exit but something to show that the code would match).
> Overall I like it, but I would want to see a unit 【test】 (does not need to go all the way to exit but something to show that the code would match).<br><br>【Thanks】 for your 【review】.<br><br>Added unit 【test】 for `LoggingPodStatusWatcher` get driver exit code after receive event.
cc @WeichenXu123
Reminder: backport to 3.4 branch.
sure, merged into master/branch-3.4
Merging this to master/3.4. 【Thanks】 for doing this!
【Thanks】 @hvanhovell
Can we minimize diff's to this file ? A large fraction is whitespace 【c<font color=blue>hang】es】 and due to the renames ... will take a look at the 【c<font color=blue>hang】es】 as well.<br><br>Also given this is an 【optimization】 change - include benchmark to quantify the 【impact】 ?
> Also given this is an 【optimization】 change - include benchmark to quantify the 【impact】 ?<br><br>I did benchmarking live in a cluster. Profiles before show ~1% of scheduler time in PercentileHeap operations. Profiles after do not have PercentileHeap operations at all.
> Can we minimize diff's to this file ? A large fraction is whitespace 【c<font color=blue>hang】es】 and due to the renames ... will take a look at the 【c<font color=blue>hang】es】 as well.<br><br>Can you treat is a new implementation? There is only 15 lines (L55-L70) that matter on the new implementation - the code inside `insert`. Outside of `insert` there is nothing meaty or interesting.
> I did benchmarking live in a cluster. Profiles before show ~1% of scheduler time in PercentileHeap operations. Profiles after do not have PercentileHeap operations at all.<br><br><br>Can you add a benchmark in the PR ? With results for best and after in description or as comment ? 【Thanks】 !<br>For example, take a look at `core/src/【test】/scala/org/apache/spark/MapStatusesSerDeserBenchmark.scala`
I ran this benchmark offline:<br><br>```<br>  【test】(\"benchmark\") {<br>    val input: Seq[Int] = 0 until 1000<br>    val numRuns = 1000<br><br>    def kernel(): Long = {<br>      val shuffled = Random.shuffle(input).toArray<br>      val start = System.nanoTime()<br>      val h = new PercentileHeap(0.95)<br>      shuffled.foreach { x =><br>        h.insert(x)<br>        for (_ <- 0 until input.length) h.percentile<br>      }<br>      System.nanoTime() - start<br>    }<br>    for (_ <- 0 until numRuns) kernel()  // warmup<br><br>    var elapsed: Long = 0<br>    for (_ <- 0 until numRuns) elapsed += kernel()<br>    val perOp = elapsed / (numRuns * input.length)<br>    println(s\"$perOp ns per op on heaps of size ${input.length}\")<br>  }<br>```<br><br>Results:<br>```<br>    BEFORE 3886 ns per op on heaps of size 1000<br>    AFTER  1703 ns per op on heaps of size 1000 (with scala PriorityQueue) <br>    AFTER    36 ns per op on heaps of size 1000 (with java PriorityQueue)<br>```<br><br>(yes 100x 【improvement】 is not a typo)<br><br>I left this 【test】 in the PR instead of a full blown benchmark.
I updated the implementation and the description. TLDR I use a comparator-less java `PriorityQueue` now for a total of 100x 【speed】up over the original implementation.<br><br>@mridulm good call on the benchmark, in my internal 【test】s I had a handrolled heap implementation that was even 【fast】er than the java one. If not for the benchmark I wouldn't have noticed that Scala's priority queue is so bad vs Java's.
The failed HealthTrackerIntegrationSuite is definitely unrelated, I'm merging it to master, thanks!
qq,<br><br>> this is a breaking change to this experimental API.<br><br>What's breaking? would be good to keep the PR desc template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)
> qq,<br>> <br>> > this is a breaking change to this experimental API.<br>> <br>> What's breaking? would be good to keep the PR desc template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)<br><br>@HyukjinKwon I have changed the description to include the breaking change and some more details.
@HyukjinKwon may I request for a 【review】 on this PR?
CC @cloud-fan @sunchao @zhengruifeng
cc @ueshin
【LGTM】 Nice catch!<br><br>
@HyukjinKwon can you 【review】 this please / find a 【review】er for me?
@cloud-fan can this be merged?
@amaliujia what existing PR cover this?
@hvanhovell this one https://github.com/apache/spark/pull/40057.  A few agg API that are built on top of that are not in Dataset.
@amaliujia those 【test】s are not exercising the new code paths. Since the change is small I am fine with merging it though.
Merging this.
@srielau Could you take a look at the PR one more time, please.
@cloud-fan @srielau Could you take a look at the PR, please.
I have lost track of the externally described rules. Do you have an updated 【summary】?<br>Especially since we recently discussed about what to do with whitespace<br>Also, do we plan to leave this OFF by default? If we don't make this default it will never get adoption because most folk just won't care.
> Do you have an updated 【summary】?<br><br>Yes, it is very easy to 【check】 - just look at the last commit.<br><br>> Especially since we recently discussed about what to do with whitespace<br><br>? @srielau Please, look at the PR 【c<font color=blue>hang】es】 first of all.<br><br>> Also, do we plan to leave this OFF by default?<br><br>Yes, Serge could you look at the PR's description and 【c<font color=blue>hang】es】 before asking the questions.
> > Do you have an updated 【summary】?<br>> <br>> Yes, it is very easy to 【check】 - just look at the last commit.<br>> <br>> > Especially since we recently discussed about what to do with whitespace<br>> <br>> ? @srielau Please, look at the PR 【c<font color=blue>hang】es】 first of all.<br>> <br>> > Also, do we plan to leave this OFF by default?<br>> <br>> Yes, Serge could you look at the PR's description and 【c<font color=blue>hang】es】 before asking the questions.<br>You sum up the root cause for a lot of our QA troubles right there....<br>Please assure that the description of the PR is accurate and up to date (or have a pointer to a google-doc that does the same).<br>We cannot reciew code against itself. We have to 【review】 it against a common understanding on what it is meant to do.<br><br><br>
Merging to master. Thank you, @srielau and @cloud-fan for 【review】.
cc @xinrong-meng , @HyukjinKwon
Thank you, @HyukjinKwon . Merged to master/3.4.
@dongjoon-hyun @holdenk Can you please 【review】 this PR?
Gentle ping @dongjoon-hyun @holdenk @srowen
Gentle ping @holdenk
Gentle ping @holdenk
Hi @dongjoon-hyun <br>The change to 【clean】 up the upload directory is not 【specific】 to HDFS. The reason we should do 【clean】up is because if the spark job is creating new directories/files, it should 【clean】 them up too just like it's being done in YARN and also for other files like shuffle spill. <br>Also, can you please explain why the approach seems to be incomplete? How is unable to prevent leftover from upload directory?
@shrprasa . <br><br>1. It seems that you have an assumption that Shutdown hook is magically 【reliable】. However, shutdown hook has a well-known limitation where JVM can be destroyed abruptly and K8s Pod can be deleted also without giving the inside processes enough time to handle business logic.<br><br>2. As I mentioned in the above, public cloud storage systems have a better and complete TTL-based solution for that issue. In that context, this PR is only trying to mitigate HDFS issue, https://issues.apache.org/jira/browse/HDFS-6382, partially.<br><br>> The change to 【clean】 up the upload directory is not 【specific】 to HDFS. The reason we should do 【clean】up is because if the spark job is creating new directories/files, it should 【clean】 them up too just like it's being done in YARN and also for other files like shuffle spill.<br>Also, can you please explain why the approach seems to be incomplete? How is unable to prevent leftover from upload directory?
@dongjoon-hyun  【Thanks】 for the clarification. But the un【reliability】 for shutdown hook is common for all other shutdown tasks also. This doesn't mean we haven't impletened them. So, why this should stop us from adding the 【clean】 up logic. <br>The  TTL-based solution is 【specific】 to the filesystem. It's nice to have but I don't understand why we should rely on external solution to perform 【clean】up on behalf of spark job and why not the job itself does the 【clean】up? What's the downside of doing this?
Gentle Ping @dongjoon-hyun @holdenk
Gentle Ping @dongjoon-hyun @holdenk
So as I think about it, what about if we did a 【clean】up as a script for the driver pod on terminate to address some of @dongjoon-hyun's concern?<br><br>The TTL based approach feels fundamentally \"better\" though. Certainly it is filesystem 【specific】, but I think (besides) HDFS all of the major filesystems we would use support TTL yes? Perhaps we should just document this?
@holdenk @dongjoon-hyun TTL based clearnce has limitations as pointed out in this comment<br>https://github.com/apache/spark/pull/40363#issuecomment-1467439787
@dongjoon-hyun @holdenk Can we have some conclusion on this issue?
I'm a soft +1, I think it's a useful incremental 【feature】 (even if imperfect) but if @dongjoon-hyun would rather not then I defer to his judgement here (I mostly run with object storage where TTLs are fine).
@dongjoon-hyun can you please comment on whether we should be doing this change or not.
For the 【review】er I have manually 【test】ed this with scala-2.13.
cc @LuciferYang
Checked the new 【test】 with Scala 2.13, all passed
Could you 【review】 this, @viirya ?
Thank you so much! Merged to master/3.4!
Could you 【review】 this too, @viirya ? I 【<font color=blue>fix】ed】 some K8s 【documentation】 issues while 【review】ing and 【test】ing 3.4.0 RC1 and updated YuniKorn 【documentation】 with the la【test】 information.
Looks good to me.
Thank you so much, @viirya . Merged to master/3.4.
cc @grundprinzip
cc @dongjoon-hyun @cloud-fan
@dongjoon-hyun I have addressed the comment, could you 【review】 again please? Thank you.<br><br>Also, do you know whom I can ping on this PR to approve DB2 SQL semantics?
@dongjoon-hyun Is there anything else left on this PR to merge?
thanks, merging to master/3.4 (bug fix)!
it has conflicts with 3.4, @sadikovi can you open a backport PR? thanks!
Yes, I will open a separate PR.
PR for 3.4: https://github.com/apache/spark/pull/40155.
cc @HyukjinKwon @xinrong-meng
Shall we add an example to **Does this PR introduce any user-facing change?** in the PR description? Like<br><br>```py<br>>>> df3.show()<br>+---+----+------+----+<br>|age|name|height|name|<br>+---+----+------+----+<br>| 16| Bob|    85| Bob|<br>| 14| Tom|    80| Tom|<br>+---+----+------+----+<br><br>### BEFORE<br>>>> df3.drop(\"name\ \"age\").columns<br>Traceback (most recent call last):<br>...<br>pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `name` is ambiguous, could be: [`name`, `name`].<br><br>### AFTER<br>>>> df3.drop(\"name\ \"age\").columns<br>['height']<br>```
which commit caused the regression?
@cloud-fan it was introduced in https://github.com/apache/spark/commit/69f402ad8085bc50837128645b61168e7d5244b9
In the JVM side, the logics of `drop(column: Column)` and `drop(columnName: String)` are different, we can not simply always convert a column name to column via `col()` method.<br><br>If there are multi columns with same name, you can use `drop(column_name)` to drop all of them;  or use `drop(df1.name)` to drop column from 【specific】 dataframe; But if you invoke `drop(col(name))`, it will fail due to ambiguous issue
Good catch @zhengruifeng 🙌
thank you all for the reivews! merged to master/branch-3.4
cc @hvanhovell
Is the fix feasible?<br><br>
【Thanks】 @zhenlineo friendly ping @HyukjinKwon @hvanhovell
A Ga task failed, let me re-trigger it
【Thanks】 @HyukjinKwon @hvanhovell @zhenlineo
cc @peter-toth
Hmm, the failed 【test】 seems a related one.
Oh is it still failing?
I think it was failing due to la【test】 commit.
The failed 【test】 【check】s invalid ordering and I've updated it.
Merged to master/3.4.<br>Thank you, @cloud-fan , @peter-toth , @viirya .
cc @cloud-fan @tgravescs
@ulysses-you to help other 【review】ers understand it, can you add more explaination in the PR description about how `boundExpr` is used in the window operator? To convince people that it's safe to skip overflow 【check】 for it.
@cloud-fan addressed, hope it is helpful
cc @cloud-fan Could you please take a look when you have a moment? 【Thanks】!
Thank you, @huaxingao and @cloud-fan .
【Thanks】 @cloud-fan @dongjoon-hyun
Looks like this also fixes SPARK-41991 (a bug that exists also in 3.3.x, but takes a little more work to reproduce than in 3.4.0).<br><br>Should the title should be `[SPARK-42286][SPARK-41991][SQL] ...etc...`?
@RunyaoChen Could you add `[3.3]` to PR title.
cc @rednaxelafx FYI
【Thanks】, merging to 3.3
cc @huaxingao too.
@SandishKumarHN, @gengliangwang : Please 【review】 when you get a chance.
+1, the new schema is more reasonable.
ping @cloud-fan cc @wangyum
@cloud-fan @dongjoon-hyun Thank you very much!
Some other things to do, will continue tomorrow
@LuciferYang thanks for the PR! Which datatypes are we still missing? I think we still some collection support?
I refer <br><br>https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala#L64-L102<br><br>the missing is `case a: Array[_]`
A couple of things:<br>- For Array I guess we can just make a nested call to lit for each element. No need to get CatalystTypeConverters involved.<br>- Bonus points if you 【check】 if all elements are the same.<br>- We do have to 【check】 what `CatalystTypeConverters` currently supports though. I think Map/Seq/Product support is also in there.<br>- You may want to put this in a separate file.
Oh and if it becomes too large I am fine with merging this first, and doing array in a follow-up.<br>
hmm... If I understand correctly, the current Literal does not support any collection type? Do we need to add some message types to support them?<br><br>https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/connector/connect/common/src/main/protobuf/spark/connect/expressions.proto#L149-L175
lol, no it does not. Let's just implement what we support, and do the rest in a different PR.
OK
> Oh and if it becomes too large I am fine with merging this first, and doing array in a follow-up.<br><br>I hope we can merge this pr first if no other need to change. In addition, I need to go to bed as soon as possible. It's 1:00 in my time zone 【:)】<br><br>
go to sleep!
@hvanhovell Is there anything else can help Scala Client? @panbingkun told me that he also wanted to take some work related to connect.
@LuciferYang @panbingkun that would be 【great】! I will create an 【epic】, with a bunch of todo's.
Is there anything else need change this pr?
@LuciferYang can you update your PR?
@LuciferYang @panbingkun I created an 【epic】 with a bunch of things you can pick up: https://issues.apache.org/jira/browse/SPARK-42554
【LGTM】 but please rebase this PR to solve conflict.
Merging to master/3.4. 【Thanks】!
cc @sunchao @AngersZhuuuu since you've worked on somewhat-related 【c<font color=blue>hang】es】 in #34690, #32887, etc.<br>cc @srowen @squito since you were involved in #24057 for the Java 9+ 【c<font color=blue>hang】es】<br>cc @HyukjinKwon @dongjoon-hyun for any general interest
Also, cc @mridulm , @cloud-fan , @rednaxelafx, @zsxwing, @kiszk , @maropu .
It makes sense to use the builtin classloader when using builtin Hive. To clarify: we still have the class loading issue if people specifies a certain hive version (not builtin), right?
Great question @cloud-fan , and actually no, we don't. For all of the other 【value】s of `spark.sql.hive.metastore.jars` besides 'builtin', the user JARs are not included at all ([refer to this section of HiveUtils](https://github.com/apache/spark/blob/87e3d5625e76bb734b8dd753bfb25002822c8585/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L451-L513)). In all of those cases, the JAR list is constructed purely from the dependencies for the specified Hive version. Whether that behavior is correct is another question -- @shardulm94 informed me that user JARs are required to support custom serdes inside of the Hive client -- but in any case, 'builtin' is the only mode that is susceptible to this issue.
lgtm if all 【test】s pass
Merged to master/branch-3.4. 【Thanks】 @xkrogen !
【Thanks】 @sunchao and @cloud-fan !
Seems like the 【test】s didn't pass .. I am reverting this as it causes a lot of 【test】 failures. e.g.)<br>- https://github.com/apache/spark/runs/11644563592<br>- https://github.com/apache/spark/actions/runs/4288729207/jobs/7471072831<br>
Thank you for recovering `master` branch by reverting, @HyukjinKwon ! The reverting unblocks other PRs.
Shall we revert it from 3.4 as well?
Yes, it was reverted here, 26009d47c1.
Hmm interesting. Somehow the 【test】s were shown all passing for me when I merged this. Sorry for the trouble.
I will take a look at the 【test】 failures, thanks @HyukjinKwon for addressing the revert!
If you folks don't mind, shall we consider this for Apache Spark 3.5 only? `ClassLoader` issue has been tricky always in the 【community】. We need enough time to stabilize in `master` branch to give a chance to be verified in several cases by different organizations.
+1. I agree with @dongjoon-hyun .
I agree as well. Posted a new PR at #40224.
Merging
The 【test】 failure seems unrelated. @HyukjinKwon can we include this PR in spark 3.4 as well?
Combined in https://github.com/apache/spark/pull/40151
@xkrogen yes, Spark Connect decouples client and server. That we synchronize jars and REPL generated classes does not mean we are re-introducing the coupling between client and server. All it means is that we will be using the server (driver) to orchistrate the execution of user defined code. This code could be executed within the same engine, however we can also use separate processes or VMs to execute this code. I do feel that adding this kind of 【functionality】 to connect keeps thing 【simple】 from the client's POV; it does not make a lot sense to me to do the synchronisation through a different service, because we need to go through the driver anyway.<br><br>I do want to call out that this mechanism is just not here for JARs and REPL generated classed, but that there are also quite a few other use cases that we need this mechanism for, e.g.: reading client local files, synchronizing other kinds of dependencies (for example python wheels), uploading models, ...<br><br>Finally it is early days for connect. We currently want to make it easy to folks to try and to move to connect. We need a REPL experience that is similar to the current experience for that. We want to do the 【simple】 thing first, and that is reuse the current UDF execution code (with some classpath isolation). I am happy to discuss the steps after that, if you are up to that please reach out to me (herman@databricks.com).
> This code could be executed within the same engine, however we can also use separate processes or VMs to execute this code.<br><br>Good point; nothing about the protocol itself precludes us from using separate UDF processes in the future.<br><br>> Finally it is early days for connect. We currently want to make it easy to folks to try and to move to connect ...<br><br>Generally agreed, but APIs and contracts can be challenging to change once adopted, so it's important to get it right on the first try, at least to the extent that it doesn't force us into bad decisions or tech debt later. That's why I'm interested in discussing a longer-term roadmap about this kind of support.<br><br>But I think you have convinced me that this protocol is generally useful!<br><br>cc @mridulm in case you're interested in taking a look
@xkrogen Thank you for all of the feedback! Do you mind doing a 【review】 of the current state? We're looking to merge and were wondering if there are any other points to address.
I am merging this. Let's address further concerns in a follow-up.
thanks for incorporating the feedback!
Merged in https://github.com/apache/spark/pull/40151
thanks, merging to master/3.4 (it's weird if some generate functions can be used as TVF and some can not)
late 【LGTM】!
Do you have any concerns about this, @Yikun ?
/【LGTM】
Thank you, @Yikun , @HyukjinKwon , @william-wang .<br>Merged to master/3.4.
cc @rithwik-db @xinrong-meng
cc @WeichenXu123 I think we should 【test】 other python versions too before the actual release.
【LGTM】<br><br><br>From https://spark.apache.org/docs/la【test】/ it says `Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+ and R 3.5+` so it makes sense to 【test】 against Python 3.7
yeah, I think we should set a scheduled job ... cc @dongjoon-hyun @Yikun too FYI
According to the Affected Version field, only Apache Spark 3.4 is affected?
Yup, this is actually rather a followup of https://github.com/apache/spark/commit/d5b08f8d99b14efa14ad706f9118762a2e570b34
Merged to master/3.4.
Although I had to manually skip a couple of 【test】s (due to my env issue), I 【test】ed Python 3.7, 3.8, 3.9 (by CI), 3.10 and the 【test】s pass.
Thank you so much, @HyukjinKwon !<br><br>May I ask which `requirements.txt` file is used there?
I used `pip install -r dev/requirements.txt`.
@cloud-fan can we have this in branch-3.4 ? since it prevent potential bug and it's friendly for developers.
@cloud-fan I opened the backport for 3.4 but there is no JDBC query builder commit in 3.4 so one of the 【test】s was removed.
thanks, merging to 3.4!
Large change because all the golden files were updated.
cc @cloud-fan  Could you help to 【review】 this PR? 【Thanks】
how do other operators support CSE in whole-stage-codegen? also cc @viirya
Hi, @cloud-fan <br>I have updated the PR description and code to support more CSE cases following the three steps.
Hi, @cloud-fan @viirya <br>I updated the PR and support subexpression elimination in FilterExec and JoinExec.<br>We can support some other operator subexpression elimination in two steps.<br>Do you have any good advice?
Use https://github.com/apache/spark/pull/40713 instead of this PR, close it.
Merging. 【Thanks】!
do we still need `message SQL` in relations.proto?
Yes, during eager execution the server generates the SQL relation if it's not a command.
merged into master/branch-3.4
I made a PR.<br>- https://github.com/apache/spark/pull/40246
【Thanks】! Merged to master.
【Thanks】! Merging to master.
@HeartSaVioR - please take a look. Thx
@HeartSaVioR - looks like the 【test】s finished fine. Not sure why the Actions result is not updated here
I am merging this one. Can you do persist in a follow-up?<br>
【LGTM】<br><br>What is the default source BTW?
> What is the default source BTW?<br><br>If format is not set, the 【value】 from SQL conf 'spark.sql.sources.default' will be used.
Merging, thanks for doing this!
cc @hvanhovell @amaliujia @LuciferYang I am a bit tired of losing `// scalastyle:ignore funsuite` when my imports get auto formatted. Let's move to this `ConnectFunSuite`
cc @vicennial too!
Merging. FYI - we are planning a refactor of Catalyst soon (post 3.4) and then we will integrate this with Spark's exception and error framework.
> Merging. FYI - we are planning a refactor of Catalyst soon (post 3.4) and then we will integrate this with Spark's exception and error framework.<br><br>Thank you for sharing the future plan, @hvanhovell !
late 【LGTM】, thanks!
Can someone please help in creating issue for this pull request ? I do not have a ASF Jira account.
[private@spark.apache.org](mailto:private@spark.apache.org)
@maropu @HyukjinKwon
re【test】 this please
Gentle ping!! @gengliangwang  @cloud-fan @viirya @Ngone51 @maropu @HyukjinKwon.
you must enable github action on yours repo https://github.com/apache/spark/pull/40171/【check】s?【check】_run_id=11594767505
Preferring https://github.com/apache/spark/pull/40173 over this PR.
@cloud-fan can you take a look?
【Thanks】 for doing this!
Thank you for doing this too!
cc @cloud-fan @ulysses-you
@srowen @dongjoon-hyun @viirya
And CC @xinrong-meng This is for updating 【documentation】 for spark 3.4 release.
To be clear, the code change itself looks okay, @bjornjorgensen .
Lastly, are you claiming a followup across `spark-website` and `spark` repositories? To me, `[FOLLOWUP]` doesn't make sense at all in that case, @bjornjorgensen .
Also, cc @HyukjinKwon .
Could you 【review】 this editorial patch, @HyukjinKwon and @viirya ?
Thank you! Merged to master/3.4.
Hi, @viirya . Could you 【review】 this PR too?
Let me close this and fix the branch first.
@wangyum
cc @cloud-fan , thanks
Now, it passed. <br>![Screenshot 2023-02-26 at 7 25 33 PM](https://user-images.githubusercontent.com/9700541/221465853-833cb047-d751-43f1-a341-7a6b01f5ce21.png)<br>
Thank you so much, @hvanhovell . Sorry for the troubles.
Looks good.
Thank you, @viirya . Sorry for missing at the first PR.
@amaliujia can you please update the 【compatibility】 【test】 for these?
@hvanhovell Compatibility 【test】s updated
@hvanhovell I should make the Compatibility list correct now.
Merging<br>
Late 【LGTM】!
If possible, this change should be ideally incorporated to the doc of Spark 3.4.0. It'd be nice if we can 【review】 and merge before 3.4.0 RC passed.
cc. @zsxwing @viirya @HyukjinKwon
cc @allanf-db FYI as we would put this Python example first too in your PR
【Thanks】! Merging to master/3.4.
cc @sunchao  @cloud-fan @huaxingao
cc @Yikun FYI
This pr often conflicts....<br><br>
cc @hvanhovell too FYI
@LuciferYang can we close this one in favor of https://github.com/apache/spark/pull/40213?
> @LuciferYang can we close this one in favor of #40213?<br><br>OK, let me close this one and focus on https://github.com/apache/spark/pull/40213<br><br>
I agree with this change. This should be ok for tracking duration of speculative tasks. cc @mridulm
This wouldn't change any user-facing results right? this is just internal? seems OK if so. If it changed, for example, the result of PERCENTILE somewhere, that could be an issue
My main concern is for tasksets with very low number of tasks, where the change can result in nontrivial difference.<br>Given the 【performance】 【impact】 is minimal overall, I am not convinced whether we want to make this change.<br><br>Does this still show up in the flamegraphs based on the earlier experiments you did @alkis ?
The pyspark failure is totally unrelated. I'm merging it to master, thanks!
@cloud-fan, @jiangxb1987, could you 【review】 this?
+1, 【LGTM】. Merging to master/3.4.<br>Thank you, @jiang13021.
@jiang13021 Congratulations with the first 【contribution】 to Apache Spark!
@jiang13021 The 【c<font color=blue>hang】es】 causes some conflicts in branch-3.3. Could you open a separate PR with a backport to Spark 3.3.
> @jiang13021 The 【c<font color=blue>hang】es】 causes some conflicts in branch-3.3. Could you open a separate PR with a backport to Spark 3.3.<br>【Thanks】 for your 【review】. Do you mean launch a new PR to branch-3.3? Here it is: https://github.com/apache/spark/pull/40253
> Do you mean launch a new PR to branch-3.3? Here it is: https://github.com/apache/spark/pull/40253<br><br>Yep. Thank you.
> > <br>> <br>> <br><br><br><br><br>> You are suggesting a breaking change here. The existing behavior is correct and consistent with old Spark versions. I guess we need to revise the 【test】 comment instead, @huangxiaopingRD .<br><br>Sorry, I don't understand what you say we should do next. I want to make this change just because I think the default format of Spark's data source is parquet, and I think the default fileformat of \"`create table`\" should be consistent with the default 【value】 (parquet) of \"spark.sql.sources.default\".
In my 【test】, the hive table format depends on the conf `hive.default.fileformat` when set `spark.sql.legacy.createHiveTableByDefault` is true . And if `spark.sql.legacy.createHiveTableByDefault` is false, it hive table format depends on the conf `spark.sql.sources.default` .<br><br>I think it's reasonable .<br><br>So maybe we should only fix the describtion of the https://github.com/apache/spark/blob/master/docs/sql-ref-syntax-ddl-create-table-datasource.md?plain=1#L121 ?
> In my 【test】, the hive table format depends on the conf `hive.default.fileformat` when set `spark.sql.legacy.createHiveTableByDefault` is true . And if `spark.sql.legacy.createHiveTableByDefault` is false, it hive table format depends on the conf `spark.sql.sources.default` .<br>> <br>> I think it's reasonable .<br>> <br>> So maybe we should only fix the describtion of the https://github.com/apache/spark/blob/master/docs/sql-ref-syntax-ddl-create-table-datasource.md?plain=1#L121 ?<br><br>I agree with you. I will launch another PR to modify the description of the document. 【Thanks】 @zzzzming95 <br><br>what do you think about this? @dongjoon-hyun
cc @srielau
+1, 【LGTM】. Merging to master.<br>Thank you, @allisonwang-db.
cc @WeichenXu123 , @HyukjinKwon
@jzhuge let's fix the PR description. This technically does not revert https://github.com/apache/spark/pull/38699 but fixes a mistake that removed the code (that author and 【review】ers thought it's a duplicate).
nit but mind fixing up the PR description? want to merge it now :-)
Updated the PR description
Merged to master, branch-3.4, branch-3.3, and branch-3.2.
@hvanhovell done.
Could you 【check】 the failures or re-trigger the CI for this PR, @aokolnychyi ?
Test failures don't seem to be related. I see similar failures on other PRs. I think the root cause has been recently reverted in master.
You need to re-trigger CI, @aokolnychyi , because we cannot re-trigger for you.
Merged to master/3.4 for Apache Spark 3.4.0.<br>Thank you, @aokolnychyi and all!
Thank you, @dongjoon-hyun @huaxingao @cloud-fan!
ping @cloud-fan
cc @huaxingao @cloud-fan @dongjoon-hyun @sunchao @viirya
@dongjoon-hyun, thanks, I overlooked. I corrected the JIRA description and added 3.3.3 as well but feel free to adjust the list of versions as needed.
Thank you!
Could you rebase to the `master` branch?
This is a good catch! I think the fix can be 【simple】r: we encode the raw type information in the root `StructField`, so we don't need to decode it recursively. We can restore the raw type at the very beginning:<br>```<br>def resolveOutputColumns(<br>    ...,<br>    expected: Seq[Attribute],<br>    ...) {<br>    val actualExpectedCols = expected.map { attr =><br>        attr.withDataType(restore(attr.metadata).getOrElse(attr.dataType))<br>    }<br>}<br>```
@cloud-fan, are you thinking of passing raw types all the way? I actually considered that but my worry was we will have to modify `【check】Field` casting logic as it seems we can't use the raw type there.<br><br>```<br>  private def 【check】Field(<br>      tableAttr: Attribute,<br>      queryExpr: NamedExpression,<br>      byName: Boolean,<br>      conf: SQLConf,<br>      addError: String => Unit,<br>      colPath: Seq[String]): Option[NamedExpression] = {<br><br>    val storeAssignmentPolicy = conf.storeAssignmentPolicy<br>    lazy val outputField = if (tableAttr.dataType.sameType(queryExpr.dataType) &&<br>      tableAttr.name == queryExpr.name &&<br>      tableAttr.metadata == queryExpr.metadata) {<br>      Some(queryExpr)<br>    } else {<br>      val casted = storeAssignmentPolicy match {<br>        case StoreAssignmentPolicy.ANSI =><br>          val cast = Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone),<br>            ansiEnabled = true)<br>          cast.setTagValue(Cast.BY_TABLE_INSERTION, ())<br>          【check】CastOverflowInTableInsert(cast, colPath.quoted)<br>        case StoreAssignmentPolicy.LEGACY =><br>          Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone),<br>            ansiEnabled = false)<br>        case _ =><br>          Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))<br>      }<br>      val exprWithStrLenCheck = if (conf.charVarcharAsString) {<br>        casted<br>      } else {<br>        CharVarcharUtils.stringLengthCheck(casted, tableAttr)<br>      }<br>      // Renaming is needed for handling the following cases like<br>      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2<br>      // 2) Target tables have column metadata<br>      Some(Alias(exprWithStrLenCheck, tableAttr.name)(explicitMetadata = Some(tableAttr.metadata)))<br>    }<br><br>    storeAssignmentPolicy match {<br>      case StoreAssignmentPolicy.LEGACY =><br>        outputField<br><br>      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =><br>        // run the type 【check】 first to ensure type errors are present<br>        val canWrite = DataType.canWrite(<br>          queryExpr.dataType, tableAttr.dataType, byName, conf.resolver, colPath.quoted,<br>          storeAssignmentPolicy, addError)<br>        if (queryExpr.nullable && !tableAttr.nullable) {<br>          addError(s\"Cannot write nullable 【value】s to non-null column '${colPath.quoted}'\")<br>          None<br><br>        } else if (!canWrite) {<br>          None<br><br>        } else {<br>          outputField<br>        }<br>    }<br>  }<br>```
@cloud-fan, passing raw types from the start would be simper and more 【efficient】 but we will have to modify `【check】Field` to work with raw types. Let me know if I got you correctly.
Yea, we will have to modify `【check】Field`. We can replace char/varchar type with string type in `【check】Field` so that most code can be unchanged, and only use the raw type for length 【check】 related logic.
Sounds good, I'll make the change today.
Thank you, @aokolnychyi , @cloud-fan , @huaxingao , @viirya !
【Thanks】 for 【review】ing, @dongjoon-hyun @cloud-fan @huaxingao @viirya!
It seems that `lint-scala` complain again. Could you apply `./dev/lint-scala`?
Scala/Java linter passed. Merged to master/3.4. Thank you, @hvanhovell and all!
cc. @HyukjinKwon @viirya Sorry to bug you but can I get another quick 【review】 for this? My bad.
cc @gengliangwang sorry for many followups ..  I don't know why I missed this ... this is the last followup.
@HyukjinKwon I also verified MathFunctionsSuite can pass with ANSI Enabled
@HyukjinKwon @grundprinzip @amaliujia @hvanhovell @ueshin
thanks for the 【review】s!
@HyukjinKwon The change is pretty 【simple】. Did I miss anything?<br><br>Should we add this to release notes? Since this PR 【c<font color=blue>hang】es】 the behavior described by SPARK-28843.
> Did you 【check】 these envs are also set correctly ?<br><br>@WeichenXu123 It is ok to set default OMP_NUM_THREADS to driver cores. \"Task\" only applies to executors.
Test `KafkaMicroBatchV1SourceWithAdminSuite` failed. Not sure it is related.
Will merge this one in few days if there aren't any objection.
@LuciferYang thanks for the hard work! Can we merge this one instead of https://github.com/apache/spark/pull/40191.
Looks 【awesome】 overall!
How would we deal with `CompatibilitySuite`?
Ah I see now that we still need to update the excluding rules.
> How would we deal with `CompatibilitySuite`<br>> Ah I see now that we still need to update the excluding rules.<br><br>Yes, just 【check】 in a different way 【:)】<br><br>
<img width=\"948\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/222325279-7ef9ec94-3e79-44c3-864c-19b2b0737c4b.png\"><br><br>The new change of `org.apache.spark.sql.Dataset#plan` in SPARK-42631 is 【check】ed as incompatible.
Alirght merging.
【Thanks】 @hvanhovell @amaliujia @zhenlineo
Looks fine for master
GA passed
【Thanks】 @srowen
This should be merged after #39931.
cc. @zsxwing @rangadi @jerrypeng @anishshri-db @chaoqin-li1123 <br>cc-ing folks who 【review】ed the code change PR. This PR is a doc change to show up what is being unblocked, like we did in https://github.com/apache/spark/pull/40188 for fixing broken late record filtering.
cc. @viirya as well who may be interested with new 【feature】 in SS.
【Thanks】 for 【review】ing! Merging to master.
Let us mention all the breaking 【c<font color=blue>hang】es】 and deprecation in both release notes and migration guides
hmm.. <br>`F05.info()`<br><br>```<br>TypeError                                 Traceback (most recent call last)<br>Cell In[12], line 1<br>----> 1 F05.info()<br><br>File /opt/spark/python/pyspark/pandas/frame.py:12167, in DataFrame.info(self, verbose, buf, max_cols, null_counts)<br>  12163     count_func = self.count<br>  12164     self.count = (  # type: ignore[assignment]<br>  12165         lambda: count_func()._to_pandas()  # type: ignore[assignment, misc, union-attr]<br>  12166     )<br>> 12167     return pd.DataFrame.info(<br>  12168         self,  # type: ignore[arg-type]<br>  12169         verbose=verbose,<br>  12170         buf=buf,<br>  12171         max_cols=max_cols,<br>  12172         memory_usage=False,<br>  12173         null_counts=null_counts,<br>  12174     )<br>  12175 finally:<br>  12176     del self._data<br><br>TypeError: DataFrame.info() got an unexpected keyword argument 'null_counts'<br><br>```<br>Hope we can get a better description as error messages here.
https://github.com/apache/spark/pull/40913 can be a fix for this.
@panbingkun can you update your PR?
@panbingkun can you update the CompatibilitySuite?
> @panbingkun can you update the CompatibilitySuite?<br><br>Done
cc @hvanhovell @HyukjinKwon
Some other things to do, will update later
@jelmerk could you please raise a bug [here](https://issues.apache.org/jira/projects/SPARK/【summary】) and use this bug id in the PR for e.g. you can refer the PR https://github.com/apache/spark/pull/40202
When would the field 【value】 have a substitution string in it - does this actually happen in practice?
> When would the field 【value】 have a substitution string in it - does this actually happen in practice?<br><br>I mention one such scenario in my description. Starting from the image on<br><br>Try and run this in a spark 3.4.x spark-shell or 12.1 databricks runtime<br><br>```<br>import org.apache.spark.sql.types._<br><br>val schema = new StructType().add(\"properties\ new StructType())<br><br>val df = Seq(\"\"\"{\"properties\":\"${foo}\"}\"\"\").toDF(\"【value】\").as[String]<br><br>spark.read.schema(schema).json(df).count()<br>```
Sorry I guess I mean I am not clear how this class, which reads error messages, is coming into play here. This is my ignorance. Seems like it wouldn't be related to just parsing user JSON, but the change fixes it?
> Sorry I guess I mean I am not clear how this class, which reads error messages, is coming into play here. This is my ignorance. Seems like it wouldn't be related to just parsing user JSON, but the change fixes it?<br><br>The example defines `properties` in the schema as a struct but in the json it is a string with the 【value】 `\"${foo}\"`<br>An exception is raised during parsing and the message is resolved by that class <br>
Ah, I get it now. I agree with this change for sure.
(Can you just file a JIRA and link it per https://spark.apache.org/contributing.html ?)
> (Can you just file a JIRA and link it per https://spark.apache.org/contributing.html ?)<br><br>Already did that a few mins ago and added it on top of the PR description
Edit the title to link it. See other PRs. Minor thing but helps
> Edit the title to link it. See other PRs. Minor thing but helps<br><br>done
@dongjoon-hyun I'd like a second opinion - should we merge to 3.4 or 3.3? Reasonable fix I think and I'm a little concerned there could be some weird way this would expand some info it shouldn't
For branch-3.4, I'm +1 for backporting.<br>Does this affect branch-3.3 too, @srowen ?
Since [SPARK-40530](https://issues.apache.org/jira/browse/SPARK-40530) added this, `branch-3.3` seems to be not affected.
Also, cc @cloud-fan , @MaxGekk , @viirya from SPARK-40530.
Oh yeah, won't affect 3.3. A quick 【test】 like your proof of concept would be 【great】 indeed.
> Oh yeah, won't affect 3.3. A quick 【test】 like your proof of concept would be 【great】 indeed.<br><br>The class itself is not present on 3.3. It is not affected.  In-fact thats how we mitigated this problem, that occurred in a production setting for our pipeline. by downgrading
Wow. Thank you for sharing the details, @jelmerk .
good catch!
The fix looks okay, but the PR description looks confusing as `What 【c<font color=blue>hang】es】 were proposed in this pull request?` describes an alternative approach to current `getErrorMessage`?
> Please make sure updating the description before merging this.<br><br>I removed the alternative solution. Imho It's a much better solution, but if experience has taught me anything, it is that getting anything but the smallest of 【c<font color=blue>hang】es】 merged is an uphill battle so I chose the 1 line fix
Simple is good. Indeed the smallest of 【c<font color=blue>hang】es】 can even break things. This looks good pending CI/CD 【test】s
> Simple is good. Indeed the smallest of 【c<font color=blue>hang】es】 can even break things. This looks good pending CI/CD 【test】s<br><br>I tried triggering it twice but those 【test】s seem un【stable】
Can you re-trigger it again? Although the CI failure looks unrelated.
Hm, no looks like something else is wrong in even setting up the environment. Github issue?<br>`Error: Error response from daemon: Get \"https://ghcr.io/v2/\": received unexpected HTTP status: 503 Service Un【available】`
Merged to master/3.4
Sounds good. Mind filing a JIRA please?
Good morning @HyukjinKwon,<br><br>I requested for a JIRA account yesterday, still waiting for confirmation.<br>I have a few questions:<br><br>- Shall we fix the data types also in 【test】s?<br>- For the moment, I have looked only for np.bool, np.object ans np.object0 but not the rest. Shall i try to find if there is something else?<br>- I have a workflow error,  Workflow run detection failed, i enabled github actions on my forked repo, how can i re run the 【test】s?<br><br>
<br>> - Shall we fix the data types also in 【test】s?<br><br>Yeah, if the 【test】 failures look related, let's fix them.<br><br>> - For the moment, I have looked only for np.bool, np.object ans np.object0 but not the rest. Shall i try to find if there is something else?<br><br>If there are not a lot, it would be 【great】 to fix them together. If the occurrences are a lot, feel free to create a separate PR.<br><br>> - I have a workflow error,  Workflow run detection failed, i enabled github actions on my forked repo, how can i re run the 【test】s?<br><br>If you rebase or merge the upstream, the 【test】 will be triggered, and the GitHub 【check】 status will be updated.<br>
@HyukjinKwon: I grepped for all the deprecated types and I list my findings below, please let me know if you see something that should not be changed.<br><br>For the deprecations introduced by numpy 1.24.0 and greping master branch as cloned yesterday:<br><br>```<br>spark % git grep np.object0<br>python/pyspark/sql/pandas/conversion.py:                                np.object0 if pandas_type is None else pandas_type<br>spark % git grep np.str0<br>spark % git grep np.bytes0<br>spark % git grep np.void0<br>spark % git grep np.int0<br>spark % git grep np.uint0<br>spark % git grep np.bool8<br>```<br><br>As we see we have only one np.object0 so we are pretty safe with these numpy 【c<font color=blue>hang】es】.<br><br>For the deprecations introduced by numpy 1.20.0 that resulted in removals in 1.24.0 and greping master branch as cloned yesterday:<br><br>```<br>spark % git grep np.float | grep -v np.float_ | grep -v np.float64 | grep -v np.float32 | grep -v np.float8 | grep -v np.float16<br>mllib/src/【test】/scala/org/apache/spark/ml/【feature】/RobustScalerSuite.scala:      X = np.array([[0, 0], [1, -1], [2, -2], [3, -3], [4, -4]], dtype=np.float)<br>python/docs/source/user_guide/pandas_on_spark/types.rst:np.float      DoubleType<br>python/pyspark/pandas/【test】s/indexes/【test】_base.py:        self.assert_eq(psidx.astype(np.float), pidx.astype(np.float))<br>python/pyspark/pandas/【test】s/【test】_series.py:        self.assert_eq(psser.astype(np.float), pser.astype(np.float))<br>python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ps.DataFrame[np.float, str]:<br>python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ps.DataFrame[np.float]:<br>python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> 'ps.DataFrame[np.float, str]':<br>python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> 'ps.DataFrame[np.float]':<br>python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> ps.DataFrame['a': np.float, 'b': int]:<br>python/pyspark/pandas/typedef/typehints.py:    >>> def func() -> \"ps.DataFrame['a': np.float, 'b': int]\":<br>spark % git grep np.str | grep -v np.str_ | grep -v np.string_<br>python/docs/source/user_guide/pandas_on_spark/types.rst:np.string\\_   BinaryType<br>python/docs/source/user_guide/pandas_on_spark/types.rst:np.str        StringType<br>python/pyspark/pandas/【test】s/【test】_typedef.py:            np.str: (np.unicode_, StringType()),<br>spark % git grep np.object | grep -v np.object_ <br>python/pyspark/sql/pandas/conversion.py:                                np.object0 if pandas_type is None else pandas_type<br>python/pyspark/sql/pandas/conversion.py:                corrected_dtypes[index] = np.object  # type: ignore[attr-defined]<br>python/pyspark/sql/【test】s/【test】_dataframe.py:        self.assertEqual(types[1], np.object)<br>python/pyspark/sql/【test】s/【test】_dataframe.py:        self.assertEqual(types[4], np.object)  # datetime.date<br>python/pyspark/sql/【test】s/【test】_dataframe.py:        self.assertEqual(types[1], np.object)<br>python/pyspark/sql/【test】s/【test】_dataframe.py:                self.assertEqual(types[6], np.object)<br>python/pyspark/sql/【test】s/【test】_dataframe.py:                self.assertEqual(types[7], np.object)<br>spark % git grep np.complex | grep -v np.complex_ <br>spark % git grep np.long  <br>spark % git grep np.unicode | grep -v np.unicode_ <br>python/docs/source/user_guide/pandas_on_spark/types.rst:np.unicode\\_  StringType<br>spark % git grep np.bool | grep -v np.bool_ <br>python/docs/source/user_guide/pandas_on_spark/types.rst:np.bool       BooleanType<br>python/pyspark/pandas/【test】s/【test】_typedef.py:            np.bool: (np.bool, BooleanType()),<br>python/pyspark/pandas/【test】s/【test】_typedef.py:            bool: (np.bool, BooleanType()),<br>python/pyspark/sql/【test】s/【test】_dataframe.py:        self.assertEqual(types[2], np.bool)<br>spark % git grep np.int | grep -v np.int_ | grep -v np.int64 | grep -v np.int32 | grep -v np.int8 | grep -v np.int16<br>connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/consumer/KafkaDataConsumer.scala:      // sadly we can't pinpoint 【specific】 data and invalidate cause we don't have 【unique】 id<br>core/src/main/resources/org/apache/spark/ui/static/vis-timeline-graph2d.min.js:<br>python/docs/source/user_guide/pandas_on_spark/types.rst:np.int        LongType<br>python/pyspark/mllib/regression.py:        return np.interp(x, self.boundaries, self.predictions)  # type: ignore[arg-type]<br>python/pyspark/pandas/groupby.py:        >>> def plus_max(x) -> ps.Series[np.int]:<br>python/pyspark/pandas/groupby.py:        >>> def plus_length(x) -> np.int:<br>python/pyspark/pandas/groupby.py:        >>> def calculation(x, y, z) -> np.int:<br>python/pyspark/pandas/groupby.py:        >>> def plus_max(x) -> ps.Series[np.int]:<br>python/pyspark/pandas/groupby.py:        >>> def calculation(x, y, z) -> ps.Series[np.int]:<br>python/pyspark/pandas/【test】s/indexes/【test】_base.py:        self.assert_eq(psidx.astype(np.int), pidx.astype(np.int))<br>python/pyspark/pandas/【test】s/【test】_series.py:        self.assert_eq(psser.astype(np.int), pser.astype(np.int))<br>```<br><br>As you can see the 2 most difficult types are the int and the float where I find even scala files and 1 js file. i will go through thoroughly the lines and let you know.
@HyukjinKwon,<br><br>I 【<font color=blue>fix】ed】 all the 【c<font color=blue>hang】es】 I could find. <br>The 【test】s are running but the base image is not working for me, consequently some of them are incomplete.<br>```<br>Run docker/login-action@v2<br>Logging into ghcr.io...<br>Error: Error response from daemon: Get \"https://ghcr.io/v2/\": received unexpected HTTP status: 503 Service Un【available】<br>```<br><br>I suggest we do the 【review】, then I squash and remove the wip tag. What do you say?
We need to file a JIRA too.<br>BTW we merged a similar change for np.bool back to Spark 3.3.x; maybe we should do the same here.
Actually I don't know, this change might theoretically be breaking? I wasn't clear
@srowen: I will create the JIRA still waiting on an answer from the mailing list.<br>Why do you think the change is a breaking one?
I dont' know, maybe it doesn't. For example if I have something like `def func() -> ps.DataFrame[np.float, str]` in my code, does it still work?
@srowen: While that code does not exist in pyspark it is only used as an example in the infer_return_type() function I can tell you that for your example using numpy>=1.24.0 will result in an attribute error.
OK. After this change, would this still work with numpy 1.20.0, for example? I think that's the question.
> OK. After this change, would this still work with numpy 1.20.0, for example? I think that's the question.<br><br>Actually to make it even more clear, <br><br>The only 【c<font color=blue>hang】es】 that are 【functional】 are related with the conversion.py file. The rest of the 【c<font color=blue>hang】es】 are inside 【test】s in the user_guide or in some docstrings describing 【specific】 functions. Since I am not an expert in these 【test】s I wait for the 【review】er and some people with more experience in the pyspark code.<br><br>But for your question, these types are aliases for classic python types so yes they should work with all the numpy versions [1](https://numpy.org/devdocs/release/1.20.0-notes.html), [2](https://stackoverflow.com/questions/74844262/how-can-i-solve-error-module-numpy-has-no-attribute-float-in-python). The error or warning comes from the call to the numpy.<br><br>I attached 2 links which explain the use-case.
Maybe let's create a JIRA .. <br><br>> I will create the JIRA still waiting on an answer from the mailing list.<br><br>BTW, what's the title of your email?
> Maybe let's create a JIRA ..<br>> <br>> > I will create the JIRA still waiting on an answer from the mailing list.<br>> <br>> BTW, what's the title of your email?<br><br>Email never arrived although i sent it to private[at]spark.apache.org like it says in the 【contribution】 guide.<br>I self registered with the other process described in the guide.<br><br>Consequently, I just created the JIRA.
Seems like linter fails (https://github.com/aimtsou/spark/actions/runs/4304579333/jobs/7506798202).
Yes but this is the original [code](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/conversion.py#L235). Shall I remove the comment and how did it pass the linter to be in master?
The line changed, and now the 'ignore' is no longer relevant - yes remove it to pass the linter
Tests are completed, shall I squash and remove wip tag from the pull request?
Yes remove WIP just for completeness. No need to squash, the script does that
Merged to master/3.4/3.3, for consistency with https://issues.apache.org/jira/browse/SPARK-40376
Merged to master/3.4. Thank you, @amaliujia and @hvanhovell .
Late 【LGTM】, in the previous PR I am not very sure about the `session.version` so didn't touch it
cc @sunchao @cloud-fan @dongjoon-hyun <br><br>Note that [this commit](https://github.com/apache/spark/pull/40224/commits/a31cd9bf95cd1282ea6fe6880693ad52332fc145) represents the delta from #40144
Merged to master, thanks @xkrogen !
Thank you, @xkrogen and @sunchao !
Thank you, @HyukjinKwon . Merged to master for Apache Spark 3.5.
Hey @dongjoon-hyun - Can you please point me to some 【documentation】 about how this benchmarking is done? I'd like to run the same benchmark locally.
Here is the official document about how to run the benchmark in your GitHub Action. Please see `Running benchmarks in your forked repository` Section.<br>- https://spark.apache.org/developer-tools.html<br><br>In addition, you can 【check】 GitHub Action benchmark job to see the logic.<br>- https://github.com/apache/spark/blob/master/.github/workflows/benchmark.yml<br><br>Lastly, each benchmark file has a command-line direction in their header file, @JohnTortugo .
【Thanks】 a lot!
thanks, merged to master/3.4
@hvanhovell
@amaliujia if you have time, let's also get this one over the line.
@hvanhovell I just addressed actionable comments
@amaliujia can you update the PR?
@hvanhovell waiting for CI
Hi @gengliangwang 👋
【LGTM】 except one comment and the pending 【test】 failures.
【Thanks】, merging to master/3.4<br>This makes the column default 【feature】 【simple】r and more reasonable.<br>cc @xinrong-meng
Could you 【review】 this PR, @HyukjinKwon ?
Thank you, @HyukjinKwon ! Merged to master/3.4.
close in favor of https://github.com/apache/spark/pull/40260
@dongjoon-hyun It is! Not sure how that happened... Should be 【<font color=blue>fix】ed】 now!
@dongjoon-hyun are you ok with this PR now?
Sure! Thank you for 【check】ing again, @hvanhovell .
Alright, merging this one.
The build is failing due to formatting issues, but due to older code: should I patch it all in this PR or create a separate one?
@MaxGekk Can you take a look at this please?
Just FYI and really not a big deal, we typically use upper-cased \"TESTS\" or \"TEST\" for PR title when the change only includes the 【test】s.
@the8thC Do you have an account of OSS JIRA (https://issues.apache.org/jira/browse/SPARK-38735)?
@MaxGekk No, I don't, but I've just requested one using \"selfserve\". Is that the right way?
> @MaxGekk No, I don't, but I've just requested one using \"selfserve\". Is that the right way?<br><br>Yep, thank you.<br>
+1, 【LGTM】. Merging to master.<br>Thank you, @the8thC and @itholic for 【review】.
@the8thC Congratulations with your first 【contribution】 to Apache Spark!
@MaxGekk could you take a look at it? Thank you very much!
@MaxGekk 【Thanks】 for 【review】ing! I guess I need to ask you to merge this PR right? Also, it should be ported into older branches with `TimestampAdd` (3.3.0, 3.3.1, 3.3.2).
+1, 【LGTM】. Merging to master/3.4.<br>Thank you, @chenhao-db.
@chenhao-db Your 【c<font color=blue>hang】es】 cause some conflicts in `branch-3.3`. Please, open a separate PR with a backport to Spark 3.3.
@chenhao-db Congratulations with your first 【contribution】 to Apache Spark!
Need add a 【test】 for the new added field?
【test】: https://github.com/hvanhovell/spark/actions/runs/4318951397
good catch, we may leverage the new proto `DDLParse` later
> we may leverage the new proto `DDLParse` later<br><br>Sounds good. Are you working on it? Please let me know once it's done. I'll address it.
@hvanhovell @LuciferYang
We may have https://github.com/apache/spark/pull/40213 merged first since that looks pretty good already.
Thank you, @hvanhovell and @HyukjinKwon . Merged to master/3.4.
CC @zhengruifeng @HyukjinKwon @ueshin @hvanhovell
After double thoughts, I add `spark.udf.registerJavaUDAF`'s implementation into this PR, since both APIs rely on the same proto message JavaUDF.
WDYT @hvanhovell ?
cc @grundprinzip , @hvanhovell , @zhengruifeng, @HyukjinKwon , @xinrong-meng
Thank you! I missed the doc【test】. Let me fix the doc【test】 too.
Hi ~ @dongjoon-hyun , could you please help 【check】 the results of local execution of <br><br>```<br>build/sbt 【clean】 \"connect-client-jvm/【test】\" -Dspark.debug.sc.jvm.client=true<br>```<br>?<br>I found the following errors:<br>```<br>[info] ClientE2ETestSuite:<br>Starting the Spark Connect Server...<br>Using jar: /Users/yangjie01/SourceCode/git/spark-mine-sbt/connector/connect/server/target/scala-2.12/spark-connect-assembly-3.5.0-SNAPSHOT.jar<br>Ready for client connections.<br>java.lang.RuntimeException: Failed to start the 【test】 server on port 15971.<br>  | => cat org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll(RemoteSparkSession.scala:129)<br>\tat org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll$(RemoteSparkSession.scala:120)<br>\tat org.apache.spark.sql.ClientE2ETestSuite.beforeAll(ClientE2ETestSuite.scala:36)<br>\tat org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)<br>\tat org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>\tat org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>\tat org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:36)<br>\tat org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>\tat org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>\tat sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>\tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>\tat java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>\tat java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>\tat java.lang.Thread.run(Thread.java:750)<br>....<br><br>Suppressed: io.grpc.StatusRuntimeException: INTERNAL: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':<br>\t\tat io.grpc.Status.asRuntimeException(Status.java:535)<br>\t\tat io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>\t\tat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)<br>\t\tat scala.collection.Iterator.find(Iterator.scala:993)<br>\t\tat scala.collection.Iterator.find$(Iterator.scala:992)<br>\t\tat scala.collection.AbstractIterator.find(Iterator.scala:1431)<br>\t\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:133)<br>\t\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1$adapted(SparkSession.scala:125)<br>\t\tat org.apache.spark.sql.SparkSession.newDataset(SparkSession.scala:258)<br>\t\tat org.apache.spark.sql.SparkSession.newDataFrame(SparkSession.scala:252)<br>\t\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:125)<br>\t\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:109)<br>\t\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:147)<br>\t\tat org.apache.spark.sql.connect.client.util.RemoteSparkSession.beforeAll(RemoteSparkSession.scala:135)<br>\t\t... 13 more<br>```<br><br>Seems also related to [SPARK-41725](https://issues.apache.org/jira/browse/SPARK-41725)? 【Thanks】 ~
Yes, you need `-Phive` after SPARK-41725.  I already 【check】ed that. I made this PR because of that, @LuciferYang .
> Yes, you need `-Phive` after [SPARK-41725](https://issues.apache.org/jira/browse/SPARK-41725). I already 【check】ed that. I made this PR because of that, @LuciferYang .<br><br>【Thanks】 ~
Thank you, @HyukjinKwon and @LuciferYang . Merged to master/3.4.
Late 【LGTM】, thank you @dongjoon-hyun !
https://github.com/apache/spark/pull/40065#issuecomment-1435520529
We already verified this.<br>- https://github.com/apache/spark/pull/40065
Ya, @LuciferYang did it. Let me close this first.
【Thanks】 @dongjoon-hyun
cc @srowen and @HyukjinKwon
Thank you, @HyukjinKwon and @LuciferYang .<br><br>The license 【test】 passed. Merged to master/3.4/3.3/3.2.<br>![Screenshot 2023-03-02 at 1 02 02 AM](https://user-images.githubusercontent.com/9700541/222381571-afcd08bf-0503-4250-8085-c2c824f570a5.png)<br>
All related 【test】s passed.<br><br>Merged to master and branch-3.4.
The jdbc API seems hard to 【test】, do we need a 【test】 case? @hvanhovell @HyukjinKwon @zhengruifeng @dongjoon-hyun
There is another kind jdbc API, see: https://github.com/apache/spark/blob/79da1ab400f25dbceec45e107e5366d084138fa8/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L316<br>I will create another PR to add proto msg and implement it.
I guess you can refer to `JDBCSuite` and `ClientE2ETestSuite` ?
@beliefer you can create a 【test】 in `PlanGenerationTestSuite`. That will at least validate the proto message we are generating, and it will validate that plan you are producing yields a valid plan in `ProtoToPlanTestSuite`.
> I guess you can refer to `JDBCSuite` and `ClientE2ETestSuite` ?<br><br>The built-in H2 running in server side and we can't start H2 database at connect API.
> @beliefer you can create a 【test】 in `PlanGenerationTestSuite`. That will at least validate the proto message we are generating, and it will validate that plan you are producing yields a valid plan in `ProtoToPlanTestSuite`.<br><br>OK. I added the 【test】 cases. But throws<br>```<br>org.h2.jdbc.JdbcSQLSyntaxErrorException: Schema \"TEST\" not found; SQL statement:<br>SELECT * FROM TEST.TIMETYPES WHERE 1=0 [90079-214]<br>```<br> when ProtoToParsedPlanTestSuite validating the golden file. So I create some schema and table in H2 database.
@dongjoon-hyun @hvanhovell It seems the build scala 2.13 failed is unrelated to this PR.
merging to master/3.4
@hvanhovell @dongjoon-hyun @LuciferYang Thank you!
@jiang13021 Thank you for the backport. Could add the following, please:<br>1. The tag `[3.3]` to PR's title.<br>2. `This is a backport of https://github.com/apache/spark/pull/40195` to PR's description.
> @jiang13021 Thank you for the backport. Could add the following, please:<br>> <br>> 1. The tag `[3.3]` to PR's title.<br>> 2. `This is a backport of https://github.com/apache/spark/pull/40195` to PR's description.<br><br>Done
+1, 【LGTM】. All GAs passed. Merging to 3.3.<br>Thank you, @jiang13021.
friendly ping @dongjoon-hyun , I found the following error message in Java11&17 maven build log<br><br>```<br>Error: [ERROR] An error occurred attempting to read POM<br>org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42) <br>    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)<br>    at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)<br>    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)<br>    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)<br>    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)<br>    at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)<br>    at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)<br>    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)<br>    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)<br>    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)<br>    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)<br>    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)<br>    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)<br>    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)<br>    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)<br>    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)<br>    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)<br>    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)<br>    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)<br>    at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)<br>    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)<br>    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)<br>    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)<br>    at java.lang.reflect.Method.invoke (Method.java:568)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)<br>    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)<br>```<br><br>- https://github.com/apache/spark/actions/runs/4324211751/jobs/7548768567<br>- https://github.com/apache/spark/actions/runs/4323972537/jobs/7548220619<br>- https://github.com/LuciferYang/spark/actions/runs/4315955520/jobs/7542233374<br><br>Will this cause errors in the build of bom?<br><br>
Recently, I often encounter Maven build failed  of Java 11&17 GA build task due to timeout ... a little strange
> friendly ping @dongjoon-hyun , I found the following error message in Java11&17 maven build log<br>> <br>> ```<br>> Error: [ERROR] An error occurred attempting to read POM<br>> org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42) <br>>     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)<br>>     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)<br>>     at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)<br>>     at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)<br>>     at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)<br>>     at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)<br>>     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)<br>>     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)<br>>     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)<br>>     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)<br>>     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)<br>>     at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)<br>>     at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)<br>>     at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)<br>>     at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)<br>>     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)<br>>     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)<br>>     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)<br>>     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)<br>>     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)<br>>     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)<br>>     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)<br>>     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)<br>>     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)<br>>     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)<br>>     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)<br>>     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)<br>>     at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)<br>>     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)<br>>     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)<br>>     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)<br>>     at java.lang.reflect.Method.invoke (Method.java:568)<br>>     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)<br>>     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)<br>>     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)<br>>     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)<br>> ```<br>> <br>> * https://github.com/apache/spark/actions/runs/4324211751/jobs/7548768567<br>> * https://github.com/apache/spark/actions/runs/4323972537/jobs/7548220619<br>> * https://github.com/LuciferYang/spark/actions/runs/4315955520/jobs/7542233374<br>> <br>> Will this cause errors in the build of bom?<br><br>I know, GA already use maven 3.9.0 to build, this is a well know issue<br><br><img width=\"961\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/222873867-bdcd885f-96bf-4abc-8106-6859ed97d25d.png\"><br>
@srowen  this one ready, all 【test】 passed
> > friendly ping @dongjoon-hyun , I found the following error message in Java11&17 maven build log<br>> > ```<br>> > Error: [ERROR] An error occurred attempting to read POM<br>> > org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42) <br>> >     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDeclWithVersion (MXParser.java:3423)<br>> >     at org.codehaus.plexus.util.xml.pull.MXParser.parseXmlDecl (MXParser.java:3345)<br>> >     at org.codehaus.plexus.util.xml.pull.MXParser.parsePI (MXParser.java:3197)<br>> >     at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog (MXParser.java:1828)<br>> >     at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl (MXParser.java:1757)<br>> >     at org.codehaus.plexus.util.xml.pull.MXParser.next (MXParser.java:1375)<br>> >     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:3940)<br>> >     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:612)<br>> >     at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read (MavenXpp3Reader.java:627)<br>> >     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:759)<br>> >     at org.cyclonedx.maven.BaseCycloneDxMojo.readPom (BaseCycloneDxMojo.java:746)<br>> >     at org.cyclonedx.maven.BaseCycloneDxMojo.retrieveParentProject (BaseCycloneDxMojo.java:694)<br>> >     at org.cyclonedx.maven.BaseCycloneDxMojo.getClosestMetadata (BaseCycloneDxMojo.java:524)<br>> >     at org.cyclonedx.maven.BaseCycloneDxMojo.convert (BaseCycloneDxMojo.java:481)<br>> >     at org.cyclonedx.maven.CycloneDxMojo.execute (CycloneDxMojo.java:70)<br>> >     at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)<br>> >     at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)<br>> >     at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)<br>> >     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)<br>> >     at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)<br>> >     at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)<br>> >     at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)<br>> >     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:260)<br>> >     at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:172)<br>> >     at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:100)<br>> >     at org.apache.maven.cli.MavenCli.execute (MavenCli.java:821)<br>> >     at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:270)<br>> >     at org.apache.maven.cli.MavenCli.main (MavenCli.java:192)<br>> >     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)<br>> >     at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)<br>> >     at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)<br>> >     at java.lang.reflect.Method.invoke (Method.java:568)<br>> >     at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)<br>> >     at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)<br>> >     at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)<br>> >     at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)<br>> > ```<br>> > <br>> > <br>> >     <br>> >       <br>> >     <br>> > <br>> >       <br>> >     <br>> > <br>> >     <br>> >   <br>> > <br>> > * https://github.com/apache/spark/actions/runs/4324211751/jobs/7548768567<br>> > * https://github.com/apache/spark/actions/runs/4323972537/jobs/7548220619<br>> > * https://github.com/LuciferYang/spark/actions/runs/4315955520/jobs/7542233374<br>> > <br>> > Will this cause errors in the build of bom?<br>> <br>> I know, GA already use maven 3.9.0 to build, this is a well know issue<br>> <br>> <img alt=\"image\" width=\"961\" src=\"https://user-images.githubusercontent.com/1475305/222873867-bdcd885f-96bf-4abc-8106-6859ed97d25d.png\"><br><br>This is not related to current pr. Let me see how to solve this problem later<br><br><br><br>
friendly ping @srowen
@LuciferYang can you update the binary 【compatibility】 【test】s?
> @LuciferYang can you update the binary 【compatibility】 【test】s?<br><br>done
Now all paased
【Thanks】 @hvanhovell @HyukjinKwon @zhengruifeng @amaliujia
Overall looks good. Thank you!
Seems like 【test】s did not pass, and it fails in the master branch (https://github.com/apache/spark/actions/runs/4319488463/jobs/7538760733).<br><br>Let me quickly revert this for now.
@srowen  Please ignore that change. It was work in 【progress】 to 【check】 few things. <br>The reason why we get ambiguous error in below scenario and why it's not correct is the result of attribute resolution returns  <br>two 【value】s but both 【value】s are same. Thus, it should not throw ambiguous error.<br><br>val df1 = sc.【parallel】ize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(\"id\\"col2\\"col3\\"col4\ \"col5\")<br>val op_cols_mixed_case = List(\"id\\"col2\\"col3\\"col4\ \"col5\ \"ID\")<br>val df3 = df1.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)<br>df3.select(\"id\").show()<br>org.apache.spark.sql.AnalysisException: Reference 'id' is ambiguous, could be: id, id.<br><br>df3.explain()<br>== Physical Plan ==<br>*(1) Project [_1#6 AS id#17, _2#7 AS col2#18, _3#8 AS col3#19, _4#9 AS col4#20, _5#10 AS col5#21, _1#6 AS ID#17]<br><br>Before the fix, attributes matched were:<br>attributes: Vector(id#17, id#17)<br>Thus, it throws ambiguous reference error. But if we consider only 【unique】 matches, it will return correct result.<br>【unique】 attributes: Vector(id#17)<br>
@srowen @dongjoon-hyun Can you please 【review】 this PR?
Gentle Ping @srowen  @dongjoon-hyun @mridulm @HyukjinKwon
I'm not sure about the change, not sure I'm qualified to 【review】 it. I think at best the error message should change; I am not clear that the result is 'wrong'
> I'm not sure about the change, not sure I'm qualified to 【review】 it. I think at best the error message should change; I am not clear that the result is 'wrong'<br><br>【Thanks】 for replying. Can you please tag someone who should be right person to 【review】 this change?
Gentle ping @dongjoon-hyun @mridulm @HyukjinKwon @yaooqinn Can you please 【review】 this PR?
Can you try `set spark.sql.caseSensitive=true`?
> Can you try `set spark.sql.caseSensitive=true`?<br><br>Yes, I have tried it. With caseSensitive set to true, it will work as then id and ID will be treated as separate columns.<br>Issue is when columns names are supposed to considered  as case insensitive.
You first defined a case-sensitive data set, then queried in a case-insensitive way, I guess the error is expected.
> You first defined a case-sensitive data set, then queried in a case-insensitive way, I guess the error is expected.<br><br>In the physical plan, both id and ID columns are projected to the same column in the dataframe: _1#6<br>_1#6 AS id#17,  _1#6 AS ID#17<br>So, there is no ambiguity,<br><br>Also, in the matched attributes, results are same: attributes: Vector(id#17, id#17)<br>Just because, we have duplicates in the matched result, it's being considered as ambiguous.<br><br>If the matched attribute result was Vector(id#17, ID#17) , then it would have been valid error.<br><br>And even if the dataset has columns in different cases, Spark being case insensitive by default, should consider both columns as same.<br>
I don't get it, it is due to case sensitivity; that's why it becomes ambiguous and that's what you see. The issue is that the error isn't super helpful because it shows the lower-cased column right? that's what I was saying. Or: does your change still result in an error without case sensitivity? it should
> I don't get it, it is due to case sensitivity; that's why it becomes ambiguous and that's what you see. The issue is that the error isn't super helpful because it shows the lower-cased column right? that's what I was saying. Or: does your change still result in an error without case sensitivity? it should<br><br>The issue is not with the error message. Problem is that in this case error should not be thrown. Select query should return result.  After this change, ambiguous error will not be thrown as we are fixing the duplicate attribute match.
Hm, how is it not ambiguous? When case insensitive, 'id' could mean one of two different columns
> Hm, how is it not ambiguous? When case insensitive, 'id' could mean one of two different columns<br><br>It's not ambiguous because the  when we are selecting using list of column names, both id and ID are getting 【value】 from same column 'id' in the source dataframe. <br>val **df1** = sc.【parallel】ize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(\"**id\"**,\"col2\\"col3\\"col4\ \"col5\")<br>val op_cols_mixed_case = List(**\"id\"**,\"col2\\"col3\\"col4\ \"col5\ **\"ID\"**)<br>val df3 = **df1**.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)<br>df3.select(\"id\").show()<br><br>df3.explain()<br>== Physical Plan ==<br>*(1) Project [**_1#6 AS id#17**, _2#7 AS col2#18, _3#8 AS col3#19, _4#9 AS col4#20, _5#10 AS col5#21, **_1#6 AS ID#17**]
That isn't relevant. You are selecting from a DataFrame with cols id and ID. Imagine for instance they do not come from the same source, it's clearly ambiguous. It wouldn't make sense if it were different in this case.
It's very much relevant as this is the only case which requires the fix. If they do not come from same source, the plan  will reflect that and it will throw the ambiguous error even after this fix.
Hm, I just don't see the logic in that. It isn't how SQL works either, as far as I understand. Here's maybe another example, imagine a DataFrame defined by `SELECT 3 as id, 3 as ID`. Would you also say selecting \"id\" is unambiguous? and it makes sense to you if I change a 3 to a 4 that this query is no longer semantically valid?
> Hm, I just don't see the logic in that. It isn't how SQL works either, as far as I understand. Here's maybe another example, imagine a DataFrame defined by `SELECT 3 as id, 3 as ID`. Would you also say selecting \"id\" is unambiguous? and it makes sense to you if I change a 3 to a 4 that this query is no longer semantically valid?<br><br>If it's valid as per the plan then yes.
Gentle ping @dongjoon-hyun @mridulm @HyukjinKwon @yaooqinn Can you please 【review】 this PR or direct it to someone who can 【review】 this PR.
That's a \"no\" from me, per the logic above
> That's a \"no\" from me, per the logic above<br><br>【Thanks】 @srowen But seems I am not able to explain the change to you. So it's better to get 【review】 from someone who is qualified to 【review】 the change and aware of this code.
I second @srowen ‘s view. cc @cloud-fan
> I second @srowen ‘s view. cc @cloud-fan<br><br>【Thanks】 @yaooqinn for replying. Can you please explain why you think it's not the right fix? <br>The fix only proposes to remove duplicates from the resolved columns. As it's incorrect to consider the only one column match as ambiguous just because it occurs more than once in the resolved column list.
I think column resolution should only look at one level, to make the behavior 【simple】 and predictable. I tried it on pgsql and it fails as well:<br>```<br>create table t(i int);<br>select id from (select i as id, i as ID from t) sub;<br>ERROR: column reference \"id\" is ambiguous Position: 8<br>```
> df3.select(\"id\").show()<br><br>@cloud-fan The example you have shared will behave the same even after this fix. It will give ambiguous error. <br>The use case which the fix is trying to solve is different. Can you please try these two cases:<br>Case 1: which works fine<br>val df1 = sc.【parallel】ize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(\"id\\"col2\\"col3\\"col4\ \"col5\")<br>val op_cols_same_case = List(\"id\\"col2\\"col3\\"col4\ \"col5\ \"id\")<br>val df3 = df1.select(op_cols_same_case.head, op_cols_same_case.tail: _*)<br>df3.select(\"id\").show()  <br><br>Case 2: which doesn't work fine and the fix is to solve this issue<br>val df2 = sc.【parallel】ize(List((1,2,3,4,5),(1,2,3,4,5))).toDF(\"id\\"col2\\"col3\\"col4\ \"col5\")<br>val op_cols_mixed_case = List(\"id\\"col2\\"col3\\"col4\ \"col5\ \"ID\")<br>val df4 = df2.select(op_cols_mixed_case.head, op_cols_mixed_case.tail: _*)<br>df4.select(\"id\").show()
@shrprasa <br>At the dataset definition phase, especially for intermediate datasets, Spark is lenient/lazy with case sensitivity. This is because the 【check】s happen in SQL Analyzing, which is not required for defining a Dataset. This gives the user more freedom but also cognitive disorders. On the other hand, in the read phase, SQL Analyzing is a mandatory step, and 【check】s will be performed, so the configuration provided by Spark at this stage is sufficient to resolve all ambiguities.
@shrprasa do you know how the case 1 works?
> @shrprasa do you know how the case 1 works?<br><br>yes. It works because the resolved column has just one match <br>attributes: Vector(id#17)<br><br>but for second case, the match result is<br>attributes: Vector(id#17, id#17)<br>Since, there are more than one 【value】 although both are exactly same, it fails. This fix proposes to fix this by taking distinct 【value】s of match result.<br><br><br>
thank you, merged into master/3.4
just a idea, in this case what about:<br>1,  Convert Global SortExec to Local SortExec;<br>2, Make `SortExec` requires `SinglePartition`;<br><br>in order to let one executor instead of driver to do the computation.
@zhengruifeng thank you for your thought.<br> <br>The original idea of driver sort is to avoid one shuffle. Requires SinglePartition seems does not help since it still requires a shuffle.<br><br>Besides, finally, the result would go to driver, i.e. `df.sort.collect` (It's the reason I match `ReturnAnswer`), so it should be fine to do at driver. Plus, do sort at driver is not the first code place except driver sort. e.g., the merge function of rdd.takeOrdered. It should be safe if the size of plan is small enough.<br><br>The idea of Convert Global SortExec to Local SortExec and make SortExec requires SinglePartition looks fine that It can avoid the `sample` of range partitioner. But it seems orthogonal with this pr.
cc @cloud-fan @viirya thank you
Meta comment: moving this to the driver has potential for destabilizing the application.<br>In SPARK-36419, we added option to move the final `treeAggregate` to executor given the scale challenges seen when it is run at driver.<br><br>One option would be to do this in an executor as @zhengruifeng suggested, and that should be much more 【scalable】 as a solution.
I did a quick 【test】 with dataset `T10I4D100K` in http://fimi.uantwerpen.be/data/ <br><br>fit:<br>```<br>scala> val df = sc.textFile(\"/Users/ruifeng.zheng/.dev/data/T10I4D100K.dat\").map(_.split(\" \")).toDF(\"items\")<br>df: org.apache.spark.sql.DataFrame = [items: array<string>]<br><br>scala> df.count<br>res16: Long = 100000<br><br>scala> val model = new FPGrowth().setMinSupport(0.01).setMinConfidence(0.01).fit(df)<br>model: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fp【growth】_92901252345a, numTrainingRecords=100000<br><br>scala> model.freqItemsets.count<br>res17: Long = 385                                                               <br><br>scala> model.associationRules.count<br>res18: Long = 21                                                                <br><br>scala> model.save(\"/tmp/fpm.model\")<br>```<br><br><br>transformation:<br>```<br>import org.apache.spark.ml.fpm._<br>val df = sc.textFile(\"/Users/ruifeng.zheng/.dev/data/T10I4D100K.dat\").map(_.split(\" \")).toDF(\"items\")<br>df.cache()<br>df.count()<br><br>val model = FPGrowthModel.load(\"/tmp/fpm.model\")<br>model.transform(df).explain(\"extended\")<br>Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up<br>val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start<br>val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start<br>```<br><br>master:<br>```<br>scala> val model = FPGrowthModel.load(\"/tmp/fpm.model\")<br>model: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fp【growth】_92901252345a, numTrainingRecords=100000<br><br>scala> model.transform(df).explain(\"extended\")<br>== Parsed Logical Plan ==<br>'Project [items#5, UDF('items) AS prediction#70]<br>+- Project [【value】#2 AS items#5]<br>   +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>      +- ExternalRDD [obj#1]<br><br>== Analyzed Logical Plan ==<br>items: array<string>, prediction: array<string><br>Project [items#5, UDF(items#5) AS prediction#70]<br>+- Project [【value】#2 AS items#5]<br>   +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>      +- ExternalRDD [obj#1]<br><br>== Optimized Logical Plan ==<br>Project [items#5, UDF(items#5) AS prediction#70]<br>+- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)<br>      +- *(1) Project [【value】#2 AS items#5]<br>         +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>            +- Scan[obj#1]<br><br>== Physical Plan ==<br>*(1) Project [items#5, UDF(items#5) AS prediction#70]<br>+- InMemoryTableScan [items#5]<br>      +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)<br>            +- *(1) Project [【value】#2 AS items#5]<br>               +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>                  +- Scan[obj#1]<br><br><br>scala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up<br><br>scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start<br>start: Long = 1678692855532<br>end: Long = 1678692860098<br>res4: Long = 4566<br><br>scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start<br>start: Long = 1678692860277<br>end: Long = 1678692862372<br>res5: Long = 2095<br>```<br><br>this PR:<br>```<br>scala> model.transform(df).explain(\"extended\")<br>== Parsed Logical Plan ==<br>'Project [items#5, CASE WHEN NOT isnull('items) THEN aggregate('prediction, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda 'y_1[antecedent], lambdafunction(array_contains('items, lambda 'x_2), lambda 'x_2, false)) THEN array_union(lambda 'x_0, array_except(lambda 'y_1[consequent], 'items)) ELSE lambda 'x_0 END, lambda 'x_0, lambda 'y_1, false), lambdafunction(lambda 'x_3, lambda 'x_3, false)) ELSE cast(array() as array<string>) END AS prediction#72]<br>+- Join Cross<br>   :- Project [【value】#2 AS items#5]<br>   :  +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>   :     +- ExternalRDD [obj#1]<br>   +- ResolvedHint (strategy=broadcast)<br>      +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]<br>         +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))<br>            +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false<br><br>== Analyzed Logical Plan ==<br>items: array<string>, prediction: array<string><br>Project [items#5, CASE WHEN NOT isnull(items#5) THEN aggregate(prediction#68, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE cast(array() as array<string>) END AS prediction#72]<br>+- Join Cross<br>   :- Project [【value】#2 AS items#5]<br>   :  +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>   :     +- ExternalRDD [obj#1]<br>   +- ResolvedHint (strategy=broadcast)<br>      +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]<br>         +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))<br>            +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false<br><br>== Optimized Logical Plan ==<br>Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(prediction#68, [], lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE [] END AS prediction#72]<br>+- Join Cross, rightHint=(strategy=broadcast)<br>   :- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)<br>   :     +- *(1) Project [【value】#2 AS items#5]<br>   :        +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>   :           +- Scan[obj#1]<br>   +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]<br>      +- Project [antecedent#57, consequent#58]<br>         +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false<br><br>== Physical Plan ==<br>AdaptiveSparkPlan isFinalPlan=false<br>+- Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(prediction#68, [], lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE [] END AS prediction#72]<br>   +- BroadcastNestedLoopJoin BuildRight, Cross<br>      :- InMemoryTableScan [items#5]<br>      :     +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)<br>      :           +- *(1) Project [【value】#2 AS items#5]<br>      :              +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>      :                 +- Scan[obj#1]<br>      +- BroadcastExchange IdentityBroadcastMode, [plan_id=117]<br>         +- ObjectHashAggregate(keys=[], functions=[collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[prediction#68])<br>            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=114]<br>               +- ObjectHashAggregate(keys=[], functions=[partial_collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[buf#95])<br>                  +- Project [antecedent#57, consequent#58]<br>                     +- Scan ExistingRDD[antecedent#57,consequent#58,confidence#59,lift#60,support#61]<br><br><br>scala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up<br><br>scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start<br>start: Long = 1678693708534<br>end: Long = 1678693713436<br>res6: Long = 4902<br><br>scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start<br>start: Long = 1678693713596<br>end: Long = 1678693713807<br>res7: Long = 211<br>```<br><br><br>the transformation is a bit slower 4566 -> 4902, but when we need to analyze the dataframe it will be much 【fast】er 2095 -> 211 since the `collect` execution is delayed.
So this seems slower on a medium-sized data set. I don't know if delaying the collect() matters much; the overall execution time matters. I'm worried that this gets much slower on 1M or 10M records. Does this buy us other benefits, like, is this necessary to support \"Safe Spark\" or something?
yes, the `BroadcastNestedLoopJoin` is slower.<br><br>Then I have another try with subquery, and it's 【fast】er in both execution and 【analysis】, but I have to create temp view and write the sql query then, see https://github.com/apache/spark/pull/40263/commits/63595ba03d9f18fe0b43bfb09f974ea50cb2c651<br><br>`model.transform(df).count()`: 4566 -> 3046<br>`model.transform(df).schema`: 2095 -> 298<br><br>So I'm trying to add a new method `Dataset.withScalarSubquery` in https://github.com/apache/spark/pull/40263/commits/c41ac094eb40520948d95108a78431694a33772d <br><br>not sure whether it is the correct way to support `ScalarSubquery` in DataFrame APIs, but it is actually a pain point to me.<br><br>```<br>scala> model.transform(df).explain(\"extended\")<br>== Parsed Logical Plan ==<br>'Project [items#5, CASE WHEN NOT isnull('items) THEN aggregate('prediction, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda 'y_1[antecedent], lambdafunction(array_contains('items, lambda 'x_2), lambda 'x_2, false)) THEN array_union(lambda 'x_0, array_except(lambda 'y_1[consequent], 'items)) ELSE lambda 'x_0 END, lambda 'x_0, lambda 'y_1, false), lambdafunction(lambda 'x_3, lambda 'x_3, false)) ELSE cast(array() as array<string>) END AS prediction#74]<br>+- Project [items#5, scalar-subquery#70 [] AS prediction#71]<br>   :  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]<br>   :     +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))<br>   :        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false<br>   +- Project [【value】#2 AS items#5]<br>      +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>         +- ExternalRDD [obj#1]<br><br>== Analyzed Logical Plan ==<br>items: array<string>, prediction: array<string><br>Project [items#5, CASE WHEN NOT isnull(items#5) THEN aggregate(prediction#71, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE cast(array() as array<string>) END AS prediction#74]<br>+- Project [items#5, scalar-subquery#70 [] AS prediction#71]<br>   :  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]<br>   :     +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))<br>   :        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false<br>   +- Project [【value】#2 AS items#5]<br>      +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>         +- ExternalRDD [obj#1]<br><br>== Optimized Logical Plan ==<br>Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(scalar-subquery#70 [], [], lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE [] END AS prediction#74]<br>:  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]<br>:     +- Project [antecedent#57, consequent#58]<br>:        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false<br>+- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)<br>      +- *(1) Project [【value】#2 AS items#5]<br>         +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>            +- Scan[obj#1]<br><br>== Physical Plan ==<br>AdaptiveSparkPlan isFinalPlan=false<br>+- Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(Subquery subquery#70, [id=#105], [], lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE [] END AS prediction#74]<br>   :  +- Subquery subquery#70, [id=#105]<br>   :     +- AdaptiveSparkPlan isFinalPlan=false<br>   :        +- ObjectHashAggregate(keys=[], functions=[collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[collect_list(struct(antecedent, consequent))#68])<br>   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=103]<br>   :              +- ObjectHashAggregate(keys=[], functions=[partial_collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[buf#97])<br>   :                 +- Project [antecedent#57, consequent#58]<br>   :                    +- Scan ExistingRDD[antecedent#57,consequent#58,confidence#59,lift#60,support#61]<br>   +- InMemoryTableScan [items#5]<br>         +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)<br>               +- *(1) Project [【value】#2 AS items#5]<br>                  +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS 【value】#2]<br>                     +- Scan[obj#1]<br><br><br>scala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up<br><br>scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start<br>start: Long = 1678788928604<br>end: Long = 1678788931650<br>res4: Long = 3046<br><br>scala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start<br>start: Long = 1678788931785<br>end: Long = 1678788932083<br>res5: Long = 298<br>```
@srowen if the la【test】 【performance】 【test】 seems fine, then I'd ask the SQL guys whether we can have a subquery method in DataFrame APIs.
If it's 【fast】er and gives the right answers, sure
TL;DR  I want to apply scalar subquery to optimize `FPGrowthModel.transform`, there are two options:<br><br>1, create temp views and use `spark.sql`, see https://github.com/apache/spark/commit/63595ba03d9f18fe0b43bfb09f974ea50cb2c651;<br><br>2, add `private[spark] def withScalarSubquery(colName: String, subquery: Dataset[_]): DataFrame`, it seems much more convenient but not sure whether it is a proper way.<br><br>cc @cloud-fan @HyukjinKwon
I don't know enough to say whether it's worth a new method. Can we start with the change that needs no new API, is it a big enough win?
@srowen sounds reasonable
@MaxGekk Please take a look, thanks for 【review】ing!
@chenhao-db Could you fix the build errors:<br>```<br>[error] /home/runner/work/apache-spark/apache-spark/sql/catalyst/src/【test】/scala/org/apache/spark/sql/catalyst/expressions/DateExpressionsSuite.scala:2047:7: not found: 【value】 【check】ErrorInExpression<br>[error]       【check】ErrorInExpression[SparkArithmeticException](TimestampAdd(\"DAY\<br>[error]       ^<br>[error] /home/runner/work/apache-spark/apache-<br>```
@MaxGekk It seems that `【check】ErrorInExpression` doesn't exist in 3.3, so I still have to use the old `【check】ExceptionInExpression`. Is that okay?
Seems like the 【test】 failure is related to the 【c<font color=blue>hang】es】:<br>```<br>[info] - SPARK-42635: timestampadd unit conversion overflow *** FAILED *** (12 milliseconds)<br>[info]   (non-codegen mode) Expected error message is `[DATETIME_OVERFLOW] Datetime operation overflow`, but `Datetime operation overflow: add 106751992 DAY to TIMESTAMP '1970-01-01 00:00:00'.` found (ExpressionEvalHelper.scala:176)<br>```
@MaxGekk I see. In old versions Spark doesn't include the error class in the error message: https://github.com/apache/spark/blob/branch-3.3/core/src/main/scala/org/apache/spark/ErrorInfo.scala#L74. I just removed the error class prefix in the expected error message.
+1, 【LGTM】. All GAs passed. Merging to 3.3.<br>Thank you, @chenhao-db.
@beliefer can you add a 【test】 to SparkPlannerSuite or ClientE2ESuite to make sure this properly works?
cc @zhengruifeng can you also take a look?
we'd better always add e2e 【test】s, since it was added in `ClientE2ESuite`, I think don't need to add one in `【test】_connect_basic`
@hvanhovell @zhengruifeng Thank you!
TPCH q21 plan change<br>|Before|After|<br>|------|----|<br>|![Web capture_3-3-2023_125513_ms web azuresynapse net_crop](https://user-images.githubusercontent.com/106726387/222658668-830d9ddb-2cc2-4013-8dbc-29967d957954.jpeg)|![Web capture_3-3-2023_12324_ms web azuresynapse net_crop](https://user-images.githubusercontent.com/106726387/222658801-f917a4cd-f825-4567-aca8-cb596dd64e51.jpeg)|
cc: @wangyum @peter-toth
This change makes sense to me and new plans look ok to me.<br>However, seemingly `InferFiltersFromConstraints` has a dedicated place in the optimizer and so there are 2 special batches `Operator Optimization before Inferring Filters` and `Operator Optimization after Inferring Filters` before and after the rule to make sure the inferred filtes are 【optimized】. It also seems like the `RewriteSubquery` batch slowly becomes larger and larger with rules from those batches (see SPARK-39511, SPARK-22662, SPARK-36280). And now you want to add `InferFiltersFromConstraints` too. So I wonder if `RewritePredicateSubquery` is at the right place or what else would make sense to be executed after `RewritePredicateSubquery`? Maybe rerunning a full `operatorOptimizationBatch` would make sense despite it comes with a cost?
I had a change like this before: https://github.com/apache/spark/pull/22778.
> I had a change like this before: https://github.com/apache/spark/pull/22778.<br><br>Ah ok, thanks @wangyum! It looks like the very same discussuion has come up before: https://github.com/apache/spark/pull/22778#【discussion】_r229178084
@wangyum @peter-toth 【Thanks】 for pointing on previous attempts.<br><br>It does seem moving `RewritePredicateSubquery` rule is right way so that in future we don't add anymore rule to that batch (`RewriteSubquery`).<br><br>In this pr https://github.com/apache/spark/pull/17520, they tried to put RewritePredicateSubquery right after `Subquery` batch (of `OptimizeSubqueries`). `operatorOptimizationBatch` will run after this.  They also added one rule to push LeftSemi/LeftAnti through join, but that has been added in 3.0 by [SPARK-19712](https://issues.apache.org/jira/browse/SPARK-19712). So now we only need to change rule position.<br><br>If this seems right to you guys, I can update this PR to move `RewritePredicateSubquery` after `Subqury` batch?
Looks like there are a few failures after moving the rule (https://github.com/apache/spark/pull/40266/commits/22e7886ff1059b98d1525380b2cb22718fd5dd09). @mskapilks, do you think you can look into those failures?
> Looks like there are a few failures after moving the rule ([22e7886](https://github.com/apache/spark/commit/22e7886ff1059b98d1525380b2cb22718fd5dd09)). @mskapilks, do you think you can look into those failures?<br><br>Yup I am working on them. I had wrong SPARK_HOME setup so missed the plan 【c<font color=blue>hang】es】
More failures. Seems this might take real effort to make it work like other rules modifications.
> More failures. Seems this might take real effort to make it work like other rules modifications.<br><br>Why was the la【test】 commit (https://github.com/apache/spark/pull/40266/commits/b1ed7bee8b91faf8286963c7867eeed9f781b487) needed?
@mskapilks, do you have any update on this? I can take over this PR and investigate the idea further if you don't have time for it.
merge it?<br>
【Thanks】! Let me merge after https://github.com/HyukjinKwon/spark/actions/runs/4323411173/jobs/7547000388. Should take less then 20 mins.
Tests look ok, removed WIP flag.<br><br>cc @wangyum, @cloud-fan
@wangyum, @cloud-fan, do you think this PR is good enough or shall we go further with 【improvement】s?<br>The following 【feature】s from https://github.com/apache/spark/pull/24553 are not in this PR currently to keep the change 【simple】, but we can add them if needed:<br>- Extend propagation from attribute => constant mapping to deterministic expression => constant mapping:<br>  e.g. `abs(a) = 5 AND b = abs(a) + 3` => `abs(a) = 5 AND b = 8`<br>- Allow substitution in other than `BinaryComparison`s:<br>  e.g. `a = 5 AND abs(a) = 1` => `a = 5 AND abs(5) = 1`<br>- Allow propagation of constant non-nullable expressions in `Project` and other nodes:<br>  e.g. `SELECT a = 5 AND b = a + 3` => `SELECT a = 5 AND b = 8`<br>  (Currently only `Filter` is supported.)<br>- Allows deep constant propagation:<br>  e.g. `WHERE IF(..., a = 5 AND b = a + 3, ...)` => `WHERE IF(..., a = 5 AND b = 8, ...)`<br>  (Currently only top level `And`/`Or`/`Not` are supported.)<br><br>@cloud-fan, I know you had concerns: https://github.com/apache/spark/pull/40093#【discussion】_r1121416560 and actually I'm not sure either how much 【performance】 【improvement】 we could expect from the above 4 in real-life usecases...
@cloud-fan, @wangyum please let me know if this PR needs further 【improvement】s.
@grundprinzip 【Thanks】 for the work. +1 for the approach.<br>Could you point out where is exactly the same as the PR https://github.com/apache/spark-website/pull/359 so that we can 【review】 this PR easier?<br><br>Also, I notice the tabs need adjustment:<br>this PR: <br><img width=\"927\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/222872786-faa168df-ffcb-428e-9f05-2d4dc8912d1e.png\"><br><br>current:<br><img width=\"963\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/222872764-37bd48fd-75c1-48a8-b781-f46acfbdb273.png\"><br><br><br>
There is also differences on the top bar and left menu when scrolling down the page:<br>Take https://spark.apache.org/docs/3.3.2/sql-ref-ansi-compliance.html as an example, of this PR:<br><img width=\"1139\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/222872915-e246493b-4dd6-45ce-b2e7-91f3aaa22944.png\"><br><br>Current:<br><img width=\"787\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/222872929-fef162d6-f033-4939-9f1b-043f16226f8e.png\"><br>
@gengliangwang I 【<font color=blue>fix】ed】 the issues you mentioned, I left an explanation in the PR desc to explain what exactly I did, but I'm not sure what you mean with:<br><br>> Could you point out where is exactly the same as the PR https://github.com/apache/spark-website/pull/359 so that we can 【review】 this PR easier?
@grundprinzip This one 【LGTM】 overall. Could you create a Spark jira for it and update the PR title?
【Thanks】, merging to master
As the #40456 has been completed, will resume this one. Let me close this PR for convenience and open a new one.
Revisit PR: https://github.com/apache/spark/pull/40507
Mind retriggering the build, please? Probably 【simple】st way to do is pushing an empty commit. You can retrigger the build in your fork but it won't be reflected here.
> Mind retriggering the build, please? Probably 【simple】st way to do is pushing an empty commit. You can retrigger the build in your fork but it won't be reflected here.<br><br>Sure done
It's taking lot much longer than usual. I've just pushed in my repo as well. Let's see the result.<br>https://github.com/HeartSaVioR/spark/actions/runs/4341254564/jobs/7580632902<br>
https://github.com/anishshri-db/spark/actions/runs/4339563807/jobs/7577302766<br><br>Here it failed \"only\" with [Run / Build modules: sql - other 【test】s](https://github.com/anishshri-db/spark/actions/runs/4339563807/jobs/7577290837#logs)<br><br>https://github.com/HeartSaVioR/spark/actions/runs/4341254564/jobs/7580668669<br><br>Here [Run / Build modules: sql - other 【test】s](https://github.com/HeartSaVioR/spark/actions/runs/4341254564/jobs/7580648546) succeeded. Although there's other failure in different module, it seems like flaky one, rather than consistent one.
@hvanhovell <br>cc @LuciferYang
The full error (even with the 【clean】 master branch):<br>```<br>build/mvn 【clean】<br>build/mvn -Pscala-2.13 compile -pl connector/connect/client/jvm -am -DskipTests<br>build/mvn -Pscala-2.13 【test】 -pl connector/connect/client/jvm -> errored here<br><br>[ERROR] [Error] /Users/zhen.li/code/spark-sbt/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/util/ConvertToArrow.scala:28: object arrow is not a member of package org.apache.spark.sql.execution<br>[ERROR] [Error] /Users/zhen.li/code/spark-sbt/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/util/ConvertToArrow.scala:46: not found: type ArrowWriter<br>[ERROR] [Error] /Users/zhen.li/code/spark-sbt/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/util/ConvertToArrow.scala:46: not found: 【value】 ArrowWriter<br>```<br><br>
【Thanks】 for your work @zhenlineo <br>If you don't mind, please give me more time to think about this pr ：）<br><br>
In the pr description, `build/mvn compile -pl connector/connect/client/jvm` should be `build/mvn compile -pl connector/connect/client/jvm -am` ?<br><br>
On the whole, it is good for me. There is only one question. Spark still uses maven for version release and deploy. But after this pr, the E2E 【test】 change to use sbt assembly server jar instead of maven shaded server jar for 【test】ing, which may weaken the maven 【test】. We may need other ways to ensure the correctness of maven shaded server jar.<br><br>In the future, we may use sbt to completely replace maven(should not be in Spark 3.4.0), including version release, deploy and other help tools, which will no longer be a problem at that time.<br><br><br>
There is another problem that needs to be confirmed, which may not related to current pr: if other Suites inherit `RemoteSparkSession`, they will share the same connect server, right? (`SparkConnectServerUtils` is an object, so `SparkConnect` will only submit once)<br><br>
@LuciferYang 【Thanks】 for your 【review】. This PR was trying to simplify the 【test】 running steps. But as you said it make the maven commands to call sbt implicitly. I will split the 【c<font color=blue>hang】es】 into smaller PRs to allow this PR only deal with the IT command change. Then we can votes if we like this change or not 【:)】
https://github.com/apache/spark/pull/40304 https://github.com/apache/spark/pull/40303
seems `SimpleSparkConnectService` startup failed, the error message is <br><br>```<br>Error: Missing application resource.<br><br>Usage: spark-submit [options] <app jar | python file | R file> [app arguments]<br>Usage: spark-submit --kill [submission ID] --master [spark://...]<br>Usage: spark-submit --status [submission ID] --master [spark://...]<br>Usage: spark-submit run-example [options] example-class [example args]<br><br>Options:<br>  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,<br>                              k8s://https://host:port, or local (Default: local[*]).<br>  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or<br>                              on one of the worker machines inside the cluster (\"cluster\")<br>                              (Default: client).<br>  --class CLASS_NAME          Your application's main class (for Java / Scala apps).<br>  --name NAME                 A name of your application.<br>  --jars JARS                 Comma-separated list of jars to include on the driver<br>...<br>```
> seems `SimpleSparkConnectService` startup failed, the error message is<br>> <br>> ```<br>> Error: Missing application resource.<br>> <br>> Usage: spark-submit [options] <app jar | python file | R file> [app arguments]<br>> Usage: spark-submit --kill [submission ID] --master [spark://...]<br>> Usage: spark-submit --status [submission ID] --master [spark://...]<br>> Usage: spark-submit run-example [options] example-class [example args]<br>> <br>> Options:<br>>   --master MASTER_URL         spark://host:port, mesos://host:port, yarn,<br>>                               k8s://https://host:port, or local (Default: local[*]).<br>>   --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or<br>>                               on one of the worker machines inside the cluster (\"cluster\")<br>>                               (Default: client).<br>>   --class CLASS_NAME          Your application's main class (for Java / Scala apps).<br>>   --name NAME                 A name of your application.<br>>   --jars JARS                 Comma-separated list of jars to include on the driver<br>> ...<br>> ```<br><br>Yeah, this was caused by the bug we had in the scripts.<br>
@hvanhovell Want to keep this or shall we skip? It helps a bit when not knowing `build/sbt -Pconnect -Phive package` before running the IT.
@LuciferYang I want support the similar `withSQLConf`.
@LuciferYang Thank you.
ping @HyukjinKwon @zhengruifeng @dongjoon-hyun
@hvanhovell @LuciferYang Thank you.
I don't know the internal of Parser well, but I guess if we want to reach 100% 【compatibility】, we may need to reuse the `.g4` files and implement a subset of  `AstBuilder` to support `singleDataType` and `singleTableSchema`.<br><br>For example, the DDL also support `NOT NULL` and `COMMENT`:<br>```<br>>>> df = spark.createDataFrame([], \"\"\"a string  COMMENT 'this is just a 【simple】 string' \"\"\")<br>>>> df.schema<br>StructType([StructField('a', StringType(), True)])<br>>>> df = spark.createDataFrame([], \"\"\"a string  NOT NULL COMMENT 'this is just a 【simple】 string' \"\"\")<br>>>> df.schema<br>StructType([StructField('a', StringType(), False)])<br>```<br><br>cc @hvanhovell @cloud-fan @HyukjinKwon  WDYT?
does it mean every spark connect client must implement a data type parser in its language? This seems a bit overkill. Can we revisit all the places that need to parse data type at client side, and see if we can delay it to the server side?
At the end of the day it is an 【optimization】. However I do think it is a sound one to have.
Close this in favor of #40260.
ping @hvanhovell @HyukjinKwon @dongjoon-hyun  cc @LuciferYang
@hvanhovell Do you have any other advice? cc @HyukjinKwon @zhengruifeng @dongjoon-hyun
@hvanhovell @zhengruifeng Thank you.
### Full stack:<br><br>INTERNAL: <br>[PARSE_SYNTAX_ERROR] Syntax error at or near '<': extra input '<'.(line 1, pos 6)<br><br>== SQL ==<br>struct<c1:struct<c1-1:string,c1-2:string>><br>------^^^<br><br>io.grpc.StatusRuntimeException: INTERNAL: <br>[PARSE_SYNTAX_ERROR] Syntax error at or near '<': extra input '<'.(line 1, pos 6)<br><br>== SQL ==<br>struct<c1:struct<c1-1:string,c1-2:string>><br>------^^^<br><br>\tat io.grpc.Status.asRuntimeException(Status.java:535)<br>\tat io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>\tat org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:61)<br>\tat org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:106)<br>\tat org.apache.spark.sql.Dataset.$anonfun$show$2(Dataset.scala:529)<br>\tat org.apache.spark.sql.Dataset.$anonfun$show$2$adapted(Dataset.scala:528)<br>\tat org.apache.spark.sql.Dataset.withResult(Dataset.scala:2752)<br>\tat org.apache.spark.sql.Dataset.show(Dataset.scala:528)<br>\tat org.apache.spark.sql.Dataset.show(Dataset.scala:444)<br>\tat org.apache.spark.sql.Dataset.show(Dataset.scala:399)<br>\tat org.apache.spark.sql.Dataset.show(Dataset.scala:408)<br>\tat org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$85(ClientE2ETestSuite.scala:608)<br>\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>\tat org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>\tat org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>\tat org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>\tat org.scala【test】.Transformer.apply(Transformer.scala:22)<br>\tat org.scala【test】.Transformer.apply(Transformer.scala:20)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>\tat org.scala【test】.TestSuite.withFixture(TestSuite.scala:196)<br>\tat org.scala【test】.TestSuite.withFixture$(TestSuite.scala:195)<br>\tat org.scala【test】.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>\tat org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>\tat org.scala【test】.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>\tat org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>\tat scala.collection.immutable.List.foreach(List.scala:431)<br>\tat org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>\tat org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>\tat org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>\tat org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>\tat org.scala【test】.Suite.run(Suite.scala:1114)<br>\tat org.scala【test】.Suite.run$(Suite.scala:1096)<br>\tat org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>\tat org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>\tat org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>\tat org.apache.spark.sql.ClientE2ETestSuite.org$scala【test】$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:34)<br>\tat org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>\tat org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>\tat org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>\tat org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:34)<br>\tat org.scala【test】.tools.SuiteRunner.run(SuiteRunner.scala:47)<br>\tat org.scala【test】.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1321)<br>\tat org.scala【test】.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1315)<br>\tat scala.collection.immutable.List.foreach(List.scala:431)<br>\tat org.scala【test】.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1315)<br>\tat org.scala【test】.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:992)<br>\tat org.scala【test】.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:970)<br>\tat org.scala【test】.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1481)<br>\tat org.scala【test】.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:970)<br>\tat org.scala【test】.tools.Runner$.run(Runner.scala:798)<br>\tat org.scala【test】.tools.Runner.run(Runner.scala)<br>\tat org.jetbrains.plugins.scala.【test】ingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:43)<br>\tat org.jetbrains.plugins.scala.【test】ingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:26)<br>
【Thanks】 @panbingkun for the nice fix!<br>Btw, think I found another `createDataFrame` bug which is not working properly with non-nullable schema as below:<br>```python<br>>>> from pyspark.sql.types import *<br>>>> schema_false = StructType([StructField(\"id\ IntegerType(), False)])<br>>>> spark.createDataFrame([[1]], schema=schema_false)<br>Traceback (most recent call last):<br>...<br>pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.<br>```<br>whereas working find with nullable schema as below:<br>```python<br>>>> schema_true = StructType([StructField(\"id\ IntegerType(), True)])<br>>>> spark.createDataFrame([[1]], schema=schema_true)<br>DataFrame[id: int]<br>```<br><br>Do you have any idea what might be causing this? Could you take a look at it if you're interested in? I have filed an issue at SPARK-42679.<br><br>Also cc @hvanhovell as an original author for `createDataFrame`.
> 【Thanks】 @panbingkun for the nice fix! Btw, think I found another `createDataFrame` bug which is not working properly with non-nullable schema as below:<br>> <br>> ```python<br>> >>> from pyspark.sql.types import *<br>> >>> schema_false = StructType([StructField(\"id\ IntegerType(), False)])<br>> >>> spark.createDataFrame([[1]], schema=schema_false)<br>> Traceback (most recent call last):<br>> ...<br>> pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.<br>> ```<br>> <br>> whereas working find with nullable schema as below:<br>> <br>> ```python<br>> >>> schema_true = StructType([StructField(\"id\ IntegerType(), True)])<br>> >>> spark.createDataFrame([[1]], schema=schema_true)<br>> DataFrame[id: int]<br>> ```<br>> <br>> Do you have any idea what might be causing this? Could you take a look at it if you're interested in? I have filed an issue at [SPARK-42679](https://issues.apache.org/jira/browse/SPARK-42679).<br>> <br>> Also cc @hvanhovell as an original author for `createDataFrame`.<br><br>Let me try to investigate it.
【Thanks】, @panbingkun !<br>By the way, I think this issue has a pretty high priority since the default nullability of a schema is `False`.<br><br>```python<br>>>> sdf = spark.range(10).schema<br>self._schema: StructType([StructField('id', LongType(), False)])<br>```<br><br>For example, even 【intuitive】 and 【simple】 code like creating a DataFrame from a pandas DataFrame fails as follows:<br>```python<br>>>> sdf = spark.range(10)<br>>>> pdf = sdf.toPandas()<br>>>> spark.createDataFrame(pdf, sdf.schema)<br>Traceback (most recent call last):<br>...<br>pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.<br>```<br><br>Please feel free to ping me anytime if you need any help!<br>【Thanks】 again for your time on investigating this :-)
> 【Thanks】, @panbingkun ! By the way, I think this issue has a pretty high priority since the default nullability of a schema is `False`.<br>> <br>> ```python<br>> >>> sdf = spark.range(10).schema<br>> self._schema: StructType([StructField('id', LongType(), False)])<br>> ```<br>> <br>> For example, even 【intuitive】 and 【simple】 code like creating a DataFrame from a pandas DataFrame fails as follows:<br>> <br>> ```python<br>> >>> sdf = spark.range(10)<br>> >>> pdf = sdf.toPandas()<br>> >>> spark.createDataFrame(pdf, sdf.schema)<br>> Traceback (most recent call last):<br>> ...<br>> pyspark.errors.exceptions.connect.AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `id` is nullable while it's required to be non-nullable.<br>> ```<br>> <br>> Please feel free to ping me anytime if you need any help! 【Thanks】 again for your time on investigating this :-)<br><br><br>I have found root cause. Let me think about how to fix it.<br>Temporary solutions: https://github.com/apache/spark/pull/40316
Awesome!! Let me take a look at your PR when it's ready.<br>【Thanks】!
cc @mridulm @Ngone51 @ulysses-you
【Thanks】 @HyukjinKwon @LuciferYang
cc @MaxGekk and @srielau
Just created ticket for SQL side: SPARK-42706 FYI.
Documentation for SQL side is get merged from https://github.com/apache/spark/pull/40336.<br><br>Note that Python side are 【simple】r compared to SQL side because we do not have SQLSTATE, and there is currently no main error class with sub-error classes. Also, the overall volume of errors is not as high as in SQL documents.
Reminder for @HyukjinKwon @srielau @MaxGekk for error class document for PySpark.
for example:<br><br>- https://github.com/apache/spark/actions/runs/4329598884/jobs/7560252913<br>- https://github.com/apache/spark/actions/runs/4329598884/jobs/7560252970<br>- https://github.com/apache/spark/actions/runs/4329004998/jobs/7559232726<br>- https://github.com/apache/spark/actions/runs/4329004998/jobs/7559232794<br><br><img width=\"1241\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/222950677-95c48561-924d-45c7-a59b-23f66a997af3.png\"><br>
For 【check】, I add a `./build/mvn -version` before in `java-11-17` GA task without this pr:<br><br><img width=\"1682\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/222950915-3dcbf4f1-6f00-4e34-a003-936197cdef57.png\"><br><br>And it print as follows:<br><br>https://github.com/LuciferYang/spark/actions/runs/4328951282/jobs/7559134224<br><br><br><img width=\"1196\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/222950899-11ca9796-11b4-400f-9a89-5f22b9367b2b.png\"><br>
no error message with this pr <br><br>- https://github.com/LuciferYang/spark/actions/runs/4335123778/jobs/7569484003<br>- https://github.com/LuciferYang/spark/actions/runs/4335123778/jobs/7569484044
also cc @HyukjinKwon
there is a known issue in Maven 3.9.0 (related to plexus-utils XML stricter reading https://github.com/codehaus-plexus/plexus-utils/issues/238 ) that is 【<font color=blue>fix】ed】 in 3.9.1-SNAPSHOT: https://issues.apache.org/jira/browse/MNG-7697<br><br>3.9.1 will be released soon: can you eventually 【check】 with 3.9.1-SNAPSHOT if you're in a different case of this \"too strict\" XML parsing?
[@cstamas ](https://github.com/cstamas) do you know if the lax parsing covers that `org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42) `?
> https://issues.apache.org/jira/browse/MNG-7697<br><br>OK, let me 【test】 3.9.1-SNAPSHOT later. @pan3793 Do you have any other issues besides those in GA task?<br><br>
> [@cstamas ](https://github.com/cstamas) do you know if the lax parsing covers that `org.codehaus.plexus.util.xml.pull.XmlPullParserException: UTF-8 BOM plus xml decl of ISO-8859-1 is incompatible (position: START_DOCUMENT seen <?xml version=\"1.0\" encoding=\"ISO-8859-1\"... @1:42) `?<br><br>It does not.  This is a separate issue which is not actually a problem in maven or plexus-utils.  See a similar fix for it in https://github.com/takari/polyglot-maven/commit/14514b672a07ec0ba582574efdcf913e308c9e91<br>It has to be 【<font color=blue>fix】ed】 in the `org.cyclonedx.maven.BaseCycloneDxMojo.readPom` method.
oh, I did not see that Spark was still using cyclonedx-maven-plugin old 2.7.3, thank you @gnodet: @LuciferYang @pan3793 you should 【upgrade】 to 2.7.5, which has completely changed the implementation and should not have the issue
> oh, I did not see that Spark was still using cyclonedx-maven-plugin old 2.7.3, thank you @gnodet: @LuciferYang @pan3793 you should 【upgrade】 to 2.7.5, which has completely changed the implementation and should not have the issue<br><br>【Thanks】 for you suggestion, in https://github.com/apache/spark/pull/40065 I discussed with @dongjoon-hyun , I know 2.7.5 【<font color=blue>fix】ed】 these issues, but 2.7.5 has other issues like https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/284 and https://github.com/CycloneDX/cyclonedx-maven-plugin/issues/289,   so we hope wait until at least 2.7.6 to 【upgrade】 `cyclonedx-maven-plugin`
【Thanks】 @dongjoon-hyun @pan3793 ~<br>Also thanks @gnodet @hboutemy
【Thanks】 @wangyum
Hi @mridulm comments addressed in the la【test】 iteration, please take a look. 【Thanks】.
> Just a doc change I had missed last time around. Rest looks good to me - can you 【check】 the proposed change, and reformulate it to something similar ? I will merge it once done<br><br>Hi @mridulm thanks a lot for the suggestion. Updated.
Can you update to la【test】 @ivoson ?<br>The style failure is not related to your change, but blocks build
> Can you update to la【test】 @ivoson ? The style failure is not related to your change, but blocks build<br><br>Hi @mridulm , branch rebased. Please take a look. 【Thanks】.
Merged to master.<br>【Thanks】 for fixing this @ivoson !
【Thanks】 for the 【review】. @mridulm
@HyukjinKwon @zhengruifeng the rationale for this change is that analyzer takes care of making lambda variables 【unique】.
@hvanhovell After my 【test】, `python/run-【test】s --【test】names 'pyspark.sql.【test】s.【test】_functions'` will not passed.<br>
I guess we will need to rewrite the lamda function in spark connect planner.<br><br>cc @ueshin as well, since existing implementation follows the fix in https://github.com/apache/spark/pull/32523
![image](https://user-images.githubusercontent.com/8486025/223014232-bf9b26ee-d0e8-4de4-a8fe-2d252813ac4d.png)<br>
It seems pyspark supports the nested lambda variables and two PR fix the issue.
@beliefer scala does support nested lambda variables as well, and they actually work. So either (my) 【test】ing on the scala side is incomplete (which might well be the case), or something weird is going on here.
@hvanhovell Scala also uses `UnresolvedNamedLambdaVariable.freshVarName(\"x\")` to get the 【unique】 names. see: <br>https://github.com/apache/spark/blob/201e08c03a31c763e3120540ac1b1ca8ef252e6b/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L4096
@beliefer here is the thing. When this was 【design】ed it was mainly aimed at sql, and there we definitely do not generate 【unique】 names in lambda functions either. This is all done in the analyzer. We should be able to follow the same path.<br><br>Do you happen to know if 【test】 failing for python also fail for scala?
> @beliefer here is the thing. When this was 【design】ed it was mainly aimed at sql, and there we definitely do not generate 【unique】 names in lambda functions either. This is all done in the analyzer. We should be able to follow the same path.<br>><br><br>It seems only the lambda functions in SQL will be transformed with analyzer. But the scala, pyspark API will not through analyzer.<br>
Ehhhh... SQL/scala/Python all use the analyzer; they are all just frontends to the same thing.
> Ehhhh... SQL/scala/Python all use the analyzer; they are all just frontends to the same thing.<br><br>I found the reason. Although the scala API use analyzer too. `object ResolveLambdaVariables extends Rule[LogicalPlan]` can't fix the issue.<br><br>If I removed the` UnresolvedNamedLambdaVariable.freshVarName(...)`<br><br>![image](https://user-images.githubusercontent.com/8486025/223439632-cd7dcd93-c60d-4e7c-844b-fddb89d00bec.png)<br><br>and 【test】 the case, see at: https://github.com/apache/spark/blob/2e7207f96e1ff848def135de63f63bcda7402517/sql/core/src/【test】/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala#L5250<br>![image](https://user-images.githubusercontent.com/8486025/223439443-efa6346d-a829-4dd2-a430-df5f24fbd819.png)<br><br>The related PR.<br>https://github.com/apache/spark/pull/32424
@hvanhovell Do we still need this change ?
If the nested lambda issue also exists in the Scala Client, do we need to fix it in the same way?
cc @tgravescs since this is a Spark Connect introduction including a note about built in authentication you [mentioned in JIRA ticket](https://issues.apache.org/jira/browse/SPARK-42374?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=17687978) before.
cc @allanf-db addressed the comments we discussed in offline
Also cc @HyukjinKwon
Let me close this for now, since the contents in this PR will be included in the future Spark Connect documents soon.<br>cc @allanf-db  FYI
cc @cloud-fan @dongjoon-hyun
Merged to branch-3.3. Thank you, @Yikf and @cloud-fan .
@hvanhovell It seems that add 【test】 cases no way.
hmmm - let me think about it.
@beliefer we should be able to create an in-memory table and append a couple of rows to that right?
@beliefer are you abandoning this one?
> @beliefer are you abandoning this one?<br><br>Because other PR implement this function.
Is that https://github.com/apache/spark/pull/40415?
> Is that #40415?<br><br>It is https://github.com/apache/spark/pull/40358
【LGTM】, merged into master/3.4
@cloud-fan @sunchao
Close it, because this change may have potential data issue. Users can `set spark.sql.legacy.typeCoercion.datetimeToString.enabled` to `true` to restore the old behavior.<br>
build timed out but succeeded on rerun: https://github.com/vitaliili-db/spark/actions/runs/4346311324/jobs/7598960402
@gengliangwang can you 【review】 this please?
@vitaliili-db 【Thanks】 for the work! <br>【LGTM】 overall. I got only one comment: the wording \"column options\" sounds weird. Shall we call it \"column descriptor\"?<br><img width=\"763\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/223539786-da8c29d9-de24-495d-add5-3575687d8a2e.png\"><br>
@gengliangwang 【great】 catch, yes, we should follow standard. Renamed.
ping @hvanhovell @zhengruifeng @HyukjinKwon
@beliefer I think it's not a `new 【feature】s` mentioned in the PR description
@HyukjinKwon @zhengruifeng Thank you.
cc @gengliangwang @dtenedor
thanks for 【review】, merging to master/3.4!
It's a good idea to provide an API that allows people to unambiguously reference metadata columns, and I like the new `Dataset.metadataColumn` function. However, I think the prepending underscore approach is a bit hacky. It's too implicit and I'd prefer a more explicit syntax like `SELECT metadata(_metadata) FROM t`. We can discuss this more and invite more SQL experts. Shall we exclude it from this PR for now?
> It's a good idea to provide an API that allows people to unambiguously reference metadata columns, and I like the new `Dataset.metadataColumn` function. However, I think the prepending underscore approach is a bit hacky. It's too implicit and I'd prefer a more explicit syntax like `SELECT metadata(_metadata) FROM t`. We can discuss this more and invite more SQL experts. Shall we exclude it from this PR for now?<br><br>@cloud-fan The prepended underscore is _NOT_ primarily intended as a user surface. Rather, it's a reliale way to get a 【unique】 column name that's still at least somewhat 【readable】 if you look at the query plan (unlike e.g. a uuid). The new `Dataset.metadataColumn` method does not even _look_ at a renamed attribute's name, for example.<br><br>I updated the PR description to not mention the 【specific】 renaming mechanism, and to indicate that a SQL user surface is out of scope.<br><br>At this point, the only remaining reference to prepended underscores is the two unit 【test】s (\"metadata name conflict resolved with leading underscores\"), which validate that the renaming reliably produces 【unique】 names as intended. If you don't think the 【test】 coverage is important, we could remove even that?
about https://github.com/apache/spark/pull/40300/files#r1129818813 , I think if `SubqueryAlias` can't propagate metadata columns, then `df.metadataColumn` should not be able to get the column, what do you think? @ryan-johnson-databricks
> about https://github.com/apache/spark/pull/40300/files#r1129818813 , I think if `SubqueryAlias` can't propagate metadata columns, then `df.metadataColumn` should not be able to get the column, what do you think? @ryan-johnson-databricks<br><br>IMO changing the behavior of `SubqueryAlias` (and other 【specific】 plan node types) is out of scope for this PR -- this PR does not change that existing situation.
Mind retriggering https://github.com/alkis/spark/runs/11797022157?
> Mind retriggering https://github.com/alkis/spark/runs/11797022157?<br><br>Done.
Would the following code achieve the same 【improvement】s?<br>```<br>if (logger.isDebugEnabled()) {<br>  logger.debug(\"string {}\ <expensive_function>);<br>}<br>```
@pan3793's suggestion is 【simple】r and less change.
Good idea! Done.
Or even better? -> https://github.com/apache/spark/pull/40305
cc @HyukjinKwon , can we merge this first before new RC? otherwise, the maven 【test】 will still fail<br><br>
【Thanks】 @HyukjinKwon 【:)】
If this PR accepted then no need to merge https://github.com/apache/spark/pull/40303 as this PR override the 【c<font color=blue>hang】es】 needed there.
【Thanks】, merging to master!
This is still WIP, but want to get early feedback.<br>+CC @Ngone51, @otterc, @waitinfuture
We are evaluating it currently @dongjoon-hyun :-)
If you don't mind, please share some results later~ 【:)】
The 【test】 failure is unrelated, so existing 【test】s work fine - will work on 【specific】ally 【check】ing for the 【c<font color=blue>hang】es】 in this PR later today.
Hi @mridulm , thanks for your 【great】 work! Apache Uniffle is similar project to Apache Celeborn.  We also patched to the Apache Spark like https://github.com/apache/incubator-uniffle/blob/master/spark-patches/spark-3.2.1_【dynamic】_allocation_support.patch.  Considering that the shuffle data is stored in in 【distributed】 filesystem or in a disaggregated shuffle cluster, maybe  we should modify the method `ShuffledRowRDD#getPreferredLocations`, too.
@jerqi locality may still have benefits when RSS works in hybrid deployments, besides, there is a dedicated configuration for that `spark.shuffle.reduceLocality.enabled`
> spark.shuffle.reduceLocality.enabled<br><br>【Thanks】, I got it. But if we want to use the locality when RSS works in hybrid deployments, should we expose an interface for ShuffleDriverComponent to provide `getPreferredLocationsForShuffle`.
> @jerqi locality may still have benefits when RSS works in hybrid deployments, besides, there is a dedicated configuration for that `spark.shuffle.reduceLocality.enabled`<br><br>`ShuffledRdd#getPreferredLocations` will call the method `MapOutputTracker#getPreferredLocationsForShuffle` and `MapOutputTracker#getMapLocation`.<br>For `MapOutpuTracker#getMapLocation`, `spark.shuffle.reduceLocality.enabled` is useless.
> This is still WIP, but want to get early feedback. +CC @Ngone51, @otterc, @waitinfuture<br><br>Hi @mridulm , thanks for the work and it really simplifies the usage of Apache Celeborn (and other similar systems)! This patch looks good to me.<br><br>I totally agree with @pan3793 that in hybrid deployed environment, it'll benefit if locality can be exploited, and it's among future works of Apache Celeborn. Maybe we can add locality-related interface in ShuffleManager, because that is the base class that remote shuffle services overwrite.
@jerqi Agree that we should have a way to specify locality preference for disaggregated shuffle implementations to spark scheduler - so that shuffle tasks are closer to the data.<br><br>@otterc is relooking at SPARK-25299, given the platform 【c<font color=blue>hang】es】 that have gone in since it was proposed - and propose 【c<font color=blue>hang】es】 in the upcoming months. We can add this to that 【discussion】/jira.<br><br>Hope that sounds fine.<br>For now, as @pan3793 suggested, turning off reduce locality for Uniffle should remove the need for the code patch (once this PR is merged) - if there are gaps, we can address those 【specific】ally to enforce the behavior of `spark.shuffle.reduceLocality.enabled`.
> > @jerqi locality may still have benefits when RSS works in hybrid deployments, besides, there is a dedicated configuration for that `spark.shuffle.reduceLocality.enabled`<br>> <br>> `ShuffledRdd#getPreferredLocations` will call the method `MapOutputTracker#getPreferredLocationsForShuffle` and `MapOutputTracker#getMapLocation`. For `MapOutpuTracker#getMapLocation`, `spark.shuffle.reduceLocality.enabled` is useless.<br><br>My mistake. I means  `ShuffledRowRdd#getPreferredLocations` instead of `ShuffledRdd#getPreferredLocations`
> @jerqi Agree that we should have a way to specify locality preference for disaggregated shuffle implementations to spark scheduler - so that shuffle tasks are closer to the data.<br>> <br>> @otterc is relooking at [SPARK-25299](https://issues.apache.org/jira/browse/SPARK-25299), given the platform 【c<font color=blue>hang】es】 that have gone in since it was proposed - and propose 【c<font color=blue>hang】es】 in the upcoming months. We can add this to that 【discussion】/jira.<br>> <br>> Hope that sounds fine. For now, as @pan3793 suggested, turning off reduce locality for Uniffle should remove the need for the code patch (once this PR is merged) - if there are gaps, we can address those 【specific】ally to enforce the behavior of `spark.shuffle.reduceLocality.enabled`.<br><br>There is some gaps. If we store data in the DFS or in disaggregated shuffle cluster. We should return `Nil` in the  `ShuffledRowRDD#getPreferredLocations`. For `CoalescedPartitionSpec`,  `spark.shuffle.reduceLocality.enabled` will help solve this problem. For `PartialReducerPartitionSpec`, `PartialMapperPartitionSpec` and `CoalescedMapperPartitionSpec `,  `spark.shuffle.reduceLocality.enabled`  can't help us. Because we call the method `MapOutpuTracker#getMapLocation`.<br>Could we modify this method in this pr like?<br>```<br>  def getMapLocation(<br>      dep: ShuffleDependency[_, _, _],<br>      startMapIndex: Int,<br>      endMapIndex: Int): Seq[String] =<br>  {<br>    val shuffleStatus = shuffleStatuses.get(dep.shuffleId).orNull<br>    if (shuffleStatus != null || shuffleLocalityEnabled) {<br>      shuffleStatus.withMapStatuses { statuses =><br>        if (startMapIndex < endMapIndex &&<br>          (startMapIndex >= 0 && endMapIndex <= statuses.length)) {<br>          val statusesPicked = statuses.slice(startMapIndex, endMapIndex).filter(_ != null)<br>          statusesPicked.map(_.location.host).toSeq<br>        } else {<br>          Nil<br>        }<br>      }<br>    } else {<br>      Nil<br>    }<br>  }<br>```<br>Or we should raise an another pr to fix this issue?
@jerqi the basic issue here is, `getPreferredLocations` in `ShuffledRowRDD` should return `Nil` at the very beginning in case `spark.shuffle.reduceLocality.enabled = false` (conceptually).<br><br>This logic is pushed into MapOutputTracker though - and `getPreferredLocationsForShuffle` honors `spark.shuffle.reduceLocality.enabled` - but `getMapLocation` does not.<br><br>So the fix would be to fix `getMapLocation` to honor the parameter.<br><br>We should fix this in a different PR though.
> @jerqi the basic issue here is, `getPreferredLocations` in `ShuffledRowRDD` should return `Nil` at the very beginning in case `spark.shuffle.reduceLocality.enabled = false` (conceptually).<br>> <br>> This logic is pushed into MapOutputTracker though - and `getPreferredLocationsForShuffle` honors `spark.shuffle.reduceLocality.enabled` - but `getMapLocation` does not.<br>> <br>> So the fix would be to fix `getMapLocation` to honor the parameter.<br>> <br>> We should fix this in a different PR though.<br><br>> <br><br>Could I raise another pr to fix this issue?
Sure ! Please go ahead :-)
Added 【test】s, will wait for CI to complete.<br><br>+CC @otterc, @Ngone51
【Thanks】 for the 【review】 @dongjoon-hyun ! Really helpful
All 【test】s passed, merging to master.
【Thanks】 for all the 【review】s @dongjoon-hyun, @otterc, @jerqi, @pan3793 and @waitinfuture :-)
cc @huaxingao @cloud-fan @dongjoon-hyun @sunchao @viirya @gengliangwang
Failures don't seem to be related.
Failures in streaming 【test】s don't seem related.
【Thanks】 for 【review】ing, @cloud-fan @huaxingao @dongjoon-hyun @viirya @johanl-db!
cc @zhengruifeng @HyukjinKwon @grundprinzip @cloud-fan @hvanhovell
【LGTM】!
It’s a good 【improvement】, especially for RSS
cc @ulysses-you
+CC @shardulm94
cc @HyukjinKwon @srowen @dongjoon-hyun ,thanks
thanks @mridulm @srowen, merged to master/3.4/3.3/3.2
ping @cloud-fan @dongjoon-hyun @HyukjinKwon
> Hi, @AngersZhuuuu .<br>> This PR seems to have insufficient information. Could you provide more details about how to validate this in what environment?<br><br>We run a client mode SparkSubmit job and throw below exception<br>```<br>23/03/07 18:34:50 INFO YarnClientSchedulerBackend: Shutting down all executors<br>23/03/07 18:34:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down<br>23/03/07 18:34:50 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped<br>23/03/07 18:34:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!<br>23/03/07 18:34:50 INFO BlockManager: BlockManager stopped<br>23/03/07 18:34:50 INFO BlockManagerMaster: BlockManagerMaster stopped<br>23/03/07 18:34:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!<br>23/03/07 18:34:50 INFO SparkContext: Successfully stopped SparkContext<br>Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Table or view not found: xxx.xxx; line 1 pos 14;<br>'GlobalLimit 1<br>+- 'LocalLimit 1<br>   +- 'Project [*]<br>      +- 'UnresolvedRelation [xxx, xxx], [], false<br><br>\tat org.apache.spark.sql.catalyst.【analysis】.package$AnalysisErrorAt.failAnalysis(package.scala:42)<br>\tat org.apache.spark.sql.catalyst.【analysis】.CheckAnalysis.$anonfun$【check】Analysis$1(CheckAnalysis.scala:115)<br>\tat org.apache.spark.sql.catalyst.【analysis】.CheckAnalysis.$anonfun$【check】Analysis$1$adapted(CheckAnalysis.scala:95)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:184)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:183)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:183)<br>\tat scala.collection.immutable.List.foreach(List.scala:392)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:183)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:183)<br>\tat scala.collection.immutable.List.foreach(List.scala:392)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:183)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:183)<br>\tat scala.collection.immutable.List.foreach(List.scala:392)<br>\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)<br>\tat org.apache.spark.sql.catalyst.【analysis】.CheckAnalysis.【check】Analysis(CheckAnalysis.scala:95)<br>\tat org.apache.spark.sql.catalyst.【analysis】.CheckAnalysis.【check】Analysis$(CheckAnalysis.scala:92)<br>\tat org.apache.spark.sql.catalyst.【analysis】.Analyzer.【check】Analysis(Analyzer.scala:155)<br>\tat org.apache.spark.sql.catalyst.【analysis】.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:178)<br>\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)<br>\tat org.apache.spark.sql.catalyst.【analysis】.Analyzer.executeAndCheck(Analyzer.scala:175)<br>\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)<br>\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)<br>\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)<br>\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)<br>\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)<br>\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)<br>\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)<br>\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)<br>\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)<br>\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)<br>\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)<br>\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:621)<br>\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)<br>\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:616)<br>\tat org.apache.spark.sql.auth.QueryAuthChecker$.main(QueryAuthChecker.scala:33)<br>\tat org.apache.spark.sql.auth.QueryAuthChecker.main(QueryAuthChecker.scala)<br>\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br>\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)<br>\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)<br>\tat java.lang.reflect.Method.invoke(Method.java:498)<br>\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)<br>\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)<br>\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)<br>\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)<br>\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)<br>\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)<br>\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)<br>\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)<br>23/03/07 18:34:50 INFO ShutdownHookManager: Shutdown hook called<br>23/03/07 18:34:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-8ce833e1-3cd4-4a9f-960d-695be85b12f4<br>23/03/07 18:34:50 INFO ShutdownHookManager: Deleting directory /hadoop/spark/sparklocaldir/spark-58bbd530-6144-4ad6-b62b-a690baac9f96<br>23/03/07 18:34:50 INFO SparkExecutionPlanProcessor: Lineage thread pool prepares to shut down<br>23/03/07 18:34:50 INFO SparkExecutionPlanProcessor: Lineage thread pool finishes to await termination and shuts down<br><br>```<br><br><br>This job failed, but with call `sparkContext.stop()`, client side failed but in AM it shows SUCCESS<br>In spark-3.1.2 the code like this<br>```<br>    try {<br>      app.start(childArgs.toArray, sparkConf)<br>    } catch {<br>      case t: Throwable =><br>        throw findCause(t)<br>    } finally {<br>      if (!isShell(args.primaryResource) && !isSqlShell(args.mainClass) &&<br>        !isThriftServer(args.mainClass)) {<br>        try {<br>          SparkContext.getActive.foreach(_.stop())<br>        } catch {<br>          case e: Throwable => logError(s\"Failed to close SparkContext: $e\")<br>        }<br>      }<br>    }<br>```<br><br>So here for normal job, I think we should pass the exit code to SchedulerBackend, right?<br><br><br>Then after your mention, I see that https://github.com/apache/spark/pull/33403 change the behavior that only k8s call `sc.stop()`, then I think for k8s and yarn mode we booth need to pass the exit code the backend.<br><br>After this pr, we also need to 【check】 if k8s backend exit code is same as client side in client mode too.<br>
Does YARN still have this issue with Spark 3.4?
cc @mridulm and @tgravescs , too
> Does YARN still have this issue with Spark 3.4?<br><br>Didn't see such fix in current code.
This seems to be a revert of https://github.com/apache/spark/pull/33403 as now we stop SparkContext in YARN environment as well. We should justify it in the PR description. This is not simply passing the exitCode. Please update the PR title as well.
> This seems to be a revert of #33403 as now we stop SparkContext in YARN environment as well. We should justify it in the PR description. This is not simply passing the exitCode. Please update the PR title as well.<br><br>DOne
@dongjoon-hyun do you have more context about https://github.com/apache/spark/pull/33403? Why do we limit the stopping spark context behavior to k8s only?
Failed UT should not related to this pr.
@cloud-fan Seems this code https://github.com/apache/spark/pull/32283 first want to fix issue in k8s, then @dongjoon-hyun make it limit in k8s env. But this also can work for yarn env....
To @cloud-fan and all. Here is the full context.<br><br>- #32081 was the initial commit.<br>- The initial commit was reverted via ed3f103ee8 due to Hive Thrift Server failure, https://github.com/apache/spark/pull/32081#issuecomment-816399871 .<br>- The second commit of SPARK-34674 excludes all `Shell` and `STS` environments to avoid the UT failures.<br><br>Three months later after merging the second commit, there was a post-commit 【review】.<br>- https://github.com/apache/spark/pull/32283#【discussion】_r670881700<br><br>    ![Screenshot 2023-03-10 at 9 42 04 AM](https://user-images.githubusercontent.com/9700541/224386302-ba1780ab-8be3-461f-aa7d-01a58d964df8.png)<br><br>Since SPARK-34674 was released already to Spark 3.1.2, according to the post-commit comment, I made a new JIRA, SPARK-36193 (#33403) which was released as 3.1.3.<br>- [SPARK-36193][CORE] Recover SparkSubmit.runMain not to stop SparkContext in non-K8s env
@AngersZhuuuu can you comment on the original 【discussion】 thread and convince related people to add back `sc.stop`? https://github.com/apache/spark/pull/32081#【discussion】_r663434289
> @AngersZhuuuu can you comment on the original 【discussion】 thread and convince related people to add back `sc.stop`? [#32081 (comment)](https://github.com/apache/spark/pull/32081#【discussion】_r663434289)<br><br>TBH, you can think it as a new 【feature】, since they first just want to only support k8s, and this pr support yarn too
I believe people in that 【discussion】 thread have the most context (some of them are committers) and I'm not comfortable merging it without them taking a look.
ping @dongjoon-hyun @HyukjinKwon @attilapiros @srowen
ping @HyukjinKwon
@amaliujia Like current? also ping @HyukjinKwon
<img width=\"797\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/223446693-3c296b56-f9aa-4b70-9eb3-5bc9059ba631.png\"><br>
cc @itholic
## Root cause<br>The data type nullable inferred from the data is true by default<br>However, on the connector server side, `to(schema: StructType)` is called, and then `Project.matchSchema (logicalPlan, schema, sparkSession. sessionState. conf)` is executed
Hi @panbingkun, Good catch and thanks for working on this!<br>But I'm afraid I don't think this is a good direction. It won't fix the other cases.<br>Let me take a look at this issue and submit a PR later.
> Hi @panbingkun, Good catch and thanks for working on this! But I'm afraid I don't think this is a good direction. It won't fix the other cases. Let me take a look at this issue and submit a PR later.<br><br>Okay.
I submitted the PR #40382.
cc @HyukjinKwon fix a maven 【test】 failed of connect server module due to dependency loss<br><br>
@HyukjinKwon @gatorsmile @cloud-fan @dongjoon-hyun this is the 101st we have broken the maven build in the last month alone. We don't 【test】 with it, but we do feel comfortable to release with it. Are we sure the dual build setup is still a thing we want to pursue? Isn't it time to start thinking about greener pastures...
> this is the 101st we have broken the maven build in the last month alone. We don't 【test】 with it, but we do feel comfortable to release with it. Are we sure the dual build setup is still a thing we want to pursue? Isn't it time to start thinking about greener pastures...<br><br>Personally, I think we should consider migrating the build to sbt only in Spark 3.5.0, also cc @pan3793 <br><br>
For the following @hvanhovell 's question, I'd ask @srowen 's advice.<br>> this is the 101st we have broken the maven build in the last month alone. We don't 【test】 with it, but we do feel comfortable to release with it. Are we sure the dual build setup is still a thing we want to pursue? Isn't it time to start thinking about greener pastures...<br><br>
While I'm a Maven person myself, I above all have also long preferred one build over two, and an SBT build is fine with me. I actually can't recall if there were strong reasons to keep one or the other in the past; seems like there were Reasons we needed both. We do use mvn in a few places in scripts to parse out stuff about the build, like to determine when new dependencies have been added. mvn and sbt dependency resolution aren't the same, but all the more reason to just pick one, even if we have to migrate some scripts
re-triggered GA
@LuciferYang Thank you for the job. https://github.com/apache/spark/pull/40291 need this one.
【Thanks】 @HyukjinKwon @hvanhovell @dongjoon-hyun @srowen @beliefer
【Thanks】 @hvanhovell @amaliujia
@justaparth 【Thanks】 for the 【contribution】. Please create a JIRA ticket on https://issues.apache.org/jira/projects/SPARK/【summary】 and change the title to <br><br>`[SPARK-?????][DOC] Update code example formatting for protobuf parsing readme` <br><br>【Thanks】.
Something went wrong with [Run spark on kubernetes integration 【test】](https://github.com/ryan-johnson-databricks/spark/actions/runs/4358877500/jobs/7620022040):<br>```<br>[info] *** Test still running after 2 minutes, 57 seconds: suite name: KubernetesSuite, 【test】 name: Test decommissioning with 【dynamic】 allocation & shuffle 【clean】ups. <br>[info] - Test decommissioning with 【dynamic】 allocation & shuffle 【clean】ups (3 minutes, 3 seconds)<br>[info] - Test decommissioning timeouts (1 minute)<br>[info] - SPARK-37576: Rolling decommissioning (1 minute, 11 seconds)<br>[info] org.apache.spark.deploy.k8s.integration【test】.KubernetesSuite *** ABORTED *** (25 minutes, 32 seconds)<br>[info]   io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/api/v1/namespaces. Message: object is being deleted: namespaces \"spark-6bff7607e9884740a4bac53b1fb655ae\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=namespaces, name=spark-6bff7607e9884740a4bac53b1fb655ae, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: namespaces \"spark-6bff7607e9884740a4bac53b1fb655ae\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).<br>[info]   at io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)<br>[info]   at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:538)<br>   ...<br>[info]   at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)<br>[info]   at org.apache.spark.deploy.k8s.integration【test】.KubernetesTestComponents.createNamespace(KubernetesTestComponents.scala:51)<br>[info]   at org.apache.spark.deploy.k8s.integration【test】.KubernetesSuite.setUpTest(KubernetesSuite.scala:202)<br>   ...<br>[info]   at org.apache.spark.deploy.k8s.integration【test】.KubernetesSuite.runTest(KubernetesSuite.scala:45)<br>   ...<br>[info]   Cause: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://192.168.49.2:8443/api/v1/namespaces. Message: object is being deleted: namespaces \"spark-6bff7607e9884740a4bac53b1fb655ae\" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=null, kind=namespaces, name=spark-6bff7607e9884740a4bac53b1fb655ae, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: namespaces \"spark-6bff7607e9884740a4bac53b1fb655ae\" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).<br>```<br>(not sure how that could be related to this PR?)
Is there a similar case on Scala connect client ？<br><br>
> Is there a similar case on Scala connect client ？<br><br>I haven't tried Scala client, but yes, it would happen, and this will fix both.
Is there a chance to add a similar case in `ClientE2ETestSuite`?<br><br>
@LuciferYang This PR fix it in the connect planner, so should also works for the Scala Client.
> @LuciferYang This PR fix it in the connect planner, so should also works for the Scala Client.<br><br>OK, got it
thank you all, merged into master/branch-3.4
It seems that this PR change is not necessary, I will close it, thank @LuciferYang @HyukjinKwon  for your 【review】
cc @zhengruifeng @ueshin @grundprinzip FYI
CC @HyukjinKwon @hvanhovell
@xinrong-meng mind rebasing this? Otherwise should be good to go
merged to master/branch-3.4
【Thanks】 @zhengruifeng !
why not using `from_json` and `from_csv` to do this?
> why not using `from_json` and `from_csv` to do this?<br><br>How to get the schema?<br><br>
not sure whether I’m missing something, but isn’t the schema already provided by users?
probably not related to this PR: <br><br>https://github.com/apache/spark/blob/39a55121888d2543a6056be65e0c74126a9d3bdf/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L63-L76<br><br>```<br>  def schema(schemaString: String): DataFrameReader = {<br>    schema(StructType.fromDDL(schemaString))<br>  }<br>```<br><br>when the user provide a DDL string, it invoke the parser. Here I think we should keep both StructType and DDL string, and pass them to the server side.
> probably not related to this PR:<br>> <br>> https://github.com/apache/spark/blob/39a55121888d2543a6056be65e0c74126a9d3bdf/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L63-L76<br>> <br>> ```<br>>   def schema(schemaString: String): DataFrameReader = {<br>>     schema(StructType.fromDDL(schemaString))<br>>   }<br>> ```<br>> <br>> when the user provide a DDL string, it invoke the parser. Here I think we should keep both StructType and DDL string, and pass them to the server side.<br><br>message `Read` seems also need to consider this？I think we can further discuss this problem in a separate pr?<br>
@LuciferYang thank you for woking on this.<br><br>merged into master/branch-3.4
【Thanks】 @zhengruifeng @hvanhovell @HyukjinKwon  ~
The failed `BasicSchedulerIntegrationSuite` is not related to this PR, I'm merging it to master/3.4, thanks for the 【review】!
This is a bug fix of a new 【feature】 in 3.4, so I won't call it a release blocker. I've set the 【<font color=blue>fix】ed】 version to 3.4.0, if rc3 passes, I'll change it to 3.4.1.
Seems like the compliation didn't pass. Let me just quickly revert this and reopen.
maybe there is a conflict right after my last commit, let me rebase
GA passes, let me merge it back.
@cloud-fan is reporting a clustered distribution still supported? Data sources should be able to report that partitions are partitioned by some columns, without reporting the actual partitioning mechanism (like hash). That fact can be reused by `groupby` or windows functions with partitioning.
thanks, merged to master
cc @srielau @MaxGekk @cloud-fan This is user-facing 【documentation】 for SQL error classes.
@itholic Did you write those files manually, or generated by a script?
> @itholic Did you write those files manually, or generated by a script?<br><br>I did both. I used the script, and for the parts that didn't print correctly, I 【<font color=blue>fix】ed】 them manually by 【check】ing the page one by one. Since the script is not very 【polish】ed for now, let me submit it as a follow-up task if we aim for automation in the future.
+1, 【LGTM】. Merging to master.<br>Thank you, @itholic and @srielau for 【review】.
@MaxGekk  should we merge it to 3.4?
@gatorsmile I stoped merging PRs related to error classes after RC0. Need to re-【check】 carefully which error classes are in branch-3.4 already. @itholic Please, backport the 【c<font color=blue>hang】es】 to branch-3.4.
It's probably ok to mention non-existing errors in the doc in 3.4
Just created PR for 3.4: https://github.com/apache/spark/pull/40433
> Looks pretty good. Mind taking a look at https://github.com/apache/spark/pull/40338/【check】s?【check】_run_id=11851128079?<br><br>@HyukjinKwon I've rebased against the la【test】 master, however it seems that workflows won't start for new first-time contributors not inside the organization. If I read the 【documentation】 [1][2] correctly, a maintainer will need to activate the workflow?<br><br>[1] https://docs.github.com/en/actions/managing-workflow-runs/approving-workflow-runs-from-public-forks<br>[2] https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-【feature】s/enabling-【feature】s-for-your-repository/managing-github-actions-settings-for-a-repository#controlling-【c<font color=blue>hang】es】-from-forks-to-workflows-in-public-repositories
Apache Spark has a custom implementation that leverages the build (and resources) from forked repository. Mind 【check】ing if the workload is enabled in your fork (https://github.com/MaicoTimmerman/spark/actions/workflows/build_and_【test】.yml), and rebase this?
cc @zero323 in case you have some feedback on this.
> cc @zero323 in case you have some feedback on this.<br><br>@HyukjinKwon I am OK with that, though there is a bigger issue here. We have `TypeVars` in `py` modules in quite a few places, which are a side effect of migrating to inline hints before dropping Python 3.8 support (looking at you, `Protocol`...). <br><br>Ideally, we'd handle all of that consistently and use consistent naming convention, but that's not something will be able to do any time soon.<br>
Is this still in draft @jerqi ?<br>Also, +CC @cloud-fan who 【review】ed this earlier.
> Is this still in draft @jerqi ? Also, +CC @cloud-fan who 【review】ed this earlier.<br><br>【Thanks】 @mridulm It's ready for 【review】.
Will wait for the CI to succeed. 【Thanks】 for fixing this @jerqi !
thanks @mridulm
The 【test】 failure is unrelated, I retrigger the 【test】.
Merged to master.<br>【Thanks】 for working on this @jerqi !<br>【Thanks】 for the 【review】s @cloud-fan, @LuciferYang, @【advanced】xy :-)
Merging to master. Thank you, @srielau and @HyukjinKwon for 【review】.
Need run `./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm` to format the code in connect modules
@LuciferYang thanks. Fixed.
fine to me, cc @zhenlineo @amaliujia @hvanhovell FYI
Hi, @rangadi and @hvanhovell . There is a `scalafmt` error. Here is a follow-up to recover CI<br>- https://github.com/apache/spark/pull/40374
@dongjoon-hyun thank you! I should have 【check】ed. missed it.
@HyukjinKwon @hvanhovell
> Manually 【test】ed<br><br>@zhenlineo would you mind supplying more details about your 【test】 steps and results? Actually, I have been trying these scripts since you added them the first time, finally, it 【success】 w/ a workaround(by adding an extra parameter `spark-internal`, I suppose this PR will fix it) after your last update. It would be good if you can leave some commands in your PR description, which makes it easier for 【review】ers or learners to verify your 【c<font color=blue>hang】es】.
@HyukjinKwon 【Thanks】 for the 【review】.<br>Merging to master/3.4
@hvanhovell do you know how to fix this<br><br>```<br>Error: ] /home/runner/work/spark/spark/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala:395: inferred existential type io.grpc.ManagedChannelBuilder[?0]( forSome { type ?0 <: io.grpc.ManagedChannelBuilder[?0] }), which cannot be expressed by wildcards,  should be enabled<br>by making the implicit 【value】 scala.language.existentials visible.<br>```
In fact, the complete error message says how to fix it....
@hvanhovell can you take another look?
This script is just a fork from the sbt repo, and the hint info for current java seems correct to me. FYI, https://github.com/sbt/sbt/blob/1.9.x/sbt#L576
【Thanks】 for comment, @yaooqinn .  The build/sbt script seems to be a simplified sbt script. I think the hint info for current java is not correct.to be honest, I think it is a copy-paste error from the hint info for sbt_version argument. I can create a PR in sbt/sbt project to see if it can be updated.
OK, so this is just changing some stuff for shell【check】. It seems OK. This is just a copy of the sbt script. Normally I'd say, does this make it harder to reason about 【c<font color=blue>hang】es】 if we pull a new copy from upstream? but probably not. This also probably doesn't fix any actual problem. I'm neutral on bothering to change these.
Yes, this PR is kind of useless. I close it now.
cc @WeichenXu123 @HyukjinKwon
The 【test】 failure seems unrelated.<br><br>Merged to master and branch-3.4.
【Thanks】 @HyukjinKwon !
In the last commit, make `BloomFilterAggregate` explicitly supported `IntegerType/ShortType/ByteType` and added corresponding updaters, then removed pass `dataType`  and `adding cast nodes` <br><br>
GA failure is not related to the current PR<br>
@WeichenXu123 in what case won't Spark Connect ML have access to the session?
> @WeichenXu123 in what case won't Spark Connect ML have access to the session?<br><br>For some APIs, like `estimator.fit(dataset)`, `model.transform(dataset)`, we can get session from the input spark dataframe, in other cases e.g. get a model attribute, we have no input dataframe, so we need `getActiveSession` to get the session and then send getting attribute requests to server side.
CC @HyukjinKwon @grundprinzip Thoughts ? ML use cases requires the global session and the session must be the same during the whole ML program execution.<br><br>And we cannot use thread-local session, it must be a global session, because some ML algorithm like \"CrossValidator\" will run estimator training in background threads.
merged to master
@WeichenXu123 mind fixing the PR title? This PR doesn't add the getActiveSession method
> @WeichenXu123 mind fixing the PR title? This PR doesn't add the getActiveSession method<br><br>Updated.
【Thanks】 for your work. I'll take a closer look tomorrow<br><br>
ping @hvanhovell @HyukjinKwon @zhengruifeng @amaliujia
ping @hvanhovell @zhengruifeng @LuciferYang
@hvanhovell Could you have time to take a look ? 【Thanks】!
ping @hvanhovell @HyukjinKwon Could you have time to take a look?
ping @hvanhovell @zhenlineo
@hvanhovell Could you take a 【review】 ?
friendly ping @HyukjinKwon @hvanhovell @zhengruifeng
@HyukjinKwon @LuciferYang @zhengruifeng @amaliujia @zhenlineo Thank you !
Merged to master and branch-3.4, thanks!
Thank you, @xinrong-meng and @HyukjinKwon .
ping @cloud-fan cc @sadikovi
thanks, merged to master! can you open a backport PR for 3.4?
> thanks, merged to master! can you open a backport PR for 3.4?<br><br>Thank you! I will create it.
Thank you for updating.
cc @cloud-fan
In our product environment, the k8s cluster is managed by other system and the port of apiserver is different between clusters. In this case, the spark.kubernetes.driver.master  on spark client side can't set a port for all clusters.
+1, 【LGTM】. Merging to master.<br>Thank you, @gengliangwang and @cloud-fan for 【review】.
@MaxGekk @cloud-fan FYI I cherry-pick this one to branch 3.4 as well.
> BTW, you can prevent the leak very easily by using TTL like S3/MinIO lifecycle rules.<br><br><br><br>We are using HDFS as the storage. <br>And the ttl is not enough to all apps, such as streaming and AI training job which runs for several days.<br>If TTL is too short, it will affect the failover for spark apps. If TTL is very long, it cases storage waste.
@thousandhu @dongjoon-hyun  @holdenk <br>The approach in this PR only handles the 【clean】up on driver side. It won't 【clean】 up the files if files were uploaded during job submission but then submission fails due to some reason. As in this driver won't be launched and 【clean】 up will not happen. <br><br>This Jira SPARK-42744 is duplicate of SPARK-42466. I had already created PR for this issue which handles 【clean】up on both driver as well client side in case of app submission failure. <br>Other than this it also optimises the file upload by creating just one upload sub directory rather than created several sub - directories for each file getting uploaded. <br><br>https://github.com/apache/spark/pull/40128<br>
cc @cloud-fan, @ulysses-you<br><br>@xinrong-meng, this seems to be a regression from 3.3 to 3.4 so could you please wait with 3.4.0-RC4 if possible?
it seems to me that:<br>1. we should skip add `PlanExpression` into `aliasMap`<br>2. make `aliasMap` as immutable, and use a local mutable map, then merge them after done multiTransform
> it seems to me that:<br>> <br>> 1. we should skip add `PlanExpression` into `aliasMap`<br>> 2. make `aliasMap` as immutable, and use a local mutable map, then merge them after done multiTransform<br><br>Oh ok, then we see the right fix a bit differently then.<br>But I'm not entirelly sure I get 2. We can make `aliasMap` immutable, by just adding a `.toMap` to here: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/AliasAwareOutputExpression.scala#L61. But what do you mean by<br><br>> and use a local mutable map, then merge them after done multiTransform<br><br>?
Unfortunately v3.4.0-rc4 has been cut. Let's ensure it to be in RC5.
@peter-toth nvm, skip adding PlanExpression and make it as immutable are kinds of `【improvement】`, the fix in this pr looks good
【Thanks】 for the 【review】!
@kenny-ddd Could you add `[SQL]` to title?
cc @wangyum
Oh, it seems to break Scala style, @wangyum .<br>```<br>[error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala:33: File line length exceeds 100 characters<br>```
To @wangyum , sometimes, the contributor's GitHub Action doesn't work properly. I also made similar mistakes before by forgetting the 【check】. It's our responsibility, the committers, are responsible to 【check】 it manually and ask them to enable it properly. 【:)】<br><br>![Screenshot 2023-03-12 at 1 39 08 PM](https://user-images.githubusercontent.com/9700541/224572230-7ba55e19-ff83-48dc-923f-1fd72c5389e9.png)<br>
@dongjoon-hyun Sorry. I will pay attention to 【check】 next time.
Oh, no problem at all. I just wanted to share my old mistakes.
cc @WeichenXu123 @srowen @huaxingao
Merged to master/3.4/3.3/3.2
@srowen Thank you for the 【review】s!
PR is mergeable @hvanhovell
@vicennial can you update?
@hvanhovell updated
Merging this one.
cc @xinrong-meng , too
Tests will not pass until Pandas 2.0.0 is released.
cc @itholic @zhengruifeng @xinrong-meng if you find some time to 【review】.
Change itself looks pretty good to me, once the CI is passed after the [initial pandas 2.0 support](https://github.com/apache/spark/pull/40658) completing.
cc @fred-db @bart-samwel
also cc @xinrong-meng , this should be included in 3.4.0
cc @itholic @HyukjinKwon
【Thanks】 for 【review】s and merging.
cc: @HeartSaVioR
cc @hvanhovell , @xinrong-meng , @HyukjinKwon
Thank you, @amaliujia .
Scala linter passed. I'll merge this to recover the branches.<br>![Screenshot 2023-03-10 at 7 42 06 PM](https://user-images.githubusercontent.com/9700541/224463100-a6ee094f-615d-4b8d-9924-5ae7820fb76b.png)<br>
【Thanks】 @dongjoon-hyun . Sorry about that. I missed that.
cc @hvanhovell @LuciferYang @HyukjinKwon @WeichenXu123
Also cc @beliefer , who is adding new function for `LiteralValueProtoConverter` to support `typedLit`<br><br>
thanks you all, merged to master/branch-3.4
Do we intentionally ignore `YearMonthIntervalType`?
> Do we intentionally ignore `YearMonthIntervalType`?<br><br>it seems that we have not support `YearMonthIntervalType` in vanilla PySpark, and the Python Client is reusing PySpark's types.<br>So it seems that we should add `YearMonthIntervalType` in PySpark first
thank you all, merged to master/branch-3.4
【Thanks】 for doing this. Small comment on the implementation.
install_app already 【check】 target
Wrong JIRA is linked, please adjust
@dongjoon-hyun FYI
If you agree with the above assessment, please remove the misleading CVE information from the PR description, @bjornjorgensen .
Well, the comment that you are refereeing to, have a link but I cant get in <br>![image](https://user-images.githubusercontent.com/47577197/224536606-58b733ab-cfb9-47e6-bf19-485fae5e3f2c.png)<br><br>3 weeks later they merged a PR https://github.com/fabric8io/kubernetes-client/commit/43b04f6cc2cde0b8cebb76c842c09de30c236780 that fix this issue. <br><br>And yesterday SNYK open a PR to my repo for this issue. https://github.com/bjornjorgensen/spark/pull/102 <br>I can always change the text for this PR, but I haven't seen anything that makes me believe that kubernets-client is not affected by this CVE.<br><br>
**The maintainers of the library contend that the application's trust would already have had to be compromised or established and therefore dispute the risk associated with this issue on the basis that there is a high bar for exploitation. Thus, no fix is expected.**<br><br>https://【security】.snyk.io/vuln/SNYK-JAVA-ORGYAML-3152153<br><br><br>This is part of snakeyaml release notes<br><br>2.0 (2023-02-26)<br><br>Fix #570: SafeConstructor ignores LoaderOptions setCodePointLimit() (thanks to Robert Patrick)<br><br>Update #565: (Backwards-incompatible) Do not allow global tags by default to fix CVE-2022-1471 (thanks to Jonathan Leitschuh)<br><br><br>1.32 (2022-09-12)<br><br>Fix #547: Set the limit for incoming data to prevent a CVE report in NIST. By default it is 3MB<br><br>https://bitbucket.org/snakeyaml/snakeyaml/wiki/Changes
@bjornjorgensen . Do you mean you can not trust `winniegy`, the fabric8io 【community】 member's comment? He close the issue after that comment.<br>![Screenshot 2023-03-12 at 1 26 33 PM](https://user-images.githubusercontent.com/9700541/224571715-ee1bd3d2-07c6-4097-9f6f-09e08d9c920f.png)<br><br>Hence, the migration happens independently from the CVE. It's just for the future release.<br><br>In short, the following claim sounds wrong to me according to the context.<br>> 3 weeks later they merged a PR https://github.com/fabric8io/kubernetes-client/commit/43b04f6cc2cde0b8cebb76c842c09de30c236780 that fix this issue.
BTW, I have two additional questions for the following PR you referred.<br><br><br>1. Do you mean it's the evidence of the previous assessment?<br><br>> SafeConstructor ignores LoaderOptions setCodePointLimit() (thanks to Robert Patrick)<br><br>2. Do you think the following major version change is safe to us?<br>```<br>- <snakeyaml.version>1.33</snakeyaml.version><br>+ <snakeyaml.version>2.5</snakeyaml.version><br>```
ok, I didn't know who this user was. I have updated the PR text now.
Now that it turns out that the information that I have been given is not correct. This change the whole picture with why we should include this PR. I think we can wait until we are done with 3.4 so that we are on the safe side.
Hey @dongjoon-hyun, <br><br>Will this fix will be backported to other maintained branches (【specific】ally the one I care about is the 3.3 branch)? <br><br>【Thanks】! <br><br>
https://github.com/apache/spark/blob/cd166243ae4e3c8aafd1062994ce9daa94f58253/pom.xml#L213 【upgrade】 from 5.12.2 to 6.5.0 thats alot of work.
Are there any reasons why I need to get this error messages like this one? <br>![image](https://user-images.githubusercontent.com/47577197/234956928-09d7f2c0-5488-47da-b6bc-b4ecca16f4cc.png)<br>
To @fryz . As @bjornjorgensen mentioned, the answer is no.<br>> Will this fix will be backported to other maintained branches (【specific】ally the one I care about is the 3.3 branch)?
So we only keep it in the window between an executor requesting it's ID and it disconnecting to restabmudh a proper connection (generally <1s). Given it's a few bytes of information per 【outstanding】 executor without an ID assigned yet I'm not super worried about the memory usage.<br><br>We could also close the connection from the driver side instead I suppose but that might cause a race condition with the executor (would have to 【check】 how we handle disconnects and if we process 【outstanding】 messages).
it would be used by in any situation where the executors don't have IDs pre-assigned to them, that might include other allocation techniques now that its pluggable but the only built in one is statefulsets.
Thank you for the answer. Let me follow the code patch again.
Let me close this in favor of the previous PR.
> Let me close this in favor of the previous PR.<br><br>Ok
To @panbingkun , you can search the JIRA or PR before making a PR in order to avoid unintentional duplication.
Yea AQE may remove materialized query stages due to 【optimization】s like empty relation propagation, but I think it's fine as the shuffle files are still there (we don't unregister the shuffle), so the reused shuffle operator can still read these shuffle files using the shuffle id. The problem with EXPLAIN is it only looks for the referenced exchange in the query plan tree, I think we can also look up from the AQE stage cache map?
@cloud-fan Yes this is purely UI and EXPLAIN issue. It does not affect query execution. <br><br>I'm not sure how AQE context stageCache map would help. The issue in EXPLAIN is that the ReusedExchange.child references a Exchange node that is not referenced anywhere else in the plan tree so we need to generate IDs on the subtree rooted at ReusedExchange.child and print them out. To do this, we need a way to 【check】 whether the ReusedExchange.child is referenced anywhere else - if they are not referenced anywhere else, we need to recursively generate IDs for subtree. I keep a HashSet of nodes with IDs already generated and 【check】 ReusedExchange.child against it to see if we need to recursively generate IDs on the subtree.
@StevenChenDatabricks 【Thanks】 for the explanation, now I understand it.<br><br>I still have some high-level questions:<br>1. Does `ReusedSubquery` have the same issue?<br>2. what if there are more than one `ReusedExchange`s that references non-existing exchange?<br><br>I have a new idea that we still display `ReusedExchange` as it is, but we add a new section to display non-existing `Ex【c<font color=blue>hang】es】`:<br>```<br>==== Adaptively Optimized Out Ex【c<font color=blue>hang】es】 ====<br>(132) Exchange<br>Input [3]: [sr_customer_sk#217, sr_store_sk#221, sum#327L]<br>Arguments: hashpartitioning(sr_store_sk#221, 200), ENSURE_REQUIREMENTS, [plan_id=1791]<br>```<br><br>Then there is no duplication even if more than one `ReusedExchange` reference it.
@cloud-fan 【Thanks】 for the idea and response!<br><br>1. I don't think this issue doesn't affects `ReusedSubquery` because of how its processed and printed. The current algorithm finds all Subquery nodes (including `ReusedSubquery`) and for each Subquery, it traverses the Subquery subtree to generate the IDs if they are missing. <br>Furthermore, a `ReusedSubquery` does not print the details of the Subquery it reuses whereas for ReusedExchange it does print the Exchange ID being reused. For a `ReusedSubquery`, all that is printed is this line:<br>```Subquery:5 Hosting operator id = 50 Hosting Expression = ReusedSubquery Subquery scalar-subquery#31, [id=#32]```<br>`Hosting operator ID` is the parent operator that contains the `ReusedSubquery`. The subtree of the `ReusedSubquery` is not printed anywhere and the `ReusedSubquery` node itself is not printed in the main plan tree either. Even if there are non-existing children, the issue is not surfaced in the Explain plan by default.<br>I guess there's still a chance it might affect Spark UI whereby the IDs in the subtree of a `ReusedSubquery` are incorrect because the IDs were generated in a previous AQE iteration... I'm not sure. I think it's best to wait and see if a ticket/bug like this is ever reported. <br><br>2. My fix detects all the ReusedEx【c<font color=blue>hang】es】 with non-existing children and generate IDs on them. I guess your question is what if multiple `ReusedExchange` reference the same non-existing `Exchange`? That's a good point and I need to account for that edge case in the code in case that is possible.<br><br>With regards to your idea for a section of non-existing `Ex【c<font color=blue>hang】es】`: we already only print each operator exactly once in the node details section. As shown in the PR description: I currently print out the plan subtree of the Non-Existing Exchange below the `ReusedExchange` (since that subtree is not shown anywhere else) and the node details while maintaining 【unique】ness.
> we already only print each operator exactly once in the node details section<br><br>What if more than one `ReusedExchange` referencing the same `Exchange`? Will we print the `Exchange` twice?
@cloud-fan It wouldn't because `collectOperatorsWithID` in ExplainUtils is responsible for collecting the list of nodes to print out. It uses a BitSet `collectedOperators` that's globally shared to ensure each node is \"collected\" and printed exactly once.
So we randomly pick one `ReusedExchange` to print its corresponding `Exchange`?
@cloud-fan I've addressed your comments. 【Thanks】 for the 【review】!
cc @wangyum and @kenny-ddd
Scala linter passed. <br><br>![Screenshot 2023-03-12 at 1 59 52 PM](https://user-images.githubusercontent.com/9700541/224573370-cbd15aa7-0cdc-43d6-8d01-9f35d77547f6.png)<br>
Hi, @bjornjorgensen . Could you 【review】 this PR?
Could you 【review】 this in order to recover master branch, @sunchao ?
Since this is a comment only change, let me merge this to recover the master branch and PRs.<br><br>![Screenshot 2023-03-12 at 2 20 11 PM](https://user-images.githubusercontent.com/9700541/224574385-6ab72bd1-4d16-42cf-a42a-72c0459f9a73.png)<br>
@dongjoon-hyun Thank you. Yes, this PR looks ok.
Thank you so much, @bjornjorgensen .
I have seen this one time before. Can you restart 【test】s on the PR that fails?
No, only the PR author can do that because we use the author's GitHub Actions for that PR.<br>> I have seen this one time before. Can you restart 【test】s on the PR that fails?<br><br>For Apache Spark branch, of course, you can because you are the committer.
Seams to be working now 【:)】
【Thanks】 for your fix @dongjoon-hyun <br><br>
Could you 【review】 this PR, @viirya ?
All 【test】s passed. Merged to master.
I guess we can just do the following with the comment:<br><br>```py<br># The implementation of pandas_udf is embedded in pyspark.sql.function.pandas_udf<br># for code reuse.<br>from pyspark.sql.functions import pandas_udf as pandas_udf<br>```<br><br>for those who want to import `pyspark.sql.connect.functions`?
Btw, what happens if `\"PYSPARK_NO_NAMESPACE_SHARE\" in os.environ`?<br><br>https://github.com/apache/spark/blob/761e0c0f6f0d00733177a869b9ecdf454e13fc9f/python/pyspark/sql/utils.py#L148-L161<br><br>Usually the env var is set, we need to explicitly import `pyspark.sql.connect.function`.
We didn't wrap `pyspark.sql.function.pandas_udf` with `try_remote_functions`, so  `\"PYSPARK_NO_NAMESPACE_SHARE\"` should be irrelevant.
It's irrelevant, that means it's an issue, no?<br><br>E.g, could you add a 【test】 in `【test】_connect_function`?
After double thoughts, https://github.com/apache/spark/pull/40388#issuecomment-1467120067 might not be what we want, as shown below<br><br>```<br>>>> from pyspark.sql.connect.functions import pandas_udf<br>>>> spark.range(2).select(pandas_udf(lambda x: x + 1, 'int')('id')).collect()<br>Traceback (most recent call last):<br>  File \"<stdin>\ line 1, in <module><br>TypeError: 'NoneType' object is not callable<br><br>```
I see your concern on `PYSPARK_NO_NAMESPACE_SHARE`, it should have affected the imports of `pandas_udf`, that is, if it is not set, `sql.functions.pandas_udf` points to `sql.connect.functions.pandas_udf`. <br>Unfortunately in our case, there is no `sql.connect.functions.pandas_udf` at all.<br><br>Considering`PYSPARK_NO_NAMESPACE_SHARE` is not a heavily-used user-facing env variable, shall we merge the change proposed in this PR first for 【clarity】? Otherwise, I am afraid that users think pandas_udf is not supported in Connect. I will work on a follow-up to support `sql.connect.functions.pandas_udf` with `PYSPARK_NO_NAMESPACE_SHARE` support.<br><br>As for `E.g, could you add a 【test】 in 【test】_connect_function?`<br><br>The unit 【test】s for pandas_udf are in `python/pyspark/sql/【test】s/connect/【test】_parity_pandas_udf.py` if that's your concern.<br><br>CC @ueshin
Also CC @HyukjinKwon
> After double thoughts, https://github.com/apache/spark/pull/40388#issuecomment-1467120067 might not be what we want, as shown below<br><br>I meant:<br><br>```diff<br>- def pandas_udf(*args: Any, **kwargs: Any) -> None:<br>-     raise NotImplementedError(\"pandas_udf() is not implemented.\")<br>+ # The implementation of pandas_udf is embedded in pyspark.sql.function.pandas_udf<br>+ # for code reuse.<br>+ from pyspark.sql.functions import pandas_udf as pandas_udf<br>```
> The unit 【test】s for pandas_udf are in `python/pyspark/sql/【test】s/connect/【test】_parity_pandas_udf.py` if that's your concern.<br><br>It's not my concern.<br>`sql.functions.pandas_udf` and `sql.connect.functions.pandas_udf` should be usable separately, for vanilla PySpark and Spark Connect, respectively, if `PYSPARK_NO_NAMESPACE_SHARE` is set.<br><br>> Considering`PYSPARK_NO_NAMESPACE_SHARE` is not a heavily-used user-facing env variable, shall we merge the change proposed in this PR first for 【clarity】? Otherwise, I am afraid that users think pandas_udf is not supported in Connect. I will work on a follow-up to support `sql.connect.functions.pandas_udf` with `PYSPARK_NO_NAMESPACE_SHARE` support.<br><br>I'd leave the decision to @HyukjinKwon or @zhengruifeng then.
Good idea! I adjusted the code and PR description accordingly. 【Thanks】 @ueshin
and, `pyspark.sql.connect.*` isn't also supposed to be called directly by users. so this PR is just more about internal refactoring.
Will update pr description later<br><br>
Should we push forward this one? @HyukjinKwon
> Should we push forward this one? @HyukjinKwon<br><br>Should, I run 【test】 on local, can't passed because I build without hive. And the 2.4.0 release can't build passed without hive. <br>This is mail list which others find problem. https://lists.apache.org/thread/11kdt36n6ytx87klcp2gd678lddqoknd
cc @grundprinzip and @hvanhovell FYI
> > Should we push forward this one? @HyukjinKwon<br>> <br>> Should, I run 【test】 on local, can't passed because I build without hive. And the 2.4.0 release can't build passed without hive. This is mail list which others find problem. https://lists.apache.org/thread/11kdt36n6ytx87klcp2gd678lddqoknd<br><br>Yes,  it will be slightly friendly to developers with this pr<br><br>
BTW, mind fxing https://github.com/apache/spark/pull/40389#【discussion】_r1136914961 though
【Thanks】 @HyukjinKwon and @Hisoka-X
@LuciferYang Thank you. Should we update https://github.com/apache/spark/blob/master/docs/building-spark.md with this info?
> @LuciferYang Thank you. Should we update https://github.com/apache/spark/blob/master/docs/building-spark.md with this info?<br><br>If needed, it may be more appropriate to update in https://github.com/apache/spark/blob/master/connector/connect/README.md
thank you @dongjoon-hyun , I'm fine to hold on this until next release. Another thought is I want to make sure all 【test】s can be passed.
Hi @dongjoon-hyun , this config was added in Spark 3.2 and we have 【<font color=blue>fix】ed】 all the known regressions, I think it's time to turn it on by default in 3.5 to 【improve】 the AQE coverage. [SPARK-42101](https://issues.apache.org/jira/browse/SPARK-42101) was only for the first query access and doesn't matter that much.
Of course, I agree with you, @cloud-fan . Thank you for pinging me.<br><br>To @ulysses-you , could you rebase this PR?
Also, cc @sunchao too
sure, thank you @dongjoon-hyun @cloud-fan
I am not sure if we should add a Executor exit code and optimize the RegisterExecutor response message in this pr.In production environment, we found sometimes only filter the exclued node when launching containers does not work as well as we want, because we found maybe driver does not request new executors for a period of time, so the execluded nodes list wont be sent to `YarnAllocator` though `requestTotalExecutorsWithPreferredLocalities`, some executors will failed and count to the failure count.<br><br>So we add the Executor exit code and optimize the RegisterExecutor response message.
@Ngone51 @tgravescs could you please help 【review】 this pr when you have time, thanks.
Hi, @HyukjinKwon . Could you 【review】 this PR when you have some time?
> ... for some executor pods to connect driver pods via IP.<br><br>Hi @dongjoon-hyun, I think it's quite useful, but in https://github.com/apache/spark/pull/39160#pullrequest【review】-1229691638, you left a concern<br><br>> ... I have a concern. Currently, Apache Spark uses K8s Service entity via [DriverServiceFeatureStep](https://github.com/apache/spark/blob/master/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/【feature】s/DriverServiceFeatureStep.scala) to access Spark driver pod in K8s environment.<br><br>do you still concern that now?
@pan3793 .<br><br>The goal of PR is different from your PR's goal.<br>- Your PR tried to add `SPARK_DRIVER_POD_NAME` to `Driver Pod` to expose it to **3rd party pods**.<br>- This PR aims to add `SPARK_DRIVER_POD_IP` to `Executor Pod` in order to help **internal communications between Spark executors and Spark driver**.<br><br>In addition, this is a kind of propagation of the information from the driver pod to the executor pods instead of exposing the executor pods' internal information.
Hi, @viirya . Could you 【review】 this PR when you have some time?
Yes, correct, @viirya ! Thank you for the approval.
Merged to master for Apache Spark 3.5.0.
+CC @otterc
@Stove-hust Thank you for reporting and the patch. Would you be able to share driver logs?
> @Stove-hust Thank you for reporting and the patch. Would you be able to share driver logs?<br><br>Sure（Add some comments）<br>--- stage 10 faield <br>22/10/15 10:55:58 WARN task-result-getter-1 TaskSetManager: Lost task 435.1 in stage 10.0 (TID 6822, zw02-data-hdp-dn21102.mt, executor 101): FetchFailed(null, shuffleId=3, mapIndex=-1, mapId=-1, reduceId=435, message=<br>org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3 partition 435<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:386) failed in 601.792 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3 partition 435<br><br>-- resubmit stage 10 && parentStage 9<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Resubmitting ShuffleMapStage 9 (processCmd at CliDriver.java:386) and ShuffleMapStage 10 (processCmd at CliDriver.java:386) due to fetch failure<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Resubmitting failed stages<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[22] at processCmd at CliDriver.java:386), which has no missing parents<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Push-based shuffle disabled for ShuffleMapStage 9 (processCmd at CliDriver.java:386) since it is already shuffle merge finalized<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[22] at processCmd at CliDriver.java:386) (first 15 tasks are for partitions Vector(98, 372, 690))<br>22/10/15 10:55:58 INFO dag-scheduler-event-loop YarnClusterScheduler: Adding task set 9.1 with 3 tasks<br><br>-- The first stage10 task completes one after another, and notifyDriverAboutPushCompletion to end stage 10, and mark finalizeTask, because the stage is not in runningStages, so the stage cannot be marked shuffleMergeFinalized.<br>22/10/15 10:55:58 INFO task-result-getter-0 TaskSetManager: Finished task 325.0 in stage 10.0 (TID 6166) in 154455 ms on zw02-data-hdp-dn25537.mt (executor 117) (494/500)<br>22/10/15 10:55:59 WARN task-result-getter-1 TaskSetManager: Lost task 325.1 in stage 10.0 (TID 6671, zw02-data-hdp-dn23160.mt, executor 47): TaskKilled (another attempt succeeded)<br>22/10/15 10:56:20 WARN task-result-getter-1 TaskSetManager: Lost task 358.1 in stage 10.0 (TID 6731, zw02-data-hdp-dn25537.mt, executor 95): TaskKilled (another attempt succeeded)<br>22/10/15 10:56:20 INFO task-result-getter-1 TaskSetManager: Task 358.1 in stage 10.0 (TID 6731) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).<br><br>--- Removed TaskSet 10.0, whose tasks have all completed<br>22/10/15 10:56:22 INFO task-result-getter-1 TaskSetManager: Ignoring task-finished event for 435.0 in stage 10.0 because task 435 has already completed 【success】fully<br>22/10/15 10:56:22 INFO task-result-getter-1 YarnClusterScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool <br><br>--- notifyDriverAboutPushCompletion stage 10<br>22/10/15 10:56:23 INFO dag-scheduler-event-loop DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:386) scheduled for finalizing shuffle merge in 0 s<br>22/10/15 10:56:23 INFO shuffle-merge-finalizer-2 DAGScheduler: ShuffleMapStage 10 (processCmd at CliDriver.java:386) finalizing the shuffle merge with registering merge results set to true<br><br>--- stage 9 finished <br>22/10/15 10:57:51 INFO task-result-getter-1 TaskSetManager: Finished task 2.0 in stage 9.1 (TID 6825) in 112825 ms on zw02-data-hdp-dn25559.mt (executor 74) (3/3)<br>22/10/15 10:57:51 INFO task-result-getter-1 YarnClusterScheduler: Removed TaskSet 9.1, whose tasks have all completed, from pool <br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: ShuffleMapStage 9 (processCmd at CliDriver.java:386) finished in 112.832 s<br><br>--- resubmit stage 10<br>2/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: looking for newly runnable stages<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: running: Set(ShuffleMapStage 11, ShuffleMapStage 8)<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: waiting: Set(ShuffleMapStage 12, ShuffleMapStage 10)<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: failed: Set()<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[36] at processCmd at CliDriver.java:386), which has no missing parents<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop OutputCommitCoordinator: Reusing state from previous attempt of stage 10.<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: Shuffle merge enabled before starting the stage for ShuffleMapStage 10 with shuffle 7 and shuffle merge 0 with 108 merger locations<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[36] at processCmd at CliDriver.java:386) (first 15 tasks are for partitions Vector(105, 288, 447, 481))<br>22/10/15 10:57:51 INFO dag-scheduler-event-loop YarnClusterScheduler: Adding task set 10.1 with 4 tasks<br><br>--- stage 10 can not finished<br>22/10/15 10:58:18 INFO task-result-getter-1 TaskSetManager: Finished task 2.0 in stage 10.1 (TID 6857) in 26644 ms on zw02-data-hdp-dn23767.mt (executor 139) (1/4)<br>22/10/15 10:58:24 INFO task-result-getter-1 TaskSetManager: Finished task 3.0 in stage 10.1 (TID 6860) in 32551 ms on zw02-data-hdp-dn23729.mt (executor 42) (2/4)<br>22/10/15 10:58:47 INFO task-result-getter-1 TaskSetManager: Finished task 0.0 in stage 10.1 (TID 6858) in 55524 ms on zw02-data-hdp-dn20640.mt (executor 134) (3/4)<br>22/10/15 10:58:58 INFO task-result-getter-0 TaskSetManager: Finished task 1.0 in stage 10.1 (TID 6859) in 66911 ms on zw02-data-hdp-dn25862.mt (executor 57) (4/4)<br>
@otterc Hello, is there anything else I should add?
@Stove-hust  Haven't had a chance to look at it yet. I'll take a look at it this week.
> @Stove-hust Haven't had a chance to look at it yet. I'll take a look at it this week.<br><br>tks
@akpatnam25 @shuwang21
So this is an interesting coincidence, I literally encountered a production job which seems to be hitting this exact same issue :-)<br>I was in the process of creating a 【test】 case, but my intuition was along the same lines as this PR.<br><br>Can you create a 【test】 case to validate this behavior @Stove-hust ?<br>Essentially it should fail with current master, and succeed after this change.<br><br>【Thanks】 for working on this fix
> So this is an interesting coincidence, I literally encountered a production job which seems to be hitting this exact same issue :-) I was in the process of creating a 【test】 case, but my intuition was along the same lines as this PR.<br>> <br>> Can you create a 【test】 case to validate this behavior @Stove-hust ? Essentially it should fail with current master, and succeed after this change.<br>> <br>> 【Thanks】 for working on this fix<br><br>No problem
> So this is an interesting coincidence, I literally encountered a production job which seems to be hitting this exact same issue :-) I was in the process of creating a 【test】 case, but my intuition was along the same lines as this PR.<br>> <br>> Can you create a 【test】 case to validate this behavior @Stove-hust ? Essentially it should fail with current master, and succeed after this change.<br>> <br>> 【Thanks】 for working on this fix<br><br>Added UT
@Stove-hust The main change in `DAGScheduler` looks good to me. Basically, [here](https://github.com/apache/spark/blob/11c9838283e98d5ebe6ce13b85e26217494feef2/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L762) we also 【check】 whether the parent stage is finalized and if it is not we submit that. The reason the parent stage is not getting finalized here is because it has no tasks. <br>Will 【review】 the UT and take another look at the code next week. 【Thanks】 for fixing this.
Instead of only 【test】ing 【specific】ally for the flag - which is subject to change as the implementation evolves, we should also 【test】 for behavior here.<br><br>This is the reproducible 【test】 I was using (with some 【c<font color=blue>hang】es】) to 【test】 approaches for this bug - and it mimics the case I saw in our production reasonably well.<br>(In DAGSchedulerSuite):<br>```<br>  for (pushBasedShuffleEnabled <- Seq(true, false)) {<br>    【test】(\"SPARK-40082: recomputation of shuffle map stage with no pending partitions should not \" +<br>        s\"hang. pushBasedShuffleEnabled = $pushBasedShuffleEnabled\") {<br><br>      if (pushBasedShuffleEnabled) {<br>        initPushBasedShuffleConfs(conf)<br>        DAGSchedulerSuite.clearMergerLocs()<br>        DAGSchedulerSuite.addMergerLocs(Seq(\"host1\ \"host2\ \"host3\ \"host4\ \"host5\"))<br>      }<br><br>      var taskIdCount = 0<br><br>      var completedStage: List[Int] = Nil<br>      val listener = new SparkListener() {<br>        override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {<br>          completedStage = completedStage :+ event.stageInfo.stageId<br>        }<br>      }<br>      sc.addSparkListener(listener)<br><br>      val fetchFailParentPartition = 0<br><br>      val shuffleMapRdd0 = new MyRDD(sc, 2, Nil)<br>      val shuffleDep0 = new ShuffleDependency(shuffleMapRdd0, new HashPartitioner(2))<br><br>      val shuffleMapRdd1 = new MyRDD(sc, 2, List(shuffleDep0), tracker = mapOutputTracker)<br>      val shuffleDep1 = new ShuffleDependency(shuffleMapRdd1, new HashPartitioner(2))<br><br>      val reduceRdd = new MyRDD(sc, 2, List(shuffleDep1), tracker = mapOutputTracker)<br><br>      // submit the initial mapper stage, generate shuffle output for first reducer stage.<br>      submitMapStage(shuffleDep0)<br><br>      // Map stage completes 【success】fully,<br>      completeShuffleMapStageSuccessfully(0, 0, 3, Seq(\"hostA\ \"hostB\"))<br>      taskIdCount += 2<br>      assert(completedStage === List(0))<br><br>      // Now submit the first reducer stage<br>      submitMapStage(shuffleDep1)<br><br>      def createTaskInfo(speculative: Boolean): TaskInfo = {<br>        val taskInfo = new TaskInfo(<br>          taskId = taskIdCount,<br>          index = 0,<br>          attemptNumber = 0,<br>          partitionId = 0,<br>          launchTime = 0L,<br>          executorId = \"\<br>          host = \"hostC\<br>          TaskLocality.ANY,<br>          speculative = speculative)<br>        taskIdCount += 1<br>        taskInfo<br>      }<br><br>      val normalTask = createTaskInfo(speculative = false);<br>      val speculativeTask = createTaskInfo(speculative = true)<br><br>      // fail task 1.0 due to FetchFailed, and make 1.1 succeed.<br>      runEvent(makeCompletionEvent(taskSets(1).tasks(0),<br>        FetchFailed(makeBlockManagerId(\"hostA\"), shuffleDep0.shuffleId, normalTask.taskId,<br>          fetchFailParentPartition, normalTask.index, \"ignored\"),<br>        result = null,<br>        Seq.empty,<br>        Array.empty,<br>        normalTask))<br><br>      // Make the speculative task succeed after initial task has failed<br>      runEvent(makeCompletionEvent(taskSets(1).tasks(0), Success,<br>        result = MapStatus(BlockManagerId(\"hostD-exec1\ \"hostD\ 34512),<br>          Array.fill[Long](2)(2), mapTaskId = speculativeTask.taskId),<br>        taskInfo = speculativeTask))<br><br>      // The second task, for partition 1 succeeds as well.<br>      runEvent(makeCompletionEvent(taskSets(1).tasks(1), Success,<br>        result = MapStatus(BlockManagerId(\"hostE-exec2\ \"hostE\ 23456),<br>          Array.fill[Long](2)(2), mapTaskId = taskIdCount),<br>      ))<br>      taskIdCount += 1<br><br>      sc.listenerBus.waitUntilEmpty()<br>      assert(completedStage === List(0, 2))<br><br>      // the stages will now get resubmitted due to the failure<br>      Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)<br><br>      // parent map stage resubmitted<br>      assert(scheduler.runningStages.size === 1)<br>      val mapStage = scheduler.runningStages.head<br><br>      // Stage 1 is same as Stage 0 - but created for the ShuffleMapTask 2, as it is a<br>      // different job<br>      assert(mapStage.id === 1)<br>      assert(mapStage.la【test】Info.failureReason.isEmpty)<br>      // only the partition reported in fetch failure is resubmitted<br>      assert(mapStage.la【test】Info.numTasks === 1)<br><br>      val stage0Retry = taskSets.filter(_.stageId == 1)<br>      assert(stage0Retry.size === 1)<br>      // make the original task succeed<br>      runEvent(makeCompletionEvent(stage0Retry.head.tasks(fetchFailParentPartition), Success,<br>        result = MapStatus(BlockManagerId(\"hostF-exec1\ \"hostF\ 12345),<br>          Array.fill[Long](2)(2), mapTaskId = taskIdCount)))<br>      Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)<br><br>      // The retries should succeed<br>      sc.listenerBus.waitUntilEmpty()<br>      assert(completedStage === List(0, 2, 1, 2))<br><br>      // Now submit the entire dag again<br>      // This will add 3 new stages.<br>      submit(reduceRdd, Array(0, 1))<br>      Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)<br><br>      // Only the last stage needs to execute, and those tasks - so completed stages should not<br>      // change.<br>      sc.listenerBus.waitUntilEmpty()<br><br>      assert(completedStage === List(0, 2, 1, 2))<br><br>      // All other stages should be done, and only the final stage should be waiting<br>      assert(scheduler.runningStages.size === 1)<br>      assert(scheduler.runningStages.head.id === 5)<br>      assert(taskSets.count(_.stageId == 5) === 1)<br><br>      complete(taskSets.filter(_.stageId == 5).head, Seq((Success, 1), (Success, 2)))<br><br>      sc.listenerBus.waitUntilEmpty()<br>      assert(completedStage === List(0, 2, 1, 2, 5))<br>    }<br>  }<br>```<br><br>Would be good to adapt/【clean】 it up for your PR, in addition to the existing 【test】 - so that the observed bug does not recur.<br><br>(Good news is, this PR works against it :-) )<br><br>
> Instead of only 【test】ing 【specific】ally for the flag - which is subject to change as the implementation evolves, we should also 【test】 for behavior here.<br>> <br>> This is the reproducible 【test】 I was using (with some 【c<font color=blue>hang】es】) to 【test】 approaches for this bug - and it mimics the case I saw in our production reasonably well. (In DAGSchedulerSuite):<br>> <br>> ```<br>>   for (pushBasedShuffleEnabled <- Seq(true, false)) {<br>>     【test】(\"SPARK-40082: recomputation of shuffle map stage with no pending partitions should not \" +<br>>         s\"hang. pushBasedShuffleEnabled = $pushBasedShuffleEnabled\") {<br>> <br>>       if (pushBasedShuffleEnabled) {<br>>         initPushBasedShuffleConfs(conf)<br>>         DAGSchedulerSuite.clearMergerLocs()<br>>         DAGSchedulerSuite.addMergerLocs(Seq(\"host1\ \"host2\ \"host3\ \"host4\ \"host5\"))<br>>       }<br>> <br>>       var taskIdCount = 0<br>> <br>>       var completedStage: List[Int] = Nil<br>>       val listener = new SparkListener() {<br>>         override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {<br>>           completedStage = completedStage :+ event.stageInfo.stageId<br>>         }<br>>       }<br>>       sc.addSparkListener(listener)<br>> <br>>       val fetchFailParentPartition = 0<br>> <br>>       val shuffleMapRdd0 = new MyRDD(sc, 2, Nil)<br>>       val shuffleDep0 = new ShuffleDependency(shuffleMapRdd0, new HashPartitioner(2))<br>> <br>>       val shuffleMapRdd1 = new MyRDD(sc, 2, List(shuffleDep0), tracker = mapOutputTracker)<br>>       val shuffleDep1 = new ShuffleDependency(shuffleMapRdd1, new HashPartitioner(2))<br>> <br>>       val reduceRdd = new MyRDD(sc, 2, List(shuffleDep1), tracker = mapOutputTracker)<br>> <br>>       // submit the initial mapper stage, generate shuffle output for first reducer stage.<br>>       submitMapStage(shuffleDep0)<br>> <br>>       // Map stage completes 【success】fully,<br>>       completeShuffleMapStageSuccessfully(0, 0, 3, Seq(\"hostA\ \"hostB\"))<br>>       taskIdCount += 2<br>>       assert(completedStage === List(0))<br>> <br>>       // Now submit the first reducer stage<br>>       submitMapStage(shuffleDep1)<br>> <br>>       def createTaskInfo(speculative: Boolean): TaskInfo = {<br>>         val taskInfo = new TaskInfo(<br>>           taskId = taskIdCount,<br>>           index = 0,<br>>           attemptNumber = 0,<br>>           partitionId = 0,<br>>           launchTime = 0L,<br>>           executorId = \"\<br>>           host = \"hostC\<br>>           TaskLocality.ANY,<br>>           speculative = speculative)<br>>         taskIdCount += 1<br>>         taskInfo<br>>       }<br>> <br>>       val normalTask = createTaskInfo(speculative = false);<br>>       val speculativeTask = createTaskInfo(speculative = true)<br>> <br>>       // fail task 1.0 due to FetchFailed, and make 1.1 succeed.<br>>       runEvent(makeCompletionEvent(taskSets(1).tasks(0),<br>>         FetchFailed(makeBlockManagerId(\"hostA\"), shuffleDep0.shuffleId, normalTask.taskId,<br>>           fetchFailParentPartition, normalTask.index, \"ignored\"),<br>>         result = null,<br>>         Seq.empty,<br>>         Array.empty,<br>>         normalTask))<br>> <br>>       // Make the speculative task succeed after initial task has failed<br>>       runEvent(makeCompletionEvent(taskSets(1).tasks(0), Success,<br>>         result = MapStatus(BlockManagerId(\"hostD-exec1\ \"hostD\ 34512),<br>>           Array.fill[Long](2)(2), mapTaskId = speculativeTask.taskId),<br>>         taskInfo = speculativeTask))<br>> <br>>       // The second task, for partition 1 succeeds as well.<br>>       runEvent(makeCompletionEvent(taskSets(1).tasks(1), Success,<br>>         result = MapStatus(BlockManagerId(\"hostE-exec2\ \"hostE\ 23456),<br>>           Array.fill[Long](2)(2), mapTaskId = taskIdCount),<br>>       ))<br>>       taskIdCount += 1<br>> <br>>       sc.listenerBus.waitUntilEmpty()<br>>       assert(completedStage === List(0, 2))<br>> <br>>       // the stages will now get resubmitted due to the failure<br>>       Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)<br>> <br>>       // parent map stage resubmitted<br>>       assert(scheduler.runningStages.size === 1)<br>>       val mapStage = scheduler.runningStages.head<br>> <br>>       // Stage 1 is same as Stage 0 - but created for the ShuffleMapTask 2, as it is a<br>>       // different job<br>>       assert(mapStage.id === 1)<br>>       assert(mapStage.la【test】Info.failureReason.isEmpty)<br>>       // only the partition reported in fetch failure is resubmitted<br>>       assert(mapStage.la【test】Info.numTasks === 1)<br>> <br>>       val stage0Retry = taskSets.filter(_.stageId == 1)<br>>       assert(stage0Retry.size === 1)<br>>       // make the original task succeed<br>>       runEvent(makeCompletionEvent(stage0Retry.head.tasks(fetchFailParentPartition), Success,<br>>         result = MapStatus(BlockManagerId(\"hostF-exec1\ \"hostF\ 12345),<br>>           Array.fill[Long](2)(2), mapTaskId = taskIdCount)))<br>>       Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)<br>> <br>>       // The retries should succeed<br>>       sc.listenerBus.waitUntilEmpty()<br>>       assert(completedStage === List(0, 2, 1, 2))<br>> <br>>       // Now submit the entire dag again<br>>       // This will add 3 new stages.<br>>       submit(reduceRdd, Array(0, 1))<br>>       Thread.sleep(DAGScheduler.RESUBMIT_TIMEOUT * 2)<br>> <br>>       // Only the last stage needs to execute, and those tasks - so completed stages should not<br>>       // change.<br>>       sc.listenerBus.waitUntilEmpty()<br>> <br>>       assert(completedStage === List(0, 2, 1, 2))<br>> <br>>       // All other stages should be done, and only the final stage should be waiting<br>>       assert(scheduler.runningStages.size === 1)<br>>       assert(scheduler.runningStages.head.id === 5)<br>>       assert(taskSets.count(_.stageId == 5) === 1)<br>> <br>>       complete(taskSets.filter(_.stageId == 5).head, Seq((Success, 1), (Success, 2)))<br>> <br>>       sc.listenerBus.waitUntilEmpty()<br>>       assert(completedStage === List(0, 2, 1, 2, 5))<br>>     }<br>>   }<br>> ```<br>> <br>> Would be good to adapt/【clean】 it up for your PR, in addition to the existing 【test】 - so that the observed bug does not recur.<br>> <br>> (Good news is, this PR works against it :-) )<br><br>Thank you for your advice on the UT I wrote, it was very important to me. I will delete my UT. thanks again very much
@Stove-hust To clarify - I meant add this as well (after you had a chance to look at it and 【clean】 it up if required - this was from my 【test】 setup).<br>We should keep the UT you had added - and it is important to 【test】 the 【specific】 code expectation as it stands today.
> @Stove-hust To clarify - I meant add this as well (after you had a chance to look at it and 【clean】 it up if required - this was from my 【test】 setup). We should keep the UT you had added - and it is important to 【test】 the 【specific】 code expectation as it stands today.<br><br>Sorry, I misunderstood what you meant。😂<br>I think the UT  written by you is 【great】, can I write your UT in my PR, I will mark this part of UT written by you。<br>I have one more question, so for this PR we will have two UT， is that right？
Technically, 3 :-)<br>The UT that I added will generate 2 【test】s - one for push based shuffle and one without.<br>And we have the initial 【test】 you added.<br><br>You dont need to mark it as written by me ! We can include it in your PR - with any 【c<font color=blue>hang】es】 you make as part of the adding it.
> Technically, 3 :-) The UT that I added will generate 2 【test】s - one for push based shuffle and one without. And we have the initial 【test】 you added.<br>> <br>> You dont need to mark it as written by me ! We can include it in your PR - with any 【c<font color=blue>hang】es】 you make as part of the adding it.<br><br>【Thanks】 for your answer, I have added all three UTs (including you wrote)
The 【test】 failure is unrelated to this PR - once the 【c<font color=blue>hang】es】 above are made, the reexecution should pass
Merged to master.<br>【Thanks】 for working on this @Stove-hust !<br>【Thanks】 for the 【review】 @otterc :-)
I could not cherry pick this into 3.4 and 3.3 - we should fix for those branches as well IMO.<br>Can you create a PR against those two branches as well @Stove-hust ? 【Thanks】
> I could not cherry pick this into 3.4 and 3.3 - we should fix for those branches as well IMO. Can you create a PR against those two branches as well @Stove-hust ? 【Thanks】<br><br>No problem
Is your apache jira id `StoveM` @Stove-hust  ?
@mridulm <br>yep，it`s me<br>Username: StoveM<br>Full name: Fencheng Mei
cc @cloud-fan, It's appreciated if it can be 【review】ed in your convenience, thanks!
Java 17 GA failed: https://github.com/apache/spark/actions/runs/4318647315/jobs/7537203682<br><br>```<br>[info] - 【test】 implicit encoder resolution *** FAILED *** (1 second, 329 milliseconds)<br>4429[info]   2023-03-02T23:00:20.404434 did not equal 2023-03-02T23:00:20.404434875 (SQLImplicitsTestSuite.scala:63)<br>4430[info]   org.scala【test】.exceptions.TestFailedException:<br>4431[info]   at org.scala【test】.Assertions.newAssertionFailedException(Assertions.scala:472)<br>4432[info]   at org.scala【test】.Assertions.newAssertionFailedException$(Assertions.scala:471)<br>4433[info]   at org.scala【test】.Assertions$.newAssertionFailedException(Assertions.scala:1231)<br>4434[info]   at org.scala【test】.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)<br>4435[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.【test】Implicit$1(SQLImplicitsTestSuite.scala:63)<br>4436[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.$anonfun$new$2(SQLImplicitsTestSuite.scala:133)<br>4437[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>4438[info]   at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>4439[info]   at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>4440[info]   at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>4441[info]   at org.scala【test】.Transformer.apply(Transformer.scala:22)<br>4442[info]   at org.scala【test】.Transformer.apply(Transformer.scala:20)<br>4443[info]   at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>4444[info]   at org.scala【test】.TestSuite.withFixture(TestSuite.scala:196)<br>4445[info]   at org.scala【test】.TestSuite.withFixture$(TestSuite.scala:195)<br>4446[info]   at org.scala【test】.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)<br>4447[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>4448[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>4449[info]   at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>4450[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>4451[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>4452[info]   at org.scala【test】.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)<br>4453[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>4454[info]   at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>4455[info]   at scala.collection.immutable.List.foreach(List.scala:431)<br>4456[info]   at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>4457[info]   at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>4458[info]   at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>4459[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>4460[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>4461[info]   at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>4462[info]   at org.scala【test】.Suite.run(Suite.scala:1114)<br>4463[info]   at org.scala【test】.Suite.run$(Suite.scala:1096)<br>4464[info]   at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>4465[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>4466[info]   at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>4467[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>4468[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>4469[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.org$scala【test】$BeforeAndAfterAll$$super$run(SQLImplicitsTestSuite.scala:34)<br>4470[info]   at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>4471[info]   at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>4472[info]   at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>4473[info]   at org.apache.spark.sql.SQLImplicitsTestSuite.run(SQLImplicitsTestSuite.scala:34)<br>4474[info]   at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>4475[info]   at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>4476[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>4477[info]   at java.base/java.util.【concurrent】.FutureTask.run(FutureTask.java:264)<br>4478[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)<br>4479[info]   at java.base/java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)<br>4480[info]   at java.base/java.lang.Thread.run(Thread.java:833) <br>```
cc @HyukjinKwon <br>also cc @bjornjorgensen who reported this issue in dev mail list<br><br>
@LuciferYang Thank you, sir. <br>【LGTM】
friendly ping @dongjoon-hyun
https://github.com/LuciferYang/spark/actions/runs/4412451542/jobs/7732979313<br><br><img width=\"1148\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/224938008-bab8acde-7a62-46bb-b3a4-57dfb83bb12a.png\"><br><br>GA passed, status not update
【Thanks】 @dongjoon-hyun @HyukjinKwon @bjornjorgensen
https://github.com/apache/spark/actions/runs/4420600519<br><br><img width=\"1191\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/225388240-9f85593f-f6d6-47dd-be07-9ab906bf53a8.png\"><br><br>The la【test】 Java 17 daily 【test】 passed. 【Thanks】 all ~<br><br>
ping @huaxingao cc @cloud-fan @sadikovi
Yea, we should mention that they can set these options to false if query fails with JDBC errors.
> Thank you for update. BTW, `JDBCDialect` is a documented developer API. We should not change the default 【value】. We should keep the original default 【value】 to avoid a breaking change to 3rd party Dialect.<br>> <br><br>Thank you for the reminder. But these new API only exists in master branch.<br><br>
@dongjoon-hyun @cloud-fan @huaxingao @sadikovi Thank you.
@cloud-fan Can we merge it to master? After it I will try to refactor HiveGenericUDTF & HiveUDAFFunction. 【Thanks】!
It's fine. Can you enable the 【test】s to run?
@srowen I have enabled it, but now I don't know how to 【progress】. Is there a \"re-run\" button to re-trigger the build? Or do I push an empty commit into this branch?
Can you push an empty commit?
@ulysses-you
lgtm
thanks for 【review】, merging to master!
can you spare some precious time to 【review】? 【Thanks】 very much @cloud-fan
Seems like there are some more unchanged docstrings in several files as below:<br>```<br>spark % git grep \"Support Spark Connect\"<br>python/pyspark/sql/column.py:            Support Spark Connect.<br>python/pyspark/sql/column.py:            Support Spark Connect.<br>python/pyspark/sql/dataframe.py:            Support Spark Connect.<br>python/pyspark/sql/dataframe.py:            Support Spark Connect.<br>python/pyspark/sql/udf.py:            Support Spark Connect.<br>python/pyspark/sql/udf.py:            Support Spark Connect.<br>```<br>Btw, you can change the all strings at once by running the shell command in terminal such as:<br>```shell<br>find . -name \"*.py\" -exec perl -pi -e 's/Support Spark Connect/Supports Spark Connect/g' {} \\;<br>```
Looks good except https://github.com/apache/spark/pull/40401#issuecomment-1467125777
【LGTM】 pending CI
thank you @allanf-db , merged into master/branch-3.4
also cc @WeichenXu123 since this PR supports `df.collect` with UDT
@zhengruifeng Sorry, I missed your comment:<br><br>> will there be another PR for the support of UDT in `createDataFrame`?<br><br>No, this also enables UDT in `createDataFrame`.
@ueshin it seem that `createDataFrame` always use the underlying `sqlType` other than the UDT itself:<br><br>```<br>In [1]: from pyspark.ml.linalg import Vectors<br><br>In [2]: df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0,<br>   ...: 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0)),], [\"label\ \"weight\ \"【feature】s\"],)<br><br>In [3]: df.schema<br>Out[3]: StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('【feature】s', StructType([StructField('type', ByteType(), False), StructField('size', IntegerType(), True), StructField('indices', ArrayType(IntegerType(), False), True), StructField('【value】s', ArrayType(DoubleType(), False), True)]), True)])<br><br>In [4]: df.collect()<br>Out[4]: :>                                                          (0 + 4) / 4]<br>[Row(label=1.0, weight=1.0, 【feature】s=Row(type=1, size=None, indices=None, 【value】s=[0.0, 5.0])),<br> Row(label=0.0, weight=2.0, 【feature】s=Row(type=1, size=None, indices=None, 【value】s=[1.0, 2.0])),<br> Row(label=1.0, weight=3.0, 【feature】s=Row(type=1, size=None, indices=None, 【value】s=[2.0, 1.0])),<br> Row(label=0.0, weight=4.0, 【feature】s=Row(type=1, size=None, indices=None, 【value】s=[3.0, 3.0]))]<br>```<br><br>while in vanilla PySpark:<br>```<br>In [1]: from pyspark.ml.linalg import Vectors<br><br>In [2]: df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0,<br>   ...:    ...: 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0)),], [\"label\ \"weight\ \"【feature】s\"],)<br><br>In [3]: df.schema<br>Out[3]: StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('【feature】s', VectorUDT(), True)])<br><br>In [4]: df.collect()<br>Out[4]:                                                                         <br>[Row(label=1.0, weight=1.0, 【feature】s=DenseVector([0.0, 5.0])),<br> Row(label=0.0, weight=2.0, 【feature】s=DenseVector([1.0, 2.0])),<br> Row(label=1.0, weight=3.0, 【feature】s=DenseVector([2.0, 1.0])),<br> Row(label=0.0, weight=4.0, 【feature】s=DenseVector([3.0, 3.0]))]<br>```<br><br><br>also cc @WeichenXu123
I save a df with UDT in pyspark, and then read it in python client, and it works fine. So I guess something is wrong in <br>`createDataFrame`<br><br>vanilla PySpark:<br>```<br>In [1]: from pyspark.ml.linalg import Vectors<br><br>In [2]: df = spark.createDataFrame([(1.0, 1.0, Vectors.dense(0.0, 5.0)), (0.0, 2.0, Vectors.dense(1.0, 2.0)), (1.0, 3.0, Vectors.dense(2.0,<br>   ...:    ...: 1.0)), (0.0, 4.0, Vectors.dense(3.0, 3.0)),], [\"label\ \"weight\ \"【feature】s\"],)<br><br>In [3]: df.write.parquet(\"/tmp/tmp.pq\")<br>```<br><br>Python Client:<br>```<br>In [6]: df = spark.read.parquet(\"/tmp/tmp.pq\")<br><br>In [7]: df.schema<br>Out[7]: StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('【feature】s', VectorUDT(), True)])<br><br>In [8]: df.collect()<br>Out[8]: <br>[Row(label=0.0, weight=4.0, 【feature】s=DenseVector([3.0, 3.0])),<br> Row(label=0.0, weight=2.0, 【feature】s=DenseVector([1.0, 2.0])),<br> Row(label=1.0, weight=3.0, 【feature】s=DenseVector([2.0, 1.0])),<br> Row(label=1.0, weight=1.0, 【feature】s=DenseVector([0.0, 5.0]))]<br>```
@zhengruifeng ah, seems like something is wrong when the schema is a column name list.<br>Could you use `StructType` to specify the schema as a workaround?<br>I'll take a look later.
@ueshin Sure, thanks!<br><br>`StructType().add(\"label\ DoubleType()).add(\"weight\ DoubleType()).add(\"【feature】s\ VectorUDT(), False)` works, but the `nullable` in column `【feature】s` must be `False`, otherwise:<br>```<br>AnalysisException: [NULLABLE_COLUMN_OR_FIELD] Column or field `【feature】s`.`type` is nullable while it's required to be non-nullable.<br>```
@zhengruifeng I submitted two PRs: #40526 and #40527.
cc @JoshRosen @rednaxelafx @xinrong-meng
@zhengruifeng @HyukjinKwon May I get a 【review】, please?
A grouped data can also be created via `df.roll/cube/pivot`, do they works with `applyInPandas` ?
As long as the input relation is GroupedData, `applyInPandas` works.<br><br>An example is as shown below:<br>```sh<br>>>> df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],(\"id\ \"v\"))<br>>>> <br>>>> def normalize(pdf):<br>...     v = pdf.v<br>...     return pdf.assign(v=(v - v.mean()) / v.std())<br>... <br><br>>>> df.cube('id').applyInPandas(normalize, schema=\"id long, v double\").show()<br>+---+-------------------+<br>| id|                  v|<br>+---+-------------------+<br>|  1|-0.7071067811865475|<br>|  1| 0.7071067811865475|<br>|  2|-0.8320502943378437|<br>|  2|-0.2773500981126146|<br>|  2| 1.1094003924504583|<br>+---+-------------------+<br><br>>>> df.rollup('id', df.v).applyInPandas(normalize, schema=\"id long, v double\").show()<br>+---+----+                                                                      <br>| id|   v|<br>+---+----+<br>|  1|null|<br>|  1|null|<br>|  2|null|<br>|  2|null|<br>|  2|null|<br>+---+----+<br><br># I didn't find `df.pivot` in vanilla PySpark.<br>```<br>The above example is verified to have the same result as vanilla PySpark.<br><br>CC @zhengruifeng
Rebased to the la【test】 master; no new 【c<font color=blue>hang】es】 after 【review】s.
It has a conflict with 3.4. mind creating a PR to backport this?
Sure, I'll do. 【Thanks】 @HyukjinKwon!
I seperate this pr because it may affect other things, e.g., a custom shuffle exchange which support both columnar and row.<br>cc @cloud-fan @dongjoon-hyun
thanks, merging to master! @ulysses-you can we create a new PR for 3.4 to make sure all 【test】s pass?
@cloud-fan  created https://github.com/apache/spark/pull/40417
1.9.0 is out now <br>I have made a new PR https://github.com/apache/spark/pull/40878<br>I close this now.
ping @cloud-fan cc @zhengruifeng
@cloud-fan @dongjoon-hyun Thank you !
Thank you, @beliefer and @cloud-fan .
@zhouyejoe can you 【review】 this PR? i would be very grateful!
@mridulm Hello, can you recruit someone to help 【review】 this pr。I would appreciate for your help
【Thanks】 for creating the PR. Will 【review】 ASAP @Stove-hust
@Stove-hust I think the 【c<font color=blue>hang】es】 will help resolve the issue described in the ticket. I am 【check】ing more about what could be causing to the race conditions where there are two Executor containers starting at almost the same time on the same node, and they will all enter this code section and try creating subdirs at the same time.
@zhouyejoe I I kept the accident scene, should be able to help you。（In our clustered machine environment，11 HDD，creating 11 * 64 subdirectories would take longer to create）<br>23/02/21 10:44:09 YarnAllocator: Launching container container_184 on host hostA<br>23/02/21 10:44:21  YarnAllocator: Launching container container_185 on host hostA<br>23/02/21 10:44:21  YarnAllocator: Container container_184 on host: hostA was preempted.<br>
@mridulm
So to make sure I understand the issue here @Stove-hust  - we have a container started on a node, and got immediately killed (pre-empted/etc) while it was in the process of creating the subdirs (started, but did not complete) ?<br><br>If yes, very interesting corner case - will take a pass through the PR later this week, thanks for the 【contribution】 !
> So to make sure I understand the issue here @Stove-hust - we have a container started on a node, and got immediately killed (pre-empted/etc) while it was in the process of creating the subdirs (started, but did not complete) ?<br>> <br>> If yes, very interesting corner case - will take a pass through the PR later this week, thanks for the 【contribution】 !<br><br>yep，that's the scenario you described
Merging to master!
As last PR mentioned, @yaooqinn @shrprasa
Please use the jira-style prefix [SPARK-42785], not a GitHub issue-like one [SPARK #42785] like apache/kyuubi. Please also answer the rest of the questions in the PR description.
@zwangsheng @yaooqinn There won't be any NPE if you try to actually submit a job without the deploy-mode because if no 【value】 is provided, it defaults to \"client\".  Please refer to org.apache.spark.internal.config/package.scala.<br><br>```<br>  private[spark] val SUBMIT_DEPLOY_MODE = ConfigBuilder(\"spark.submit.deployMode\")<br>    .version(\"1.5.0\")<br>    .stringConf<br>    .createWithDefault(\"client\")<br>```
> @zwangsheng @yaooqinn There won't be any NPE if you try to actually submit a job without the deploy-mode because if no 【value】 is provided, it defaults to \"client\". Please refer to org.apache.spark.internal.config/package.scala.<br>> <br>> ```<br>>   private[spark] val SUBMIT_DEPLOY_MODE = ConfigBuilder(\"spark.submit.deployMode\")<br>>     .version(\"1.5.0\")<br>>     .stringConf<br>>     .createWithDefault(\"client\")<br>> ```<br><br>@shrprasa 【Thanks】 for reminder<br><br>I saw this default 【value】, you can do local 【test】 with writing following code<br>![popo_2023-03-14  17-50-46](https://user-images.githubusercontent.com/52876270/224981649-5e46838a-644e-4f2e-9566-e693f82473e5.jpg)<br><br>The default 【value】 `client` you mean, will be set in `prepareSubmitEnvironment` function, which be called in `runMain()` after 【check】 <br>```<br>args.deployMode.equals(\"client\") &&<br>```<br>
> Please use the jira-style prefix [[SPARK-42785](https://issues.apache.org/jira/browse/SPARK-42785)], not a GitHub issue-like one [SPARK #42785] like apache/kyuubi<br><br>Sure. XD<br><br>
I was able to reproduce the issue. 【Thanks】 @zwangsheng for fixing it.
@beliefer is there a chance you can this one moving again?
> @beliefer is there a chance you can this one moving again?<br><br>But https://github.com/apache/spark/pull/40358 already do it.
@dongjoon-hyun The reason is that we only support table cache query stage at master branch, so that 【test】 case is not valid. For branch-3.4, actually, this change only affects developers.
> actually, this change only affects developers.<br><br>Can we be a bit more 【specific】? I think it's a problem for table cache because it supports both row and columnar output. But shuffle/broadcast won't have the same issue.
If people build a custom shuffle exchange which support both row and columnar output. Logically, we have `ShuffleExchangeLike` for developers.
ping @huaxingao cc @cloud-fan
@srowen Thank you! @cloud-fan @huaxingao Thank you too!
Cc @Yikun too if you find some time to 【review】.
Could you resolve mypy 【check】? You can run the static 【analysis】 by running `dev/lint-python` locally.
> Could you resolve mypy 【check】? You can run the static 【analysis】 by running `dev/lint-python` locally.<br><br>@itholic  I've 【<font color=blue>fix】ed】 the issue with mypy in my 【c<font color=blue>hang】es】, but I still see three problems that don't relate to 【c<font color=blue>hang】es】. Should I fix them?<br><br>```<br>annotations failed mypy 【check】s:<br>python/pyspark/pandas/namespace.py:162: error: Cannot assign multiple types to name \"_range\" without an explicit \"Type[...]\" annotation  [misc]<br>python/pyspark/pandas/indexes/base.py:2093: error: unused \"type: ignore\" comment<br>python/pyspark/pandas/indexes/base.py:2163: error: unused \"type: ignore\" comment<br>Found 3 errors in 2 files (【check】ed 507 source files)<br>```<br>
Can you rebase to master and try running linter again??<br>If the problem still exists, yes, let's fix the it. We should make the PR pass the CI anyway.
cc @huaxingao @HeartSaVioR @cloud-fan @dongjoon-hyun @sunchao @viirya @gengliangwang
Let me fix failures.
The error classes were not ordered. Fixed that. The other Python failure does not seem related.
+1, 【LGTM】 【Thanks】 for the PR @aokolnychyi
+1, 【LGTM】 for Apache Spark 3.5.
Thank you, @aokolnychyi and all. Merged to master for Apache Spark 3.5
【Thanks】 for 【review】ing, @dongjoon-hyun @huaxingao @viirya @cloud-fan @sunchao!<br><br>I will follow up to address the comments (most likely on Monday).
I think we should file a jira to tracking this
> I think we should file a jira to tracking this<br><br>Done, pls 【check】 - https://issues.apache.org/jira/browse/SPARK-42803
The pr title should be `[SPARK-42803][CORE][SQL][ML] Use ... `
@NarekDW Are there any more similar cases?<br><br>cc @srowen FYI<br>
> @NarekDW Are there any more similar cases?<br>> <br>> cc @srowen FYI<br><br>@LuciferYang  no, these are all
FYI: https://github.com/rithwik-db/spark/runs/12002967878
This ticket will be closed for now; related 【c<font color=blue>hang】es】 may be in a V2 of the TorchDistributor
cc @hvanhovell, @HyukjinKwon
Thank you, @HyukjinKwon !
BTW, `branch-3.4` GitHub CI failure is still under 【investigation】 independently.
👌
cc @HeartSaVioR
@HeartSaVioR - 【test】s look good. Pls merge when you get a chance. Thx
Hi @LuciferYang, @MaxGekk, could you help me 【review】 this PR? Or do you know who would be more suitable to 【review】 it? 【Thanks】 a lot!
Looks OK pending 【test】s, but re-run 【test】s
rebased and re-run 【test】s, let's wait ci
All 【test】 passed
cc @bjornjorgensen, @yaooqinn , @srowen
Thank you, @yaooqinn !
Thank you, @srowen .
Thank you, @LuciferYang .
Thank you all! All 【test】s passed. Merged to master/3.4/3.3/3.2.
@dongjoon-hyun Thank you, sir. 👍
CC @panbingkun <br>so you too are aware of this and hopefully don't make the same mistake I did.
https://github.com/apache/spark/pull/40452 : I added a comment in `pom.xml` to prevent us from forgetting this
`sql - slow` failed, not sure whether it is related to 【c<font color=blue>hang】es】 in `modules.py`, let me investigate it first
Is it ready to merge ?
@WeichenXu123 not ready.<br><br>`sql slow` failed with message related to `mllib-common`:<br>```<br>[error] /home/runner/work/spark/spark/mllib/common/src/【test】/scala/org/apache/spark/ml/attribute/AttributeGroupSuite.scala:35:11: exception during macro expansion: <br>[error] java.util.MissingResourceException: Can't find bundle for base name org.scalactic.ScalacticBundle, locale en<br>[error] \tat java.util.ResourceBundle.throwMissingResourceException(ResourceBundle.java:1581)<br>[error] \tat java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1396)<br>[error] \tat java.util.ResourceBundle.getBundle(ResourceBundle.java:782)<br>[error] \tat org.scalactic.Resources$.resourceBundle$lzycompute(Resources.scala:8)<br>[error] \tat org.scalactic.Resources$.resourceBundle(Resources.scala:8)<br>[error] \tat org.scalactic.Resources$.pleaseDefineScalacticFillFilePathnameEnvVar(Resources.scala:256)<br>[error] \tat org.scalactic.source.PositionMacro$PositionMacroImpl.apply(PositionMacro.scala:65)<br>[error] \tat org.scalactic.source.PositionMacro$.genPosition(PositionMacro.scala:85)<br>[error] Caused by: java.io.IOException: Stream closed<br>[error] \tat java.util.zip.InflaterInputStream.ensureOpen(InflaterInputStream.java:67)<br>```<br><br><br>the current 【test】 seems fine, but let's wait for the CI
all 【test】s passed, merged into master
cc @MaxGekk @cloud-fan
+1, 【LGTM】. Merging to 3.4.<br>Thank you, @itholic and @cloud-fan @dongjoon-hyun @HyukjinKwon for 【review】.
cc @HyukjinKwon , @hvanhovell
Thank you, @HyukjinKwon and @LuciferYang . The `connect` module UT passed. Let me merge this because this is a removal of 【test】 coverage.
cc @itholic and @Yikun if you find some time to 【review】.
Looks pretty good., but let's wait until the [initial pandas 2.0 support](https://github.com/apache/spark/pull/40658) is done.
@cloud-fan Please take a look if you have time, thanks.
Friendly ping @cloud-fan , Please take a look if you find a time, thanks
I am not sure why we must keep consistent with Hive for such a case,<br><br>1. this is just output from the command line interface, not a programming API.<br>2. the `hive` CLI itself is already deprecated.<br><br>Why do we not always use schemas of spark's commands as the CLI header?<br>
@yaooqinn this is a good point. If we are sure this is only for CLI display, not thriftserver protocol, I agree we don't need to follow Hive.
> If we are sure this is only for CLI display,<br><br>Yes. hiveResultString is only used in spark-sql CLI. The thrift server-side always uses command output schema. Maybe this is the inconsistent issue we face internally
Yes. `hiveResultString` is added to ensure 【compatibility】 with hive output.<br><br>`hiveResultString` is only used by the spark-sql CLI. It is used only as the CLI display.<br><br>`thriftServer` always outputs as spark's schema,<br>1. `hiveResultString` is not used for the thrift protocol.<br>2. spark-sql output from the CLI and `thriftServer` is inconsistent.<br><br>I'm not sure why spark-sql CLI has to be compatible with hive output, personally, I don't think it's necessary. Maybe we should display spark's schema as is, just like thriftSever?
> I'm not sure why spark-sql CLI has to be compatible with hive output, personally, I don't think it's necessary. Maybe we should display spark's schema as is, just like thriftSever?<br><br>+1
Please take a look, thanks @cloud-fan @yaooqinn
Seems like we just need a small change: only invoke `hiveResultString` if a 【test】ing config is true, and we only set that 【test】ing config in `HiveComparisonTest`.
> Seems like we just need a small change: only invoke `hiveResultString` if a 【test】ing config is true, and we only set that 【test】ing config in `HiveComparisonTest`.<br><br>Return the result as a hive compatible sequence of strings of `hiveResultString`  currently does two things:<br>1. `HiveComparisonTest` used to compare 【compatibility】 with hive output;<br>2. Spark generates golden files for `SQLQueryTestSuite`;<br><br>2 is a friendly form of output, and I think we should keep it.<br><br>Edit:<br>Like `Array` datatype would output WrappedArray(col1, col2) instead of [col1, col2]
> Spark generates golden files for SQLQueryTestSuite;<br><br>I think golden files there should match `df.show` instead of hive result.
> > Spark generates golden files for SQLQueryTestSuite;<br>> <br>> I think golden files there should match `df.show` instead of hive result.<br><br>This may be a bit of a regression, maybe...  first, golden generation and validation are done with `hiveResultString `, it won't have what problem, then, the output is more 【readable】, finally, `ThriftServerQueryTestSuite` 【test】 case generation also uses the independent `hiveString`, this also needs to reconstruct. So what's the benefit of removal?
Like following:<br><img width=\"1440\" alt=\"image\" src=\"https://user-images.githubusercontent.com/51110188/230561881-177790a2-3b0b-4081-ba95-7e902af6b05f.png\"><br><br>And expression describes `ExpressionInfo`, it uses NULL instead of null (It caused `ExpressionInfoSuite` to fail); <br><br>I 【check】ed the output of `hiveResultString` and `df.show`, and the current display is just `null`<br>
I'm looking for consistency. `df.show` is what users see, and `hiveResultString` is for golden files. Shouldn't the golden file match what users really see? Why do we 【test】 something that is almost invisible to users?
> I'm looking for consistency. `df.show` is what users see, and `hiveResultString` is for golden files. Shouldn't the golden file match what users really see? Why do we 【test】 something that is almost invisible to users?<br><br>In fact, `SQLQueryTestSuite` does not 【test】 `df.show`, it is actually something under the `DataSet`, and `hiveResultString` only prints out the data of the `DataSet`, but I agree with you. It is better to print with spark's resultString
also cc @AngersZhuuuu
Adjusting `df.show` may need to change the output of `show` first. Some data 【value】s do not have a nice string representation yet
> Some data 【value】s do not have a nice string representation yet<br><br>Let's fix `df.show` first.
Hmm, how about add a legacy conf to indicate whether we should follow the hive format here? and default we use false.  I'm not sure if there will be users who use this place to connect to the internal system strictly in accordance with the hive format
Does `ThriftServerQueryTestSuite` also need to use sparkResultString golden file format?
@AngersZhuuuu are you programmatically parsing the output of the CLI? I thought it displays 【value】s for humans only.
https://github.com/apache/spark/pull/40699 is merged, can we continue this PR?
> #40699 is merged, can we continue this PR?<br><br>Sure
@cloud-fan This PR currently makes spark-sql CLI use the same spark result string as df.show.<br><br>And remove the use of hiveResultString by non-HiveComparisonTest. Like ExpressionInfoSuite, currently, only SQLQueryTestHelper and HiveComparisonTest use hiveResultString, and SQLQueryTestHelper is used for golden file generation. I try to make SQLQueryTestHelper also use sparkResultString, but ThriftServerQueryTestSuite also uses golden files.<br><br>And Beautify the output of cast StructType to StringType, Such as {1, 2} -> {a:1, b:2}.  (It has nothing to do with the theme of this PR, Just in passing when making ExpressionInfoSuite to use sparkResultString, If you mind, I can send another PR to do this).
https://github.com/apache/spark/pull/40922 has been merged, @Yikf can you open a PR to update `ToPrettyString` for all the needed 【c<font color=blue>hang】es】 to adjust the behavior of `df.show`?
> #40922 has been merged, @Yikf can you open a PR to update `ToPrettyString` for all the needed 【c<font color=blue>hang】es】 to adjust the behavior of `df.show`?<br><br>Sure, I will open PR to make `ToPrettyString` eval nice strings.<br><br>And, I will close this PR since I think `spark-sql` and `df.show` are two different REPLs, don't have to make them exactly the same to break 【compatibility】.
Overall 【LGTM】
【Thanks】 @amaliujia
friendly ping @HyukjinKwon @hvanhovell
rebased
【Thanks】 @HyukjinKwon @hvanhovell @amaliujia
@LuciferYang thanks for doing this.
BTW, @hvanhovell please take another look for a posthoc 【review】. I just took a look to make sure the impl is matched with PySpark side so I might have missed few things.
@HeartSaVioR  Please help 【review】 this PR, 【Thanks】.
Thank you all!
4.8.1 【<font color=blue>fix】ed】 a [compilation issue](https://github.com/davidB/scala-maven-plugin/issues/673) with Scala 3.x after 4.7.2
I do have some problems with this 【upgrade】. <br><br><br># with <scala-maven-plugin.version>4.8.0</scala-maven-plugin.version><br>./build/mvn -DskipTests 【clean】 package<br><br>[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:<br>[INFO] <br>[INFO] Spark Project Parent POM ........................... SUCCESS [  2.993 s]<br>[INFO] Spark Project Tags ................................. SUCCESS [  5.352 s]<br>[INFO] Spark Project Sketch ............................... SUCCESS [  6.497 s]<br>[INFO] Spark Project Local DB ............................. SUCCESS [  6.266 s]<br>[INFO] Spark Project Networking ........................... SUCCESS [  8.869 s]<br>[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  7.967 s]<br>[INFO] Spark Project Unsafe ............................... SUCCESS [  9.179 s]<br>[INFO] Spark Project Launcher ............................. SUCCESS [  4.749 s]<br>[INFO] Spark Project Core ................................. SUCCESS [01:55 min]<br>[INFO] Spark Project ML Local Library ..................... SUCCESS [ 22.125 s]<br>[INFO] Spark Project GraphX ............................... SUCCESS [ 26.436 s]<br>[INFO] Spark Project Streaming ............................ SUCCESS [ 39.594 s]<br>[INFO] Spark Project Catalyst ............................. SUCCESS [02:00 min]<br>[INFO] Spark Project SQL .................................. SUCCESS [02:39 min]<br>[INFO] Spark Project ML Common ............................ SUCCESS [ 19.772 s]<br>[INFO] Spark Project ML Library ........................... SUCCESS [01:46 min]<br>[INFO] Spark Project Tools ................................ SUCCESS [  4.027 s]<br>[INFO] Spark Project Hive ................................. SUCCESS [ 58.614 s]<br>[INFO] Spark Project REPL ................................. SUCCESS [ 15.526 s]<br>[INFO] Spark Project Assembly ............................. SUCCESS [  5.192 s]<br>[INFO] Kafka 0.10+ Token Provider for Streaming ........... SUCCESS [ 13.213 s]<br>[INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 19.317 s]<br>[INFO] Kafka 0.10+ Source for Structured Streaming ........ SUCCESS [ 28.580 s]<br>[INFO] Spark Project Examples ............................. SUCCESS [ 33.875 s]<br>[INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [ 11.099 s]<br>[INFO] Spark Avro ......................................... SUCCESS [ 25.772 s]<br>[INFO] Spark Project Connect Common ....................... SUCCESS [ 33.313 s]<br>[INFO] Spark Project Connect Server ....................... SUCCESS [ 27.918 s]<br>[INFO] Spark Project Connect Client ....................... SUCCESS [ 30.611 s]<br>[INFO] Spark Protobuf ..................................... SUCCESS [ 28.972 s]<br>[INFO] ------------------------------------------------------------------------<br>[INFO] BUILD SUCCESS<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Total time:  16:38 min<br>[INFO] Finished at: 2023-03-17T18:09:50+01:00<br>[INFO] ------------------------------------------------------------------------<br><br><br><br><br><br><br><br># with <scala-maven-plugin.version>4.8.1</scala-maven-plugin.version><br>./build/mvn -DskipTests 【clean】 package<br><br>[INFO] --- maven-compiler-plugin:3.11.0:compile (default-compile) @ spark-core_2.12 ---<br>[INFO] Not compiling main sources<br>[INFO] <br>[INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---<br>[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar<br>[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)<br>[INFO] compiling 597 Scala sources and 103 Java sources to /home/bjorn/github/spark/core/target/scala-2.12/classes ...<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: 【value】 sun<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: 【value】 sun<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: 【value】 Signal<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: 【value】 Signal<br>[ERROR] [Error] /home/bjorn/github/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: 【value】 Signal<br>[ERROR] 19 errors found<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:<br>[INFO] <br>[INFO] Spark Project Parent POM ........................... SUCCESS [  3.848 s]<br>[INFO] Spark Project Tags ................................. SUCCESS [ 12.106 s]<br>[INFO] Spark Project Sketch ............................... SUCCESS [ 10.685 s]<br>[INFO] Spark Project Local DB ............................. SUCCESS [  8.743 s]<br>[INFO] Spark Project Networking ........................... SUCCESS [  9.362 s]<br>[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  7.828 s]<br>[INFO] Spark Project Unsafe ............................... SUCCESS [  9.071 s]<br>[INFO] Spark Project Launcher ............................. SUCCESS [  4.776 s]<br>[INFO] Spark Project Core ................................. FAILURE [ 17.228 s]<br>[INFO] Spark Project ML Local Library ..................... SKIPPED<br>[INFO] Spark Project GraphX ............................... SKIPPED<br>[INFO] Spark Project Streaming ............................ SKIPPED<br>[INFO] Spark Project Catalyst ............................. SKIPPED<br>[INFO] Spark Project SQL .................................. SKIPPED<br>[INFO] Spark Project ML Common ............................ SKIPPED<br>[INFO] Spark Project ML Library ........................... SKIPPED<br>[INFO] Spark Project Tools ................................ SKIPPED<br>[INFO] Spark Project Hive ................................. SKIPPED<br>[INFO] Spark Project REPL ................................. SKIPPED<br>[INFO] Spark Project Assembly ............................. SKIPPED<br>[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED<br>[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED<br>[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED<br>[INFO] Spark Project Examples ............................. SKIPPED<br>[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED<br>[INFO] Spark Avro ......................................... SKIPPED<br>[INFO] Spark Project Connect Common ....................... SKIPPED<br>[INFO] Spark Project Connect Server ....................... SKIPPED<br>[INFO] Spark Project Connect Client ....................... SKIPPED<br>[INFO] Spark Protobuf ..................................... SKIPPED<br>[INFO] ------------------------------------------------------------------------<br>[INFO] BUILD FAILURE<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Total time:  01:24 min<br>[INFO] Finished at: 2023-03-17T18:13:26+01:00<br>[INFO] ------------------------------------------------------------------------<br>[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile (scala-compile-first) on project spark-core_2.12: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile failed: org.apache.commons.exec.ExecuteException: Process exited with an error: 255 (Exit 【value】: 255) -> [Help 1]<br>[ERROR] <br>[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.<br>[ERROR] Re-run Maven using the -X switch to enable full debug logging.<br>[ERROR] <br>[ERROR] For more information about the errors and possible solutions, please read the following articles:<br>[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException<br>[ERROR] <br>[ERROR] After correcting the problems, you can resume the build with the command<br>[ERROR]   mvn <args> -rf :spark-core_2.12
hmm... I run `./build/mvn -DskipTests 【clean】 package` three times with `4.8.1` on my Mac, they all executed 【success】fully ... I can't reproduce your issue ... <br><br>So, in what environment did you fail to compile? @bjornjorgensen
hmm.. ok. <br>On one PC it's manjaro with java -version<br>openjdk version \"17.0.6\" 2023-01-17<br>OpenJDK Runtime Environment (build 17.0.6+10)<br>OpenJDK 64-Bit Server VM (build 17.0.6+10, mixed mode)<br> <br>and the other that have the same problem it's ubuntu with openjdk 17. This one is running on github https://github.com/bjornjorgensen/jupyter-spark-master-docker/actions/runs/4448461787/jobs/7811305871#step:7:42144
When use Java 17.0.6 with 4.8.1, I can reproduce this issue<br><br>```<br>[INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---<br>[INFO] Compiler bridge file: /Users/yangjie01/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__61.0-1.8.0_20221110T195421.jar<br>[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)<br>[INFO] compiling 597 Scala sources and 103 Java sources to /Users/yangjie01/SourceCode/git/spark-mine-12/core/target/scala-2.12/classes ...<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: 【value】 sun<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: 【value】 sun<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: 【value】 Signal<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: 【value】 Signal<br>[ERROR] [Error] /Users/yangjie01/SourceCode/git/spark-mine-12/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: 【value】 Signal<br>[ERROR] 19 errors found<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:<br>[INFO] <br>[INFO] Spark Project Parent POM ........................... SUCCESS [  1.591 s]<br>[INFO] Spark Project Tags ................................. SUCCESS [  3.560 s]<br>[INFO] Spark Project Sketch ............................... SUCCESS [  4.014 s]<br>[INFO] Spark Project Local DB ............................. SUCCESS [  4.817 s]<br>[INFO] Spark Project Networking ........................... SUCCESS [  6.146 s]<br>[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  5.401 s]<br>[INFO] Spark Project Unsafe ............................... SUCCESS [  5.420 s]<br>[INFO] Spark Project Launcher ............................. SUCCESS [  3.365 s]<br>[INFO] Spark Project Core ................................. FAILURE [  9.098 s]<br>```<br><br>also cc @HyukjinKwon @panbingkun
I think GA can pass due to `-Djava.version=${JAVA_VERSION/-ea}`, I run `./build/mvn -DskipTests -Djava.version=17 package` with Java 17 can build pass @bjornjorgensen
./build/mvn -DskipTests -Djava.version=17 【clean】 package<br><br>[INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-tags_2.12 ---<br>[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar<br>[INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)<br>[INFO] compiling 2 Scala sources and 8 Java sources to /home/bjorn/github/spark/common/tags/target/scala-2.12/classes ...<br>[ERROR] '17' is not a valid choice for '-release'<br>[ERROR] bad option: '-release'<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:<br>[INFO] <br>[INFO] Spark Project Parent POM ........................... SUCCESS [  3.074 s]<br>[INFO] Spark Project Tags ................................. FAILURE [  1.211 s]<br>[INFO] Spark Project Sketch ............................... SKIPPED
```<br>[INFO] Compiler bridge file: /home/bjorn/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__55.0-1.8.0_20221110T195421.jar<br>```<br><br>@bjornjorgensen You need to make sure that the Java you're using is really 17(should be 2.12.17__61.0-1.8.0, not 55.0)
archlinux-java status<br>Available Java environments:<br>  java-11-openjdk<br>  java-17-openjdk (default)<br>[bjorn@amd7g ~]$ java --version<br>openjdk 17.0.6 2023-01-17<br>OpenJDK Runtime Environment (build 17.0.6+10)<br>OpenJDK 64-Bit Server VM (build 17.0.6+10, mixed mode)<br><br>This problem is why I wrote the email \"Failed to build master google protobuf protoc-3.22.0-linux-x86_64.exe\" to dev@spark.org <br><br><br>
I open a ticket for this at https://github.com/davidB/scala-maven-plugin/issues/684
> I open a ticket for this at [davidB/scala-maven-plugin#684](https://github.com/davidB/scala-maven-plugin/issues/684)<br><br>Next week, I will try to find a minimum case to reproducer problem. `Spark` is too complex for others 【:)】<br><br><br>
<img width=\"1118\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/226252485-27d82655-fdda-48dd-9611-dce44de69d66.png\"><br><br><br>@HyukjinKwon @bjornjorgensen @panbingkun In addition to the issues @bjornjorgensen  mentioned, when compiling using Java 8+Scala 2.13, I saw compilation errors as shown in the figure: `[ERROR] -release is only supported on Java 9 and higher<br>`, It seems that 4.8.1 and Java 8 are not compatible well, although it does not cause compilation failures<br><br><br>More, I saw https://github.com/davidB/scala-maven-plugin/issues/686, So I think we should revert this pr @HyukjinKwon <br><br>
https://github.com/apache/spark/pull/40482 I give a pr to revert this one
All 【test】s passed.<br><br>Merged to master and branch-3.4.
cc @slothspot @dongjoon-hyun @yaooqinn, please take a look when you get time, thanks.
@dongjoon-hyun UT is added, please take a look again, thanks.
Kindly ping @dongjoon-hyun
@dongjoon-hyun thank you, I updated the log message to<br>```<br>\"Application $appName with application ID $appId and submission ID $sId finished\"<br>```
Merged to master branch for Apache Spark 3.5.0.<br>Thank you, @pan3793 and @yaooqinn .
Thank you, @dongjoon-hyun and @yaooqinn
cc @viirya @cloud-fan thank you
Merged to master/branch-3.4. Thank you, @grundprinzip and all!
@mridulm @xkrogen @akpatnam25 @shuwang21 Please help 【review】.
AppVeyor failure (`【continuous】-integration/appveyor/pr`) should be fine to ignore for now.
cc @mridulm
【LGTM】. thanks!
Thank you for update. Merged to master/3.4.
Thank you @dongjoon-hyun @HyukjinKwon @shuwang21 @yaooqinn
Hi @gengliangwang this should be ready for a first look!
@dtenedor Since we already have `SQLQueryTestSuite` which has good basic Spark SQL 【feature】s coverage, shall we combine both? E.g. let `SQLQueryTestSuite` show analyzed plan/【optimized】 plan/execution results for comparison, and port more 【test】s from [zetasql](https://github.com/google/zetasql/tree/master/zetasql/analyzer/【test】data)<br>What is the reason for having a new 【analysis】 【test】 only?
@gengliangwang Sure, I was thinking about this too. We can reuse the same input SQL query files if we want, and just generate and 【test】 against different analyzer 【test】 output files. Let me update the PR to do that and I can ping the thread again.
@gengliangwang from past experience we will want to keep the query plans separate from the SQL results, otherwise the SQL results become hard to read. I will put the analyzer results in separate files.
> I will put the analyzer results in separate files.<br><br>Sounds 【great】! 【Thanks】 for the work!
@gengliangwang alright I made this change, please look again when you are ready.
【LGTM】 except for minor comments. 【Thanks】 for the work!
Seems GA break after this one:<br><br>- https://github.com/apache/spark/actions/runs/4467843445/jobs/7847830877<br>- https://github.com/apache/spark/actions/runs/4468075788/jobs/7848369978<br>- https://github.com/apache/spark/actions/runs/4468868780/jobs/7850199229<br><br><img width=\"874\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/226384714-ee30f2c5-495c-4709-8a82-78361e1d35b3.png\"><br>
I'm AFK at this time. @gengliangwang can you help to revert it if @dtenedor can't fix the failure soon?
Seems would be ok to regenerate the golden files<br><br>
https://github.com/apache/spark/pull/40492
Looks like some extra 【test】s got added just as this was getting merged! 【Thanks】 @LuciferYang for this fix 👍
【Thanks】 @dongjoon-hyun @yaooqinn
【LGTM】, thanks @williamhyun @dongjoon-hyun
It seems that there is some GitHub Action setting issue on William side.<br><br>Actually, I was the release manager of Apache ORC 1.8.3 and 【test】ed this here in my repo.<br>- https://github.com/dongjoon-hyun/spark/pull/13
Let me merge this~ Merged to master/3.4.
This function a bit special, It was discussed a long time ago ...<br>https://github.com/apache/spark/pull/37862#issuecomment-1249419779
> This function a bit special, It was discussed a long time ago ... [#37862 (comment)](https://github.com/apache/spark/pull/37862#issuecomment-1249419779)<br><br>If it's a public API, we should mark it; If not, we should correct our problem sooner or later.<br><br>cc @srowen
Personally, I think it's definitely possible to change it in Spark 4.0, but I wasn't sure before.<br><br>
Not a public API but probably not worth 'fixing' before spark 4 indeed
> Not a public API but probably not worth 'fixing' before spark 4 indeed<br><br>OK, Let's fix it after spark 4 release.<br>I will close it.
@HeartSaVioR - pls take a look. Thx
https://github.com/anishshri-db/spark/actions/runs/4434196358/jobs/7780002037<br><br>Looks like PySpark build is stuck but this change does not involve 【impact】 on PySpark. (If there is one, it should have failed in SQL/SS 【test】s.)
cc @cloud-fan @hvanhovell FYI
BTW, according to the JIRA, this is only for Apache Spark 3.5?
cc @HyukjinKwon @cloud-fan @dongjoon-hyun, thanks
Thank you for pinging me, @yaooqinn .
thank you @dongjoon-hyun
You're welcome!
Instead of proposing that the user uses another PySpark version, I think it's better to suggest that the user creates a Spark Driver session instead of a Spark Connect session.<br><br>So instead of:<br>[JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `_jsc` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, use the original PySpark instead of Spark Connect.<br><br>Perhaps something more like:<br>[JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute `_jsc` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session.
> [JVM_ATTRIBUTE_NOT_SUPPORTED] Attribute _jsc is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session.<br><br>Sounds good. Just applied the comments on error message.<br>【Thanks】!
@ueshin who last touched the type hints
Can you please explain more on scenarios when rebalancepartitions becomes child of locallimit? i tried  <br>SELECT  * FROM t WHERE id > 1 LIMIT 5; with spark 2.4.4 version and rebalancepartitions is not part of the plan.
> Can you please explain more on scenarios when rebalancepartitions becomes child of locallimit? i tried SELECT * FROM t WHERE id > 1 LIMIT 5; with spark 2.4.4 version and rebalancepartitions is not part of the plan.<br><br>Spark 2.4.4  does not have `RebalancePartitions`. Please see: https://issues.apache.org/jira/browse/SPARK-35786
> > Can you please explain more on scenarios when rebalancepartitions becomes child of locallimit? i tried SELECT * FROM t WHERE id > 1 LIMIT 5; with spark 2.4.4 version and rebalancepartitions is not part of the plan.<br>> <br>> Spark 2.4.4 does not have `RebalancePartitions`. Please see: https://issues.apache.org/jira/browse/SPARK-35786<br>Got it.thanks!
Merged to master for Apache Spark 3.5.
I don't quite get the rationale. For `SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5;`, the user explicitly requires to do a rebalance before limit, why do we remove it? It's a physical hint and we should respect it.
rebalance before limit feels meaningless.<br>If the user really wants to take n rows from each partition, the SQL should look like this:<br>```sql<br>SELECT *<br>FROM   (SELECT *,<br>               Row_number() OVER(partition BY pmod(Hash(id, 42), 200) ORDER BY id) AS rn<br>        FROM   tbl)<br>WHERE  rn <= 5;<br>```
The optimizer can remove meanless logical operators as it won't change the query result and it's usually more 【efficient】 to remove operators. But for physical APIs like rebalance and repartition, the users explicitly ask to take control of 【performance】 tuning and we should trust them. Besides, I don't think this is an always-beneficial 【optimization】, what if the limit is very large? We should let the user make the decision: they can remove the rebalance hint if they believe it makes the perf worse.  What do you think? @wangyum @dongjoon-hyun
Could we push the `LocalLimit` through `RebalancePartitions`?<br>```<br>== Optimized Logical Plan ==<br>GlobalLimit 5<br>+- LocalLimit 5<br>   +- RebalancePartitions<br>      +- LocalLimit 5<br>         +- Filter (isnotnull(id#0L) AND (id#0L > 1))<br>            +- Relation spark_catalog.default.tbl[id#0L] parquet<br>```
I don't think so. Local limit is per partition and rebalance 【c<font color=blue>hang】es】 partitioning.
oh wait, I thought the hint applies as the last operator. `SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5;` should rebalance be the root node for this query?
`HINT` is a part of `selectClause`.<br>https://github.com/apache/spark/blob/a3d9e0ae0f95a55766078da5d0bf0f74f3c3cfc3/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4#L530-L532<br><br>That's the reason why `LIMIT` is the root of the query.<br>```<br>scala> sql(\"SELECT /*+ REBALANCE */ * FROM t WHERE id > 1 LIMIT 5\").explain(true)<br>== Parsed Logical Plan ==<br>'GlobalLimit 5<br>+- 'LocalLimit 5<br>   +- 'UnresolvedHint REBALANCE<br>      +- 'Project [*]<br>         +- 'Filter ('id > 1)<br>            +- 'UnresolvedRelation [t], [], false<br>```
If such queries cannot be 【optimized】, the 【performance】 of such queries will be very poor. We use a partition to fetch data from MySQL, and increase its 【parallel】ism for downstream computing after fetching the data:<br><br>```sql<br>CREATE VIEW full_query_log<br>AS<br>SELECT h.* FROM query_log_hdfs h<br>UNION ALL<br>SELECT /*+ REBALANCE */ q.*, DATE(start) FROM query_log_mysql q;<br><br>SELECT * FROM full_query_log limit 5;<br>```<br><br>
OK now I got the use case. At the time when we add the rebalance hint, we don't know what the final query is. Shall we make this 【optimization】 a bit more conservative to match both global and local limit? I still think it's risky to remove rebalance below local limit.
+1 for @cloud-fan 's suggestion.
Could you make a follow-up, @wangyum ?
How about reverting this PR directly? Otherwise, we need to change `LimitPushDown`, which is not 【elegant】:<br>```scala<br>case Limit(exp, u: Union) if u.children.exists(_.isInstanceOf[HasPartitionExpressions]) =><br>  val newChildren = u.children.map {<br>    case r: RebalancePartitions =><br>      maybePushLocalLimit(exp, r.child)<br>    case r: RepartitionByExpression<br>        if r.partitionExpressions.nonEmpty && r.partitionExpressions.forall(_.deterministic) =><br>      maybePushLocalLimit(exp, r.child)<br>    case o =><br>      maybePushLocalLimit(exp, o)<br>  }<br>  Limit(exp, u.copy(children = newChildren))<br>```
why do we need to change `LimitPushDown`? We can't push limit through repartition.
It is meaningless to only optimize this case, because the user can manually remove the hint:<br>```sql<br>SELECT /*+ REBALANCE */ * FROM tbl LIMIT 5;<br>```<br><br>I'd like to optimize this case, since the user cannot manually remove the hint, because the view may be maintained by others:<br>```sql<br>CREATE VIEW full_query_log<br>AS<br>SELECT h.* FROM query_log_hdfs h<br>UNION ALL<br>SELECT /*+ REBALANCE */ q.*, DATE(start) FROM query_log_mysql q;<br><br>SELECT * FROM full_query_log limit 5;<br>```
Reverted this commit: https://github.com/apache/spark/commit/58facc16536cb2784aace38d6cca77bd055a3053.
Thank you for the decision, @wangyum .
@LuciferYang I am trying to get to your PRs in the next couple of days. My apologies for the delay.
> I am trying to get to your PRs in the next couple of days. My apologies for the delay.<br><br>No problem, thanks ~
This is very trivial so probably doesn't need a JIRA ticket. We can add [MINOR] in the title.
> This is very trivial so probably doesn't need a JIRA ticket. We can add [MINOR] in the title.<br><br>Thank you @cloud-fan. Done!
cc @dongjoon-hyun @sunchao @viirya
Thank you for pinging me, @kazuyukitanimura .
cc @cloud-fan and @HyukjinKwon , too.
Thank you, @kazuyukitanimura and all.<br>Merged to master for Apache Spark 3.5.
Just a general question which is for my self education: do we expect the results of `Column.explain` are 【stable】?
> Just a general question which is for my self education: do we expect the results of `Column.explain` are 【stable】?<br><br>Personally, I think Spark should keep the 【stable】 output and do not break change for users.
Can we use the existing golden file framework? I think we just need to add a bunch of 【test】 queries like `SELECT a + b`.<br><br>EDIT: sorry I just realized that this PR is to 【test】 the result of `Column.explain`. Why do we care about its 【stability】? The API doc says `Prints the expression to the console for 【de<font color=blue>bug】ging】 purposes`. We don't have 【stability】 【test】s for the plan EXPLAIN result either.
> EDIT: sorry I just realized that this PR is to 【test】 the result of `Column.explain`. Why do we care about its 【stability】? The API doc says `Prints the expression to the console for 【de<font color=blue>bug】ging】 purposes`. We don't have 【stability】 【test】s for the plan EXPLAIN result either.<br><br>Got it. I will close this PR.
ping @hvanhovell cc @HyukjinKwon @zhengruifeng @amaliujia @ueshin @LuciferYang
Do we need python side compatible API?
> Do we need python side compatible API?<br><br>Maybe. But PySpark does not have the API now.
https://github.com/apache/spark/pull/40528 used to replace this one.
Mind filing a JIRA please? cc @MaxGekk @itholic FYI
SQLQueryTestSuite failed, need re-generate the golden files
Found JIRA: SPARK-42838.<br><br>Can you attach the JIRA number to title?<br><br>e.g.<br>\"[SPARK-42838][SQL] changed error class name _LEGACY_ERROR_TEMP_2000\"
> Found JIRA: [SPARK-42838](https://issues.apache.org/jira/browse/SPARK-42838).<br>> <br>> Can you attach the JIRA number to title?<br>> <br>> e.g. \"[[SPARK-42838](https://issues.apache.org/jira/browse/SPARK-42838)][SQL] changed error class name _LEGACY_ERROR_TEMP_2000\"<br>@itholic <br>done!
Mind filling the PR description as well?
Nit: typo `DataFraem` in PR title and PR description 【:)】
late lgtm
cc @sigmod as well
cc @peter-toth @cloud-fan <br>Also cc @xinrong-meng for this being a potential Spark 3.4.0 release blocker.
【Thanks】 @rednaxelafx for the fix and pinging me.<br>I think you are right that `EquivalentExpressions.addExpr()` should be guarded by `supportedExpression()` if we guard `getExprState()`. But, I'm not sure it is right that we don't deduplicate the `max(transform(array(id), x -> x))` in your example query.<br>Probably the real issue here is that in `PhysicalAggregation` the class `EquivalentExpressions` is used for simply deduplicating whole expressions while on executors we use it for common subexpression elimination. In the former case we don't need the `LambdaVariable` guard but in the latter one we need it. So maybe we should add a argument to `EquivalentExpressions` to enable/disable the guards and in `PhysicalAggregation` we should disable it?
Just seeing this group of PRs (most notably https://github.com/apache/spark/pull/39046), was there a real reason `NamedLambdaVariable` was added into all this mix? If I understand right, it 【effective】ly eliminates all subexpression elimination involving any expressions containing higher-order functions at any nested level, even though it's perfectly valid to pull out a complete high-order function, you just can't pull out the `LambdaFunction` by itself. <br><br>Currently the 【check】 for `CodegenFallback` is what prevents the `LambdaFunction`'s from being considered for subexpression elimination. Quick plug for my 1.5 year old PR for adding codegen to HOFs https://github.com/apache/spark/pull/34558 simply adds `HigherOrderFunction` as a special case to only consider the arguments and not the functions themselves for subexpression elimination
Hm, I think you are right @Kimahriman, `LambdaVariable` and `NamedLambdaVariable` are very different and `NamedLambdaVariable` seem to be used only in `LambdaFunction`s, so https://github.com/apache/spark/pull/39046 doesn't make sense and actually it can prevent pulling out higher order functions and so cause 【performance】 regression... I think that PR should be reverted.<br>Update: I've filed a revert PR here: https://github.com/apache/spark/pull/40475<br><br><br>But I feel that is orthogonal to the issue that we use `EquivalentExpressions` for different purposes in `PhysicalAggregation` (the only place where we use `.addExpr()`) and in executors (`.addExprTree()` for subexpression elimination).<br>
@peter-toth could you please clarify why `supportedExpression()` was needed in `getExprState()` in the first place? i.e. why isn't it sufficient to add it to `addExprTree()`?
@Kimahriman I'd love to see a good CSE implementation for higher-order functions too. But for backporting the fix (which is this PR's primary intent) that would have been too much. For this one (or the one @peter-toth forked off) we're just aiming for a narrow fix that allows the aggregate to work again.
> @Kimahriman I'd love to see a good CSE implementation for higher-order functions too. But for backporting the fix (which is this PR's primary intent) that would have been too much. For this one (or the one @peter-toth forked off) we're just aiming for a narrow fix that allows the aggregate to work again.<br><br>Yeah I was just commenting on the related PR that broke CSE for anything using a HOF. I had plans for trying to do CSE inside a HOF but that stalled when I didn't get any traction on the initial adding codegen support
The 【check】 was added to `getExprState` in https://github.com/apache/spark/pull/39010, which is to avoid canonicalizing a subquery expression and leading to NPE.<br><br>I agree that we should be consistent and this PR 【LGTM】. Can we update the 【test】 case to use `LambdaVariable` as `NamedLambdaVariable` has been removed in https://github.com/apache/spark/pull/40475 ?
【Thanks】 for filing a separate JIRA.
> 【Thanks】 for filing a separate JIRA.<br><br>【Thanks】 @HyukjinKwon for the quick 【review】!<br>Actually I've just noticed that the SPARK-41468 follow-up PR was only merged to `master` (3.4 at that time) only: https://github.com/apache/spark/pull/39046#issuecomment-1347975823.<br>So probably a 【simple】 revert commit (wihtout a new ticket) on 3.4 and `master` would have been sufficient... For the same reason I'm removing the 3.3.2 affect version from SPARK-42852. Please let me know if you disagree.
Yeah so I just reverted it from master and 3.4 ;-).
@HyukjinKwon @LuciferYang @MaxGekk  @cloud-fan  cc
Thank you, @cloud-fan !
【Thanks】, @dongjoon-hyun @cloud-fan!
cc @zhengruifeng FYI
All passed
【Thanks】 @zhengruifeng @HyukjinKwon
cc @HyukjinKwon @panbingkun @bjornjorgensen FYI
【Thanks】, merged to master
【Thanks】 @yaooqinn @HyukjinKwon @bjornjorgensen @panbingkun
also cc @beliefer , https://github.com/apache/spark/pull/40355 maybe need rebase after this one
This PR is almost a pure cherry-pick of https://github.com/apache/spark/commit/fee47c77e5d31bce592bf6e2bd33c2dabfc57bd3, with minor 【c<font color=blue>hang】es】 on 【test】s to adapt to branch-3.4.
Merged to branch-3.4.
【Thanks】!
May I get a 【review】 please @zhengruifeng @HyukjinKwon ?
cc @LuciferYang  thanks!
It has a conflict w/ branch-3.4. mind creating a backport PR please?
@rednaxelafx, @cloud-fan let me know it this PR is a viable alternative to https://github.com/apache/spark/pull/40473. Or maybe if I should do a little 【clean】up like https://github.com/peter-toth/spark/commit/90421cb74dd0d0cfe2693cab39a25cd7892c0d45 in this or in a follow-up PR...
Before the recent rounds of 【c<font color=blue>hang】es】 to EquivalentExpressions, the old `addExprTree` used to call `addExpr` in its core:<br>https://github.com/apache/spark/blob/branch-2.1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/EquivalentExpressions.scala#L90<br>That was still the case when `PhysicalAggregation` started using this mechanism to deduplicate expressions. I guess it started becoming \"detached\" from the main path when the recent refactoring happened that allows updating a separate equivalence map instead of the \"main\" one.<br><br>Your proposed PR here further orphans that function from any actual use. Which is okay for keeping binary 【compatibility】 as much as possible.<br>The inlined dedup logic in `PhysicalAggregation` looks rather ugly though. I don't have a strong opinion about that as long as other 【review】ers are okay. I'd prefer still retaining some sort of encapsulated collection for the dedup usage.<br><br>BTW I updated my PR's 【test】 case because it makes more sense to 【check】 the return 【value】 from `addExpr` on the second invocation rather than on the first (both \"not supported expression\" and actual new expression cases would have gotten a `false` return 【value】 if we add that guard to the `addExpr()` function).<br>https://github.com/apache/spark/pull/40473/commits/28d101ee6765c5453189fa62d6b8ade1568d99d2
@LuciferYang if you can resolve the conflict (I merged your other change) I'll merge this
done, let's wait CI
【Thanks】 @srowen @HyukjinKwon
cc @viirya , too.
Looks good to me. +1
【Thanks】 @dongjoon-hyun @viirya ~
Hi, thanks @dongjoon-hyun for looking at it.<br>Our use case: we have an application which is using the `InProcessLauncher` class several times (to launch multiple spark jobs).<br>The first one is launched correctly but the next ones fail, trying to use the same configMaps created for the previous job.<br><br>I think the call chain is `InProcessLauncher` -> `InProcessSparkSubmit` -> `SparkSubmit` -> `KubernetesClientApplication` -> [Client](https://github.com/apache/spark/blob/ba1badd2adfd175c6680dff90e14b8aaa6cecd78/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L105) -> `KubernetesClientUtils`
Got it. Thank you.
Hi, sorry for the delay.<br><br>I will add some Unit Testing in order to validate the 【c<font color=blue>hang】es】 as you mention.
Any updates, @DHKold ?
> Any updates, @DHKold ?<br><br>Hello. We have the same issue (i'm the one who reported the bug in the JIRA issue). We have to maintain a fork with the change from val to def, and would be 【great】 if this fix was in the original source code.<br><br>If there is no update maybe we can help. Thx
Thank you for sharing, @ejblanco .
To collect more feedback, we are in the same situation as @ejblanco . Can also offer helping hand in case of need.
Hi, sorry, Iw was away for some time. What remains to do:<br>- add some Unit Tests for the propose 【c<font color=blue>hang】es】 (I'll try to do that this week)<br>- 【check】 if the configmap for the driver should also be generated 【dynamic】ally (I was not able to determine it, on our scenario it does not seem to produce any error)<br><br>If someone can investigate the second point, that would help.<br><br>【Thanks】
On our side we duplicated the code also because of driver config map being hard-coded as well. We are using same launcher to start multiple spark sessions hence it would lead to conflict
cc @cloud-fan @gengliangwang @dtenedor
+1, 【LGTM】. Merging to master.<br>Thank you, @LuciferYang and @dongjoon-hyun @gengliangwang @dtenedor for 【review】.
【Thanks】 All ~
@srowen FYI <br>
I did enable action on my forked repo and rebase to `apache/spark` master. <br>Can anyone re-run the workflows?
Mind double 【check】ing? Seems they are not running https://github.com/sudoliyang/spark/actions
@HyukjinKwon 【Thanks】. I ran the 【test】s at https://github.com/sudoliyang/spark/pull/2. <br>I would like to rebase and force push again to run all 【test】s here.
【LGTM】 if 【test】s pass
@HyukjinKwon the 【test】s are passing now, this is ready to merge if you are ready 【:)】
@dtenedor Wait a few minutes for me to 【check】 with Scala 2.13 manually
> @dtenedor Wait a few minutes for me to 【check】 with Scala 2.13 manually<br><br>done, should be ok ~
I think ANSI 【test】 fails after this PR:<br><br>```<br>[info] - timestampNTZ/datetime-special.sql_analyzer_【test】 *** FAILED *** (31 milliseconds)<br>[info]   timestampNTZ/datetime-special.sql_analyzer_【test】<br>[info]   Expected \"...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ...\ but got \"...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ...\" Result did not match for query #1<br>[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)<br>[info]   org.scala【test】.exceptions.TestFailedException:<br>[info]   at org.scala【test】.Assertions.newAssertionFailedException(Assertions.scala:472)<br>[info]   at org.scala【test】.Assertions.newAssertionFailedException$(Assertions.scala:471)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.Assertions.assertResult(Assertions.scala:847)<br>[info]   at org.scala【test】.Assertions.assertResult$(Assertions.scala:842)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.assertResult(AnyFunSuite.scala:1564)<br>[info]   at org.apache.spark.sql.SQLQueryTestSuite.$anonfun$readGoldenFileAndCompareResults$3(SQLQueryTestSuite.scala:777)<br>[info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)<br>```<br><br>https://github.com/apache/spark/actions/runs/4496107425/jobs/7910457741<br><br>@dtenedor mind taking a look please? cc @gengliangwang
Sure, I can take a look.<br><br>On Fri, Mar 24, 2023 at 3:12 AM Hyukjin Kwon ***@***.***><br>wrote:<br><br>> I think ANSI 【test】 fails after this PR:<br>><br>> [info] - timestampNTZ/datetime-special.sql_analyzer_【test】 *** FAILED *** (31 milliseconds)<br>> [info]   timestampNTZ/datetime-special.sql_analyzer_【test】<br>> [info]   Expected \"...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ...\ but got \"...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ...\" Result did not match for query #1<br>> [info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)<br>> [info]   org.scala【test】.exceptions.TestFailedException:<br>> [info]   at org.scala【test】.Assertions.newAssertionFailedException(Assertions.scala:472)<br>> [info]   at org.scala【test】.Assertions.newAssertionFailedException$(Assertions.scala:471)<br>> [info]   at org.scala【test】.funsuite.AnyFunSuite.newAssertionFailedException(AnyFunSuite.scala:1564)<br>> [info]   at org.scala【test】.Assertions.assertResult(Assertions.scala:847)<br>> [info]   at org.scala【test】.Assertions.assertResult$(Assertions.scala:842)<br>> [info]   at org.scala【test】.funsuite.AnyFunSuite.assertResult(AnyFunSuite.scala:1564)<br>> [info]   at org.apache.spark.sql.SQLQueryTestSuite.$anonfun$readGoldenFileAndCompareResults$3(SQLQueryTestSuite.scala:777)<br>> [info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)<br>><br>> https://github.com/apache/spark/actions/runs/4496107425/jobs/7910457741<br>><br>> @dtenedor <https://github.com/dtenedor> mind taking a look please? cc<br>> @gengliangwang <https://github.com/gengliangwang><br>><br>> —<br>> Reply to this email directly, view it on GitHub<br>> <https://github.com/apache/spark/pull/40496#issuecomment-1482557118>, or<br>> unsubscribe<br>> <https://github.com/notifications/unsubscribe-auth/AXU4PODW66JIEV5CNJWXEALW5VXQJANCNFSM6AAAAAAWBOMAJQ><br>> .<br>> You are receiving this because you were mentioned.Message ID:<br>> ***@***.***><br>><br><br><br>-- <br>Please write anonymous feedback for Daniel at any time (form<br><https://docs.google.com/forms/d/e/1FAIpQLSc-Nd9JOncZTMb2hlhj9GxQO0igStEBgtFclLXFsQleamA0ag/viewform?vc=0&c=0&w=1&flr=0><br>).<br>
@HyukjinKwon I ran the 【test】 locally and it passes. Maybe it is 【<font color=blue>fix】ed】 at head now?
```<br>[info] - timestampNTZ/datetime-special.sql_analyzer_【test】 *** FAILED *** (11 milliseconds)<br>[info]   timestampNTZ/datetime-special.sql_analyzer_【test】<br>[info]   Expected \"...date(999999, 3, 18, [false) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, fals]e) AS make_date(-1, ...\ but got \"...date(999999, 3, 18, [true) AS make_date(999999, 3, 18)#x, make_date(-1, 1, 28, tru]e) AS make_date(-1, ...\" Result did not match for query #1<br>[info]   select make_date(999999, 3, 18), make_date(-1, 1, 28) (SQLQueryTestSuite.scala:777)<br>[info]   org.scala【test】.exceptions.TestFailedException:<br>```<br><br>@dtenedor<br><br>Seems not 【<font color=blue>fix】ed】,  run <br><br>`SPARK_ANSI_SQL_MODE=true build/sbt \"sql/【test】Only org.apache.spark.sql.SQLQueryTestSuite\"`<br><br>can reproduce the failure
Looks like @LuciferYang 【<font color=blue>fix】ed】 it with https://github.com/apache/spark/pull/40552. 【Thanks】 so much for the fix!
@amaliujia can you add a 【test】 that 【check】s if the options are properly propagated from client to server?
@hvanhovell existing codebase uses this to verify the options:<br> <br>```<br>  【test】(\"SPARK-32844: DataFrameReader.table take the specified options for V1 relation\") {<br>    withSQLConf(SQLConf.USE_V1_SOURCE_LIST.key -> \"parquet\") {<br>      withTable(\"t\") {<br>        sql(\"CREATE TABLE t(i int, d double) USING parquet OPTIONS ('p1'='v1', 'p2'='v2')\")<br><br>        val df = spark.read.option(\"P2\ \"v2\").option(\"p3\ \"v3\").table(\"t\")<br>        val options = df.queryExecution.analyzed.collectFirst {<br>          case r: LogicalRelation => r.relation.asInstanceOf[HadoopFsRelation].options<br>        }.get<br>        assert(options(\"p2\") == \"v2\")<br>        assert(options(\"p3\") == \"v3\")<br>      }<br>    }<br>  }<br>  ```<br>  <br>  However this won't work for Spark Connect. Do you know if we have another way to achieve it?
@hvanhovell <br><br>oh are you thinking about golden file based 【test】s?
also cc @xinrong-meng this is from api auditing
【LGTM】, thank you @zhengruifeng !
@xinrong-meng  it seems this PR was already merged?
Merged to branch-3.4, thanks!
cc @viirya FYI
Yarn NM injects spark.yarn.app.container.log.dir as a system property, so we use ${sys:xxx} to refer it during logging initialization. <br><br>https://logging.apache.org/log4j/2.x/manual/lookups.html#system-properties-lookup
cc @wangyum @cloud-fan FYI
cc @cloud-fan @wangyum @LuciferYang <br>I run benchmark - org.apache.spark.sql.execution.datasources.json.JsonBenchmark, result as follow:<br><br>- CodeGen for get_json_object (on github)<br>https://github.com/panbingkun/spark/actions/runs/4489492515/jobs/7895384637<br><img width=\"922\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/226918094-4a943e82-151f-4d70-b29c-a16ef4087f30.png\"><br><br>- CodeGen for get_json_object (on local):<br><img width=\"899\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/226931986-8d8892b3-4f39-4e86-bd61-17f73f324184.png\"><br><br>- No CodeGen for get_json_object (on github)<br>https://github.com/panbingkun/spark/actions/runs/4489490118/jobs/7895379568<br><img width=\"920\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/226918535-df933355-bc2e-4b07-bbed-7f6010bcd42b.png\"><br><br>- No CodeGen for get_json_object (on local)<br><img width=\"898\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15246973/226931458-9f71d338-0b09-439a-8007-24c102ff5db6.png\"><br><br>
hmm... I think we should refactor `JsonBenchmark` to make get_json_object run w/ and w/o  code gen in one
> hmm... I think we should refactor `JsonBenchmark` to make get_json_object run w/ and w/o code gen in one<br><br>Ok, Let me do it.
@panbingkun I think we should also update `JsonBenchmark-jdk11-results.txt`, `JsonBenchmark-jdk17-results.txt` and `JsonBenchmark-results.txt` in this pr due to `JsonBenchmark` updated
> @panbingkun I think we should also update `JsonBenchmark-jdk11-results.txt`, `JsonBenchmark-jdk17-results.txt` and `JsonBenchmark-results.txt` in this pr due to `JsonBenchmark` updated<br><br>Done
> @panbingkun Could you resolve conflicts, please.<br><br>Let me update the results of `JsonBenchmark` again. Waiting for it.<br>- https://github.com/panbingkun/spark/actions/runs/5080796638<br>- https://github.com/panbingkun/spark/actions/runs/5080799367<br>- https://github.com/panbingkun/spark/actions/runs/5080801858<br><br>Thank you for 【review】ing it! @MaxGekk
> @panbingkun Could you resolve conflicts, please.<br><br>This is done.
@cloud-fan @grundprinzip @HyukjinKwon Could you 【review】 this PR, please.
Merging to master/3.4. Thank you, @cloud-fan @srielau @grundprinzip for 【review】.
Thank you, @amaliujia . <br><br>Also, could you 【review】 this PR, @viirya ?
Thank you, @viirya . Merged to master for Apache Spark 3.5.
@dongjoon-hyun , may I ask for your 【review】, since you did the original import of the GCS connector in [SPARK-33605](https://issues.apache.org/jira/browse/SPARK-33605)/#37745? Thank you!
Merged to master/3.4 for Apache Spark 3.4.0. This will be a part of next Apache Spark 3.4.0 RC.<br>I added you to the Apache Spark contributor group and assigned SPARK-42888 to you.<br>Welcome to the Apache Spark 【community】, @cnauroth .<br><br>Also, cc @sunchao
【LGTM】 too, thanks @cnauroth @dongjoon-hyun !
@dongjoon-hyun and @sunchao , thank you for the commit and the warm welcome!
@hvanhovell @cloud-fan
would you mind adding a 【simple】 【test】 here? <br>https://github.com/apache/spark/blob/149e020a5ca88b2db9c56a9d48e0c1c896b57069/python/pyspark/sql/【test】s/connect/【test】_connect_function.py#L1077-L1089
@dongjoon-hyun officially is a bit a broad term. As far as I am concerned ammonite is just a way to use the connect JVM client, it is not meant as a change for all of Spark (although I do think it might make sense). We use other libraries all the time, and I don't think we have ever discussed those in the mailing list.<br><br>As it stands there are two reasons for using ammonite:<br>- It is a better and easier to use REPL than the Scala REPL ever was.<br>- It is easier to customize. Connect is a good illustration, for the Scala REPL I will probably need to the same thing as we did for the spark REPL project and that is fork the source code. In ammonite this was a minor modification.<br><br>I do intent to add support for the regular REPL at some point, this was just easier to get started with.<br><br>If you feel strongly about this then I am happy to send a message to the dev-list.
Ya, I'm not against this nice 【improvement】. Just please shoot one email to the dev mailing list to give a headup. That's what I'm thinking that we need.
@dongjoon-hyun I send the email.
Thank you so much, @hvanhovell .
Interesting!<br><br>
cc @HyukjinKwon @hvanhovell FYI
Merged to master/3.4.<br>Thank you, @LuciferYang and @HyukjinKwon .
【Thanks】 @dongjoon-hyun @HyukjinKwon
cc @HyukjinKwon @WeichenXu123
merged into master
rebase due to https://github.com/apache/spark/pull/40516 merged
@LuciferYang nit: you need to update the PR description. There is an old file name `storage_level.proto`.
> @LuciferYang nit: you need to update the PR description. There is an old file name `storage_level.proto`.<br><br>【Thanks】 ~ 【<font color=blue>fix】ed】
> @LuciferYang . This looks worthy of having a new JIRA. Please create a new JIRA for this PR and use it. This PR is a good 【contribution】 of yours. 😄<br><br>@dongjoon-hyun 【Thanks】 for your suggestion ~ I created SPARK-42901 and updated the pr title 😄
GA passed ~
【Thanks】 @HyukjinKwon @dongjoon-hyun @ueshin
+1, late 【LGTM】.
branch-3.4 is handled via https://github.com/apache/spark/pull/40500 yesterday.
thanks for reivews
To address @HyukjinKwon 's concern about optimizer, <br><br>can we add `is_barrier` attribute into `UnaryNode`,<br>and if optimizer find a node marking `is_barrier` as True, then skip all 【optimization】s around the node.<br><br>CC @cloud-fan @mengxr WDYT ?<br><br><br>Barrier mode is only used in 【specific】 ML case, i.e. in model training routine, we will only use it in one pattern:<br><br>`dataset.mapInPandas(..., is_barrier=True).collect()`<br><br>and we don't need complex 【optimization】 for it.
hmmm why do we need to care about the optimizer? The optimizer is not sensitive to the physical execution engine, e.g. Presto, Spark, Flink have many similar SQL 【optimization】s.
> hmmm why do we need to care about the optimizer? The optimizer is not sensitive to the physical execution engine, e.g. Preso, Spark, Flink have many similar SQL 【optimization】s.<br><br>I am not familier with optimizer details but it is concern from @HyukjinKwon <br>But note this PR also 【c<font color=blue>hang】es】 logical plan operator like `MapInPandas`
Predicate pushdown is just an example. e.g., you might want to combine adjacent `MapInPandas`s but it would need a special handling if `is_barrier` flag is added.
I am saying that real 【power】 of Catalyst optimizer is to optimize/reorder these logical plans, and I believe that's the reason why barrier execution wasn't introduced in SQL. The barrier has to be created exactly when the call is invoked, in which basically it requires something like https://github.com/apache/spark/pull/19873 to have a sound implementation.<br><br>However, I am fine with having this as an exception if you guys are fine with this.
> Barrier mode is only used in 【specific】 ML case, i.e. in model training routine, we will only use it in one pattern:<br>> <br>> dataset.mapInPandas(..., is_barrier=True).collect()<br><br>> To simply the implementation, we can implement a barrierMapInPandasAndCollect instead, and define a execution plan stage like BarrierMapInPandasAndCollectExec<br><br>If it is the only usage case, i think it will be safe to add dedicated logical plan and physical plan for it.
> I am saying that real 【power】 of Catalyst optimizer is to optimize/reorder these logical plans, and I believe that's the reason why barrier execution wasn't introduced in SQL. The barrier has to be created exactly when the call is invoked, in which basically it requires something like #19873 to have a sound implementation.<br>> <br>> However, I am fine with having this as an exception if you guys are fine with this.<br><br>@cloud-fan What do you think of this ?
From a SQL engine's point of view, running all tasks at once or batch by batch doesn't matter. It doesn't change the semantics of the SQL operator, and the optimizer doesn't care about it. However, `mapInPandas` is a public API and you are free to define what's the expectation of the `is_barrier` parameter. To make our life easier, we can just define it as \"the tasks of running the pandas function must all be launched at once\ and it's not a barrier to the SQL operators. Then I think it's fine to just add a flag to the existing `MapInPandas` operator.
merged to master 【:)】
@zhengruifeng Hi, Would you mind have a 【review】?
Those are actually not real JIRAs or TODOs. These are the pointers of the original fix or ticket (that contains examples or code change). So I guess it's fine as is.
I think it is fine if we don't have avaliable ticket link.<br>It seems that those links point to issues before moving Kolas to Apache Spark.
【Thanks】 for your 【review】. I think some examples that cannot be linked to the correct places, which can make confused, and the original points provides some clear references.
Got it. 【Thanks】. +1, 【LGTM】 too.
Hi, @c21 @cloud-fan  this seems to be SMJ full outer join codegen bug, could you have a look at this issue ? 【Thanks】
【LGTM】, I would just add a unit 【test】 to CastSuite with relevant asserts for `Cast.needsTimeZone(from, to)` to prevent regressions
@MaxGekk This might take a little while. I don't want to delete the 【test】, but I also cannot just switch the data types over to Timestamp because the partition string is not always stored in a format that is compatible with TimestampType. So I ma going to have to spend some more time understanding the 【test】 to see if there is a good way to update it for TimestampType.
The remaining task at hand is to address [numerous mypy annotation issues](https://github.com/itholic/spark/actions/runs/4493385095/jobs/7904557221). If you have any good ideas for resolving linter, please feel free to let me know at any time :-)
CI passed. cc @HyukjinKwon @ueshin @xinrong-meng @zhengruifeng PTAL when you find some time.<br><br>I summarized key 【c<font color=blue>hang】es】 into PR description for 【review】.
Applied the comments. 【Thanks】 @HyukjinKwon and @zhengruifeng  for the 【review】.
@itholic Thank you, 【great】 work 【:)】 <br><br>After this PR <br>`from pyspark import pandas as ps `<br><br>ModuleNotFoundError                       Traceback (most recent call last)<br>File /opt/spark/python/pyspark/sql/connect/utils.py:45, in require_minimum_grpc_version()<br>     44 try:<br>---> 45     import grpc<br>     46 except ImportError as error:<br><br>ModuleNotFoundError: No module named 'grpc'<br><br>The above exception was the direct cause of the following exception:<br><br>ImportError                               Traceback (most recent call last)<br>Cell In[1], line 11<br>      9 import pyarrow<br>     10 from pyspark import SparkConf, SparkContext<br>---> 11 from pyspark import pandas as ps<br>     12 from pyspark.sql import SparkSession<br>     13 from pyspark.sql.functions import col, concat, concat_ws, expr, lit, trim<br><br>File /opt/spark/python/pyspark/pandas/__init__.py:59<br>     50     warnings.warn(<br>     51         \"'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to \"<br>     52         \"set this environment variable to '1' in both driver and executor sides if you use \"<br>   (...)<br>     55         \"already launched.\"<br>     56     )<br>     57     os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"<br>---> 59 from pyspark.pandas.frame import DataFrame<br>     60 from pyspark.pandas.indexes.base import Index<br>     61 from pyspark.pandas.indexes.category import CategoricalIndex<br><br>File /opt/spark/python/pyspark/pandas/frame.py:88<br>     85 from pyspark.sql.window import Window<br>     87 from pyspark import pandas as ps  # For running doc【test】s and reference resolution in PyCharm.<br>---> 88 from pyspark.pandas._typing import (<br>     89     Axis,<br>     90     DataFrameOrSeries,<br>     91     Dtype,<br>     92     Label,<br>     93     Name,<br>     94     Scalar,<br>     95     T,<br>     96     GenericColumn,<br>     97 )<br>     98 from pyspark.pandas.accessors import PandasOnSparkFrameMethods<br>     99 from pyspark.pandas.config import option_context, get_option<br><br>File /opt/spark/python/pyspark/pandas/_typing.py:25<br>     22 from pandas.api.extensions import ExtensionDtype<br>     24 from pyspark.sql.column import Column as PySparkColumn<br>---> 25 from pyspark.sql.connect.column import Column as ConnectColumn<br>     26 from pyspark.sql.dataframe import DataFrame as PySparkDataFrame<br>     27 from pyspark.sql.connect.dataframe import DataFrame as ConnectDataFrame<br><br>File /opt/spark/python/pyspark/sql/connect/column.py:19<br>      1 #<br>      2 # Licensed to the Apache Software Foundation (ASF) under one or more<br>      3 # contributor license agreements.  See the NOTICE file 【distributed】 with<br>   (...)<br>     15 # limitations under the License.<br>     16 #<br>     17 from pyspark.sql.connect.utils import 【check】_dependencies<br>---> 19 【check】_dependencies(__name__)<br>     21 import datetime<br>     22 import decimal<br><br>File /opt/spark/python/pyspark/sql/connect/utils.py:35, in 【check】_dependencies(mod_name)<br>     33 require_minimum_pandas_version()<br>     34 require_minimum_pyarrow_version()<br>---> 35 require_minimum_grpc_version()<br><br>File /opt/spark/python/pyspark/sql/connect/utils.py:47, in require_minimum_grpc_version()<br>     45     import grpc<br>     46 except ImportError as error:<br>---> 47     raise ImportError(<br>     48         \"grpc >= %s must be installed; however, \" \"it was not found.\" % minimum_grpc_version<br>     49     ) from error<br>     50 if LooseVersion(grpc.__version__) < LooseVersion(minimum_grpc_version):<br>     51     raise ImportError(<br>     52         \"gRPC >= %s must be installed; however, \"<br>     53         \"your version was %s.\" % (minimum_grpc_version, grpc.__version__)<br>     54     )<br><br>ImportError: grpc >= 1.48.1 must be installed; however, it was not found.        <br><br>`pip install grpc`<br><br>Collecting grpc<br>  Downloading grpc-1.0.0.tar.gz (5.2 kB)<br>  Preparing metadata (setup.py) ... error<br>  error: subprocess-exited-with-error<br>  <br>  × python setup.py egg_info did not run 【success】fully.<br>  │ exit code: 1<br>  ╰─> [6 lines of output]<br>      Traceback (most recent call last):<br>        File \"<string>\ line 2, in <module><br>        File \"<pip-setuptools-caller>\ line 34, in <module><br>        File \"/tmp/pip-install-vp4d8s4c/grpc_c0f1992ad8f7456b8ac09ecbaeb81750/setup.py\ line 33, in <module><br>          raise RuntimeError(HINT)<br>      RuntimeError: Please install the official package with: pip install grpcio<br>      [end of output]<br>  <br>  note: This error originates from a subprocess, and is likely not a problem with pip.<br>error: metadata-generation-failed<br><br>× Encountered error while generating package metadata.<br>╰─> See above for output.<br><br>note: This is an issue with the package mentioned above, not pip.<br>hint: See above for details.<br>Note: you may need to restart the kernel to use updated packages.<br><br>After <br>`pip install grpcio`  <br>then it works. <br><br><br>I don't think that ever pandas users that try pandas API on spark will use spark connect. So can we change this back?
Thank you for the feedback, @bjornjorgensen !<br><br>IMHO, it seems more reasonable to add `grpcio` as a dependency for the Pandas API on Spark instead of reverting all this change back (Oh, btw seems like you already open a https://github.com/apache/spark/pull/40716 for fixing the existing issue 😄)<br><br>From my understanding, the purpose of Spark Connect is to allow users to use existing PySpark project without any code 【c<font color=blue>hang】es】 through a remote client. Therefore, if a user is using the `pyspark.pandas` module in their existing code, it should work the same way through the remote client as well.<br><br>So I think we should support all the 【functionality】 of PySpark as much as possible including pandas API on Spark, since nobody can not sure whether existing PySpark users will use the Pandas API on Spark through Spark Connect or not at this point, not only the existing pandas users.<br><br>Alternatively, we might be able to create completely separate package path for the Pandas API on Spark for Spark Connect. This would allow the existing Pandas API on Spark to be used without installing `grpcio`, but IMHO it would be much more overhead than simply changing the policy to add one package as an additional installation.<br><br>WDYT? also cc @HyukjinKwon @grundprinzip @ueshin @zhengruifeng FYI
The problem is that you don't use Spark Connect but it complains that it needs `grpcio`
For example, you can't just import `from pyspark.sql.connect.column import Column as ConnectColumn` at `pyspark/pandas/_typing.py`. Even you don't use Spark connect, it will 【check】 the dependency. Can we avoid this?
Got it. Then I think we need to modify the current code to import the `ConnectColumn` only when `is_remote()` is `True` across all code paths. For example:<br><br>**Before**<br>```python<br>from pyspark.sql.column import Column as PySparkColumn<br>from pyspark.sql.connect.column import Column as ConnectColumn<br>from pyspark.sql.utils import is_remote<br><br>def example():<br>    Column = ConnectColumn if is_remote() else PySparkColumn<br>    return Column.__eq__<br>```<br><br>**After**<br>```python<br>from pyspark.sql.column import Column as PySparkColumn<br>from pyspark.sql.utils import is_remote<br><br>def example():<br>    if is_remote():<br>        from pyspark.sql.connect.column import Column as ConnectColumn<br>        Column = ConnectColumn<br>    else:<br>        Column = PySparkColumn<br>    return Column.__eq__<br>```<br><br>Also, we should remove `GenericColumn` and `GenericDataFrame`.<br><br>Can you happen to think of a better solution? As of now, I haven't come up with a better way other than this.<br><br>【Thanks】!
\"The problem is that you don't use Spark Connect but it complains that it needs grpcio\"<br>Yes, this is correct 【:)】 <br>https://github.com/apache/spark/pull/40716 was for a typo so users of connect will know how to install the package.<br><br>The is_remote() can probably work. <br>Perhaps we can look at how pyspark did solve this?
We need to be careful in changing the imports too early or too late. The way that the is remote 【functionality】 works 【check】s for an environment variable. If it's not set during the import setting it later yields undesired consequences. <br><br>Is it so had to add the dependency for grpc when using the pandas API? <br><br>What are we achieving with this? GRPC is a 【stable】 protocol and not a random library. It's 【available】 throughout all platforms. <br><br>What's the benefit of trying this pure approach?
One of the key thing with pandas API on spark is that users only have to change `pd` to `ps` in the import. But now they also have to install GRPC. <br><br>Does this PR introduce any user-facing change?<br>Yes, every pandas API on spark user need to install GRPC.
That's only partially true, they still have to install PySpark. If they pip install PySpark[connect] the dependencies are properly resolved.
Actually even today Pandas on Spark users have to install 【specific】 dependencies that are not part of the default PySpark installation. <br><br>See https://github.com/apache/spark/blob/master/python/setup.py<br><br>Now, the quickest fix is to simply add grpcio to the pandas on spark dependency list. <br><br>Given that we're forcing Pandas users into 【specific】 Java versions etc adding grpcio does not make it worse in my opinion. <br><br>The user surface remains the same.
Those modules that pandas Api on spark needs are pandas, pyarrow, numpy. Witch every pandas users already have installed. <br>And now they have to add connect, grpcio, grpcio-status and googleapis-common-protos.<br><br>Why does pandas API on spark need such high coupling on the connect module?
IIUC only grpcio is needed. The rest is localized to the spark connect client. <br><br>
> Is it so had to add the dependency for grpc when using the pandas API?<br><br>It's not super hard. But it's a bit odd to add this alone to pandas API on Spark. We should probably think about adding grpcio as a hard dependency for whole PySpark project, but definitely not alone for pandas API on Spark.<br><br>> What are we achieving with this? GRPC is a 【stable】 protocol and not a random library. It's 【available】 throughout all platforms.<br>> What's the benefit of trying this pure approach?<br><br>So for the current status, we're trying to add the dependencies that the module need so users won't need to install the unnecessary dependency. In addition, adding the dependency breaks existing applications when they migrate from 3.4 to 3.5. It matters when PySpark is installed without `pip` (which is actually the official release channel of Apache Spark).<br><br>
So my proposal is to keep it consistent for now (that dose not require `grpcio` when we don't use Spark Connect). And separately discuss if we should switch them to a hard required dependency.
> We should probably think about adding grpcio as a hard dependency for whole PySpark project, but definitely not alone for pandas API on Spark<br><br>I think this point is particularly crucial even though all of your opinions make sense to me.<br>To summarize, I will made a fix following the suggestion in https://github.com/apache/spark/pull/40525#issuecomment-1501097772 so that the Pandas API on Spark can work without `grpcio` for now. Then, we can discuss adding the `grpcio` dependency to the entire PySpark project if needed in the future.<br>Thank you for all the feedback!
also cc @WeichenXu123
thank you @ueshin  for resolve this issue !<br><br>merged into master/branch-3.4
This PR has not been implemented yet. But we can see the implementation method.<br>@hvanhovell Could you take a look? Can this meet your expectations?<br><br>cc @HyukjinKwon @ueshin @zhengruifeng
ping @hvanhovell
ping @hvanhovell Could you have time to take a look?
@hvanhovell Could you take a look again?
cc @HyukjinKwon @cloud-fan @dongjoon-hyun thanks
cc @dongjoon-hyun @yaooqinn @Yikun
> To @pan3793 and @yaooqinn . IMO, what we need is only one additional line at the end of the replacement. WDYT?<br>> <br>> ```scala<br>> .replaceAll(\"^[0-9]\ \"x\")<br>> ```<br><br>Yes, this approach does not introduce breaking change, updated as suggested
Hi @dongjoon-hyun thanks for pinging me, SGTM
Merged to master/3.4/3.3/3.2.
It doesn't have UT in vanilla PySpark, so I only add the doc【test】
It has a conflict with branch-3.4. mind creating a backport please?
@HyukjinKwon 【Thanks】 for 【review】s. What about only adding it to master? since it 【c<font color=blue>hang】es】 the dependency, so I want to be a bit conservative 【:)】
Sure that's fine.
btw, the examples in \"Does this PR introduce any user-facing change?\" are the same??
> the examples in \"Does this PR introduce any user-facing change?\" are the same??<br><br>No, previously we still see `py4j.protocol.Py4JJavaError` or `SparkConnectGrpcException` and now we only see the actual exception classes.
Ah I see. One for regular Spark session and the other for remote Spark session.
@ueshin it has a conflict w/ branch-3.4. would you mind creating a backport PR?
@HyukjinKwon #40547
【Thanks】 for the 【review】 @zhengruifeng ! Will address the comments soon
Updated!
@itholic it has a conflict against branch-3.4. Mind creating a backport PR please
Backport: https://github.com/apache/spark/pull/40554
@xinrong-meng
cc @cloud-fan @HyukjinKwon @dongjoon-hyun thanks
> If there is a DBMS-【specific】 magic-number, shall we make a constant for that in our directs?<br><br>I agree. But the LOC may be trivial, while the behavior change is critical. So I suggest we introduce another thread for this change.
cc @cloud-fan @dongjoon-hyun <br>After SPARK-43049 and SPARK-42943, which bidirectionally redirect StringType to other types, the round-trip char/varchar type mapping can be safely applied. Please retake a look.
There is a minor build failure (unused import) - but please feel free to merge after fixing it and CI passes.
@mridulm 【Thanks】 for the 【review】.<br>Merging to master
cc @dtenedor @HyukjinKwon FYI
I am so sorry to break the build again :| thanks for fixing it! It looks like we need separate regular and ANSI 【test】 cases now!
【Thanks】 @HyukjinKwon @dtenedor
@rxin @ScrapCodes @maropu  can you please 【review】 . thanks!
Hi @VindhyaG, this might be useful - may be we can benefit from the usecase you have for this. Is it just for logging? <br>Not sure what others think, it might be good to limit the API surface.
> Hi @VindhyaG, this might be useful - may be we can benefit from the usecase you have for this. Is it just for logging? Not sure what others think, it might be good to limit the API surface.<br><br>@ScrapCodes thanks for 【review】ing. Bug lists three ways API could be used which i have added in PR description above. Do you have any suggestions to limit API surface?
I see this as developer facing API, So just having <br>```<br>def getString(numRows: Int, truncate: Int): String =<br>     getString(numRows, truncate, vertical = false)<br>```<br>would suffice.
Do you think, a more interesting way can be returning a JSON representation?
> I see this as developer facing API, So just having<br>> <br>> ```<br>> def getString(numRows: Int, truncate: Int): String =<br>>      getString(numRows, truncate, vertical = false)<br>> ```<br>> <br>> would suffice.<br><br>my intention was to keep it consistent with show() where if numrows and truncate have default 【value】s
> Do you think, a more interesting way can be returning a JSON representation?<br>@ScrapCodes  If use case is to send the it via rest api(as listed in use case above) JSON would make more sense but for logging i suppose string in tabular form is more useful? may be have a separate API for that ?
@ScrapCodes can you pls suggest how can we go ahead on this?<br>@jaceklaskowski  have updated the versions. can you pls 【review】 again.
Oh im sorry. Actually we don't need this for 3.4 since pandas API in spark will be in 3.5 only
np!
Shall we close this PR because Apache Parquet 1.12.4 vote seems to fail due to the technical issue.<br><br>For the record, the release manage creates it from the wrong branch.<br>- https://github.com/apache/parquet-mr/commit/22069e58494e7cb5d50e664c7ffa1cf1468404f8<br>```<br>- <version>1.13.0-SNAPSHOT</version><br>+ <version>1.12.4</version><br>```
TPC-DS benchmark result:<br>Query | Parquet 1.13.0(first time) | Parquet 1.12.3(first time) | Parquet 1.13.0(second time) | Parquet 1.12.3(second time) | Parquet 1.13.0(third time) | Parquet 1.12.3(third time)<br>-- | -- | -- | -- | -- | -- | --<br>q1.sql | 37.819 | 37.786 | 36.322 | 37.59 | 37.772 | 36.776<br>q2.sql | 42.132 | 41.513 | 43.189 | 42.274 | 42.859 | 42.605<br>q3.sql | 5.933 | 6.1 | 6.082 | 6.071 | 6.128 | 6.094<br>q4.sql | 335.051 | 319.173 | 322.396 | 320.977 | 324.464 | 326.822<br>q5.sql | 78.41 | 76.631 | 76.841 | 76.37 | 78.257 | 76.502<br>q6.sql | 9.006 | 9.11 | 8.737 | 8.577 | 8.729 | 9.05<br>q7.sql | 12.881 | 12.731 | 12.685 | 12.662 | 12.606 | 12.675<br>q8.sql | 10.122 | 10.092 | 10.035 | 10.853 | 10.277 | 10.841<br>q9.sql | 72.562 | 71.942 | 73.649 | 73.04 | 72.899 | 72.01<br>q10.sql | 14.127 | 13.075 | 14.276 | 13.913 | 13.281 | 13.229<br>q11.sql | 111.334 | 111.612 | 110.952 | 110.776 | 111.686 | 112.27<br>q12.sql | 3.138 | 3.854 | 3.187 | 3.613 | 3.437 | 3.306<br>q13.sql | 13.131 | 12.676 | 12.516 | 12.417 | 12.739 | 12.987<br>q14a.sql | 217.664 | 213.632 | 214.655 | 213.333 | 217.601 | 213.341<br>q14b.sql | 191.553 | 182.775 | 184.35 | 187.004 | 188.313 | 189.876<br>q15.sql | 10.308 | 10.46 | 10.304 | 9.901 | 10.175 | 10.307<br>q16.sql | 81.97 | 82.059 | 82.41 | 81.263 | 83.179 | 82.042<br>q17.sql | 28.876 | 28.905 | 30.41 | 29.573 | 29.555 | 28.837<br>q18.sql | 14.183 | 13.929 | 14.11 | 14.466 | 13.969 | 14.022<br>q19.sql | 6.611 | 7.593 | 6.652 | 6.659 | 6.446 | 6.533<br>q20.sql | 3.263 | 3.701 | 3.56 | 3.503 | 3.53 | 3.627<br>q21.sql | 2.252 | 2.188 | 2.249 | 2.128 | 2.161 | 2.252<br>q22.sql | 14.809 | 14.715 | 14.324 | 14.266 | 14.567 | 14.123<br>q23a.sql | 554.385 | 544.75 | 546.213 | 542.194 | 553.784 | 547.388<br>q23b.sql | 781.236 | 768.367 | 770.584 | 776.065 | 776.502 | 776.006<br>q24a.sql | 196.806 | 193.989 | 197.608 | 194.416 | 194.71 | 192.817<br>q24b.sql | 176.56 | 183.084 | 177.486 | 177.936 | 177.776 | 177.389<br>q25.sql | 22.323 | 22.089 | 22.665 | 22.049 | 22.248 | 22.317<br>q26.sql | 8.574 | 8.356 | 8.174 | 8.753 | 8.186 | 8.302<br>q27.sql | 9.056 | 8.252 | 8.37 | 8.319 | 8.516 | 8.38<br>q28.sql | 102.185 | 102.382 | 102.344 | 103.058 | 102.024 | 102.786<br>q29.sql | 75.655 | 75.604 | 75.217 | 75.532 | 75.835 | 76.024<br>q30.sql | 12.476 | 12.966 | 13.039 | 14.108 | 12.19 | 13.143<br>q31.sql | 26.343 | 27.632 | 26.337 | 26.791 | 26.74 | 26.098<br>q32.sql | 3.251 | 3.41 | 3.378 | 3.333 | 3.371 | 3.516<br>q33.sql | 7.143 | 6.125 | 6.85 | 6.718 | 7.067 | 6.615<br>q34.sql | 8.53 | 8.656 | 8.536 | 8.866 | 8.358 | 8.589<br>q35.sql | 35.212 | 35.571 | 35.659 | 37.631 | 36.292 | 35.603<br>q36.sql | 9.264 | 9.166 | 9.748 | 9.488 | 9.45 | 9.469<br>q37.sql | 36.368 | 35.881 | 37.023 | 36.578 | 35.823 | 36.7<br>q38.sql | 74.58 | 73.472 | 72.926 | 73.823 | 71.097 | 73.329<br>q39a.sql | 8.596 | 7.637 | 8.036 | 7.984 | 7.849 | 7.88<br>q39b.sql | 7.233 | 6.641 | 6.278 | 7.06 | 6.595 | 6.691<br>q40.sql | 17.34 | 16.558 | 16.448 | 16.864 | 16.432 | 16.413<br>q41.sql | 1.223 | 1.105 | 1.103 | 1.182 | 1.232 | 1.304<br>q42.sql | 2.464 | 2.441 | 2.554 | 2.544 | 2.314 | 2.393<br>q43.sql | 7.477 | 7.396 | 7.394 | 7.764 | 7.381 | 7.534<br>q44.sql | 30.228 | 30.516 | 30.859 | 31.057 | 30.372 | 29.008<br>q45.sql | 9.93 | 10.089 | 9.874 | 10.075 | 9.802 | 9.838<br>q46.sql | 9.544 | 9.949 | 9.503 | 9.755 | 9.395 | 9.25<br>q47.sql | 27.322 | 26.952 | 26.974 | 26.83 | 27.087 | 26.991<br>q48.sql | 14.266 | 14.39 | 14.517 | 14.684 | 14.471 | 14.61<br>q49.sql | 21.279 | 21.733 | 20.286 | 20.945 | 22.388 | 21.52<br>q50.sql | 191.416 | 194.256 | 196.701 | 194.113 | 193.354 | 191.004<br>q51.sql | 37.552 | 37.767 | 38.317 | 37.731 | 37.369 | 38.187<br>q52.sql | 2.206 | 2.406 | 2.235 | 2.362 | 2.337 | 2.278<br>q53.sql | 5.282 | 5.131 | 5.465 | 5.137 | 5.142 | 5.069<br>q54.sql | 13.039 | 12.655 | 13.047 | 12.382 | 12.992 | 12.988<br>q55.sql | 2.534 | 2.39 | 2.375 | 2.867 | 2.623 | 2.546<br>q56.sql | 7.365 | 7.087 | 6.902 | 7.406 | 7.586 | 7.081<br>q57.sql | 18.064 | 17.945 | 18.699 | 17.664 | 18.362 | 18.222<br>q58.sql | 6.198 | 6.702 | 6.109 | 6.211 | 5.9 | 6.101<br>q59.sql | 28.266 | 28.195 | 27.876 | 28.748 | 29.027 | 28.543<br>q60.sql | 6.847 | 7.143 | 7.322 | 7.1 | 7.207 | 7.215<br>q61.sql | 7.258 | 7.62 | 7.317 | 7.781 | 7.616 | 7.669<br>q62.sql | 10.334 | 11.523 | 10.389 | 10.378 | 10.072 | 10.583<br>q63.sql | 4.631 | 4.944 | 4.947 | 5.124 | 4.61 | 4.865<br>q64.sql | 249.694 | 252.117 | 254.359 | 254.813 | 253.236 | 250.401<br>q65.sql | 78.742 | 79.184 | 78.559 | 78.305 | 78.985 | 78.515<br>q66.sql | 14.98 | 14.854 | 14.794 | 14.767 | 14.781 | 14.696<br>q67.sql | 1019.744 | 1048.439 | 987.894 | 972.062 | 927.566 | 1002.206<br>q68.sql | 8.903 | 8.915 | 8.277 | 8.709 | 9.349 | 9.178<br>q69.sql | 13.097 | 13.01 | 14.352 | 12.036 | 12.302 | 12.843<br>q70.sql | 21.175 | 21.085 | 21.102 | 20.471 | 20.129 | 19.678<br>q71.sql | 15.13 | 15.526 | 14.929 | 15.231 | 15.406 | 15.487<br>q72.sql | 76.463 | 75.851 | 72.002 | 72.356 | 72.676 | 74.798<br>q73.sql | 5.894 | 6.09 | 5.877 | 6.051 | 6.365 | 6.634<br>q74.sql | 99.106 | 99.356 | 100.291 | 99.51 | 96.766 | 97.292<br>q75.sql | 126.625 | 128.094 | 127.364 | 128.575 | 127.418 | 125.806<br>q76.sql | 35.172 | 33.601 | 34.752 | 34.764 | 34.228 | 35.748<br>q77.sql | 8.394 | 8.01 | 7.951 | 8.061 | 7.839 | 8.348<br>q78.sql | 289.061 | 287.508 | 283.615 | 288.768 | 288.448 | 288.661<br>q79.sql | 10.048 | 9.251 | 9.396 | 9.81 | 8.607 | 8.341<br>q80.sql | 59.68 | 59.458 | 60.234 | 60.415 | 61.325 | 60.744<br>q81.sql | 17.822 | 18.815 | 18.488 | 18.95 | 17.911 | 18.113<br>q82.sql | 64.781 | 63.957 | 63.621 | 64.38 | 63.637 | 64.488<br>q83.sql | 4.686 | 4.922 | 4.635 | 4.827 | 4.678 | 5.071<br>q84.sql | 10.987 | 10.629 | 10.841 | 11.151 | 10.646 | 10.6<br>q85.sql | 12.689 | 13.304 | 13.362 | 13.19 | 13.779 | 12.657<br>q86.sql | 6.48 | 6.491 | 6.722 | 6.667 | 6.833 | 6.52<br>q87.sql | 77.589 | 77.377 | 77.177 | 77.011 | 78.339 | 78.399<br>q88.sql | 83.876 | 83.676 | 84.044 | 83.761 | 84.201 | 84.089<br>q89.sql | 6.741 | 6.564 | 6.755 | 6.708 | 6.704 | 6.794<br>q90.sql | 7.79 | 7.812 | 7.882 | 7.88 | 7.875 | 7.854<br>q91.sql | 4.072 | 3.728 | 3.883 | 3.976 | 4.151 | 4.035<br>q92.sql | 3.05 | 3.155 | 3.336 | 3.067 | 2.942 | 3.099<br>q93.sql | 356.412 | 360.731 | 358.14 | 356 | 356.108 | 358.011<br>q94.sql | 43.202 | 43.561 | 44.63 | 44.486 | 43.993 | 42.693<br>q95.sql | 197.185 | 199.657 | 193.975 | 195.843 | 201.801 | 196.113<br>q96.sql | 12.765 | 12.481 | 12.682 | 12.799 | 12.528 | 12.505<br>q97.sql | 82.895 | 82.067 | 81.754 | 82.799 | 81.788 | 81.572<br>q98.sql | 7.338 | 7.066 | 7.133 | 7.005 | 7.254 | 7.047<br>q99.sql | 18.431 | 17.874 | 17.826 | 17.861 | 17.705 | 17.878<br>total | 7105.675 | 7091.391 | 7030.209 | 7021.7 | 6992.413 | 7047.295<br><br>
Thank you for sharing!
As the first glance, there is no noticeable significant perf difference (in both directions: 【speed】up or regression). What is your opinion, @wangyum ?
@dongjoon-hyun Yes. It's no noticeable significant perf difference.
Thank you for the confirmation.
BTW, if you mind, please revise the PR description.<br>1. Removing `Maybe it can 【improve】 read 【performance】.` from the PR description.<br>2. Coping https://github.com/apache/spark/pull/40555#issuecomment-1501603107 to the PR descrition.
> BTW, if you mind, please revise the PR description.<br>> <br>> 1. Removing `Maybe it can 【improve】 read 【performance】.` from the PR description.<br>> 2. Coping [[SPARK-42926][BUILD][SQL] Upgrade Parquet to 1.13.0 #40555 (comment)](https://github.com/apache/spark/pull/40555#issuecomment-1501603107) to the PR descrition.<br><br>OK.
Looks fine to me, but +CC @srowen
【Thanks】 @mridulm @srowen
I guess you forgot to run `dev/connect-gen-protos.sh` ?
+1, @LuciferYang 【Thanks】 for the work!
【Thanks】 @gengliangwang ~
cc. @zsxwing @viirya @rangadi Please take a look. 【Thanks】 in advance!
The error only occurred from linter - it now does not allow a new PR to introduce a new public API \"without adding to spark-connect\". This PR intentionally postpones addressing PySpark in separate JIRA ticket, hence addressing spark-connect should go to there as well.
Just added a dummy implementation.
@HyukjinKwon @amaliujia <br>Would you mind if I ask what happens with the mima 【check】 for this PR? <br>https://github.com/HeartSaVioR/spark/actions/runs/4536405777/jobs/7993077860<br><br>Is it required to add PySpark API in this PR to pass Spark connect 【check】? At least MiMa 【check】 failed in Scala codebase.
I think you need update at connector/connect/client/jvm/src/【test】/scala/org/apache/spark/sql/connect/client/CheckConnectJvmClientCompatibility.scala
I was wondering what is different from dropDuplicates and this one. I don't see dropDuplicates being handled separately. Is it because the PySpark implementation of dropDuplicates is 【available】?<br><br>If this method has to be excluded, could you please guide how to do that? 【Thanks】 in advance.
hmm I am not sure what you already did but I am thinking if you don't add anything into https://github.com/apache/spark/blob/master/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/Dataset.scala, then you need update `CheckConnectJvmClientCompatibility`.<br><br>
Sigh I didn't indicate we already took a step of Scala API with Spark connect. I thought there's only in PySpark. 【Thanks】 for correcting me.
cc. @zsxwing @viirya @rangadi Friendly reminder.
> What is the decision about batch support?<br><br>I just added support of batch in the la【test】 commit. It needs be more 【test】 coverage for batch query support so that's why we have new FIXMEs. All FIXMEs should be resolved before merging.
cc. @zsxwing @viirya @rangadi Could you please 【review】 this again? I feel this is very close to the final shape.<br>
Just filed another JIRA ticket https://issues.apache.org/jira/browse/SPARK-43027 to support PySpark. Once we merge this in I'll work on PySpark side.
I just added the pyspark implementation in this PR. It doesn't seem to be worthwhile to have another round of 【review】 【specific】ally for pyspark, given that the 【review】 phase is not going 【fast】 enough.
The last update is to rebase with master branch - just to make sure CI is happy with the change before merging this.
Confirmed CI passed for last commit. https://github.com/HeartSaVioR/spark/runs/12606973127
【Thanks】 all for 【review】ing! Merging to master.
@HeartSaVioR Confirming that the 【documentation】 in the initial PR description is accurate and up to date, as I'll be using it at the example I base my doc updates on.<br><br>Also: I don't see these doc 【c<font color=blue>hang】es】 live in the SS programming guide. What Spark versions should they go live with?
@dstrodtman-db We will release this 【feature】 in Spark 3.5.0. We don't have the tentative date to release Spark 3.5.0.
@HyukjinKwon @zhengruifeng Hi, Would you mind have a 【review】?
【Thanks】 all first, I had made a global 【check】 about key words which is 'reference/api/pyspark.pandas', and hasn't find another instance like this.
ping @zhengruifeng @HyukjinKwon @dongjoon-hyun cc @infoankitp @navinvishy
cc @MaxGekk
Can we update the PR description? `array_append` is untouched now.
After a second thought, this makes the 【performance】 worse. We should 【improve】 `ArrayInsert` to generate more 【efficient】 code if the position argument is constant. For example, if the position argument is constant 1, then we don't need to generate code to 【check】 if position is negative or not.
> Can we update the PR description? `array_append` is untouched now.<br><br>Updated.
> After a second thought, this makes the 【performance】 worse. We should 【improve】 `ArrayInsert` to generate more 【efficient】 code if the position argument is constant. For example, if the position argument is constant 1, then we don't need to generate code to 【check】 if position is negative or not.<br><br>You means create another PR to simplify the code of `ArrayInsert`.
@cloud-fan 【Thanks】 !
cc @zhenlineo can you take a look?
cc @LuciferYang Can we merge this?
The pr title should be `[SPARK-42519][CONNECT][TESTS] ...`
@zhenlineo 【Thanks】 for ping me ~ Let me do some manual 【check】s<br>
```<br>### How was this patch 【test】ed?<br>Yes<br>```<br><br>I think You can say `Add new 【test】s`<br>
cc @hvanhovell @HyukjinKwon @zhengruifeng FYI
【Thanks】 @zhenlineo @LuciferYang @HyukjinKwon
Highly likely, the GA `【continuous】-integration/appveyor/pr` is not related to my 【c<font color=blue>hang】es】. I am going to merge this PR.
Merging to master. Thank you, @cloud-fan for 【review】.
cc @dongjoon-hyun FYI<br><br>If you have time, please help verify this change. I am not sure if only my environment can reproduce this issue. 【Thanks】 ~<br><br>
Thank you for pinging me, @LuciferYang .
I verified this manually via Maven. Merged to master/3.4/3.3/3.2.
【Thanks】 very much @dongjoon-hyun 😄
cc @dongjoon-hyun @cloud-fan @viirya @yaooqinn thank you
+CC @srowen
I think it's fine. These do look like better usages of RNGs. Let's see what 【test】s say.
According to the `Affected Version` in JIRA, I also agree with backporting to the applicable release branches.
Merged to master/3.4/3.3
【Thanks】 for the 【review】s everyone !<br>And thanks for merging it @srowen :-)
cc @allisonwang-db
I updated the description to mention that the issue occurs only when both wholestage codegen and adaptive execution is disabled.
lgtm, it seems we missed update non-AQE part when we optimize out the DPP broadcast 【value】 in https://github.com/apache/spark/pull/34051.
According to the Affected Versions in Jira, I merged this to master/3.4/3.3.
cc @WeichenXu123 and @ueshin
Discussed in https://github.com/apache/spark/pull/35278#issuecomment-1033927506, PTAL. @HyukjinKwon 【Thanks】 a lot.<br>
【Thanks】 all !
cc @cloud-fan @dongjoon-hyun @HyukjinKwon, thanks
https://github.com/yaooqinn/spark/runs/12325165688 action passed, and the last one which blocks hours for python 【test】, removed tailing `:` only. <br><br>Merged to master, thanks
Nice!
@allisonwang-db can we do something similar for the scala side?
> @allisonwang-db can we do something similar for the scala side?<br><br>Same question: do we plan to do the same on the scala side?
@grundprinzip I added a config for the stack trace size (default 4096). Could you take another look at this PR?
@amaliujia Scala side does not have an existing SQL conf. We can certainly add this with a new config on the Scala side as well.
@HyukjinKwon @allisonwang-db  it seems `UserDefinedFunctionE2ETestSuite` keeps failing after this PR
Reverting this out for now.
> @HyukjinKwon @allisonwang-db it seems `UserDefinedFunctionE2ETestSuite` keeps failing after this PR<br><br>This is because the default max HTTP header size is 8k and having 2k messages and 4k stack trace exceeds this limit. For now I changed the default max stack trace size to be 2k. We should consider moving to a different error framework to remove such constraints.
merged to master again
cc @HyukjinKwon @gengliangwang @cloud-fan thanks
thanks, merge to master/3.4
@srowen Can you help 【review】 this PR? We found this problem in the use of connecting AD for authentication. This problem is very similar to [36784](https://github.com/apache/spark/pull/36784), because STS LDAP authentication lacks some functions.
It's hard to make out the change because you moved the code. What is the important change?
Was this file originally copied from hive? Is there a corresponding fix in the hive?<br><br>
> Was this file originally copied from hive? Is there a corresponding fix in the hive?<br><br>difficult to synchronize, this file has changed too much in hive<br><br>
> It's hard to make out the change because you moved the code. What is the important change?<br><br>The original code will continue to construct principals according to the DN pattern after processing the domain. After I modify it, when encountering a domain, it will no longer go to the logic of the DN pattern, and the createCandidatePrincipals function will return directly.
> > Was this file originally copied from hive? Is there a corresponding fix in the hive?<br>> <br>> difficult to synchronize, this file has changed too much in hive<br><br>You can 【check】 this file.   [LdapUtils](https://github.com/apache/hive/blob/4d21ba11c893038dbecd98c512a81ccf1d4fc6d0/service/src/java/org/apache/hive/service/auth/ldap/LdapUtils.java#L203)
I think the 【community】 needs to fully synchronize the la【test】 code of hive in the end, but at present this PR can solve LDAP authentication including domain.
The major LDAP 【enhancement】 in Apache Hive occurred in HIVE-14713, which also brought UT. There is a Scala version porting in Apache Kyuubi https://github.com/apache/kyuubi/pull/4152. If wanted, I can contribute those parts to Spark, the change should be similar w/ Kyuubi.
> cc @sunchao too<br><br>Can this modification be merged?
cc @ulysses-you @cloud-fan
Merging to master
cc @xinrong-meng
```<br>TODO: Need to fix the 【test】 for maven.<br>```<br><br>Should we make this one 3.5.0 only? I think Maven 【test】 failure will become a blocker for 3.4.0 release<br><br><br><br>
thank you for 【review】s, merged into master
cc @cloud-fan @HyukjinKwon
cc @HyukjinKwon @hvanhovell
`avro/functions` not in sql module, so `CheckConnectJvmClientCompatibility` cannot perform mima 【check】 between `client.jar` and `avro.jar`, created SPARK-42958 to tracking this<br><br>
【Thanks】 @HyukjinKwon @yaooqinn
cc @HyukjinKwon and @xinrong-meng
Thank you for 【check】ing.<br><br>Yes, JSON was fine.<br>```<br>spark-rm@1ea0f8a3e397:/opt/spark-rm/output/spark/spark-repo-glCsK/org/apache/spark$ find . | grep json<br>./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.json<br>./spark-graphx_2.13/3.4.1-SNAPSHOT/spark-graphx_2.13-3.4.1-SNAPSHOT-cyclonedx.json<br>./spark-kvstore_2.13/3.4.1-SNAPSHOT/spark-kvstore_2.13-3.4.1-SNAPSHOT-cyclonedx.json<br>...<br>```<br><br>I found that I missed that `maven-metadata-local.xml`. <br>```<br>spark-rm@1ea0f8a3e397:/opt/spark-rm/output/spark/spark-repo-glCsK/org/apache/spark$ find . | grep xml<br>./spark-core_2.13/3.4.1-SNAPSHOT/maven-metadata-local.xml<br>./spark-core_2.13/3.4.1-SNAPSHOT/spark-core_2.13-3.4.1-SNAPSHOT-cyclonedx.xml<br>./spark-core_2.13/maven-metadata-local.xml<br>```<br><br>Let me make a follow-up to use `cyclonedx.xml` and `cyclonedx.json` 【specific】ally. Thank you so much, @viirya and @HyukjinKwon .
cc: @HyukjinKwon, @pengzhon-db, @WweiL, @grundprinzip <br>This is ready for 【review】. I will fix the conflicts.
I have less expertises in protobuf, otherwise 【LGTM】
The proto side overall looks good.
@rangadi It seems the doc【test】 `pyspark.sql.connect.dataframe.DataFrame.writeStream` is not 【stable】, would you mind taking a look?<br><br>https://github.com/apache/spark/actions/runs/4815479364/jobs/8574219355<br><br>https://github.com/apache/spark/actions/runs/4805357601/jobs/8551705966<br><br>https://github.com/apache/spark/actions/runs/4783998036/jobs/8504986700
Thank you, @HyukjinKwon . I'll merge this because GitHub Action doesn't cover this.
For the record, to sum up, SPARK-42957 made the following change. `cyclonedx` didn't exist before across all file names and only `-cyclonedx.json` and `-cyclonedx.xml` are newly generated for SBOM according to our snapshot result.<br>```<br>   # Remove any extra files generated during install<br>-  find . -type f |grep -v \\.jar |grep -v \\.pom | xargs rm<br>+  find . -type f |grep -v \\.jar |grep -v \\.pom |grep -v cyclonedx | xargs rm<br>```
I verified that Apache Spark 3.4.0 RC5 【success】fully has SBOM artifacts.<br>- https://repository.apache.org/content/repositories/orgapachespark-1439/org/apache/spark/spark-core_2.12/3.4.0/spark-core_2.12-3.4.0-cyclonedx.json<br>- https://repository.apache.org/content/repositories/orgapachespark-1439/org/apache/spark/spark-core_2.13/3.4.0/spark-core_2.13-3.4.0-cyclonedx.json<br><br>Thank you, @HyukjinKwon , @viirya , @xinrong-meng .
Cool. 【Thanks】 @dongjoon-hyun
cc @cloud-fan @dongjoon-hyun @yaooqinn
cc @sunchao , too
Oh my bad. I realized that @ulysses-you re-used SPARK-38697 mistakenly.<br>Let me revert this.
This is reverted from master branch.<br>```<br>$ git log --oneline -n3<br>9287a5e7db (HEAD -> master, apache/master, apache/HEAD) Revert \"[SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE query stage optimizer\"<br>6f8b068151 [SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE query stage optimizer<br>```<br><br>To @ulysses-you , please create a new JIRA and new PR.
Just for the record, SPARK-38697 was a commit of one-year ago.<br>```<br>$ git show 883596a4ba | head<br>commit 883596a4bab36ddf0e1a5af0ba98325ca8582550<br>Author: ulysses-you <ulyssesyou18@gmail.com><br>Date:   Fri Apr 15 16:02:00 2022 +0800<br><br>    [SPARK-38697][SQL] Extend SparkSessionExtensions to inject rules into AQE Optimizer<br>```
oh sorry, not sure why that happened, I actually have created https://issues.apache.org/jira/browse/SPARK-42963.<br><br>created a new pr https://github.com/apache/spark/pull/40653
Merging to master/3.4/3.3/3.2
It has a conflict with branch-3.4. Would you mind creating a backport please?
#40595
cc @HyukjinKwon @sadikovi
【Thanks】 @HyukjinKwon @sadikovi
cc @sadikovi @srowen @HyukjinKwon
@HeartSaVioR - PTAL when you get a chance. Thx<br>
This change makes sense to me. This is a breaking change, then shall we add a migration guide for it?
The change makes sense, but I'd say this is a legacy 【feature】 and the existing behavior doesn't make sense at all. For string +/- internal, the string can be timestamp, timestamp_ntz and interval, there is no correct behavior here.<br><br>My suggestion is don't touch it to keep legacy workloads running. We should update the SQL queries to not use String so extensively.
Or we should probably fail it in ANSI mode, cc @gengliangwang
+1 for fail it in ANSI mode.
> My suggestion is don't touch it to keep legacy workloads running. We should update the SQL queries to not use String so extensively.<br><br>>we should probably fail it in ANSI mode<br><br>+1, totally agree!
cc @cloud-fan @HyukjinKwon thanks
Ping on this @HyukjinKwon or @hvanhovell?
Another question: will the raw data in `LocalRelation` also shown in Spark UI?   It might be sensitive
> Another question: will the raw data in `LocalRelation` also shown in Spark UI? It might be sensitive<br><br>This is similar to what we do for SQL queries we put the literal string into the description.
Actually it would also have a 【security】 concern as it exposes the local data as is.
Let me make a PR to redact it for now at least.
also cc @xinrong-meng , this is not a blocker but it's better if we can make it into 3.4.0.
+1 for reverting decision. Thank you, @cloud-fan and all.
@HyukjinKwon @hvanhovell @yaooqinn Can this on move forward?<br><br>
@LuciferYang is it good to go?
> @LuciferYang is it good to go?<br><br>Yes, I think so
Alright let's merge it then.
Was this opened by mistake?
Yes.
cc @WeichenXu123 @HyukjinKwon  I think it is ready to 【review】
Followup tasks:<br><br>* We should replace `mapInPandas` with `mapInArrow` for better 【performance】<br>* For each spark task, we should save each partition data into local disk (in arrow format) before starting torch process, and provide utility reading function (returning `torch.utils.data.Dataset` instance) that torch program (running as a child process) can invoke it to iterate the partition data inside torch program.
```<br>pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:<br>\tstatus = StatusCode.UNKNOWN<br>\tdetails = \"Java heap space\"<br>\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:39429 {grpc_message:\"Java heap space\ grpc_status:2, created_time:\"2023-04-04T10:54:38.384319228+00:00\"}\"<br>><br>```<br>it fails again even I switch back to the initial approach, the error is raised in the server side, so should not be related to the way to concat. I guess there is no enough RAM.<br><br>cc @HyukjinKwon are those UTs 【test】ed in 【parallel】 or one by one?
the GA status is missing, https://github.com/zhengruifeng/spark/actions/runs/4607449033
In my local env, the failed 【test】 can pass with even bigger model size.<br>but let me try to reduce the model size for GA to see what will happen.
I am hitting a weird failure of `TorchDistributorDistributedUnitTestsOnConnect.【test】_parity_torch_distributor`, it appeared after I rebase this PR yesterday, but I don't find any suspicious commits merged recently.<br><br>```<br>======================================================================<br>ERROR [18.362s]: 【test】_end_to_end_run_【distributed】ly (pyspark.ml.【test】s.connect.【test】_parity_torch_distributor.TorchDistributorDistributedUnitTestsOnConnect)<br>----------------------------------------------------------------------<br>Traceback (most recent call last):<br>  File \"/__w/spark/spark/python/pyspark/ml/torch/【test】s/【test】_distributor.py\ line 457, in 【test】_end_to_end_run_【distributed】ly<br>    output = TorchDistributor(num_processes=2, local_mode=False, use_gpu=False).run(<br>  File \"/__w/spark/spark/python/pyspark/ml/torch/distributor.py\ line 749, in run<br>    output = self._run_【distributed】_training(framework_wrapper_fn, train_object, *args)<br>  File \"/__w/spark/spark/python/pyspark/ml/torch/distributor.py\ line 607, in _run_【distributed】_training<br>    self.spark.range(start=0, end=self.num_tasks, step=1, numPartitions=self.num_tasks)<br>  File \"/__w/spark/spark/python/pyspark/sql/connect/dataframe.py\ line 1354, in collect<br>    table, schema = self._session.client.to_table(query)<br>  File \"/__w/spark/spark/python/pyspark/sql/connect/client.py\ line 668, in to_table<br>    table, schema, _, _, _ = self._execute_and_fetch(req)<br>  File \"/__w/spark/spark/python/pyspark/sql/connect/client.py\ line 982, in _execute_and_fetch<br>    for response in self._execute_and_fetch_as_iterator(req):<br>  File \"/__w/spark/spark/python/pyspark/sql/connect/client.py\ line 963, in _execute_and_fetch_as_iterator<br>    self._handle_error(error)<br>  File \"/__w/spark/spark/python/pyspark/sql/connect/client.py\ line 1055, in _handle_error<br>    self._handle_rpc_error(error)<br>  File \"/__w/spark/spark/python/pyspark/sql/connect/client.py\ line 1095, in _handle_rpc_error<br>    raise SparkConnectGrpcException(str(rpc_error)) from None<br>pyspark.errors.exceptions.connect.SparkConnectGrpcException: <_MultiThreadedRendezvous of RPC that terminated with:<br>\tstatus = StatusCode.UNKNOWN<br>\tdetails = \"Java heap space\"<br>\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:35071 {created_time:\"2023-04-05T10:13:52.254507275+00:00\ grpc_status:2, grpc_message:\"Java heap space\"}\"<br>><br>```<br><br><br>In my local env, I can only repro this by decreasing the driver memory (e.g. \"spark.driver.memory\ \"512M\"), And this issue can be simply resolve by increase the driver memory to 1024M.<br>I 【test】s different combinations locally like:<br>`spark.driver.memory=1024M, spark.executor.memory=512M`<br>`spark.driver.memory=1024M, spark.executor.memory=1024M`<br>etc<br>and they also works as expected.<br><br>But in Github Action (this resource limitation seems to be https://github.com/apache/spark/blob/0b45a5278026c2ea9ce2b127333514f7a7a933f4/.github/workflows/build_and_【test】.yml#L1028), no matter how larger driver memory I set (3G, 4G), this 【test】 just keeps failing with this error message.<br><br>Do you have any thoughts on this? @WeichenXu123 @HyukjinKwon <br>
@HyukjinKwon WDYT ? Can we increase CI bot memory capacity ?
【Thanks】 all for the 【review】s, merged into master
@holdenk @MaxGekk
【LGTM】 +1
@HyukjinKwon Changed the implementation to `df.explain(\"codegen\")` and added it to the connector
Hi @HyukjinKwon Does the PR approach make sense to you?
@Hisoka-X Please, update PR's description and fix the error class name.<br><br>Is `fanjia` at https://issues.apache.org/jira/browse/SPARK-42316 your JIRA account ?
> @Hisoka-X Please, update PR's description and fix the error class name.<br><br>Done<br><br>> Is `fanjia` at https://issues.apache.org/jira/browse/SPARK-42316 your JIRA account ?<br><br>Yes<br>
+1, 【LGTM】. Merging to master.<br>Thank you, @Hisoka-X.
Thank @MaxGekk
```<br>2023-03-30T16:09:39.9363333Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m- Dataset result destructive iterator *** FAILED *** (84 milliseconds)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9382605Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.vectorized.ColumnarBatch@3f5ed920 equaled org.apache.spark.sql.vectorized.ColumnarBatch@3f5ed920 (ClientE2ETestSuite.scala:819)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9383550Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.scala【test】.exceptions.TestFailedException:\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9384640Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Assertions.newAssertionFailedException(Assertions.scala:472)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9385936Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Assertions.newAssertionFailedException$(Assertions.scala:471)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9387002Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Assertions$.newAssertionFailedException(Assertions.scala:1231)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9388072Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9389136Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$106(ClientE2ETestSuite.scala:819)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9390070Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9391014Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9392246Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9393644Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Transformer.apply(Transformer.scala:22)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9394518Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Transformer.apply(Transformer.scala:20)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9395634Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9396587Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.TestSuite.withFixture(TestSuite.scala:196)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9397490Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.TestSuite.withFixture$(TestSuite.scala:195)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9398540Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9399811Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9400900Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9401857Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9402904Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9403910Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9405036Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9406066Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9407048Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9407986Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at scala.collection.immutable.List.foreach(List.scala:431)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9409109Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9410244Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9411594Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9412839Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9414688Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9416225Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9416768Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Suite.run(Suite.scala:1114)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9417907Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.Suite.run$(Suite.scala:1096)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9419535Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9420225Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9421710Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9422826Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9423713Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9436163Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.apache.spark.sql.ClientE2ETestSuite.org$scala【test】$BeforeAndAfterAll$$super$run(ClientE2ETestSuite.scala:41)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9437232Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9455327Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9456678Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9457909Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.apache.spark.sql.ClientE2ETestSuite.run(ClientE2ETestSuite.scala:41)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9459138Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9460291Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9461481Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9462543Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at java.util.【concurrent】.FutureTask.run(FutureTask.java:266)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9463721Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9464883Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001B[0m\u001B[0m<br>2023-03-30T16:09:39.9465861Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  at java.lang.Thread.run(Thread.java:750)\u001B[0m\u001B[0m<br>```<br>@ivoson The new 【test】 failed
@ivoson any update of this one ?
La【test】 commits addressed the comments above. cc @hvanhovell @LuciferYang please take a look when you have time. 【Thanks】.
looking into the 【test】 failure.
It is not related to the current PR. It seems that the `SparkResult ` instance is not thread-safe, do we need to consider this? @hvanhovell <br><br><br>
Hi @hvanhovell , could you please take a look at this PR? 【Thanks】.
friendly ping @hvanhovell , is this one ok?
@ivoson Can you resolve the conflict?<br><br>
friendly ping @srowen @sadikovi
Can we push forward this one ? @srowen @sadikovi <br>
@srowen Does this not need to be merged into branch-3.4?<br><br>
I don't see a strong reason to. We don't have a 【specific】 issue this solves right?
@srowen I think there are two reasons:<br>1. branch-3.3 and the master both use `spark.util.ShutdownHookManager`, but 3.4 not, I think they should keep the same.<br>2. @sadikovi mentioned earlier(https://github.com/apache/spark/pull/36529#【discussion】_r1152687590) `DeleteOnExit 【functionality】 can cause 【performance】 issues due memory leaks under heavy load in certain JDK versions because while the temporary files are deleted, the entries in the map are never cleared.`<br><br>So I think branch-3.4 may be worth having this one<br><br>WDYT  @sadikovi ?
Right, but we haven't observed this actual problem as far as I know.<br>I'm not against back-porting, go for it
I'have corrected the problem that cause build error in github workflow
cc @SandishKumarHN and @rangadi
@rangadi The adition of `any` should be added in wich table? Protobuf -> Spark SQL or Spark SQL -> Protobuf?
@rangadi the first two fixes were made
@lucaspompeun thanks for the updates. 【LGTM】.<br><br>`Any` is for 'Protobuf -> Spark SQL' type table.
@rangadi Done!
【Thanks】.<br>@HyukjinKwon please merge when you get a chance.
@dtenedor FYI, I updated the 【test】s and am just missing one for empty input table, and one for merging sparse/dense sketches. Once I get the build to be green, I'm going to remove the WIP tag from the PR and send an e-mail back on that initial spark-dev thread (or maybe start a new thread) letting everyone know that the implementation is open for 【review】. I think renaming functions is still do-able at this point, so let me know if you'd like to setup another sync to discuss updated function names?
Hi @mkaravel thank you for the 【review】! I'll respond to your comments in-line.<br><br>>more in favor of an aggregate function that merges sketches and returns the merged sketch<br><br>I'm not opposed to building out an agg function that merges sketches without estimating the cardinality; I think this would be beneficial for multi-stage re-aggregations. I think this topic (unfortunately) begs the question of function naming, and here's the resultant naming scheme I'd propose:<br><br>Aggregate functions:<br>- HllSketch (IntegerType|LongType|StringType|...) -> BinaryType<br>- HllUnion (BinaryType) -> BinaryType<br><br>Normal functions<br>- HllSketchEstimate (BinaryType) -> LongType<br><br>I think this API is 【simple】 and the lack of an aggregate function that returns the estimated cardinality is fine. What do you think?
> Hi @mkaravel thank you for the 【review】! I'll respond to your comments in-line.<br>> <br>> > more in favor of an aggregate function that merges sketches and returns the merged sketch<br>> <br>> I'm not opposed to building out an agg function that merges sketches without estimating the cardinality; I think this would be beneficial for multi-stage re-aggregations. I think this topic (unfortunately) begs the question of function naming, and here's the resultant naming scheme I'd propose:<br>> <br>> Aggregate functions:<br>> <br>> * HllSketch (IntegerType|LongType|StringType|...) -> BinaryType<br>> * HllUnion (BinaryType) -> BinaryType<br>> <br>> Normal functions<br>> <br>> * HllSketchEstimate (BinaryType) -> LongType<br>> <br>> I think this API is 【simple】 and the lack of an aggregate function that returns the estimated cardinality is fine. What do you think?<br><br>Semantically I like this API. There is one more scalar expression that I would add, it's purpose being to merge two sketches. Let me give some more details below.<br><br>In terms of naming I am more in favor of dropping the \"Sketch\" part from the names and using an underscore to separate the \"Hll\" part from the rest. More 【specific】ally this is what I would choose (this is a personal opinion, and I am not reflecting opinions of anybody else other than myself):<br><br>Aggregate functions:<br><br>* hll_collect (IntegerType|LongType|StringType|BinaryType|...) -> BinaryType<br>* {hll_merge_agg, hll_union_agg}(BinaryType) -> BinaryType<br><br>Scalar functions:<br>* {hll_merge, hll_union}(BinaryType, BinaryType) -> BinaryType<br>* hll_estimate(BinaryType) -> LongType<br><br>Above I propose hll_merge* or hll_union* for the functions that perform the union (or merge operation) on sketches. I have a slight preference for the \"merge\" versions.
Hello @mkaravel ! <br><br>I've updated the PR to provide the following functions:<br><br>Aggregate functions:<br>- hll_sketch_agg(IntegerType|LongType|StringType|BinaryType) -> BinaryType<br>- hll_union_agg(BinaryType) -> BinaryType<br><br>Scalar functions<br>- hll_sketch_estimate(BinaryType) -> LongType<br>- hll_union(BinaryType, BinaryType) -> BinaryType<br><br>Naming wise, I felt it was valuable to keep the function names aligned with the Datasketches objects they utilize, and be explicit about the operation being applied. Hopefully these function names are a good middle ground for us? I'll continue working on getting all the 【test】s to pass, and then open the PR up for wide 【review】.<br><br>
A few 【test】s are failing due to some connectivity issues unrelated to the 【c<font color=blue>hang】es】 in this PR - is there an easy way to re-run without pushing a new commit?
> A few 【test】s are failing due to some connectivity issues unrelated to the 【c<font color=blue>hang】es】 in this PR - is there an easy way to re-run without pushing a new commit?<br><br>I am not aware how to do that. @gatorsmile any ideas?
Are you able to see the button \"Re-run all jobs\" ?<br><img width=\"1206\" alt=\"image\" src=\"https://user-images.githubusercontent.com/11567269/233427975-c8ecf0cd-d1c2-43cf-b367-1183c8911d44.png\"><br>
@mkaravel regarding your comment about 'mixing sketches with different lgk 【value】s',[ this is the Union implementation which handles merging sketches with different configs](https://github.com/apache/datasketches-java/blob/master/src/main/java/org/apache/datasketches/hll/Union.java#L317-L340); my assumption is that the 【functionality】 is 【test】ed and 【stable】, but let me know if you think we should try to limit the union operation to only support sketches with the same config?
> @mkaravel regarding your comment about 'mixing sketches with different lgk 【value】s',[ this is the Union implementation which handles merging sketches with different configs](https://github.com/apache/datasketches-java/blob/master/src/main/java/org/apache/datasketches/hll/Union.java#L317-L340); my assumption is that the 【functionality】 is 【test】ed and 【stable】, but let me know if you think we should try to limit the union operation to only support sketches with the same config?<br><br>I am not questioning the correctness of the DataSketches implementation.<br><br>My concern is accidental mistakes that can happen if the user does not pay attention to the `lgk` 【value】s of the input sketches. I would argue that merging two sketches with different `lgk` 【value】s is \"【advanced】\" usage of sketches, and the user should be aware that mixing such sketches comes with caveats (loosing precision with respect to the sketch with higher `lgk`). The current API hides this complexity and caveats.<br><br>Let's consider another alternative (I want your opinion on this): Let's say we have two overloads (we can extend this to the aggregate version)<br>* hll_merge(sketch1: BinaryType, sketch2: BinaryType)<br>* hll_merge(sketch1: BinaryType, sketch2: BinaryType, allowDifferentLgKs: BooleanType)<br><br>The first errors out if the `lgk` 【value】s are different.<br>The second errors out if the `lgk` 【value】s are different and `allowDifferentLgKs` is `false`. However, if `allowDifferentLgk` is `true` then the second overload behaves as your current implementation.<br><br>Clearly, I am talking about adding a third boolean argument, with the default 【value】 being `false`. With these two overloads, if the user tries to merge two sketches with different precision, the query will fail. If they really need to merge them, they have the opportunity to do that by means of  setting the third argument to `true`, and it will not happen accidentally without them noticing. It will happen because they force it this way, they will be proactive about the loss of precision. A nice error message for the first overload and good 【documentation】 will make it very clear what is going on.
>about adding a third boolean argument, with the default 【value】 being false<br><br>I'm open to making this change, as this is in-line with the limitation I was planning on imposing on the original approx_count_distinct_sketch merge 【functionality】. @mkaravel do you have an opinion on whether we should preserve the lgMaxK argument supported by the union operation, given this change? [Here's a ref to the docs](https://github.com/apache/datasketches-java/blob/b2c62bf050d8eb37b20e523a35bcd4f8c9a6cdf2/src/main/java/org/apache/datasketches/hll/Union.java#L45-L49). I'm leaning towards removing this argument in favor of relying on the default behavior, to further reduce complexity.
FYI it looks like [another similar implementation of a Datasketches/Spark integration already exists in its own repo ](https://github.com/Gelerion/spark-sketches/tree/spark-3.0)- I've invited the owner of that implementation to provide feedback on this PR.
@mkaravel I've updated the implementation based on your 【review】 comments. We're now returning the updatable binary representation, no longer support the tgtHllType parameter, and defer initialization of the Union instance until we've ingested the first HllSketch such that we can throw an exception when union/sketch lgConfigKs don't match. Let me know when you've had chance to re-【review】?
@RyanBerti nit in the PR description:<br>> Yes, this PR introduces two new aggregate functions, and two new non-aggregate functions:<br><br>Maybe use \"scalar\" instead of \"non-aggregate\".
While browsing the dev-mailing list, I came across this PR and I am excited to see that data-sketches will be built-in in Spark. Interestingly, a year ago, I created a similar [project](https://github.com/Gelerion/spark-sketches) that adds sketch support for Spark versions 2.4 to 3.4. Even the implementation path is very similar 【:)】
@RyanBerti I created a PR against this PR for updating `sql-expression-schema.md` (I regenerated the golden file).<br>Feel free to either merge my PR into yours, or just copy the file in my PR into your code manually.<br>Just trying to help here 【:)】<br><br>My PR: https://github.com/RyanBerti/spark/pull/4
@mkaravel @dtenedor Finally got all the 【test】s passing, thanks for all your help! Think I covered all of the most recent 【review】 comments, let me know if you need anything else from me for the merge?
@RyanBerti this is super 【awesome】. Just wanted to thank you again for working on this. I'm sure lots of users will appreciate it.
@dtenedor thanks for all the help, excited to be able to utilize sketches natively in Spark!
As a follow-up, should you add a 【check】 to ensure a foldable `lgConfigK`?:<br><br>```<br>spark-sql (default)> create or replace temp view v1 as<br>select * from 【value】s<br>(1, 12),<br>(1, 12),<br>(2, 12),<br>(2, 12),<br>(3, 12)<br>as tab(col, logk);<br>Time taken: 1.665 seconds<br>spark-sql (default)> select hex(hll_sketch_agg(col, logk)) from v1;<br>23/05/09 16:25:25 ERROR Executor: Exception in task 4.0 in stage 0.0 (TID 4)<br>java.lang.NullPointerException<br>\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$4(InternalRow.scala:138)<br>\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$4$adapted(InternalRow.scala:138)<br>\tat org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:40)<br>\tat org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.lgConfigK$lzycompute(datasketchesAggregates.scala:65)<br>\tat org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.lgConfigK(datasketchesAggregates.scala:64)<br>\tat org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.createAggregationBuffer(datasketchesAggregates.scala:116)<br>\tat org.apache.spark.sql.catalyst.expressions.aggregate.HllSketchAgg.createAggregationBuffer(datasketchesAggregates.scala:55)<br>...<br>```
@bersprockets good catch - I thought relying on ExpectsInputTypes would be sufficient. Looking at other example functions like ApproximatePercentile, I see that the foldable 【check】 is applied after super.【check】InputDataTypes() in an overridden 【check】InputDataTypes() implementation. I'll follow this pattern and open up a new PR.
cc @cloud-fan @gengliangwang
@wangyum Do you mind writing more details about the reason why making this change in the PR description? <br>On second thought: as I will propose enabling ANSI mode by default in Spark 4.0, this one adds more gaps between the default behavior and ANSI SQL mode.
@gengliangwang Has updated the description.
Applied the comments, thanks @zhengruifeng !
Can you make a little JIRA for this and https://github.com/apache/spark/pull/40620 ? they're minor bugs
@thyecust Good catch! 【Thanks】 for fixing it!
I submitted a PR #40650 to fix the failed 【test】s.
Indeed a bug. It happens to work for 【value】s that aren't 0, I think
@Ngone51 @VasilyKolpakov <br>please help me 【review】 the PR
@Ngone51 @VasilyKolpakov<br>would  you please help me 【review】 the PR?
@HyukjinKwon would you please help me 【review】 this PR?<br>
+CC @HeartSaVioR
Hey, I think all these typo PRs should have a 【test】 if this is an actual issue.
And please file a JIRA if this is an issue, see also https://spark.apache.org/contributing.html
@thyecust <br>The failed 【test】 is not related to this, can you restart failed 【test】? <br>
<br><br><br><br>> Yeah it's more of a bug fix, good catch. File a JIRA and link in the title please<br><br>Thank you, this is my first time submitting code for Spark. I have applied for a JIRA account and I will file a bug later.
cc @tgravescs
definitely looks like a typo, thanks for catching and fixing
@grundprinzip Spark Connect is not released yet, I think we can still change it? This PR should go to Spark 3.4. cc @xinrong-meng
If it's guaranteed to be included in 3.4 it's not a breaking change.
It has conflicts with 3.4, @MaxGekk can you create a backport PR? 【Thanks】!
@cloud-fan I am working on the backport ...
Here is the backport to `branch-3.4`: https://github.com/apache/spark/pull/40666
【Thanks】, merged into master/branch-3.4
【Thanks】 for working on that! We will enable `foreach` and `foreachPartition` in Python Client's DataFrame based on that.
Merging, thanks!
Hi @cloud-fan @jiangxb1987  could you pls help 【review】 this?
It is unclear to me why we need this from the description, what the existing issues being solved are, and how much this approach will help.<br>Btw, if the variable being broadcasted is that small - simply inline it in the task and remove the roundtrip entirely ?
@mridulm the use case we found so far is the broadcasting of hadoop conf: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L161-L162<br><br>Simply inline the variable does not 【improve】 the perf because:<br>1. hadoop conf is not that small<br>2. if the executor is 【power】ful like 16 cores, the hadoop conf has 16 copies in the executor JVM, which is a waste.<br><br>We might find more cases in the future, as SQL operators need to broadcast small data (but not as small as a single integer) sometimes.
Why not handle a single chunk case in `TorrentBroadcast` to inline it ?<br>For the cases mentioned, this should come within 4mb.<br><br>This will prevent adding a new codepath, while also making all cases of broadcast 【fast】er for smaller blocks.<br>It also allows us to seemlessly go from smaller to larger payloads without changing api's in code.
A strawman proposal:<br>```<br>diff --git a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala<br>index 7b430766851..d9632964e3d 100644<br>--- a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala<br>+++ b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala<br>@@ -29,6 +29,7 @@ import scala.util.Random<br> import org.apache.spark._<br> import org.apache.spark.internal.{config, Logging}<br> import org.apache.spark.io.CompressionCodec<br>+import org.apache.spark.network.util.JavaUtils<br> import org.apache.spark.serializer.Serializer<br> import org.apache.spark.storage._<br> import org.apache.spark.util.{KeyLock, Utils}<br>@@ -95,12 +96,16 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO<br> <br>   private val broadcastId = BroadcastBlockId(id)<br> <br>-  /** Total number of blocks this broadcast variable contains. */<br>-  private val numBlocks: Int = writeBlocks(obj)<br>-<br>   /** The 【check】sum for all the blocks. */<br>   private var 【check】sums: Array[Int] = _<br> <br>+  /** Total number of blocks this broadcast variable contains. */<br>+  private val (singleBlockData: Array[Byte], numBlocks: Int) = writeBlocks(obj)<br>+  assert(1 != numBlocks || null != singleBlockData)<br>+  assert(1 == numBlocks || null == singleBlockData)<br>+  assert(null != 【check】sums || null != singleBlockData)<br>+<br>+<br>   override protected def getValue() = synchronized {<br>     val memoized: T = if (_【value】 == null) null.asInstanceOf[T] else _【value】.get<br>     if (memoized != null) {<br>@@ -135,7 +140,23 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO<br>    * @param 【value】 the object to divide<br>    * @return number of blocks this broadcast variable is divided into<br>    */<br>-  private def writeBlocks(【value】: T): Int = {<br>+  private def writeBlocks(【value】: T): (Array[Byte], Int) = {<br>+<br>+    val blocks = {<br>+      try {<br>+        TorrentBroadcast.blockifyObject(【value】, blockSize, SparkEnv.get.serializer, compressionCodec)<br>+      } catch {<br>+        case t: Throwable =><br>+          logError(s\"Store broadcast $broadcastId failed, cannot serialize object\")<br>+          throw t<br>+      }<br>+    }<br>+<br>+    if (1 == blocks.length) {<br>+      // no 【check】sum<br>+      return (JavaUtils.bufferToArray(blocks(0)), 1)<br>+    }<br>+<br>     import StorageLevel._<br>     val blockManager = SparkEnv.get.blockManager<br>     if (serializedOnly && !isLocalMaster) {<br>@@ -156,8 +177,6 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO<br>       }<br>     }<br>     try {<br>-      val blocks =<br>-        TorrentBroadcast.blockifyObject(【value】, blockSize, SparkEnv.get.serializer, compressionCodec)<br>       if (【check】sumEnabled) {<br>         【check】sums = new Array[Int](blocks.length)<br>       }<br>@@ -172,7 +191,7 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO<br>             s\"in local BlockManager\")<br>         }<br>       }<br>-      blocks.length<br>+      (null, blocks.length)<br>     } catch {<br>       case t: Throwable =><br>         logError(s\"Store broadcast $broadcastId fail, remove all pieces of the broadcast\")<br>@@ -186,6 +205,14 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long, serializedO<br>     // Fetch chunks of data. Note that all these chunks are stored in the BlockManager and reported<br>     // to the driver, so other executors can pull these chunks from this executor as well.<br>     val blocks = new Array[BlockData](numBlocks)<br>+    if (null != singleBlockData) {<br>+      assert(1 == numBlocks)<br>+      blocks(0) = new ByteBufferBlockData(<br>+        new ChunkedByteBuffer(ByteBuffer.wrap(singleBlockData)), false)<br>+      return blocks<br>+    }<br>+<br>+    assert(1 != numBlocks)<br>     val bm = SparkEnv.get.blockManager<br> <br>     for (pid <- Random.shuffle(Seq.range(0, numBlocks))) {<br><br>```
I think inline it in TorrentBroadcast still suffer from the multiple copies issue. Need to send a id and fetch-when-needed. Might be possible to implement on TorrentBroadcast with slightly API change, but not sure whether this will add more complexity, since there are not too much that we would like to share with current TorrentBroadcast implementation
Did you 【check】 the implementation proposal above @liuzqt ? There should not be a multiple copies issue - it will also get inlined into the task binary - which is already broadcast.<br>If I am missing something here, please let me know the details.
Circling back to this - to make sure I clarify: please feel free to use the proposal I detailed above (or some variation of it) @liuzqt !<br>Please ping me when you are done, and I will help 【review】 it.
Hi @mridulm  sorry for the late reply. I'm still confused about the mutiple copies issue, the taskBinary broadcast is created [here](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1554), when inlining into small broadcast, the taskBinary contains the real data block, and will be sent to executor per-task. Pls correct me if I understand it wrong...
yea it seems like the same as not using broadcast at all, then the multi-copy issue will appear. We need to still use the block manager to transfer the broadcast data.
A few points to consider:<br><br>a) Task binary is already broadcasted - so the inlined versoin should not have an overhead by itself.<br><br>Ignore (a) for time being, <br><br>b) the poc above was a strawman proposal to inline, we can use other strategies for the small block case in torrent broadcast itself - either inline (as in the example), or fetch from block manager directly, etc.<br><br>Essentially, what I am trying to get to is, having users explicitly try to reason about whether their broadcast data is small or large is brittle - this is something that needs to be handled automatically by the broadcast impl seemlessly.<br>When data is small, use more 【efficient】 paths, and 【progress】ively move to more expensive options when data is larger - without any user code change.
Hi @mridulm agree that the broadcast impl under the hood should not be exposed to user if possible, let me see how we can inline the small broadcast within current broadcast code path based on your poc
cc  @cloud-fan @dongjoon-hyun @viirya @huaxingao @sunchao
FYI, this PR landed only at master branch because SPARK-42997 is filed as 'Improvement`. <br><br>![Screenshot 2023-04-03 at 3 26 17 PM](https://user-images.githubusercontent.com/9700541/229640458-0cbecb85-a8ae-4b2f-84f6-913dfbc6ad51.png)<br>
kindly ping @MaxGekk
cc @gengliangwang too
@MaxGekk Plz 【review】 this PR, thx
@Leibnizhu Could you fix PR's description and title according to your actual 【c<font color=blue>hang】es】, please.
> @Leibnizhu Could you fix PR's description and title according to your actual 【c<font color=blue>hang】es】, please.<br><br>done
+1, 【LGTM】. Merging to master.<br>Thank you, @Leibnizhu.
@Leibnizhu Congratulations with your first 【contribution】 to Apache Spark!
> @Leibnizhu Congratulations with your first 【contribution】 to Apache Spark!<br><br>thx 😁😁😁
>  But generating optimised logical plan sometimes takes more time.<br><br>Why don't you just print out `df.queryExecution.analyzed`? This sounds not very common case to have a separate user-facing API for this.
> > But generating optimised logical plan sometimes takes more time.<br>> <br>> Why don't you just print out `df.queryExecution.analyzed`? This sounds not very common case to have a separate user-facing API for this.<br><br>This is a unresolved task that I found on Jira [SPARK-42860](https://issues.apache.org/jira/browse/SPARK-42860), which some people \"Start watching this issue\". Additionally, its<br>priority is set to \"Blocked\ so I completed it. In fact, I also have a little doubt about the usage scenario of<br>this API.
> > But generating optimised logical plan sometimes takes more time.<br>> <br>> Why don't you just print out `df.queryExecution.analyzed`? This sounds not very common case to have a separate user-facing API for this.<br><br>In your opinion, should I close this pr？
Mind filing a JIRA please? See also https://spark.apache.org/contributing.html.
【Thanks】 @ShreyeshArangath for this! I think it helps clear a lot of unnecessary noise from user logs and keeps the logs manageable.<br><br>One thing I noticed is that we set `spark.yarn.report.logging.frequency` to `30` by default. I think it is a much more sensible default, especially given a) Spark jobs submitted to YARN would usually run longer than a minute including the overhead of launching Spark driver b) We always report any state change immediately e.g. `ACCEPTED` -> `RUNNING`. I would be pro defaulting to `30`. But if we want to eliminate behavior change, we can always default it to `1`.<br><br>I saw the 【test】s are failing with what seems to be a transient Github connection issue.<br>```<br>/usr/bin/git -c protocol.version=2 fetch --prune --【progress】 --no-recurse-submodules origin +refs/heads/*:refs/remotes/origin/* +refs/tags/*:refs/tags/*<br>  Error: fatal: unable to access 'https://github.com/apache/spark/': The requested URL returned error: 429<br>  The process '/usr/bin/git' failed with exit code 128<br>```<br>Can you push an empty commit to re-run the 【test】s?
The GIthub Actions 【test】s now pass. The AppVeyor build is failing, but it looks to be unrelated.<br><br>Tagging folks with recent commits/【review】s of this file<br>cc: @sunchao @dongjoon-hyun @mridulm @tgravescs Can you help take a look?
how is this different from:<br>```<br><br>  private[spark] val REPORT_INTERVAL = ConfigBuilder(\"spark.yarn.report.interval\")<br>    .doc(\"Interval between reports of the current app status.\")<br>    .version(\"0.9.0\")<br>    .timeConf(TimeUnit.MILLISECONDS)<br>    .createWithDefaultString(\"1s\")<br><br>```<br><br>?
> how is this different from:<br>><br>>  private[spark] val REPORT_INTERVAL = ConfigBuilder(\"spark.yarn.report.interval\")<br>>   .doc(\"Interval between reports of the current app status.\")<br>>    .version(\"0.9.0\")<br>>    .timeConf(TimeUnit.MILLISECONDS)<br>>    .createWithDefaultString(\"1s\")<br>><br>> ?<br><br>@tgravescs The new property allows for the ability to reduce the log noise without reducing the 【responsiveness】 when something 【c<font color=blue>hang】es】 because the status 【check】 interval is not reduced like if `spark.yarn.report.interval` was used alone. For example, if a job failed right away waiting 30 seconds to get that feedback slows down the iteration cycle.
> how is this different from:<br>> <br>> ```<br>> <br>>   private[spark] val REPORT_INTERVAL = ConfigBuilder(\"spark.yarn.report.interval\")<br>>     .doc(\"Interval between reports of the current app status.\")<br>>     .version(\"0.9.0\")<br>>     .timeConf(TimeUnit.MILLISECONDS)<br>>     .createWithDefaultString(\"1s\")<br>> ```<br>> <br>> ?<br><br>@tgravescs this property dictates the how often we poll yarn for an application report. We still want to 【check】 for the application state every second, if we change this property then we might not know of a change in state until the `x` seconds have elapsed. <br><br>This is why, I decided to create a new logging 【specific】 property so that the current behavior of polling application report every second is not affected.<br>
lgtm to me pending build/【test】
@dongjoon-hyun / @HyukjinKwon, could you help me merge this PR? thanks 【:)】
Got it, @ShreyeshArangath .
Merged to master for Apache Spark 3.5.0.<br><br>Thank you, @ShreyeshArangath , @HyukjinKwon , @shardulm94, @tgravescs , @robreeves .<br><br>Also, welcome to the Apache Spark 【community】, @ShreyeshArangath !<br>I added you to the Apache Spark contributor group and assigned SPARK-43002 to you.
I applied the suggested code 【c<font color=blue>hang】es】. can you 【review】 the code one more time in your spare time, thanks a lot @jaceklaskowski <br><br>
Could you please take a look when you have time? thanks! @cloud-fan
I applied the suggested code 【c<font color=blue>hang】es】. can you 【review】 the code one more time in your spare time, thanks a lot @jaceklaskowski
【Thanks】 @dongjoon-hyun <br>OK~ let's wait for some more days
Merged to master. Thank you for waiting.
Sorry for post-【review】. It'd be nice if you don't mind running below benchmark as well.<br>sql/core/src/【test】/scala/org/apache/spark/sql/execution/benchmark/StateStoreBasicOperationsBenchmark.scala<br>The 【test】 is more likely about 【performance】 of WriteBatch rather than RocksDB itself but that's something we use in RocksDB state store provider.
Thank you, @HeartSaVioR . To @LuciferYang , could you address the above comment?
> Thank you, @HeartSaVioR . To @LuciferYang , could you address the above comment?<br><br>OK, Let me update the results of `StateStoreBasicOperationsBenchmark ` in a follow-up<br><br>
Need to update later because<br><br><img width=\"1200\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/230563757-cf26102c-3f87-43a9-95f3-84f8a65b530c.png\"><br>
cc @cloud-fan @itholic
@cloud-fan @MaxGekk would you mind taking another look?
+1, 【LGTM】. Merging to master/3.4.<br>Thank you, @zhengruifeng and @cloud-fan @itholic for 【review】.
Merging to master/3.4. Thank you, @cloud-fan @HyukjinKwon @LuciferYang for 【review】.
Failed 【test】s are irrelevant
cc @yaooqinn @dongjoon-hyun
> I understand what you aim to achieve in this PR. In case of K8s Cluster Mode, the driver pod has a driver `ConfigMap` containing `spark.app.submitTime` already. So, you want to keep that. However, this is not safe because of the timezone between the user machine and the K8s control plane server.<br>> <br>> If we want to propose this 【improvement】 to include the driver pod pending time, we need to add a configuration to control this 【feature】, @zhouyifan279 . Could you try to add a configuration and add an integration 【test】 case too?<br><br>@dongjoon-hyun Can you elaborate about how timezone affects the computed driver pod pending time.<br><br>As I understand, both `spark.app.submitTime` and `spark.app.startTime` is milliseconds between the current time and midnight, January 1, 1970 UTC.<br>
Sorry for misleading you. You are right about timezone. What I imagined was more like the following case.<br>```<br>$ docker run -it --rm --cap-add SYS_TIME openjdk:la【test】 bash<br><br>bash-4.4# date -s '@2147483647'<br>Tue Jan 19 03:14:07 UTC 2038<br><br>bash-4.4# jshell<br>|  Welcome to JShell -- Version 18.0.2.1<br>|  For an introduction type: /help intro<br><br>jshell> java.lang.System.currentTimeMillis()<br>$1 ==> 2147483657595<br>```
> Any updates, @zhouyifan279 ?<br><br>@dongjoon-hyun sorry for response late. <br><br>If I did not misunderstand, you are talking about user 【c<font color=blue>hang】es】 system time by intention.<br><br>Speaking from my limited experience, I think very few users may encounter this case. <br>We'd better not to handle this case in Spark's code. <br><br>Besides, if user changed system time by intention, he can handle it by himself easily:<br>1. Inject a custom spark property, such as `spark.app.systemTimeBias` into SparkConf. <br>2. The accurate driver pod pending time would be: `startTime - systemTimeBias - submitTime`.<br><br>
No because it will be a new regression in terms of the vulnerability. You are aiming to fix one thing and open another bug.<br>> We'd better not to handle this case in Spark's code.<br><br>That's the reason why I counter-propose you to add `a configuration` to support this proposed behavior and the existing behavior, @zhouyifan279 .<br>- https://github.com/apache/spark/pull/40645#pullrequest【review】-1370202973<br>
@dongjoon-hyun Added a new configuration `spark.kubernetes.setSubmitTimeInDriver` to control which `submitTime` Spark should use.<br>Now Spark will set `spark.app.submitTime` in the following cases:<br><br>1. The application is deployed using spark-submit in client/cluster mode. <br>SubmitTime is the time spark-submit executed.<br>2. The application is deployed by other ways, such as [spark-on-k8s-operator](https://googlecloudplatform.github.io/spark-on-k8s-operator/)<br>SubmitTime is the time driver pod started.<br>3. The application is deployed using spark-submit in cluster mode and `spark.kubernetes.setSubmitTimeInDriver` is true. <br>SubmitTime is the time driver pod started.
@dongjoon-hyun do you have any more inputs on this PR?
Gently ping @dongjoon-hyun
Unfortunately, it seems to fail again.
> Unfortunately, it seems to fail again.<br><br>Let me trigger it again.
Thank you, @zhouyifan279 .<br><br>BTW, when I said the following.<br>> Please re-trigger the failed 【test】 pipeline.<br><br>It means this. `Re-run failed jobs` is always 【fast】er.<br>![Screen Shot 2023-05-09 at 9 25 51 AM](https://github.com/apache/spark/assets/9700541/59d15792-5d5e-4ee5-9ab8-61341391cb23)<br>
> Thank you, @zhouyifan279 .<br>> <br>> BTW, when I said the following.<br>> <br>> > Please re-trigger the failed 【test】 pipeline.<br>> <br>> It means this. `Re-run failed jobs` is always 【fast】er. ![Screen Shot 2023-05-09 at 9 25 51 AM](https://user-images.githubusercontent.com/9700541/237159446-59d15792-5d5e-4ee5-9ab8-61341391cb23.png)<br><br>【Thanks】 for the information. Didn't know that before. 😂
> This PR never passes the unit 【test】s. According to the GitHub Action log, you had better verify the following 【test】 suite locally, @zhouyifan279 . In the 【community】, we are unable to merge a PR with UT failures.<br>> <br>> ```<br>> [info] ReplE2ESuite:<br>> sh: 1: cannot open /dev/tty: No such device or address<br>> sh: 1: cannot open /dev/tty: No such device or address<br>> sh: 1: cannot open /dev/tty: No such device or address<br>> sh: 1: cannot open /dev/tty: No such device or address<br>> sh: 1: cannot open /dev/tty: No such device or address<br>> sh: 1: cannot open /dev/tty: No such device or address<br>> ```<br>@dongjoon-hyun CI succeeded after I rebase on master branch.<br>
Great! Thank you, @zhouyifan279 .
cc @wangyum FYI
> Add configuration spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.<br><br>hmm... what happens if machines that support AVX 512 are prohibited from using AVX 512?
```<br>[error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala:4512:7: method parquetAggregatePushDown is defined twice;<br>[error]   the conflicting method parquetAggregatePushDown was defined at line 4510:7<br>[error]   def parquetAggregatePushDown: Boolean = getConf(PARQUET_VECTOR512_READ_ENABLED)<br>[error]       ^<br>[error] one error found<br>```<br><br>compile failed
> > Add configuration spark.sql.parquet.vector512.read.enabled, If true and CPU contains avx512vbmi & avx512_vbmi2 instruction set, parquet decodes using Java Vector API. For Intel CPU, Ice Lake or newer contains the required instruction set.<br>> <br>> hmm... what happens if machines that support AVX 512 are prohibited from using AVX 512?<br><br>@LuciferYang this is a good question.  As far as I know, Cascade Lake currently used by most users. Compared with Ice lake, Cascade Lake is Intel's previous generation CPU and it does not contains avx512vbmi & avx512_vbmi2 instruction set.  If running the parquet vector 【optimization】 on Cascade Lake, it will become very slowly(0.01x), but there is a JDK [Patch](https://bugs.openjdk.org/browse/JDK-8290322) to fix this problem, with the patch, it will be 1.7x.  Under normal situation, it will be 5.5x on Ice Lake with Java17. The patch has been merged Java17.
> ```<br>> [error] /home/runner/work/spark/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala:4512:7: method parquetAggregatePushDown is defined twice;<br>> [error]   the conflicting method parquetAggregatePushDown was defined at line 4510:7<br>> [error]   def parquetAggregatePushDown: Boolean = getConf(PARQUET_VECTOR512_READ_ENABLED)<br>> [error]       ^<br>> [error] one error found<br>> ```<br>> <br>> compile failed<br><br>@LuciferYang 【Thanks】, I will fix it
Since avx256 has been supported by more types of x86 servers, could this PR also supports avx256 ?
cc @srowen @sadikovi backport to 3.4
thanks @dongjoon-hyun @sadikovi
【Thanks】 @HyukjinKwon @hvanhovell
@grundprinzip @zhenlineo @LuciferYang Can you help me see if there is a problem? 【Thanks】.
I suggest placing the 【design】 doc on Google doc and initiating 【discussion】s in the dev mail list for more people to participate. <br><br>Additionally, Spark Connect is not limited to Scala clients, so Python clients should also be considered.<br><br>Meanwhile, there is still a lot of unfinished work on Spark Connect (in order to maintain the same behavior as the native Spark API),  so I am not sure if everyone has the energy to discuss this new 【feature】 at the moment.<br><br>
> I suggest placing the 【design】 doc on Google doc and initiating 【discussion】s in the dev mail list for more people to participate.<br>> <br>> Additionally, Spark Connect is not limited to Scala clients, so Python clients should also be considered.<br>> <br>> Meanwhile, there is still a lot of unfinished work on Spark Connect (in order to maintain the same behavior as the native Spark API), so I am not sure if everyone has the energy to discuss this new 【feature】 at the moment.<br><br>【Thanks】 for suggestion, I will add python 【design】 and move doc to google doc later. Then send mail. Before start to do this 【feature】, I will try to do other Spark Connect missing 【feature】s that need to be added
@Hisoka-X thanks for the write up. We should be able to support most of this at the moment. GRPC supports this type of execution out of the box. The reason we did not really go for this, is because of API 【compatibility】. The `SparkResult` does support incremental collect and can collect results in the background though.<br><br>The thing that Martin was getting at in the ticket is more about what to do when disconnect happen. You probably want to reconnect in these cases, this does require some architectural rework. We are discussing how we should do this, there are quite a few trade offs here. Do you mind shelving this until we can provide a bit more 【clarity】? Please let me know if you want in on these conversations.
> @Hisoka-X thanks for the write up. We should be able to support most of this at the moment. GRPC supports this type of execution out of the box. The reason we did not really go for this, is because of API 【compatibility】. The `SparkResult` does support incremental collect and can collect results in the background though.<br>> <br>> The thing that Martin was getting at in the ticket is more about what to do when disconnect happen. You probably want to reconnect in these cases, this does require some architectural rework. We are discussing how we should do this, there are quite a few trade offs here. Do you mind shelving this until we can provide a bit more 【clarity】? Please let me know if you want in on these conversations.<br><br>Ok for me. I would be happy if I could join the 【discussion】
Oh darn the 【test】s passed in the original PR. OK I'll merge this when 【test】s finish.
This PR should be ready to 【review】 now.
Merging. Please address remaining comments (if any) in a follow-up
Hi @gengliangwang here is the correctness bug fix 🙏
【Thanks】, merging to master/branch-3.4<br>cc @xinrong-meng
Wait https://github.com/apache/spark/pull/40605 to add mima 【check】
cc @rangadi fyi
hmm... `SparkConnectPlanner ` has had another conflict... I will fix it and merge this pr as soon as possible
Merged to master. 【Thanks】 @hvanhovell @HyukjinKwon @rangadi
cc @gengliangwang @cloud-fan @dongjoon-hyun @viirya @huaxingao @sunchao
@dongjoon-hyun, let me look into 【test】 failures.
Ok, all 【test】s have been adapted. This PR is ready for a detailed 【review】.
@aokolnychyi @cloud-fan  I am +0 for changing the behavior since I haven't heard complaints about this from end-users.  Instead, relaxing the strict compiler 【check】 can bring complaints.<br> <br>Do we consider other alternatives? For example, we can have a new function `as_not_null` appending on the input 【value】/columns to bypass the static 【check】s? E.g.<br>```<br>insert into target select as_not_null(null_column_name) from source<br>```
@gengliangwang, this PR is based on the consensus we reached in [this](https://github.com/apache/spark/pull/40308#【discussion】_r1127081206) thread. Each approach has its own pros/cons. The primary problem is that our behavior is not consistent (e.g. inserts and updates behave differently). In that thread, it seemed the best way forward is to use runtime 【check】s everywhere. If I were to pick one approach, I think runtime 【check】s are a bit better as they only fail if we really have null 【value】s. Otherwise, we rely on null propagation, which may not be that 【reliable】. The primary motivation for this PR is to have consistent behavior rather than replace static 【check】s with runtime 【check】s as those are better.<br><br>Let me know what you think!
@aokolnychyi Yes I got it. My concern was around the behavior change. I am OK with the idea and merging this one.
@gengliangwang, got it. I was initially concerned as well but I believe this is the right thing to do after we discussed it. 【Thanks】 for taking a look!
thanks, merging to master
This one just for Spark 3.5.0
@MaxGekk , I have updated the PR for all your 【review】 except the \"Could you trigger the error from user code, please. For example sql(\"select parse_url(...)\")\".<br>I can't use sql function in StringExpressionsSuite.scala, don't know how to import the sql function. <br>I have tried in UrlFunctionsSuite.scala using sql function. The 【test】 result reported:<br>```<br>[info] - url parse_url function *** FAILED *** (3 seconds, 922 milliseconds)<br>[info]   Expected exception org.apache.spark.SparkIllegalArgumentException to be thrown, but no exception was thrown (UrlFunctionsSuite.scala:75)<br>```  <br><br>Actually I firstly added 【test】 case in UrlFunctionSuite like I do now.scala to try to get the error, but I failed. I have tried to assign the sql code result to a variable and then call show method. The 【test】 case reported following error:<br>```<br>[info] - url parse_url function *** FAILED *** (4 seconds, 65 milliseconds)<br>[info]   Expected exception org.apache.spark.SparkIllegalArgumentException to be thrown, but org.apache.spark.SparkException was thrown (UrlFunctionsSuite.scala:75)<br>```<br><br>Then I found StringExpressionsSuite.scala and tried in it. The error can be reproduced. So I don't know it can't be reproduced from user space or my method is wrong.
@liang3zy22 You need to run an action to trigger the execution exception like:<br>```scala<br>    withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {<br>      val url = \"inva lid://user:pass@host/file;param?query;p2\"<br>      【check】Error(<br>        exception = intercept[SparkException] {<br>          sql(s\"SELECT parse_url('$url', 'HOST')\").collect()<br>        }.getCause.asInstanceOf[SparkThrowable],<br>        errorClass = \"INVALID_URL\<br>        parameters = Map(<br>          \"url\" -> url,<br>          \"ansiConfig\" -> toSQLConf(SQLConf.ANSI_ENABLED.key)))<br>    }<br>```
@liang3zy22 Do you have an account at https://issues.apache.org/ ? I would like to resolve it and assign it to you.
> @liang3zy22 Do you have an account at https://issues.apache.org/ ? I would like to resolve it and assign it to you.<br><br>I have. I already added comment in the JIRA issue SPARK-42844.<br>
+1, 【LGTM】. Merging to master.<br>Thank you, @liang3zy22 and @itholic for 【review】.
@liang3zy22 Congratulations with your first 【contribution】 to Apache Spark!
@MaxGekk and @itholic  , thanks for your patient 【review】.
https://pandas.pydata.org/pandas-docs/version/2.0.1/whatsnew/v2.0.1.html
【Thanks】, @bjornjorgensen !
just FYI, 2.0.1 has been released
【Thanks】 for the notice. Let me address 2.0.0 first, and made a follow-up for 2.0.1 right after then to avoid extra complexity.
I just have opened a [new PR](https://github.com/apache/spark/pull/41211) that does not include any behavior 【c<font color=blue>hang】es】 related to the pandas API on Spark.<br><br>Since there are already lots of behavior 【c<font color=blue>hang】es】 included in this PR, let me resume working on it during a future major release Instead of reverting all the 【c<font color=blue>hang】es】.<br><br>Also changed the PR title from \"Upgrade pandas to 2.0.0\" to \"Matching the behavior of pandas API on Spark to pandas 2.0.0\"
Let me close this PR for now, will revisit when we ready for the next Apache Spark major release.
cc @dongjoon-hyun @cloud-fan  thank you
@allisonwang-db Could you fix the R 【test】:<br>```<br>══ Failed ══════════════════════════════════════════════════════════════════════<br>── 1. Failure ('【test】_sparkSQL.R:3963'): Setting and getting config on SparkSessi<br>`sparkR.conf(\"completely.dummy\")` threw an error with unexpected message.<br>Expected match: \"Config 'completely.dummy' is not set\"<br>Actual message: \"Unknown error: Error in handleErrors(returnStatus, conn): org.apache.spark.SparkNoSuchElementException: <br>```
+1, 【LGTM】. Merging to master.<br>Thank you, @allisonwang-db and @srielau for 【review】.
I think @peter-toth did something similar before, can you share some ideas @peter-toth ?
> I think @peter-toth did something similar before, can you share some ideas @peter-toth ?<br><br>I guess @peter-toth did the similar thing for scalar subquery, but this one try to fix non-scalar subquery.
> > I think @peter-toth did something similar before, can you share some ideas @peter-toth ?<br>> <br>> I guess @peter-toth did the similar thing for scalar subquery, but this one try to fix non-scalar subquery.<br><br>Sorry, I haven't got time to fully 【review】 the PR (maybe next week) but at first sight it seems to copy some fuctions (e.g. `【check】IdenticalPlans()`, `mergeNamedExpressions()`) from `MergeScalarSubqueries` so there seems to be some room for 【improvement】 and we could share the common functions. Also, some names (e.g. `subqueryIndex`) might need some 【c<font color=blue>hang】es】 here.<br><br>This PR combines UNION ALL legs if they return disjoint set of rows from the same source node. I think this makes sense in those cases when there are overlaping scans in the legs (despite the disjoint filters), and by \"overlapping\" I mean that the scans use some common set of files.<br>So seems like the only case when this change doesn't bring 【improvement】 is when the filter is a pushed-down partitioning/bucketing column filter and the scans in union legs doesn't overlap. But even in that case I'm not sure if this PR has any disadvantage, just doesn't 【improve】 anything...<br><br>BTW, `MergeScalarSubqueries` (https://github.com/apache/spark/pull/32298) does very similar merging, but we run that only once because merging can be costly when there are many candidates. Do we need `EliminateUnions` in `operatorOptimizationBatch`?<br>Sidenote: `MergeScalarSubqueries` doesn't work with different filters currently. This is because merging filters in subqueries is more comlicated as we need to propogate the filters up to an aggregate, and because it can cause 【performance】 degradation when we have non-overlapping scans. (See this WIP PR: https://github.com/apache/spark/pull/37630).
@peter-toth Thank you for your first look.<br>In fact, this PR references some functions as you mentioned above (e.g. `【check】IdenticalPlans`). If we can share these functions, that be good.<br><br>The partitioning/bucketing column filter seems doesn't 【improve】 anything. I will optimize it in further.
cc @gengliangwang @viirya @allisonwang-db
@viirya sorry I wrote it wrong. The attr IDs do not change. It should be<br>> However, with metadata columns, the analyzer will copy a scan relation with new output attributes to include metadata cols.
thanks for the 【review】, merging to master (it doesn't fix any actual bug so no need to backport)!
Ping @LuciferYang @JoshRosen that previously discussed this issue in https://github.com/apache/spark/pull/37206 let me know if would be a better approach to reopen the old PR rather then creating a new one. And/or if my description of the data race does not sounds accurate to you.<br>
It's ok to me to further 【discussion】 in this one <br>
I found that before Scala 2.13.6(include) seems no this issue and the new 【test】 will failed after 2.13.7.  <br><br>@eejbyfeldt I am not sure if this is caused by change of https://github.com/scala/scala/pull/9258, as it has been added to Scala 2.13.4.<br><br>also cc @srowen @mridulm and @xinrong-meng <br>
> I found that before Scala 2.13.6(include) seems no this issue and the new 【test】 will failed after 2.13.7.<br>> <br>> @eejbyfeldt I am not sure if this is caused by change of [scala/scala#9258](https://github.com/scala/scala/pull/9258), as it has been added to Scala 2.13.4.<br><br><br>【Thanks】 for looking in to that closer. Should probably formulated myself more clearly that was only a guess on my part and not something I had verified. But now that you narrowed it down to 2.13.7 another possible candidate change could be https://github.com/scala/scala/pull/9786 that changed how that mutations are tracked in ArrayBuffer. <br><br>EDIT: Even if [scala/scala#9258](https://github.com/scala/scala/pull/9258) was tagged milestone 2.13.4 at some point it looks to me like it actually landed in 2.13.7 with this commit https://github.com/scala/scala/commit/5f250021e7e55dd36e45d0b224a6cb6967e67ebd<br><br>
This makes sense.<br>The TaskRunner is visible to heartbeater since it gets added to `runningTasks` before the task binary is deserialized.<br>During `TaskRunner.run`, we are registering the accumulators as part of the deserialization - which can result in race with the heartbearter reading the metrics.<br><br>So this is essentially a bug waiting to happen for two reasons:<br><br>* `externalAccums` is getting used in an MT-unsafe way by the task thread and the reporter thread.<br>* This is broken in 2.12 as well - we were just not aware of it.<br>  * There always was potential visibility issues in heartbeat for external accumulators.<br><br>Note, the task itself is visible only after the deserialization is complete, since it is volatile - but the registration is not covered by it.<br>
@eejbyfeldt `unused-imports` 【check】 failed, please fix it<br><br>```<br>[error] /home/runner/work/spark/spark/core/src/【test】/scala/org/apache/spark/executor/ExecutorSuite.scala:48:41: Unused import<br>[error] import org.apache.spark.internal.config.Network<br>[error]                                         ^<br>[error] one error found<br>```
> I found that before Scala 2.13.6(include) seems no this issue and the new 【test】 will failed after 2.13.7.<br>> <br>> @eejbyfeldt I am not sure if this is caused by change of [scala/scala#9258](https://github.com/scala/scala/pull/9258), as it has been added to Scala 2.13.4.<br>> <br>> also cc @srowen @mridulm and @xinrong-meng<br><br>also cc @dongjoon-hyun due to Spark 3.2.x also use Scala 2.13.8 and maybe Spark 3.2.4 should include this one
@HyukjinKwon branch-3.3 and branch-3.2 may also require this one, they also use Scala 2.13.8, need @eejbyfeldt  to submit an independent PRs?
Thank you, @eejbyfeldt , @mridulm , @HyukjinKwon , and @LuciferYang .<br>I also backported to branch-3.3.<br><br>However, we need a new PR for branch-3.2 due to the compilation error, @LuciferYang and @eejbyfeldt .<br>```<br>[error] /Users/dongjoon/APACHE/spark-merge/core/src/【test】/scala/org/apache/spark/executor/ExecutorSuite.scala:33:19: object logging is not a member of package org.apache<br>[error] import org.apache.logging.log4j._<br>```
To @LuciferYang  and all.<br><br>After double-【check】ing, I found that Apache Spark 3.2.x is not affected because it uses Scala 2.13.5. <br><br>https://github.com/apache/spark/blob/7773740e4141444bf78ba75dcee9f3fade7f6e11/pom.xml#L3389<br><br>SPARK-35496 (Scala 2.13.7) landed at Apache Spark 3.3.0+. We don't need to backport this to branch-3.2.<br>
Please let me know if this is still valid in `branch-3.2`.<br>> branch-3.3 and branch-3.2 may also require this one, they also use Scala 2.13.8
@dongjoon-hyun Scala 2.13.5 does not require this fix. I apologize for providing incorrect information earlier
No problem at all. Thank you always, @LuciferYang !
【Thanks】 for 【check】ing @dongjoon-hyun and @LuciferYang !<br>Great to finally have this issue 【<font color=blue>fix】ed】 :-)
cc @itholic @zhengruifeng @xinrong-meng @Yikun if you find some time to 【review】.
cc @zhengruifeng @ueshin FYI
cc @allanf-db @zhengruifeng @ueshin FYI
Merged to master and bracnh-3.4.
Nice. cc @itholic
Will merge this in few days. Please let me know if there's any concern.
Seems working fine.<br><br>Merged to master.
Find an error related to ReplE2ESuite on [GA 【test】 task](https://pipelines.actions.githubusercontent.com/serviceHosts/c184045e-b556-4e78-b8ef-fb37b2eda9a3/_apis/pipelines/1/runs/69161/signedlogcontent/23?urlExpires=2023-04-18T10%3A18%3A24.5673026Z&urlSigningMethod=HMACV1&urlSignature=IZ4kWbB8mtkvxvyxojX3%2FxIz43j%2FVRKl7Ghp2Y52nnE%3D):<br><br>```<br>023-04-18T03:29:20.8938544Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32mReplE2ESuite:\u001B[0m\u001B[0m<br>2023-04-18T03:29:24.5685770Z sh: 1: cannot open /dev/tty: No such device or address<br>...<br>2023-04-18T03:29:25.5551653Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-18T03:29:25.5631256Z sh: 1: cannot create /dev/tty: No such device or address<br>...<br>2023-04-18T03:29:43.5473148Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.lang.RuntimeException: REPL Timed out while running command: \u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5473697Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mclass A(x: Int) { def get = x * 5 + 19 }\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5474147Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mdef dummyUdf(x: Int): Int = new A(x).get\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5474578Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mval myUdf = udf(dummyUdf _)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5480701Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mspark.range(5).select(myUdf(col(\"id\"))).as[Int].collect()\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5481161Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m      \u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5481539Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mConsole output: \u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5482081Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mSpark session 【available】 as 'spark'.\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5484862Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m   _____                  __      ______                            __\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5488022Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5491144Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5494244Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5497421Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5500566Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m    /_/\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.5502973Z sh: 1: cannot open /dev/tty: No such device or address<br>...<br>2023-04-18T03:29:43.7154910Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mError output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7155688Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mCompiling /home/runner/work/spark/spark/connector/connect/client/jvm/(console)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7156273Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mjava.lang.RuntimeException: Nonzero exit 【value】: 2\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7156688Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.package$.error(package.scala:30)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7157121Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:138)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7157538Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:108)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7158104Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.TTY$.stty(Utils.scala:103)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7158794Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.TTY$.withSttyOverride(Utils.scala:114)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7160239Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.Terminal$.readLine(Terminal.scala:41)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7162012Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.AmmoniteFrontEnd.readLine(AmmoniteFrontEnd.scala:133)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7163249Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.AmmoniteFrontEnd.action(AmmoniteFrontEnd.scala:28)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7163556Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.$anonfun$action$4(Repl.scala:194)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7163863Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.$anonfun$flatMap$1(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7164158Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Signaller.apply(Signaller.scala:28)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7171303Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.flatMap(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7172151Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.flatMap$(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7172894Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Signaller.flatMap(Signaller.scala:16)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7174248Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.$anonfun$action$2(Repl.scala:178)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7176993Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.util.Catching.flatMap(Res.scala:115)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7177811Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.action(Repl.scala:170)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7178715Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.loop$1(Repl.scala:212)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7179009Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.run(Repl.scala:227)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7179272Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.Main.$anonfun$run$1(Main.scala:236)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7179543Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.Option.getOrElse(Option.scala:189)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7179795Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.Main.run(Main.scala:224)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7184102Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:100)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7185205Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.application.ReplE2ESuite$$anon$1.run(ReplE2ESuite.scala:59)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7185569Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.Executors$RunnableAdapter.call(Executors.java:511)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7185885Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.FutureTask.run(FutureTask.java:266)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7191750Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7192249Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001B[0m\u001B[0m<br>2023-04-18T03:29:43.7192662Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.lang.Thread.run(Thread.java:750)\u001B[0m\u001B[0m<br>```<br><br>I am currently unsure why this error occurred<br><br>
@LuciferYang It looks related to the TTY issues (https://github.com/com-lihaoyi/Ammonite/issues/276) we were hitting in the CI pipelines earlier. (`2023-04-18T03:29:24.5685770Z sh: 1: cannot open /dev/tty: No such device or address` in the logs is the indicator) <br><br>[These](https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253) 【c<font color=blue>hang】es】 (taken from [here](https://github.com/com-lihaoyi/Ammonite/issues/276#issuecomment-439273906)) (helped us mitigate the issue for the CI pipelines but perhaps the GA 【test】 pipeline uses a different config? Any idea about the pipeline config @HyukjinKwon?<br><br>Also for further clarification, this issue only appears during the CI 【test】s. Local 【test】ing does not face this issue.
Is this just being flaky? All GA jobs share the same thing (that you 【<font color=blue>fix】ed】)
@HyukjinKwon I would not think it is flaky. It is probably the GA builds also missing the TTY setup in the build steps. So we need to copy the following 【c<font color=blue>hang】es】 to the steps:<br>```<br>      run: |<br>        # Fix for TTY related issues when launching the Ammonite REPL in 【test】s.<br>        export TERM=vt100 && script -qfc 'echo exit | amm -s' && rm typescript<br>```<br>https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253
@LuciferYang which GitHub job failed? The link seems stale now so I can't see. It is a scheduled build?
> @LuciferYang which GitHub job failed? The link seems stale now so I can't see. It is a scheduled build?<br><br>I found this issue from GA task of user pr<br><br>The last failed build on https://github.com/apache/spark/pull/40783  also had this issue, I don't know if @rangadi  can provide a new link
@HyukjinKwon https://github.com/rangadi/spark/actions/runs/4737137341/jobs/8409588363 this one
Ah, i think it just happened because of unsynced fork. It should be 【<font color=blue>fix】ed】 if their fork is synced to the las【test】 master branch.
Hmm... GA should merge the current pr to the la【test】 master before 【test】ing, right? <br><br>It's okay, Let's wait and see if there are any new cases happening.<br><br>
> @HyukjinKwon I would not think it is flaky. It is probably the GA builds also missing the TTY setup in the build steps. So we need to copy the following 【c<font color=blue>hang】es】 to the steps:<br>> <br>> ```<br>>       run: |<br>>         # Fix for TTY related issues when launching the Ammonite REPL in 【test】s.<br>>         export TERM=vt100 && script -qfc 'echo exit | amm -s' && rm typescript<br>> ```<br>> <br>> https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253<br><br>@zhenlineo is there some reference about such magic command? better to briefly explain the mechanism or leave a link in comments
> Hmm... GA should merge the current pr to the la【test】 master before 【test】ing, right?<br><br>Yes except the workflow file. For the 【c<font color=blue>hang】es】 in workflow, they have to manually update to the la【test】 master branch.
> > Hmm... GA should merge the current pr to the la【test】 master before 【test】ing, right?<br>> <br>> Yes except the workflow file. For the 【c<font color=blue>hang】es】 in workflow, they have to manually update to the la【test】 master branch.<br><br>Got it ~
@vicennial I found `ReplE2ESuite` always failed in Java 17 GA daily 【test】:<br><br>- https://github.com/apache/spark/actions/runs/4726264540/jobs/8385681548<br>- https://github.com/apache/spark/actions/runs/4737365554/jobs/8410097712<br>- https://github.com/apache/spark/actions/runs/4748319019/jobs/8434392414<br>- https://github.com/apache/spark/actions/runs/4759278349/jobs/8458399201<br><br><img width=\"1307\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/233674106-5cf0c4cf-ed4f-4d75-be42-3b7c39dc2936.png\"><br>
@zhenlineo @hvanhovell
The client script still does<br>```<br>CONNECT_CLASSPATH=\"$(build/sbt \"${SCALA_ARG}\" -DcopyDependencies=false \"export connect-client-jvm/fullClasspath\" | grep jar | tail -n1)\"<br>SQL_CLASSPATH=\"$(build/sbt \"${SCALA_ARG}\" -DcopyDependencies=false \"export sql/fullClasspath\" | grep jar | tail -n1)\"<br>```<br>and these sbt commands still take almost a minute (20-30s each) before it drops me into the client shell...<br>I wonder if something is wrong with my setup.
Updated PR description.
Also added possibility to provide saved classpath without waiting 1 minute for sbt to compute it.
Hm .. this only got into the master branch but the original fix wend down to branch-3.4. (https://github.com/apache/spark/pull/40257). Would probably need a separate JIRA in such case next time ;-).
I think it's fine in master only. It's dev scripts and probably majority of dev will be in master moving forward.
Yeah, merging it to master only is fine. The point I was making was to have a different JIRA so we know which version the fix has landed to (so as to make it easier when we revert, etc.)
cc @cloud-fan @dongjoon-hyun @beliefer
also cc @yaooqinn
Let me also 【check】 `NoSuchIndexException`, `IndexAlreadyExistsException`. It may be a bit harder to restore those.
cc @dongjoon-hyun @viirya @huaxingao @gengliangwang @cloud-fan @gatorsmile @sunchao
Just curious. Why wasn't this caught by MiMa?
@hvanhovell, I think because the Catalyst package is excluded in MiMa 【check】s.<br><br>```<br>ProblemFilters.exclude[Problem](\"org.apache.spark.sql.catalyst.*\"),<br>```<br><br>Exceptions are in `org.apache.spark.sql.catalyst.【analysis】`.
I'll 【check】 【test】 failures in a bit.
【Thanks】 for 【review】ing, @gengliangwang!
let me just directly revert.
cc @dongjoon-hyun @cloud-fan thanks
@HyukjinKwon @amaliujia @hvanhovell Hi, I add support for python. PTAL again. 【Thanks】!
@HyukjinKwon @hvanhovell Hi, kindly ask, can merge this PR now?
kindly ping @HyukjinKwon
I am fine. but I would defer to @hvanhovell to merge.
kindly ping @hvanhovell
Hi, can we merge this PR now? @HyukjinKwon @hvanhovell @xinrong-meng 😁
【Thanks】! @HyukjinKwon @amaliujia @hvanhovell @grundprinzip @xinrong-meng
cc @cloud-fan
@dongjoon-hyun Both branch-2.x and branch-3.x have this issue.
3.2 will be EOL soon, shall we backport to 3.3+?
or shall we include this fix in 3.2 as it's the last chance? I don't have a strong opinion. @dongjoon-hyun can you make a call here since you are the release manager of 3.2.4?
I'd prefer to consider this for `branch-3.3`+ because this was not a regression at 3.2, @cloud-fan ~
BTW, are you going to block Apache Spark 3.4 RC7, @cloud-fan ?<br><br>cc @xinrong-meng , too
Since it's not a regression, we don't need to block 3.4 either.
Ya, I think so too~
This patch can wait for Apache Spark 3.4.1 and 3.3.3.
cc @dongjoon-hyun @huaxingao @viirya to take a look. 【Thanks】 for 【contribution】.
adding @rangadi as well, as they seem to have been a recent committer on a lot of the protobuf parsing logic!
FYI: I sent another 【improvement】 for Protobuf here: https://github.com/apache/spark/pull/40983
@rangadi thanks for the comments! i've updated the pr quite a bit, namely i've split it up into two commits:<br>- commit 1: add 【test】s that show the current state<br>- commit 2: add flag + modify 【test】s to show what changed<br><br>and i addressed many of your comments as well along the way 🙏
【Thanks】 a bunch @pang-wu and @rangadi for the thoughtful 【discussion】, really appreciate all the feedback and input 🙏
okay @rangadi @pang-wu (and cc @viirya)  i think i've addressed all comments; i've referenced the external libraries and also removed the mentions of \"ambiguity\" in the 【documentation】. i've also renamed the option `emit.default.【value】s` which i think is clearer than `materialize.default.【value】s`. would y'all be able to take another look?
@justaparth thanks for the PR. Please ping @gengliangwang to merge this once the 【test】s pass (or @viirya if he can merge).
thanks @rangadi!<br><br>cc @viirya or @gengliangwang , the 【test】s have passed! do you mind taking a look and helping us merge this? thanks a bunch!
@HeartSaVioR could you merge this? You don't need to 【review】.
QQ: When does description actually become `null` ? (other than synthetic cases like 【test】s)
> QQ: When does description actually become `null` ? (other than synthetic cases like 【test】s)<br><br>I don't know when filename would be null. My guess would be compiler removed filename info or different JVM implementation. I just saw this in real event log. Doing defensive null handling is no harm here.
Filename can be null, my query was about description :-)
> Filename can be null, my query was about description :-)<br><br>What do you mean by \"description\" here?
> BTW, according to JIRA, is this a regression at Apache Spark 3.3.2, @warrenzhu25 ?<br><br>I don't think so.
Do you happen to know when this bug starts, @warrenzhu25 ?
> Do you happen to know when this bug starts, @warrenzhu25 ?<br><br>Sorry, I have no idea. It's 1st time I have seen this.
Thank you for your answers, @warrenzhu25 .
@dongjoon-hyun @mridulm Do you have more comments on this?
Looks like my comments were not addressed ?
> Looks like my comments were not addressed ?<br><br>Sorry, forget to push. Updated.
+CC @dongjoon-hyun for 【review】, since you took a look at this PR before
Merging to master.<br>【Thanks】 for fixing this @warrenzhu25 <br>【Thanks】 for the 【review】 @HyukjinKwon, @srowen :-)
cc @imback82, @cloud-fan , @viirya , @sunchao
> We may need adding 【test】 case.<br><br>yeah , i will add UT later
One more question , it time to make the default 【value】 of `SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED`  as true ?
Maybe, no? If this is not working properly before, we cannot enable this configuration at Apache Spark 3.5.0. Since we need to wait for one release cycle, we may be able to do that at Apache Spark 3.6.0 if we want.<br>> One more question , it time to make the default 【value】 of `SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED` as true ?<br><br>
> Maybe, no? If this is not working properly before, we cannot enable this configuration at Apache Spark 3.5.0. Since we need to wait for one release cycle, we may be able to do that at Apache Spark 3.6.0 if we want.<br>> <br>> > One more question , it time to make the default 【value】 of `SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED` as true ?<br><br>Yes, this is the more logical way.
The CI build failure doesn't seem to be caused by this patch, can you take a look?<br><br>@dongjoon-hyun @viirya
Please rebase to the `master` branch once more, @zzzzming95 .<br>> The CI build failure doesn't seem to be caused by this patch, can you take a look?
@cloud-fan @dongjoon-hyun @viirya  Please merge to master . 【Thanks】 ~
To be clear, this PR didn't get any approval yet, @zzzzming95 .<br>> Please merge to master . 【Thanks】 ~
I was the OP of the issue in the jira.<br><br>Thank you for the fix, but I discovered a weird behavior when hints applied and I don't know how to interpret it. Please 【check】 [SPARK-43326](https://issues.apache.org/jira/browse/SPARK-43326) I filled
> I was the OP of the issue in the jira.<br>> <br>> Thank you for the fix, but I discovered a weird behavior when hints applied and I don't know how to interpret it. Please 【check】 [SPARK-43326](https://issues.apache.org/jira/browse/SPARK-43326) I filled<br><br>Okay, I will follow up on this issue
cc: @HyukjinKwon please merge.
This happens on a benchmark job generating a large number of very tiny blocks. When the job is finished, the cluster tries to shutdown the idle executors and migrate all the blocks to other active executors, the driver acts like hanging, then resumed after a while.
You are right about the null -> Int, should have 【check】ed better :-)
Since this is `Performance` PR, do you think you can contribute a micro-benchmark which is similar to your case? Maybe, as an independent JIRA issue?<br>> This happens on a benchmark job generating a large number of very tiny blocks. When the job is finished, the cluster tries to shutdown the idle executors and migrate all the blocks to other active executors, the driver acts like hanging, then resumed after a while.<br><br>
@dongjoon-hyun I created https://issues.apache.org/jira/browse/SPARK-43515 as a followup task to add a micro-benchmark.
Merged to master/3.4, thanks all!
Also, +1 for backporting decision of @jiangxb1987 on this 【improvement】 PR.
CC @rangadi @pengzhon-db
Hi @HyukjinKwon could you please take another look? 【Thanks】!
@HyukjinKwon can you merge this? Thank you!
Just FYI, vanilla PySpark's DataFrame.toPandas also has this issue https://issues.apache.org/jira/browse/SPARK-41971<br>Is it possible to move the 【c<font color=blue>hang】es】 to `ArrowUtils` to fix them all?
> Just FYI, vanilla PySpark's DataFrame.toPandas also has this issue [issues.apache.org/jira/browse/SPARK-41971](https://issues.apache.org/jira/browse/SPARK-41971)<br>Is it possible to move the 【c<font color=blue>hang】es】 to ArrowUtils to fix them all?<br><br>Yes, I'm aware of the issue, but let me hold on it to the following PRs.<br>(【Thanks】 for filing the ticket, btw. 😄 )<br><br>TL;DR<br><br>Actually this PR still has an issue with `toPandas`.<br><br>```py<br>>>> spark.sql(\"【value】s (1, struct(1 as a, 2 as a)) as t(x, y)\").toPandas()<br>   x                     y<br>0  1  {'a_0': 1, 'a_1': 2}<br>```<br><br>The duplicated fields have suffix `_1`, `_2`, and so on.<br><br>Also, handling struct type in `toPandas` was not well-defined and there are behavior difference even between Arrow enabled/disabled in PySpark.<br><br>```py<br>>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)<br>>>> spark.sql(\"【value】s (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()<br>   x       y<br>0  1  (1, 2)<br>>>> spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)<br>>>> spark.sql(\"【value】s (1, struct(1 as a, 2 as b)) as t(x, y)\").toPandas()<br>   x                 y<br>0  1  {'a': 1, 'b': 2}<br>```<br><br>Currently PySpark with Arrow enabled, and Spark Connect, use a map for the struct type object as a result, whereas `Row` object in PySpark without Arrow.<br><br>The options are:<br><br>1. It's ok to be different, also with suffix.<br>    - In this case, the suffix is a must because a map object will hold only one 【value】 for the duplicates.<br>2. `Row` object should be used for the struct.<br>    - In this case, we will lose the benefit of Arrow -> pandas 【fast】 conversion.
We also need to change `TorchDistributor._run_local_training` implementation.<br><br>The existing code it executes pytorch code in client side, but in spark connect case, we should execute pytorch code in server side (we can reuse _run_【distributed】_training code in the case, but local mode spark job does not support GPU schdeduling, instead, we can broadcast the selected driver GPU list to all tasks and each task select its GPU id via task rank).
On second thought,<br><br>I propose to make TorchDistributor._run_local_training only supports spark legacy mode,<br><br>but for ` TorchDistributor._run_【distributed】_training` , we should make it support both legacy mode and spark connect mode, i.e., when running on spark local mode cluster, but user set `TorchDistributor.local_mode=False`, it executes `TorchDistributor._run_【distributed】_training`, in this case, current master code does not handle GPU allocation correctly, you need to fix it (we can broadcast the selected driver GPU list to all tasks and each task select its GPU id via task rank).
> The existing code it executes pytorch code in client side, but in spark connect case, we should execute pytorch code in server side<br><br>yes, I feel it is non-trivial to execute pytorch code in server side, since we need to launch a new Python process in the server side and then communicate with it.<br><br>> I propose to make TorchDistributor._run_local_training only supports spark legacy mode<br><br>agree<br><br>> running on spark local mode cluster, but user set TorchDistributor.local_mode=False<br><br>why not just failing it?<br><br>
> running on spark local mode cluster, but user set TorchDistributor.local_mode=False<br><br>I think we need to support this, because for spark ML algorithm implemented atop TorchDistributor, we hope to support either spark local mode or spark cluster mode.
> yes, I feel it is non-trivial to execute pytorch code in server side, since we need to launch a new Python process in the server side and then communicate with it.<br><br>I think it does not require too much work, we can reuse most code of `TorchDistributor._run_【distributed】_training`, we just need to fix one issue:<br>current master code does not handle GPU allocation correctly, we can broadcast the selected driver GPU list to all tasks and each task select its GPU id via task rank.
Summary: for spark connect mode:<br><br>If torchDistributor.local_mode is True, raise error saying no support.<br><br>If torchDistributor.local_mode is False, and spark server side is spark local mode, we need to fix the issue https://github.com/apache/spark/pull/40695#issuecomment-1501315354 <br><br>If torchDistributor.local_mode is False, and spark server side is spark cluster mode, current master code works fine either with GPU or without GPU config.
What is local mode and why would you not support it on the client?
> What is local mode<br><br>Let me clarify it,<br><br>TorchDistributer has \"local mode\" configure, if true, it just run torch program in client side, if False, it launches a spark job to run the torch program. So the TorchDistributer side \"local mode\" has nothing to do with spark master local mode.<br><br><br>> why would you not support it on the client?<br><br>I assume you mean to run torch program in spark connect client machine, we can support this, but I think it is less meaningful, because client machine should usually run lightweight workloads, but torch programs are heavy workloads and they often requires GPU, which client machine are hard to satisfy the condition.<br><br><br>
> I assume you mean to run torch program in spark connect client machine, we can support this, but I think it is less meaningful, because client machine should usually run lightweight workloads, but torch programs are heavy workloads and they often requires GPU, which client machine are hard to satisfy the condition.<br><br>I don't think this is a valid assumption. With Spark Connect you can actually build an environment in which you have GPU locally but don't have a GPU on your cluster. In this case you still want to leverage the same execution flow. I've previously talked to users that were looking for an EC2 setup with a GPU attached and running the workloads from there against Sprk using Spark Connect. <br><br>This is very similar to running sklearn locally on the client side.<br><br>It's not the only way, but it's a very valid way.
> I don't think this is a valid assumption. With Spark Connect you can actually build an environment in which you have GPU locally but don't have a GPU on your cluster. In this case you still want to leverage the same execution flow. I've previously talked to users that were looking for an EC2 setup with a GPU attached and running the workloads from there against Sprk using Spark Connect.<br><br><br>OK make sense, we can support it too @zhengruifeng , in this case, we can read client side environment variable `CUDA_VISIBLE_DEVICES` to determine which GPU devices we can use.
@grundprinzip would you mind taking another look at the 【c<font color=blue>hang】es】 in protos?
@WeichenXu123 mind taking another look?
@HeartSaVioR - PTAL. 【Thanks】
CI fails with odd errors.<br>@anishshri-db Could you please rebase so that CI is retriggered? If the new trial fails again, maybe good to post to dev@ and see whether someone encountered this before, and/or someone is willing to volunteer to fix if that's not a transient issue.
>  Could you please rebase so that CI is retriggered? If the new trial fails again, maybe good to post to dev@ and see whether someone encountered this before, and/or someone is willing to volunteer to fix if that's not a transient issue.<br><br>Seems like a known issue someone already posted to dev channel - https://github.com/sbt/sbt/issues/7202
@HeartSaVioR - all 【test】s passed. Please merge when you get a chance. Thx
Missed the notification. 【Thanks】! Merging to master.
cc @viirya @dongjoon-hyun @yaooqinn @beliefer
Thank you for pinging me, @cloud-fan .
@dongjoon-hyun This is a bit of a low-level change and ideally we should run all end-to-end 【test】s twice with the config on and off. However, it seems not worthwhile to double the 【test】 resource for this change. How about we turn it on by default after we have enough SQL operator coverage?
Got it. +1 for the 【test】ing plan.
@cloud-fan Thank you for ping me. What is the 【performance】 before or after this PR?
I'm just curious about the prefix Task for Evaluator. Is it more 【specific】 to Partition, Split for SQL/DataFrame, or RDD? `Task` more likely belongs to the scheduler. Before 【review】ing the PR description and implementation, I thought it was to evaluate the cost of task execution or something. 【:)】
@beliefer This is not a 【performance】 【feature】. It's just to avoid people making mistakes referencing extra objects in the closure, which can slow down task serialization and increase query latency.<br><br>@yaooqinn I don't have a strong opinion. how about PartitionEvaluator?
`PartitionEvaluator` looks better to me, altho I don't have a strong option either.
> @beliefer This is not a 【performance】 【feature】. It's just to avoid people making mistakes referencing extra objects in the closure, which can slow down task serialization and increase query latency.<br><br>Got it.<br>
thanks for the 【review】, merging to master!
Thank you, @cloud-fan and all!
@Yikf Can you re-trigger GA?
The change 【LGTM】. Can we also 【check】 other systems like Preso/Trino/Impala and see how they display null 【value】s?<br><br>cc @HyukjinKwon @viirya
> The change 【LGTM】. Can we also 【check】 other systems like Preso/Trino/Impala and see how they display null 【value】s?<br><br>@Yikf Can you help 【check】  Preso/Trino ? Also 【check】 PostgreSQL?<br><br>
> The change 【LGTM】. Can we also 【check】 other systems like Preso/Trino/Impala and see how they display null 【value】s?<br>> <br>> cc @HyukjinKwon @viirya<br><br><br>> > The change 【LGTM】. Can we also 【check】 other systems like Preso/Trino/Impala and see how they display null 【value】s?<br>> <br>> @Yikf Can you help 【check】 Preso/Trino ? Also 【check】 PostgreSQL?<br><br>Sure, As following,<br>**Trino**<br>```shell<br>trino> select ARRAY[1, null];<br>   _col0<br>-----------<br> [1, NULL]<br>(1 row)<br><br>Query 20230412_023104_00007_unnu6, FINISHED, 1 node<br>Splits: 1 total, 1 done (100.00%)<br>0.03 [0 rows, 0B] [0 rows/s, 0B/s]<br><br>trino> select null;<br> _col0<br>-------<br> NULL<br>(1 row)<br><br>Query 20230412_023108_00008_unnu6, FINISHED, 1 node<br>Splits: 1 total, 1 done (100.00%)<br>0.07 [0 rows, 0B] [0 rows/s, 0B/s]<br>```<br>**PostgreSQL**<br>```shell<br>postgres=# select array[1, null];<br>  array<br>----------<br> {1,NULL}<br>(1 row)<br>```
Actually, this is a breaking change for CAST expression. It affects the string representation of complex types and not just `df.show` which could cause issues when comparing the string representation of a row:<br><br>```sql<br>select cast(struct(null) as string) as c0, null as c1<br>```<br><br>```<br>-- df.show()<br>+------+----+<br>|    c0|  c1|<br>+------+----+<br>|{NULL}|NULL|<br>+------+----+<br><br>-- df.collect.toList<br>List([{NULL},null])<br>```<br><br>It also affects CSV writes:<br>```scala<br>val df = spark.sql(<br>  \"\"\"<br>  select cast(array(1,2) as string) as c0, 1 as c1<br>  union all<br>  select cast(array(3,null) as string) as c0, null as c1<br>  \"\"\"<br>).coalesce(1)<br>df.write.csv(\"file:/tmp/ivan_【test】.csv\")<br>```<br><br>Before this PR:<br>```<br>\"[1, 2]\1<br>\"[3, null]\<br>```<br><br>After this PR:<br>```<br>\"[1, 2]\1<br>\"[3, NULL]\<br>```<br><br>And there is no option to re-enable null format.<br><br>On a broader side of things: It would be good to clarify why spark-sql shell is expected to be visually compatible with spark-shell, those are two different REPLs with their own ways of displaying data. For example, we don't display a table the same way in those REPLs.
@sadikovi CSV does not accept struct/array/map columns, your example generates a string column and writes it to CSV. That said, any behavior change of that \"string generation\" function will change the 【value】 we write to CSV, but I wouldn't call it a CSV behavior change.<br><br>It's intentional that the cast struct/array/map to string behavior is changed w.r.t. nulls. If third-party libraries rely on this behavior, then we need to revisit this PR.<br><br>BTW, I think PR description should not say it's for consistency with spark-sql shell. `df.show` displays top-level null 【value】 as `NULL`, but inner field null 【value】 as `null`. This is the major motivation of the fix.
I just tried without the change and df.show() seems to return correct results: <br>```scala<br>spark.sql(<br>  \"\"\"<br>  select struct('a'), map('a', 1), array(1, 2), 'a'<br>  union all<br>  select struct(null), map('a', null), array(null, null), null <br>  \"\"\").show()<br>```<br><br>```<br>+---------+-----------+------------+----+<br>|struct(a)|  map(a, 1)| array(1, 2)|   a|<br>+---------+-----------+------------+----+<br>|      {a}|   {a -> 1}|      [1, 2]|   a|<br>|   {null}|{a -> null}|[null, null]|null|<br>+---------+-----------+------------+----+<br>```<br><br>Could you provide an example of inconsistency in df.show() that this PR addresses?<br><br>The reason I brought the CSV example is because that is typically how users would write structs/arrays/maps to a CSV file, they would transform that to a string, potentially via CAST. That is why I think this is a behaviour change in writing a CSV file.
> BTW, I think PR description should not say it's for consistency with spark-sql shell. `df.show` displays top-level null 【value】 as `NULL`, but inner field null 【value】 as `null`. This is the major motivation of the fix.<br><br>Actually, previously, the top-level is consistent with the inner null display with `null`. This PR mainly thinks that the mainstream database uses `NULL` instead of `null` when displaying null, and `NULL` is a better nice string representation.<br><br>If it's affecting something else, I think we should look at it again, sorry ~ : )<br><br><br>> For example, we don't display a table the same way in those REPLs.<br><br>BTW, For apache spark, spark-sql and spark-shell are two different REPLs, but they are only user interaction interfaces. Shouldn't the displayed content be the same? At least in terms of the result set.
> which could cause issues when comparing the string representation of a row:<br><br>Sorry, I'm not sure what that means, does it mean compared to what was written before this PR? Any problems if cast(null as string) with NULL, I see that trino is also NULL<br>``` SQL<br>trino> select cast(null as varchar) as c0, null as c1;<br>  c0  |  c1<br>------+------<br> NULL | NULL<br>(1 row)<br>```
@sadikovi Sorry I was wrong, `Dataset.show` also explicitly prints null as `null` before this PR.<br><br>Being consistent with other databases is good, but if people rely on the cast behavior to write struct/array/map to CSV, then it's more important to not break it.<br><br>Shall we revert this PR then? We can change spark-sql shell to print `null` to be consistent with `df.show` instead.
+1 for the revert.<br><br>BTW, Should spark-sql keep consistent with df.show, this also seems to break a lot of things, since spark-sql previously used hiveResultString, not only null display.
If people programmatically parse the output of spark-sql shell, then we shouldn't change it. If it's only for display and consumed by humans, I think it's OK to change.
> If people programmatically parse the output of spark-sql shell<br><br>I want to keep it the way it is. Look at what other people think.
To be honest, I don't understand why spark-sql shell is expected to be consistent with spark-shell or pyspark shell. Can someone elaborate? I can see making spark-sql shell consistent with Presto/Trino/MySQL/Postgres, etc. but I don't understand why Scala REPL should be visually consistent with SQL terminal in terms of displaying results - they serve different purposes.<br><br>If you would like to have a consistent visual behaviour for NULLs/nulls, it is also fine just as long as it does not break other 【feature】s like Cast or `collect.toString`. Maybe we could simply add a conversion method to display 【value】s in a DataFrame in whatever format we need when calling `.show` instead of changing Cast. In fact, we can refactor it into a separate class and reuse it in spark-sql and spark-shell. <br><br>
@maropu Could you help to 【review】 this pr? 【Thanks】
Mind retriggering https://github.com/caican00/spark/runs/12587259773?
https://github.com/apache/spark/pull/40437 might be related. We want to remove `hiveResultString` from CLI and only use it in hive 【compatibility】 【test】s.
【Thanks】 for 【review】ing and merging!
@cloud-fan @HyukjinKwon @gatorsmile @srielau @entong @hvanhovell Could you 【review】 this PR when you have time, please.
This is introduced from 3.4 hence ideal to land the fix to 3.4, but the possibility to trigger the bug is relatively very low, hence probably not urgent.
I'll just merge this since the fix is super straightforward and CI passed.
> This is introduced from 3.4 hence ideal to land the fix to 3.4, but the possibility to trigger the bug is relatively very low, hence probably not urgent.<br><br>So this is not a blocker released in 3.4, is it?<br><br>
No blocker for current 3.4.0 release.
cc @Ngone51 @jiangxb1987 too FYI
According to the two ideas provided by @cloud-fan  on how to differentiate user-facing errors and user-triggered errors ([have a special prefix] or [create a base trait for transient errors]), in the implementation process, I think having a special prefix may be a more good idea.<br>I defined a new `SparkThrowable#isTransientError` method with reference to `SparkThrowable#isInternalError`, and decided whether to skip the retry logic based on the return 【value】 of the `SparkThrowable#isTransientError`.<br>```scala<br>  def isInternalError(errorClass: String): Boolean = {<br>    errorClass == \"INTERNAL_ERROR\"<br>  }<br><br>  def isTransientError(errorClass: String): Boolean = {<br>    errorClass.startsWith(\"TRANSIENT\")<br>  }<br>```<br><br>can you re-【review】 the code when you are free, and make some comments. @cloud-fan @aokolnychyi
OK, I'm on the fence now. On one hand, the number of transient errors should be much smaller than the number of user-triggered errors, so it's better to find out these transient errors and mark them. On the other hand, not retrying the task can be a regression that leads to job failure, so we should make sure we only skip task retry when the error is definitely user-triggered.<br><br>To be conservative, now I'm leaning towards picking some errors and marking them as \"can skip task retry\". I like the idea from @aokolnychyi that we can add a JSON field for it.
I would be on the conservative side and skip retry when we are absolutely certain that a retry will not help.<br>SerDe failures, for example, are good candidates (which is already handled), and similar ... note that in non deterministic tasks, a retry can succeed which earlier failed with a user exception
+1 for being safe
> OK, I'm on the fence now. On one hand, the number of transient errors should be much smaller than the number of user-triggered errors, so it's better to find out these transient errors and mark them. On the other hand, not retrying the task can be a regression that leads to job failure, so we should make sure we only skip task retry when the error is definitely user-triggered.<br>> <br>> To be conservative, now I'm leaning towards picking some errors and marking them as \"can skip task retry\". I like the idea from @aokolnychyi that we can add a JSON field for it.<br><br>I'm trying to change the code now
> I would be on the conservative side and skip retry when we are absolutely certain that a retry will not help. SerDe failures, for example, are good candidates (which is already handled), and similar ... note that in non deterministic tasks, a retry can succeed which earlier failed with a user exception<br><br>I see, I'm trying to change the code now
I have modified the code, can you re-【review】 the code when you are free, and make some comments. @cloud-fan @aokolnychyi @mridulm
According to the suggestions provided by @cloud-fan @aokolnychyi .I modified the code.<br>I added the `isTransient ` attribute to some `error_classes` <br>such as:<br>-  `AMBIGUOUS_LATERAL_COLUMN_ALIAS`<br>- `CANNOT_PARSE_DECIMAL`<br>- `DATATYPE_MISMATCH` <br>- `DIVIDE_BY_ZERO`<br>- `_LEGACY_ERROR_TEMP_3043(npe)`<br>```java<br>  \"CANNOT_PARSE_DECIMAL\" : {<br>    \"message\" : [<br>      \"Cannot parse decimal.\"<br>    ],<br>    \"sqlState\" : \"22018\<br>    \"isTransient\" : false<br>  },<br>```<br><br>When these errors occur, the retry logic is skipped.<br>```java<br>   if (!ef.isTransient) {<br>        // if the exception has an error class which means a non-transient error, not retry<br>        logError(s\"$task has a non-transient exception: ${ef.description}; not retrying\")<br>        sched.dagScheduler.taskEnded(tasks(index), reason, null, accumUpdates, metricPeaks, info)<br>        abort(s\"$task has a non-transient exception: ${ef.description}\ ef.exception)<br>        return<br>    }<br>```<br>hope you leave some comments in your free time. @cloud-fan @aokolnychyi @mridulm, thanks a lot.<br><br><br><br>
useful 【feature】, any updates here?
> useful 【feature】, any updates here?<br><br>I modified the code according to the suggestions provided by @cloud-fan @aokolnychyi @mridulm , next step may be to seek help from people who worked on the error framework. Can you give some suggestions on the next work?
Could you 【review】 this, @viirya ?<br><br>Although the build system seems to be recovering now, I want to reduce the chance of failures in the future by switching the repo.<br>- https://github.com/apache/spark/commits/master<br><br>```<br>- addSbtPlugin(\"com.typesafe.sbteclipse\" % \"sbteclipse-plugin\" % \"5.2.4\")<br>+ addSbtPlugin(\"com.github.sbt\" % \"sbt-eclipse\" % \"6.0.0\")<br>```
> This PR aims to use set-eclipse instead of sbteclipse-plugin.<br><br>One typo `set-eclipse` in the description.
Thank you, @viirya . The description is 【<font color=blue>fix】ed】 now.
I 【test】ed this manually. Merged to master/3.4/3.3/3.2.
Yes, correctly. Apache Spark 3.2.0+ uses SBT 1.5.0+ via SPARK-34959.
Documentation generation GitHub Action job passed.<br><br>![Screenshot 2023-04-07 at 3 56 39 PM](https://user-images.githubusercontent.com/9700541/230689883-597b8e03-b929-4c68-88a3-025358192793.png)<br>
Could you 【review】 this PR, @huaxingao ?
Thank you so much!
Merged to master for Apache Spark 3.5. Thank you, @huaxingao and @amaliujia
I will come up with screenshots from branch-3.4.<br>The markdown tables in the master branch are not showing properly. cc @grundprinzip <br><img width=\"919\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/230692781-af14c6a9-b458-4c21-826c-9851868c830d.png\"><br>
cc @xinrong-meng it would be 【great】 to include this in the doc of Spark 3.4.0. <br>(Document 【c<font color=blue>hang】es】 won't fail RC vote)
【Thanks】 @grundprinzip for the 【review】s, merged into master
cc @dongjoon-hyun @HeartSaVioR FYI
【Thanks】 @HeartSaVioR @dongjoon-hyun
@LuciferYang @wangyum @frankliee Since parquet-mr has released 1.13.0, So I resubmit the PR. The original PR is https://github.com/apache/spark/pull/40646
@frankliee Sorry for delay. The PR only supports AVX512, does not support AVX256.<br>Your question \"Do we need to create SparkContext in static code ?\"  because I want to get the SQL configuration sql.parquet.vector512.read.enabled
@LuciferYang @wangyum @frankliee I have added a benchmark.<br><br>This is the result:<br>```<br>Java HotSpot(TM) 64-Bit Server VM 17.0.5+9-LTS-191 on Linux 5.15.0-60-generic<br>Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz<br>Selection:                                Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>------------------------------------------------------------------------------------------------------------------------<br>Without Java Vector API                            4696           4802          89         21.3          47.0       1.0X<br>With Java Vector API                               3742           3927         230         26.7          37.4       1.3X<br>```
Please wait for me to 【check】 and update the benchmark results of `ZStandardBenchmark`<br><br>
> New results look reasonable.<br><br>I have been in a team meeting this morning.<br><br>It seems that the results of `ZStandardBenchmark` are somewhat related to the CPU model.
The CPU model used in micro-bench is the same as before, and the results have not changed much now<br><br>
cc @HyukjinKwon @bjornjorgensen FYI<br><br>We don't need to install `grpcio` for pandas API on Spark with regular session after this PR.
CI passed. @HyukjinKwon Could you take a look when you find some time??
@HyukjinKwon Test passed. There is anything else we should address here?
@itholic Thank you. Great work.
@itholic can you double 【check】 this https://github.com/apache/spark/pull/40722#【discussion】_r1170976266?
> Could you file a JIRA for this, @LuciferYang ? This 【contribution】 looks enough to have a JIRA issue.<br><br>@dongjoon-hyun thanks for your suggestion ~ created SPARK-43090
what if there are two input datasets, one for training and one for validation?
> what if there are two input datasets, one for training and one for validation?<br><br>We can add a \"is_validation\" boolean column to mark it is for training or for validation.
@mengxr raises another suggestion: uses petastorm to load data from DBFS / HDFS /.. .(so that it can make torch distributor has a 【simple】r interfaces). But there’s a shortcoming that it is low performant for sparse vector 【feature】s. We haven’t made final decision yet.
> @mengxr raises another suggestion: uses petastorm to load data from DBFS / HDFS /.. .(so that it can make torch distributor has a 【simple】r interfaces). But there’s a shortcoming that it is low performant for sparse vector 【feature】s. We haven’t made final decision yet.<br><br>Finally, after offline 【discussion】, we decided to adopt the approach of this PR, because this PR approach has significant benefits including:<br><br> - It does not need to dump training dataset to 【distributed】 file system but just saving partition data in local disk, which is much 【fast】er.<br> - If we uses petastorm or pytorch parquet / arrow loader, we have to densify sparse 【feature】 input data, it makes data exploded before saving dataset, this causes further 【performance】 deterioration. But current approach in this PR it dumps sparse data to local disk and when loading for training, it densifies the data.<br><br>Btw, we decided to make it a private API because the API currently will be only used by pyspark MLv2 (a new module we plan to add soon) code.<br>
`【test】_data_loader` should also be added to `modules.py`
CI failed because of <br>```<br>Run echo \"APACHE_SPARK_REF=$(git rev-parse HEAD)\" >> $GITHUB_ENV<br>fatal: detected dubious ownership in repository at '/__w/spark/spark'<br>To add an exception for this directory, call:<br><br>\tgit config --global --add safe.directory /__w/spark/spark<br>fatal: detected dubious ownership in repository at '/__w/spark/spark'<br>To add an exception for this directory, call:<br><br>\tgit config --global --add safe.directory /__w/spark/spark<br>Error: Process completed with exit code 128.<br>```
@HyukjinKwon @zhengruifeng Would you please take a look? Thank you!
cc @ueshin FYI
R failure on AppVeyor is irrelevant to this PR.
Could you 【review】 this PR, @viirya ? I verified manually.<br><br>```<br>$ ls -alt<br>total 67688<br>-rw-r--r--@  1 dongjoon  staff      1955 Apr 10 15:27 maven-metadata-local.xml<br>-rw-r--r--@  1 dongjoon  staff       492 Apr 10 15:27 _remote.repositories<br>-rw-r--r--@  1 dongjoon  staff   4451845 Apr 10 15:27 spark-core_2.12-3.5.0-SNAPSHOT-javadoc.jar<br>-rw-r--r--@  1 dongjoon  staff    362584 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-cyclonedx.json<br>-rw-r--r--@  1 dongjoon  staff    312338 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-cyclonedx.xml<br>-rw-r--r--@  1 dongjoon  staff   1423015 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-【test】-sources.jar<br>-rw-r--r--@  1 dongjoon  staff   2919170 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-sources.jar<br>-rw-r--r--@  1 dongjoon  staff   8078262 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT-【test】s.jar<br>-rw-r--r--@  1 dongjoon  staff  14038165 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT.jar<br>-rw-r--r--@  1 dongjoon  staff     44355 Apr 10 15:26 spark-core_2.12-3.5.0-SNAPSHOT.pom<br>drwxr-xr-x@  4 dongjoon  staff       128 Apr 10 14:56 ..<br>drwxr-xr-x@ 12 dongjoon  staff       384 Apr 10 14:56 .<br><br>$ head spark-core_2.12-3.5.0-SNAPSHOT-cyclonedx.xml<br><?xml version=\"1.0\" encoding=\"UTF-8\"?><br><bom serialNumber=\"urn:uuid:14185927-0ca0-4589-92aa-268c9c81873d\" version=\"1\" xmlns=\"http://cyclonedx.org/schema/bom/1.4\"><br>  <metadata><br>    <timestamp>2023-04-10T22:26:52Z</timestamp><br>    <tools><br>      <tool><br>        <vendor>OWASP Foundation</vendor><br>        <name>CycloneDX Maven plugin makeBom compile+provided+runtime+system</name><br>        <version>2.7.6</version><br>        <hashes><br>```
Hm, for some reasons, it shows @LuciferYang as a primary author. I manually changed it to @dongjoon-hyun.
Oh, it was intentional https://github.com/apache/spark/pull/40726#pullrequest【review】-1378012264, but thank you!<br><br>Thank you, @HyukjinKwon and @viirya !
late 【LGTM】 ~ 【Thanks】 @dongjoon-hyun and all ~
this working OK on the la【test】 maven releases that homebrew is pushing out?
To @steveloughran , this is done independently from Maven.<br>I verified in Apache ORC and Spark 【community】 only for now.
Could you 【review】 this PR when you have some time, @huaxingao ?
Thank you, @huaxingao !
I also confirmed the moved `*StateStoreSuite` output in the GitHub Action log on this PR.<br>- https://github.com/dongjoon-hyun/spark/actions/runs/4661381120/jobs/8250624115<br><br>Merged to master/3.4.
cc @xinrong-meng @grundprinzip For the proto change.
CC @amaliujia @vicennial Be free to give any 【review】 feedbacks.
【Thanks】 @zhenlineo , the proto 【c<font color=blue>hang】es】 look nice.
A nit on the PR description, would you please indicate which API is supported under `Does this PR introduce any user-facing change?` section?  That would be helpful for future API auditing. 【Thanks】!
Merging to master.
@dongjoon-hyun @mridulm @Ngone51 Help take a look?
> I understand the intention but there is a chance of in【stability】 due to `OutOfDisk` and sometimes `OutOfMemory`. In addition, bin-packed executors could work slower due to the network traffic congestion. Do you have some production results about the real benefits, @warrenzhu25 ?<br><br>In prod, we have seen 20~50 percent resource save based on executor size, the more saving with more cpu cores per executor. For the issue could be caused by bin-packed executors, it's still possible to happen when all executor cores are occupied. If it indeed happen, the better solution might be large partition num. <br><br>The purpose of change is to provide one more scheduling option, customer still can use the old way.
Please put your observation to the PR description too in order to make it a permanent commit log, @warrenzhu25 .
> Please put your observation to the PR description too in order to make it a permanent commit log, @warrenzhu25 .<br><br>Done
I have been wondering if it's better to allow `spark.task.cpus` to set a floating number like `0.2` so I/O intensive tasks can benefit from that as well as the case like this.
cc @tgravescs too FYI
Overall I'm fine with adding in more scheduling algorithms, we have talked about it before but never done it.  this has been requested beofre like https://issues.apache.org/jira/browse/SPARK-17637<br><br>Ideally perhaps we could make a pluggable scheduling algorithm, but I think that is much more complex, if we make the config more generic like proposed it doesn't exclude it from happening in the future
I'm also curious what do you have locality set to?  What if you set wait=0 as I thought that kind of got you this
there is already a PR doing the same thing: https://github.com/apache/spark/pull/40688
Close this one.
@huaxingao @cloud-fan @dongjoon-hyun @sunchao @HyukjinKwon @viirya @gengliangwang, could you take a look at the approach used in this PR and let me know what you think? If it seems reasonable, I'll add more 【test】s.
@cloud-fan, could you take another look? I have changed the approach and updated the PR description.
This approach 【LGTM】. Can we also 【check】 the SQL UI manually and see if there is any problem?
> Can we also 【check】 the SQL UI manually and see if there is any problem?<br><br>Checking.
There is a difference in terms of what is shown in the UI as the new approach uses nested execution.<br><br>**Prior to this change**<br><br><img width=\"1490\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6235869/231557959-cb44436e-0f34-4393-88ed-90fcc141381c.png\"><br><img width=\"193\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6235869/231558107-32e96225-dcb4-4379-b33d-efc767cdb36d.png\"><br><br>**After this change**<br><br><img width=\"891\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6235869/231558265-1a14ad48-a596-4b63-ba96-d12647bea921.png\"><br><img width=\"270\" alt=\"image\" src=\"https://user-images.githubusercontent.com/6235869/231558323-98f26730-7e8f-4401-ad56-570bf659d91b.png\"><br><br>It seems reasonable as we do use nested execution. What do you think, @cloud-fan?
yea the UI looks good!
Failures seem unrelated. Retrying [here](https://github.com/aokolnychyi/spark/actions/runs/4680956607).
【Thanks】 for 【review】ing, @cloud-fan! I updated the PR. Let me also know if you see any issues regarding [this proposal](https://github.com/apache/spark/pull/40734#【discussion】_r1164418931).
all 【test】s passed, and the pyspark issue is unrelated: https://github.com/aokolnychyi/spark/runs/12709198717<br><br>merging to master, thanks!
【Thanks】, @cloud-fan!
hmm I am not sure about this. I think there are some practice already that add the API but throw unsupported exception.<br><br>so I am not sure if we want those unsupported API are just missing or add those but throws.<br><br>BTW the python side I believe add the APIs but throw.<br><br>cc @hvanhovell
I think the behavior of throwing `UnsupportedOperationException` other than compilation failure is also acceptable
This is a minor issue, let me close this one, thanks @zhengruifeng @amaliujia <br><br>
Also I think we need to add unit 【test】 for this by reusing 【test】s here https://github.com/apache/spark/pull/37894<br><br>You can follow [my PR](https://github.com/apache/spark/pull/40691/files#r1162341092) to <br>1. create a new mixin class that contains all 【test】 cases from the original one but don't extend `ReusedSQLTestCase`, and <br>2. create a new class below with the original name and extend this mixin class and `ReusedSQLTestCase`<br>3. create a parity 【test】 class that extends this mixin class and `ReusedConnectTestCase`<br>
> Also I think we need to add unit 【test】 for this by reusing 【test】s here #37894<br>> <br>> You can follow [my PR](https://github.com/apache/spark/pull/40691/files#r1162341092) to<br>> <br>> 1. create a new mixin class that contains all 【test】 cases from the original one but don't extend `ReusedSQLTestCase`, and<br>> 2. create a new class below with the original name and extend this mixin class and `ReusedSQLTestCase`<br>> 3. create a parity 【test】 class that extends this mixin class and `ReusedConnectTestCase`<br><br>@WweiL I added a 【test】 file 【test】_parity_pandas_grouped_map_with_state.py. However, I had to skip all 【test】s due to spark.streams not supported in connect for now
Seems that there is lint error, you can run `PYTHON_EXECUTABLE=python3.9 ./dev/lint-python` or just `./dev/lint-python` before commit to make sure
@HyukjinKwon can u help merge this?
Hi, after bumping to Maven 3.9.1 in Apache Kyuubi (https://github.com/apache/kyuubi/pull/4647), there are notable increasing failures in Maven builds when resolving Maven dependencies from the central mirror.<br>We are trying to investigate and 【improve】 the situation in https://github.com/apache/kyuubi/pull/4692, by increasing Maven Resolver Transport connection timeout back to original default connection timeout 60000ms.
The property `aether.connector.connectTimeout` seems not working, and there are still maven compilation 【performance】 issues, need more 【investigation】 ...<br><br>
Agree. Same on kyuubi.
Thank you all.<br><br>FYI, @steveloughran . Apache Maven 3.9.1 seems to have issues still according to the above 【discussion】.
<img width=\"1293\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/231329527-32cb53b8-6724-4461-8e26-5411ff81f000.png\"><br><br>The last build took 1 hour and 7 meters, let me re-run GA more times<br><br>
noted. interestingly for cutting hadoop releases i have had to turn off connection pooling so as to guarantee 【reliable】 builds (docker container having to download **/*.jar from maven central is too brittle otherwise). that's even older versions.<br><br>* spark &c with 【<font color=blue>fix】ed】 versions should stay on the good ones<br>* if the la【test】 version of the SBOR is happy on recent maven builds then we can reinstate in hadoop.
@dongjoon-hyun @steveloughran Adding `-Dmaven.resolver.transport=wagon` seems useful, as recent Java 11&17 Maven build have all finished within 1 hour and there have been no more timeouts.  Let me 【check】 for two more days<br><br><br><br><br><br>
I mean, is it worth updating if 3.9.1 introduces small issues that we have to work around? or just wait?
Long time no see @srowen , thanks for your 【review】 ~<br><br>I am not sure if the native http client will change the default timeout 【value】 to be same with wagon in the future. But I don't object to letting us wait and see if there will be any 【c<font color=blue>hang】es】 of the new version of Apache Maven 【:)】
@steveloughran Do you have any other questions? I am planning to close this PR and 【test】 Apache Maven 3.9.2+ in the future to 【check】 if it is possible not to add `-Dmaven.resolver.transport=wagon`. Apache Spark can continue to use 3.8.7 now.<br><br><br><br>
me, I'm happy with whatever suits, just noting that 3.9.1 was unwelcome trouble for me as it ended up hurting the 3.3.5 release process -not the docker builds, which were still on a lower version, but my local dev builds blew up. joy
【Thanks】 for your attention to this pr @bowenliang123 @dongjoon-hyun @srowen @steveloughran, close first ~<br>
【Thanks】 for the ping. And feel free to cc me when Spark has the next attempt to use maven 3.9.x/4.x.<br>I was pessimistic that whether Maven will change the default timeout as it's a default config (`aether.connector.connectTimeout=10000`) of Maven Artifact Resolver not in Maven itself. And as for Maven, it has changed to Artifact Resolver as the default resolver already, which fewer 【improvement】s we could expect from that. And we are also difficult to reduce possible timeout to maven central mirrors.
Sorry for missing pings, @LuciferYang . I'm back from my vacation.<br>I'm also +1 for the decision (waiting for new better version). Thank you for closing.
Hi guys, I want implement support sql with columns first, to avoid [sql with dataframes problem](https://github.com/apache/spark/pull/40741#【discussion】_r1163580352) before have better solution. If ok for you please let me know. 【Thanks】.
qq:<br><br>> because the unwrapped expression contains a UDF<br><br>Where is the UDF, and how it's related to UDF?
@HyukjinKwon The `EqualNullSafe(Cast(ts, DateType), date)` shoud rewrite to `If(IsNull(ts), FalseLiteral, And(GreaterThanOrEqual(ts, Cast(date, TimestampType)), LessThan(ts, Cast(date + 1, TimestampType))))`.  It contains `If`.  So there is no benefit if rewrite `EqualNullSafe(Cast(ts, DateType), date)`.
@wangyum I think we should mention two things:<br>1. the current rewrite is incorrect for EqualNullSafe<br>2. even if we fix the rewrite, it will contain `If` and can't be pushed down to the data source.
shall we still rewrite `EqualNullSafe` if column is not nullable?
This PR is WIP as it contains https://github.com/apache/spark/pull/40856. Once that PR is merged I will rebase and remove the WIP flag.
https://github.com/apache/spark/pull/40856 got merged and I've rebased this PR. I'm removing the WIP flag and the PR is ready for 【review】.<br><br>cc @cloud-fan, @wangyum, @maryannxue, @sigmod
~I would give the ssh repl a try too. Maybe starting the repl this way do not need the tyy?<br>https://github.com/com-lihaoyi/Ammonite/issues/276#issuecomment-267302917<br>Also see http://ammonite.io/api/sshd/ammonite/sshd/SshdRepl.html~
@alexjinghn there is a JAVA17 job on the OSS and it seems working<br>https://github.com/apache/spark/blob/master/.github/workflows/build_java17.yml<br>https://github.com/apache/spark/actions/workflows/build_java17.yml<br><br>How do you reproduce the issue?
I can reproduce this on java17 with spark-shell locally. Can we add a 【test】 for anonymous UDFs?
> I can reproduce this on java17 with spark-shell locally. Can we add a 【test】 for anonymous UDFs?<br><br>+1 for add a 【test】 for anonymous UDFs
I think it works probably we build with JDK 8 and run with JDK 17. I guess this is only reproducible when it's both built and runs with JDK 17.
>  I guess this is only reproducible when it's both built and runs with JDK 17.<br><br>my case was built on 8 but run on 17
@yaooqinn @LuciferYang 【Thanks】 for the suggestion. I've added a 【test】 and verified that the 【test】 captures the failure (fails w/o my change, passes w/ my change).<br><br>@HyukjinKwon this should be able to be reproduced as long as it runs on JDK15+ since the root cause is the classloader API behavior change.
cc @mridulm @thejdeep
also cc @grundprinzip
ok, on second thought, I think we should narrow this PR to abbreviation only.<br><br>I think we can support redaction as followings in the future:<br><br>```<br>{<br>...<br>      case (field: FieldDescriptor, relation: proto.LocalRelation)<br>          if field.getJavaType == FieldDescriptor.JavaType.MESSAGE && relation != null =><br>        builder.setField(field, redactLocalRelation(relation))<br><br>      case (field: FieldDescriptor, msg: Message)<br>          if field.getJavaType == FieldDescriptor.JavaType.MESSAGE && msg != null =><br>      ...<br>...<br>}<br><br>private def redactLocalRelation(relation: proto.LocalRelation): proto.LocalRelation = {<br><br>....<br>}<br><br>```
【Thanks】 @zhengruifeng
cc @rangadi due to initially he set `shadeTestJar` of protobuf module to true<br><br>also cc @HyukjinKwon <br><br>
Yeah ~ all 【test】 passed
【Thanks】 @sunchao @dongjoon-hyun @HyukjinKwon @rangadi
cc @beliefer @cloud-fan
@ulysses-you Thank you for ping me.<br>Should we put `numOutputRows += 1` into `BaseLimitIterator.next()` ?
yea, I'm fine move it to iterator. jsut want to make less change, though.
the failed pyspark 【test】 seems irrelevant..
【LGTM】.
cc @zhenlineo since you are working on this on the connect side.
cc @cloud-fan @viirya for 【review】, thanks!
@kings129 can you open a new PR for branch 3.3? 【Thanks】!
> @kings129 can you open a new PR for branch 3.3? 【Thanks】!<br><br>【Thanks】 for the quick 【review】, @cloud-fan!<br>Yes, here is the pull request for branch 3.3: https://github.com/apache/spark/pull/40858
One more time @HyukjinKwon ... I don't know how this chmod did not get committed last time.
The operation was canceled after 6 hours running.
Can you rerun? it should be 【<font color=blue>fix】ed】 now
@HyukjinKwon yes, now its works 【:)】
cc @LuciferYang FYI
Some 【c<font color=blue>hang】es】 seems unrelated to the current pr, and not ready to 【review】 yet? @zhenlineo
@LuciferYang Yeah, adding the whole client jars will fix the issue, but we are looking at how could we fix the issue with only the selected files using artifact sync. So this PR is still experimental at this moment.
@LuciferYang @hvanhovell @vicennial <br>This fixes maven 【test】 failures to run UDF E2E 【test】s.
+1
I will close this PR. Though there is problematic behaviour if the column name is present in column list as well as in partition clause, and in case of hive tables, that situation should be detected . But it is not that severe a bug, as for hive table format the partition col should not be present in both the places ( i was not aware of that).
It has some minor conflicts against branch-3.3. @bersprockets wanna make a pr for it?
>It has some minor conflicts against branch-3.3. @bersprockets wanna make a pr for it?<br><br>【Thanks】! Will do.
cc @cloud-fan @dongjoon-hyun @HyukjinKwon thanks
please retake a look, @cloud-fan ,thanks.
@srielau how do you think of this new TVF `sql_keywords`?<br>```<br>@ExpressionDescription(<br>  usage = \"\"\"_FUNC_() - Get Spark SQL keywords\"\"\<br>  examples = \"\"\"<br>    Examples:<br>      > SELECT * FROM _FUNC_() LIMIT 2;<br>       ADD  false<br>       AFTER  false<br>  \"\"\<br>  since = \"3.5.0\<br>  group = \"generator_funcs\")<br>```
> @srielau how do you think of this new TVF `sql_keywords`?<br>> <br>> ```<br>> @ExpressionDescription(<br>>   usage = \"\"\"_FUNC_() - Get Spark SQL keywords\"\"\<br>>   examples = \"\"\"<br>>     Examples:<br>>       > SELECT * FROM _FUNC_() LIMIT 2;<br>>        ADD  false<br>>        AFTER  false<br>>   \"\"\<br>>   since = \"3.5.0\<br>>   group = \"generator_funcs\")<br>> ```<br><br>I'm not opposed to it. What is the reference to JDBC compliance?
> What is the reference to JDBC compliance?<br><br>To implement java.sql.DatabaseMetaData#getSQLKeywords at thriftserver side
@yaooqinn Could you re-run the 【test】?
thanks, @wangyum for the notification. I didn't notice there was a transient failure
the 【test】 failures seem not related， can you retake a look? @wangyum @cloud-fan
thanks @cloud-fan @wangyum @srielau for the help, merged to master
@HyukjinKwon  do we have a timeout parameter for python unit【test】?<br><br>The issue happens occasionally, I want to fail the 【test】 if it is too long (maybe 10min) so that 【review】s at least will be aware of that the failure is unrelated<br><br>
We don't have. We have `utils.eventually`  but I think it cannot be used in this case. Would need to implement it separately.
@HeartSaVioR - please take a look. 【Thanks】
cc  @dongjoon-hyun @yaooqinn @Yikun, can anyone help 【review】?
> The cpu limits are set by spark.kubernetes.{driver,executor}.limit.cores. The cpu is set by spark.{driver,executor}.cores. The memory request and limit are set by summing the 【value】s of spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead. Other resource limits are set by spark.{driver,executor}.resources.{resourceName}.* configs.<br><br>Referring to the doc, we can actually set driver pod memory alone
> > The cpu limits are set by spark.kubernetes.{driver,executor}.limit.cores. The cpu is set by spark.{driver,executor}.cores. The memory request and limit are set by summing the 【value】s of spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead. Other resource limits are set by spark.{driver,executor}.resources.{resourceName}.* configs.<br>> <br>> Referring to the doc, we can actually set driver pod memory alone<br><br>It`s about container memory request&limit, not about the driver or exec.<br>Current code is setting the pod request.memory and limit.memory in the **same 【value】** from by summing the 【value】s of spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead.<br>But request.memory and limit.memory are different kind of params from k8s, always keep same 【value】 may not a good practice. In most case the request memory quota we can get is always smaller than limit.memory from infrastructure team. If spark pods request.memory can only same as limit.memory, then total memory we can use is based on the smaller one, thus can not fully use the memory resource.<br>requests 定义了对应的容器所需要的最小资源量。<br>limits 定义了对应容器最大可以消耗的资源上限。
There was a 【discussion】 about this on the spark dev mailing list earlier, hope it helps you.<br>[spark executor pod has same memory 【value】 for request and limit](https://www.mail-archive.com/search?l=dev@spark.apache.org&q=subject:%22spark+executor+pod+has+same+memory+【value】+for+request+and+limit%22&o=newest)
> <br><br>@zwangsheng do we have any timeline to add this 【feature】？
> spark executor pod has same memory 【value】 for request and limit<br><br>You can found this in the mail 【discussion】:<br>> There is a very good reason for this. It is recommended using k8s that you set <br>memory request and limit to the same 【value】, set a cpu request, but not a cpu <br>limit. More info here https://home.【robust】a.dev/blog/kubernetes-memory-limit
@dongjoon-hyun  sorry, my bad, `【test】_parity_torch_distributor` became flaky, I haven't find the root cause for now.<br>After disabling the related cases (https://github.com/apache/spark/pull/40775 and https://github.com/apache/spark/pull/40787), the CI should be fine
Thank you for merging, @sunchao !
【Thanks】 @sunchao @dongjoon-hyun
> This adds 【functionality】 similar to YARN[1] to K8s.<br><br>They are not exactly the same. Can we clarify both the similarities and differences? It seems that tracking failures on hosts has not been ported to k8s
also cc @dongjoon-hyun
> They are not exactly the same.  ... It seems that tracking failures on hosts has not been ported to k8s<br><br>let me try to port the \"tracking failures on hosts\" 【feature】 as well<br><br>update <br>------<br>this is kind of an independent and big 【feature】, and seems out of the scope of this PR, documented the differences between YARN and K8s instead
can we rename the title to `Port executor failure tracker from Spark on YARN to K8s`?
【Thanks】, @yaooqinn
cc @cloud-fan, please let me know if you have a better idea.
shall we update `ConvertToLocalRelation` to support `CommandResult` as well?
or a more surgical way is to update `Dataset.getRows`, convert `CommandResult` to `LocalRelation` in the logical plan, and then execute the new logical plan.
> or a more surgical way is to update `Dataset.getRows`, convert `CommandResult` to `LocalRelation` in the logical plan, and then execute the new logical plan.<br><br>【Thanks】, that's a good idea!
Updated to `LocalRelation` conversion in `Dataset.getRows()` in https://github.com/apache/spark/pull/40779/commits/98bf071cfb016b6461545a964ec07576419a6bb6
【Thanks】 all for the 【review】!
@martin-g @HyukjinKwon PTAL. 【Thanks】
Could you add a 【test】 case for this scenario?<br><br>
hmm.. there is no `[SERVER]` tag, we should remove if from pr title
@Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`?
> @Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`?<br><br>Sorry, I don't get it. What do you mean about `drop the abbreviate`? This PR just do null 【check】.
> > @Hisoka-X @HyukjinKwon Any reason we drop the `abbreviate`?<br>> <br>> Sorry, I don't get it. What do you mean about `drop the abbreviate`? This PR just do null 【check】.<br><br>Ah sorry! I didn't see you did the abbreviate in the top, please ignore this!
@Hisoka-X I think we also change this line?<br>https://github.com/apache/spark/pull/40780/files#diff-d21a96890e660df398527183a191b0d0c73521aada856e2491c33243f269fdceR128<br><br>I could add it to my ongoing PR https://github.com/apache/spark/pull/40785
> @Hisoka-X I think we also change this line? https://github.com/apache/spark/pull/40780/files#diff-d21a96890e660df398527183a191b0d0c73521aada856e2491c33243f269fdceR128<br>> <br>> I could add it to my ongoing PR #40785<br><br>OK for me, but I just want to remind, add null 【check】 just want to avoid throw NPE in `setMessage`, not want to change `null` to  `\"\"`
> > @Hisoka-X I think we also change this line? https://github.com/apache/spark/pull/40780/files#diff-d21a96890e660df398527183a191b0d0c73521aada856e2491c33243f269fdceR128<br>> > I could add it to my ongoing PR #40785<br>> <br>> OK for me, but I just want to remind, add null 【check】 just want to avoid throw NPE in `setMessage`, not want to change `null` to `\"\"`<br><br>I see. With additional look I think here it's better to keep what it is. as Status's description is nullable:<br><br>``` <br>private Status(Code code, @Nullable String description, @Nullable Throwable cause) {<br>    this.code = 【check】NotNull(code, \"code\");<br>    this.description = description;<br>    this.cause = cause;<br>  }<br><br>public Status withDescription(String description) {<br>    if (Objects.equal(this.description, description)) {<br>      return this;<br>    }<br>    return new Status(this.code, description, this.cause);<br>  }<br><br>```
cc @yaooqinn
Test failures unrelated. Merged to master/branch-3.4, thanks!
Thank you so much, @sunchao ! Ya, I agree with you.
late +1
The failure GA is unrelated to this PR.<br>ping @hvanhovell @zhengruifeng cc @dongjoon-hyun @amaliujia
sorry, `【test】_parity_torch_distributor` became flaky, I am going to disable related cases for now
So this happens when <br>```<br>val df = createDataFrame(...)<br>df.collect()<br>```<br>?
> So this happens when<br>> <br>It happens when the root plan is `LocalRelation`.<br><br><br>
@hvanhovell The failed GA is unrelated to this PR.
Because there are difference suggestion from @hvanhovell and @ueshin, I don't know how to continue this job.
@ueshin @hvanhovell Recently, https://github.com/apache/spark/pull/41064 added the rowCount statistics to `LocalRelation`. In this PR, @ueshin also suggested to add the row count as optional to `LocalRelation` message.<br>So I think this is a chance to add optional row count to `LocalRelation` message.
cc: @amaliujia, @zhenlineo, @WweiL, @pengzhon-db
The PR failed with formatting, just run the following command as it suggested on the failed build:<br>```<br>./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm<br>```
Good work ~ @rangadi please add new mima 【check】 to `CheckConnectJvmClientCompatibility`, thanks
https://github.com/apache/spark/pull/40757/files already re-chmod `connector/connect/bin/spark-connect-scala-client-classpath`
```<br>ERROR: Comparing client jar: /__w/spark/spark/connector/connect/client/jvm/target/scala-2.12/spark-connect-client-jvm-assembly-3.5.0-SNAPSHOT.jar and and sql jar: /__w/spark/spark/sql/core/target/scala-2.12/spark-sql_2.12-3.5.0-SNAPSHOT.jar <br>problems: <br>method logName()java.lang.String in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method log()org.slf4j.Logger in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logInfo(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logDebug(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logTrace(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logWarning(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logError(scala.Function0)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logInfo(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logDebug(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logTrace(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logWarning(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method logError(scala.Function0,java.lang.Throwable)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method isTraceEnabled()Boolean in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method initializeLogIfNecessary(Boolean)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method initializeLogIfNecessary(Boolean,Boolean)Boolean in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>synthetic method initializeLogIfNecessary$default$2()Boolean in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>method initializeForcefully(Boolean,Boolean)Unit in class org.apache.spark.sql.streaming.DataStreamReader does not have a correspondent in client version<br>static method SOURCES_ALLOW_ONE_TIME_QUERY()scala.collection.Seq in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>static method SOURCE_NAME_NOOP()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>static method SOURCE_NAME_TABLE()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>static method SOURCE_NAME_CONSOLE()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>static method SOURCE_NAME_FOREACH_BATCH()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>static method SOURCE_NAME_FOREACH()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>static method SOURCE_NAME_MEMORY()java.lang.String in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>method toTable(java.lang.String)org.apache.spark.sql.streaming.StreamingQuery in class org.apache.spark.sql.streaming.DataStreamWriter does not have a correspondent in client version<br>Exceptions to binary 【compatibility】 can be added in 'CheckConnectJvmClientCompatibility#【check】MiMaCompatibility'<br>```<br><br>Some 【compatibility】 issues need to be 【<font color=blue>fix】ed】<br><br>
@LuciferYang PTAL recent updates. <br>I will fix the ReplE2ESuite failure (not yet sure if that is related to this PR).
> I will fix the ReplE2ESuite failure (not yet sure if that is related to this PR).<br><br>This should be not related to the current PR. I told @vicennial  yesterday<br><br>https://github.com/apache/spark/pull/40675#issuecomment-1512936430<br>https://github.com/apache/spark/pull/40675#issuecomment-1513102087<br><br>@rangadi you can try to re-trigger the failed GA task again<br><br><br><br>
@rangadi Could you try merging master into your branch and rerun the 【test】s? I'm not sure if the github workflow that's running here is picking up [these](https://github.com/apache/spark/pull/40675/files#diff-48c0ee97c53013d18d6bbae44648f7fab9af2e0bf5b0dc1ca761e18ec5c478f2R250-R253) 【c<font color=blue>hang】es】 from the original PR (which 【<font color=blue>fix】ed】 this issue in the original PR)
@LuciferYang, @HyukjinKwon, @hvanhovell  please merge this when you get chance. All 【test】s pass.
In fact the compilation is failing on 【test】s. let me fix it first.
Test issues should have been 【<font color=blue>fix】ed】.
@HyukjinKwon @rangadi @pengzhon-db<br><br>Hey guys, PTAL when you get a chance, thanks!
@HyukjinKwon Can you take a look? 【Thanks】!
@WweiL mind rebasing this one please?
Fetched and Merged with master. I noticed stack trace was added. I'll create a SPARK ticket to include it
@hvanhovell or @grundprinzip actually mind taking a look please when you find some time?
I see .. sorry for a bit of back and forth:<br><br>```<br>python/pyspark/sql/connect/streaming/query.py:72: error: Missing return statement  [return]<br>python/pyspark/sql/connect/streaming/query.py:108: error: Missing return statement  [return]<br>python/pyspark/sql/connect/streaming/query.py:140: error: Missing return statement  [return]<br>```<br><br>Let's add them back :-) ..
@HyukjinKwon Can you merge this : ) 【Thanks】!
Thank you always!
For the record, I verified that this commit excluded `branch-3.2` correct like the following in the la【test】 `Snapshot Publishing`.<br><br>- https://github.com/apache/spark/actions/runs/4694946818<br><br>![Screenshot 2023-04-13 at 10 22 09 PM](https://user-images.githubusercontent.com/9700541/231948512-81799483-8617-4039-b90b-1db0fd61231e.png)<br>
also track it in https://issues.apache.org/jira/browse/SPARK-43122
cc @HyukjinKwon @WeichenXu123 @dongjoon-hyun
```<br>Starting 【test】(python3.9): pyspark.ml.【test】s.connect.【test】_parity_torch_distributor (temp output: /__w/spark/spark/python/target/69e3939a-22ef-4a97-a09c-c5d3e66930de/python3.9__pyspark.ml.【test】s.connect.【test】_parity_torch_distributor__zke1oq_a.log)<br>Finished 【test】(python3.9): pyspark.ml.【test】s.connect.【test】_parity_torch_distributor (192s) ... 10 【test】s were skipped<br>```<br><br>right now the local mode related 【test】s are disabled in connect
@dongjoon-hyun I created https://issues.apache.org/jira/browse/SPARK-43122 for the disabled 【test】s
Great! Thank you, @zhengruifeng ! That's all we need in order to not to forget.
Test first,  need wait until 3.4 release at least <br><br>
Also, cc FYI, @sunchao
> Test first, need wait until 3.4 release at least<br><br>3.4.0 is uploaded to mirrors.
> > Test first, need wait until 3.4 release at least<br>> <br>> 3.4.0 is uploaded to mirrors.<br><br><br>I know, but let's wait for the official website update<br><br>
Personally I like this move, but not sure whether there are other users in the 【community】 that still depend on Hadoop 2.x. Has this been discussed before?
> Personally I like this move, but not sure whether there are other users in the 【community】 that still depend on Hadoop 2.x. Has this been discussed before?<br><br>yes, like this one [Dropping Apache Spark Hadoop2 Binary Distribution?](https://lists.apache.org/thread/z4jdy9959b6zj9t726zl0zcrk4hzs0xs)
Oh cool, it's good then. Forgot about this thread even though I replied on it ..
Just sent an email to the 【community】 in case anyone still have concerns on this.
friendly ping @dongjoon-hyun @sunchao
【Thanks】 @sunchao @dongjoon-hyun @bjornjorgensen ~
ping @infoankitp @navinvishy cc @cloud-fan
https://github.com/apache/spark/pull/40833 merged, so close this one.
Not sure how CI passes, or if this only happens in my local. But I think we should exclude them in any event.<br><br>cc @LuciferYang
Manually run `dev/sbt-【check】style` can reproduce, but it seems that `dev/sbt-【check】style` does not always execute.<br><br>https://github.com/apache/spark/blob/4c938d62d791742b9f0c6a77b66fc06a90d7c0ad/dev/run-【test】s.py#L611-L618<br><br>https://github.com/apache/spark/blob/4c938d62d791742b9f0c6a77b66fc06a90d7c0ad/dev/run-【test】s.py#L641-L647<br><br>`lint-java` use mvn 【check】style, It did not 【check】ed this error.<br><br>So should we let `lint-java` use  `sbt-【check】style` to unify behavior?<br><br>Additionally, it seems that the generated proto java files of the connect-common module have not been 【check】ed? it is a little strange.<br><br>
Yeah .. let me merge this in first and fix them to be consistent separately ..
Let me merge this one first.
hmm...branch-3.4 not require this fix?<br><br>
actually yeah. let's put this in branch-3.4 too.
Merged to branch-3.4 too.
> Yeah .. let me merge this in first and fix them to be consistent separately ..<br><br>`sbt-【check】style-plugin` will not print wrong fmt issue to the console,  we can only 【check】 the problem from `target/【check】style-output.xml`  and they are in different module directories. On the other hand, `sbt-【check】style-plugin`  has not been updated for a long time, It seems that no one is maintaining it ...<br><br>`maven-【check】style-plugin` will print out the problem to the console, which is more 【intuitive】.<br><br><br><br>
let me try merging commits from master several times, to see whether this fix is 【stable】 enough
This PR actually only change `setUp & tearDown` to `setUpClass & tearDownClass`.<br><br>In all the 6 runs, the PyTorch related 【test】s all passed, so I think it is ready for 【review】.<br><br>@HyukjinKwon @dongjoon-hyun @WeichenXu123
I see a failure in `pyspark.ml.clustering`, should be unrelated.<br>but let me re-run `pyspark.ml.clustering` and 【test】 torch one more time.<br>right now, I didn't see the previous torch-related issue.
in all the 10 runs, `【test】_parity_torch_distributor` and `【test】_distributor` passed without being hanged.<br>so I think it is 【stable】 enough to re-enable.
Maybe @cloud-fan or @hvanhovell? You last 【review】ed 【c<font color=blue>hang】es】 here.<br><br>It's a one-line fix: `DslAttr.attr` now returns the wrapped `UnresolvedAttribute` instead of creating a new one.
https://github.com/apache/spark/actions/runs/4765094614/jobs/8470442826<br><br><img width=\"1278\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/233662686-1bfb0633-bbd6-4c4a-a9b9-ecdd8e2f0ffc.png\"><br><br><br>@cloud-fan many 【test】 failed after this one, should we revert this one first?
> https://github.com/apache/spark/actions/runs/4765094614/jobs/8470442826<br>> <br>> <img alt=\"image\" width=\"1278\" src=\"https://user-images.githubusercontent.com/1475305/233662686-1bfb0633-bbd6-4c4a-a9b9-ecdd8e2f0ffc.png\"><br>> <br>> @cloud-fan many 【test】 failed after this one, should we revert this one first?<br><br>also ping @HyukjinKwon
let's revert first. Seems GA wrongly reported green for this PR.
Reverted: https://github.com/apache/spark/commit/3523d83ac472b330bb86a442365c0a15f7e53f8c.
Damn, thank you for reverting guys. Unsure why GA didn't 【test】 the last commit.
@cloud-fan, maybe let's consider multi-part attribute references as fine or at least separate from this? What do you think?<br><br>I opened another PR just changing `DslAttr.attr` to not break on special characters #40902.
cc @amaliujia Could you 【review】 the agg and reduce 【c<font color=blue>hang】es】. a.k.a. the last two commits.
Overall looks reasonable to me. I only have questions over the proto validation in the server side.
cc @hvanhovell @HyukjinKwon Can we merge this? 【Thanks】!
@rangadi @pengzhon-db @HyukjinKwon PTAL, thank you!
Why do you need the change in `dev/tox.ini`?
> Why do you need the change in `dev/tox.ini`?<br><br>@amaliujia Ah thanks for pointing it out. I'll revert it.<br><br>The change and reason is in this PR: https://github.com/apache/spark/pull/40801
@DerekTBrown Mind creating a JIRA and add it to the PR title please? See also https://spark.apache.org/contributing.html
@DerekTBrown I have an alternative solution https://github.com/apache/spark/pull/40831, which should cover your case
Looks good. Closing in favor of #40831
@cloud-fan can you help 【review】 this? thank you!
Not sure if this would break github 【test】s, let's see
cc @wangyum @sunchao
@gengliangwang I just 【success】 to run this `pip install --【upgrade】 -r dev/requirements.txt` in a 【clean】 conda env, and can not reproduce the issue related to pytorch version.<br><br>@HyukjinKwon @WeichenXu123 have you seen this version issue?
I haven't seen this before. but I think loosing the version restrictions looks fine.
> @gengliangwang I just 【success】 to run this pip install --【upgrade】 -r dev/requirements.txt in a 【clean】 conda env, and can not reproduce the issue related to pytorch version.<br><br>@zhengruifeng I am using python 3.11.2<br>And I am actually running with <br>```<br>pip3 install --【upgrade】 -r dev/requirements.txt<br>```
@wangyum can we mention the idea a bit more? Do we push down the cast in join condition to the join children?
@cloud-fan OK, updated the description of the PR. We do not push down the cast to the  join children. This rule is the same as `CoalesceBucketsInJoin`. Must also must before `EnsureRequirements`. The join's children must be the filter, project and stream side of broadcast join.
Can we fix this issue by pulling out expressions of join conditions to Projects under Join? IIRC you had a PR for it and then the join condition should only contains attributes.
Pulling out join condition can't fix this issue because the [output partitioning](https://github.com/apache/spark/blob/72922adc8a78e8d31f03205a148b89291a9a4d19/sql/core/src/main/scala/org/apache/spark/sql/execution/AliasAwareOutputExpression.scala#L57) is `UnknownPartitioning(Bucket Number)` which can't satisfy the `SortMergeJoin`, and also needs to introduce shuffle.<br><br>```sql<br>SELECT *<br>FROM   (SELECT Cast(i AS DECIMAL(20, 0)) AS i FROM t2) tmp1<br>JOIN   (SELECT Cast(i AS DECIMAL(20, 0)) AS i FROM t3) tmp2<br>ON     tmp1.i = tmp2.i; <br>```<br><br>```<br>== Physical Plan ==<br>AdaptiveSparkPlan isFinalPlan=false<br>+- SortMergeJoin [i#23], [i#24], Inner<br>   :- Sort [i#23 ASC NULLS FIRST], false, 0<br>   :  +- Exchange hashpartitioning(i#23, 200), ENSURE_REQUIREMENTS, [plan_id=130]<br>   :     +- Project [cast(i#19L as decimal(20,0)) AS i#23]<br>   :        +- Filter isnotnull(cast(i#19L as decimal(20,0)))<br>   :           +- FileScan parquet spark_catalog.default.t2[i#19L] Batched: true, Bucketed: false (disabled by query planner)<br>   +- Sort [i#24 ASC NULLS FIRST], false, 0<br>      +- Exchange hashpartitioning(i#24, 200), ENSURE_REQUIREMENTS, [plan_id=135]<br>         +- Project [cast(i#20 as decimal(20,0)) AS i#24]<br>            +- Filter isnotnull(cast(i#20 as decimal(20,0)))<br>               +- FileScan parquet spark_catalog.default.t3[i#20] Batched: true, Bucketed: false (disabled by query planner)<br>```
Shall we 【improve】 the partitioning propagation to support cast?
cc @Ngone51 @jiangxb1987 FYI
Small ping on this. The PR was previously a draft since I was waiting for me to 【test】 it on a real workload, but that have now been done.<br><br>
@itholic @MaxGekk
@kori73 Could you update the example (output) according to the recent commit, please.
> @kori73 Could you update the example (output) according to the recent commit, please.<br><br>updated the example according to the recent commit
+1, 【LGTM】. Merging to master.<br>Thank you, @kori73.
@kori73 Congratulations with your first 【contribution】 to Apache Spark!
@allisonwang-db @cloud-fan
is this a long-standing bug? Also cc @viirya
Yes, this is a long-standing bug.
FYI this bug affected both the current DecorrelateInnerQuery framework and the old code (with spark.sql.optimizer.decorrelateInnerQuery.enabled = false), and this PR fixes both.
The streaming failure is unrelated. 【Thanks】, merging to master/3.4!
+CC @cloud-fan, @dongjoon-hyun who have 【review】ed work on explain output earlier.
Thank you for pinging me, @mridulm .<br><br>Also, cc @sunchao
I agree that we do need to copy the cached plan when getting it, but I feel the current change is hard to understand and reason about.<br><br>I think a better place to fix is `CacheManager#useCachedDataInternal`. It does copy `InMemoryRelation` but does not copy the physical plan:<br>https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L298-L306<br><br>We can add a new method `InMemoryRelation#freshCopy` which takes new output attributes and copy the physical plan, then we change `cached.cachedRepresentation.withOutput(currentFragment.output)` to `cached.cachedRepresentation.freshCopy(currentFragment.output)`
> I agree that we do need to copy the cached plan when getting it, but I feel the current change is hard to understand and reason about.<br>> <br>> I think a better place to fix is `CacheManager#useCachedDataInternal`. It does copy `InMemoryRelation` but does not copy the physical plan: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L298-L306<br>> <br>> We can add a new method `InMemoryRelation#freshCopy` which takes new output attributes and copy the physical plan, then we change `cached.cachedRepresentation.withOutput(currentFragment.output)` to `cached.cachedRepresentation.freshCopy(currentFragment.output)`<br><br>The challenge with this approach is that `InMemoryRelation` does not get the physical plan passed in directly. It is through `CachedRDDBuilder`. I originally tried making a new `CachedRDDBuilder` with a copy of the physical plan, but it broke other 【test】s due to the private state it maintains.
I see, can we change `def cachedPlan: SparkPlan = cacheBuilder.cachedPlan` to `lazy val cachedPlan: SparkPlan = cacheBuilder.cachedPlan.clone` with comments to explain it? This makes sure that a new instance of `InMemoryRelation` will have a new instance of the physical plan.
> <br><br>@cloud-fan Cloning the cachedPlan is also problematic because it contains state (accumulators in private fields) when it includes a `CollectMetricsExec` operator. `CollectMetricsExec.collect` 【specific】ally looks at the `InMemoryRelation.cachedPlan` to get the stateful metrics.<br><br>I verified this is an issue by cloning `InMemoryRelation.cachedPlan`. Then I modified a unit 【test】 that uses `Dataset.observe` to cache the dataset. This breaks the unit 【test】. When I revert cloning the cachedPlan it passes. Here is how I modified the unit 【test】 to prove the issue.<br>```<br>diff --git a/sql/core/src/【test】/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala b/sql/core/src/【test】/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala<br>index f046daacb91..3de33e3e1b2 100644<br>--- a/sql/core/src/【test】/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala<br>+++ b/sql/core/src/【test】/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala<br>@@ -277,7 +277,7 @@ class DataFrameCallbackSuite extends QueryTest<br>         max($\"id\").as(\"max_val\"),<br>         // Test unresolved alias<br>         sum($\"id\"),<br>-        count(when($\"id\" % 2 === 0, 1)).as(\"num_even\"))<br>+        count(when($\"id\" % 2 === 0, 1)).as(\"num_even\")).cache()<br>       .observe(<br>         name = \"other_event\<br>         avg($\"id\").cast(\"int\").as(\"avg_val\"))<br>```<br><br>So I think we should stick with only cloning it for the innerChildren since the only usage is the ExplainUtils and statefulness doesn't matter, besides the `TreeNode.tag` 【value】s.
Merged to branch-3.3 too.
Sorry for a forth and back. We don't actually need this - it was my mistake. reverted.
cc @mingyangge-db @HyukjinKwon FYI
I can't find a method to trigger this error_class using sql command. This error_class are only thrown by \"mergeExpressions\" field of AggregateWindowFunction. But this field isn't called anywhere in all aggregate window function. Is it an internal error?
@MaxGekk , any comment?
+1, 【LGTM】. Merging to master.<br>Thank you, @liang3zy22.
One of the 【test】s fail but I wonder why this should not be due to import:<br>`starting mypy annotations 【test】...<br>annotations failed mypy 【check】s:<br>python/pyspark/broadcast.py:100: error: Overloaded function implementation does not accept all possible arguments of signature 3  [misc]<br>Found 1 error in 1 file (【check】ed 505 source files)`<br><br>I am getting also some Java 【test】s failing but this is really weird since I have not touched any Java code
Looks like the failed 【test】 is a flaky one so not relevant to this change.
Yea, I can re-trigger if it's necessary.
I guess better to trigger to have it pass<br>
Surprisingly after commiting of naming 【improvement】s (no logic 【c<font color=blue>hang】es】) the build failed. I think it's not related to my change:<br>```<br>[error] running /home/runner/work/spark/spark/build/sbt -Phadoop-3 -Pconnect sql-kafka-0-10/【test】 protobuf/【test】 connect/【test】 connect-client-jvm/【test】 mllib/【test】 ; received return code 1<br>```<br><br>It happened at [Run / Build modules: streaming, sql-kafka-0-10, streaming-kafka-0-10, mllib-local, mllib, yarn, mesos, kubernetes, hadoop-cloud, spark-ganglia-lgpl, connect, protobuf](https://github.com/woj-i/spark/actions/runs/4762682139/jobs/8474612155#logs)<br>Can you please help me with fixing the build?<br><br>The status result is:<br>```<br>ReplE2ESuite.Simple query<br>java.lang.RuntimeException: <br>REPL Timed out while running command: <br>spark.sql(\"select 1\").collect()<br>      <br>Console output: <br>Spark session 【available】 as 'spark'.<br>   _____                  __      ______                            __<br>  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_<br>  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/<br> ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_<br>/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/<br>    /_/<br><br>```<br><br>I extracted some logs about 【summary】 of the failure: <br>```<br>2023-04-21T08:59:45.6811001Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[36mTests: succeeded 739, failed 3, canceled 0, ignored 0, pending 0\u001B[0m\u001B[0m<br>2023-04-21T08:59:45.6811459Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m*** 3 TESTS FAILED ***\u001B[0m\u001B[0m<br>2023-04-21T08:59:45.6841148Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0mFailed 【test】s:\u001B[0m<br>2023-04-21T08:59:45.6845148Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0m\torg.apache.spark.sql.application.ReplE2ESuite\u001B[0m<br>2023-04-21T08:59:45.9076588Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0m(connect-client-jvm / Test / \u001B[31m【test】\u001B[0m) sbt.TestsFailedException: Tests un【success】ful\u001B[0m<br>2023-04-21T08:59:45.9270454Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0mTotal time: 140 s (02:20), completed Apr 21, 2023 8:59:45 AM\u001B[0m<br>2023-04-21T08:59:46.7253579Z \u001B[0J[error] running /home/runner/work/spark/spark/build/sbt -Phadoop-3 -Pconnect sql-kafka-0-10/【test】 protobuf/【test】 connect/【test】 connect-client-jvm/【test】 mllib/【test】 ; received return code 1<br>2023-04-21T08:59:46.9853017Z ##[error]Process completed with exit code 18.<br>```<br><br>and some exceptions/errors that were very frequent in this 【test】<br>```<br>2023-04-21T08:58:47.7065800Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32mUserDefinedFunctionE2ETestSuite:\u001B[0m\u001B[0m<br>2023-04-21T08:58:47.9937562Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed filter (225 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.0428439Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed filter - java (48 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.1508396Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed map (98 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.2447937Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- filter with condition (92 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.3442084Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- filter with col(*) (98 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.4067957Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed map - java (55 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.4836489Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed flat map (74 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.5347903Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed flat map - java (51 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:48.8260858Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed map partition (288 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.0958006Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset typed map partition - java (270 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.1921226Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset foreach (92 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.2289858Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset foreach - java (34 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.3390396Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset foreachPartition (109 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.4353655Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset foreachPartition - java (99 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.4971434Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- Dataset foreach: change not visible to client (65 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:49.5121205Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32mReplE2ESuite:\u001B[0m\u001B[0m<br>2023-04-21T08:58:53.2505694Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.2692727Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.3262467Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.3544612Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.3646754Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.3896517Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.4061277Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.4268730Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.4353509Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.4546791Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.4808386Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.4964638Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.5198604Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.5300108Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.5383343Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.5494860Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.5631736Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.5913690Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.6397803Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.6769519Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.7221075Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.7631252Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.8129701Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.8289367Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.8541965Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.8848448Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9060169Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9152965Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9257073Z sh: 1: cannot create /dev/tty: No such device or address<br>2023-04-21T08:58:53.9294986Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9379865Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9483798Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9579639Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9644462Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9701922Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9739622Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9775897Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9830054Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9870461Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9910296Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:53.9947627Z sh: 1: cannot open /dev/tty: No such device or address<br>```<br>...<br><br>```<br>2023-04-21T08:58:59.6221061Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.6255708Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.6282881Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.6349493Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.6627557Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.6787449Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.7065904Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m- Simple query *** FAILED *** (10 seconds, 75 milliseconds)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.7248880Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.7820090Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.7821918Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.7870724Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.7941758Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8165642Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8305760Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8376055Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8407369Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8439122Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8541147Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8651384Z sh: 1: cannot open /dev/tty: No such device or address<br>2023-04-21T08:58:59.8652177Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.lang.RuntimeException: REPL Timed out while running command: \u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8653000Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mspark.sql(\"select 1\").collect()\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8653943Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m      \u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8654470Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mConsole output: \u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8655695Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mSpark session 【available】 as 'spark'.\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8656304Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m   _____                  __      ______                            __\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8657167Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  / ___/____  ____ ______/ /__   / ____/___  ____  ____  ___  _____/ /_\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8657763Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  \\__ \\/ __ \\/ __ `/ ___/ //_/  / /   / __ \\/ __ \\/ __ \\/ _ \\/ ___/ __/\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8663578Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m ___/ / /_/ / /_/ / /  / ,<    / /___/ /_/ / / / / / / /  __/ /__/ /_\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8664196Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m/____/ .___/\\__,_/_/  /_/|_|   \\____/\\____/_/ /_/_/ /_/\\___/\\___/\\__/\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8664916Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m    /_/\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8665558Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8666266Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8666734Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8667377Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8667825Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8668458Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8668901Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8669534Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8669960Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8670722Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.8671179Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>```<br>...<br><br>```<br>2023-04-21T08:58:59.9409151Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mError output: Compiling (synthetic)/ammonite/predef/ArgsPredef.sc\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9409484Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mCompiling /home/runner/work/spark/spark/connector/connect/client/jvm/(console)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9409764Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mjava.lang.RuntimeException: Nonzero exit 【value】: 2\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9410063Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.package$.error(package.scala:30)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9410464Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:138)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9410846Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:108)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9411103Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.TTY$.stty(Utils.scala:103)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9411387Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.TTY$.withSttyOverride(Utils.scala:114)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9411672Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.Terminal$.readLine(Terminal.scala:41)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9412007Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.AmmoniteFrontEnd.readLine(AmmoniteFrontEnd.scala:133)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9412374Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.AmmoniteFrontEnd.action(AmmoniteFrontEnd.scala:28)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9412650Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.$anonfun$action$4(Repl.scala:194)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9412936Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.$anonfun$flatMap$1(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9413217Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Signaller.apply(Signaller.scala:28)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9413496Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.flatMap(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9413765Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.flatMap$(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9414044Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Signaller.flatMap(Signaller.scala:16)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9414308Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.$anonfun$action$2(Repl.scala:178)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9414621Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.util.Catching.flatMap(Res.scala:115)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9414875Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.action(Repl.scala:170)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9415122Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.loop$1(Repl.scala:212)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9415366Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.run(Repl.scala:227)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9415618Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.Main.$anonfun$run$1(Main.scala:236)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9415866Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.Option.getOrElse(Option.scala:189)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9416095Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.Main.run(Main.scala:224)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9416446Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:100)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9416845Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.application.ReplE2ESuite$$anon$1.run(ReplE2ESuite.scala:59)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9417169Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.Executors$RunnableAdapter.call(Executors.java:511)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9417461Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.FutureTask.run(FutureTask.java:266)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9417825Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9418212Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9445731Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.lang.Thread.run(Thread.java:750)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9446013Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9446291Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31mjava.lang.RuntimeException: Nonzero exit 【value】: 2\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9446689Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.package$.error(package.scala:30)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9447082Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(ProcessBuilderImpl.scala:138)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9447463Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(ProcessBuilderImpl.scala:108)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9447714Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.TTY$.stty(Utils.scala:103)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9448002Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.TTY$.withSttyOverride(Utils.scala:114)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9448285Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.terminal.Terminal$.readLine(Terminal.scala:41)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9448611Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.AmmoniteFrontEnd.readLine(AmmoniteFrontEnd.scala:133)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9448994Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.AmmoniteFrontEnd.action(AmmoniteFrontEnd.scala:28)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9449261Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.$anonfun$action$4(Repl.scala:194)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9449548Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.$anonfun$flatMap$1(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9449820Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Signaller.apply(Signaller.scala:28)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9450091Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.flatMap(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9450362Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Scoped.flatMap$(Signaller.scala:45)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9450642Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Signaller.flatMap(Signaller.scala:16)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9451184Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.$anonfun$action$2(Repl.scala:178)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9451519Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.util.Catching.flatMap(Res.scala:115)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9451770Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.action(Repl.scala:170)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9452017Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.loop$1(Repl.scala:212)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9452260Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.repl.Repl.run(Repl.scala:227)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9452503Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.Main.$anonfun$run$1(Main.scala:236)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9452752Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  scala.Option.getOrElse(Option.scala:189)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9452981Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  ammonite.Main.run(Main.scala:224)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9453327Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.application.ConnectRepl$.doMain(ConnectRepl.scala:100)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9453685Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  org.apache.spark.sql.application.ReplE2ESuite$$anon$1.run(ReplE2ESuite.scala:59)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9454018Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.Executors$RunnableAdapter.call(Executors.java:511)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9454310Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.FutureTask.run(FutureTask.java:266)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9454671Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9455071Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9455325Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m  java.lang.Thread.run(Thread.java:750)\u001B[0m\u001B[0m<br>2023-04-21T08:58:59.9455486Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[31m\u001B[0m\u001B[0m<br><br>```
I merged this branch with the most recent master branch hoping it solves the build issue. I have another build failure, now different error, IMO still unrelated to the change:<br><br>```<br>ThriftServerWithSparkContextInHttpSuite.(It is not a 【test】 it is a sbt.【test】ing.SuiteSelector)<br>java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.<br>This stopped SparkContext was created at:<br><br>org.apache.spark.sql.hive.thriftserver.ThriftServerWithSparkContextInHttpSuite.beforeAll(ThriftServerWithSparkContextSuite.scala:226)<br>org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)<br>org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:67)<br>org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>java.lang.Thread.run(Thread.java:750)<br><br>The currently active SparkContext was created at:<br><br>(No active SparkContext.)<br>```<br>
> Make some special sql can be parsed. Like SELECT 1 UNION SELECT 1.<br><br>cc @wangyum FYI
cc @cloud-fan @MaxGekk and @gengliangwang FYI
Can you provide more background to help people understand and 【review】? I have no idea why catching one more exception type can fix this parser issue.
> Can you provide more background to help people understand and 【review】? I have no idea why catching one more exception type can fix this parser issue.<br><br>【Thanks】 for point that. I updated the description.
Replaced by https://github.com/apache/spark/pull/40835
Do we have a 【design】 doc? This is a big 【feature】.
> Do we have a 【design】 doc? This is a big 【feature】.<br><br>+1
@cloud-fan @hvanhovell
```<br>[error] spark-catalyst: Failed binary 【compatibility】 【check】 against org.apache.spark:spark-catalyst_2.12:3.3.0! Found 2 potential problems (filtered 1415)<br>[error]  * static method canWrite(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,Boolean,scala.Function2,java.lang.String,scala.Enumeration#Value,scala.Function1)Boolean in class org.apache.spark.sql.types.DataType does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.types.DataType.canWrite\")<br>[error]  * method canWrite(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,Boolean,scala.Function2,java.lang.String,scala.Enumeration#Value,scala.Function1)Boolean in object org.apache.spark.sql.types.DataType does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.types.DataType.canWrite\")<br>```<br><br>Failing because of this.<br><br>In fact this method should be marked as private sql method....
Could we change this method as private sql?
@amaliujia IMO it is fine to move this and break 【compatibility】 a bit. Let's just update the MiMa 【check】s and move on.
I updated the MIMA 【check】.
@grundprinzip @hvanhovell @zhengruifeng @HyukjinKwon Could you 【review】 this PR, please.
Are the Python 【c<font color=blue>hang】es】 done in a follow up?
> Are the Python 【c<font color=blue>hang】es】 done in a follow up?<br><br>Yep, in a separate PR.<br>
【LGTM】, merged to master
It seems that there will be behavior difference:<br>```<br>In [3]: spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)<br><br>In [4]: spark.sql(\"【value】s (1, struct(1 as a, 2 as a, 3 as b)) as t(x, y)\").toPandas()<br>Out[4]: <br>   x          y<br>0  1  (1, 2, 3)<br>```
That's an existing behavior difference as we discussed before at https://github.com/apache/spark/pull/40692#issuecomment-1499840711.<br>This PR targets to save the case where there are duplicated field names with Arrow enabled.
Close this in favor of #40988.
cc @dongjoon-hyun @HyukjinKwon FYI
`SPARK_USER_NAME` is introduced in SPARK-26015(https://github.com/apache/spark/pull/23017), and I guess supporting 【dynamic】 user name is one of the author initial intention
cc @Yikun
@Yikun I suppose it's a K8s-only 【feature】.<br><br>As mentioned in the PR description, the main purpose is to reduce the gap between Spark on YARN and K8s, to allow users seamlessly migrate Spark jobs from YARN to K8s.<br><br>I don't have much knowledge about docker/container technology, and I agree w/ you it looks not easy to 【dynamic】ally switch user based on the Docker Official Image rule
Also cc @yaooqinn
Just for others 【review】er infomation, I also wanna share some considerations about this (also include some idea in offline 【discussion】 with @pan3793 ):<br>1. (-0.5) As per docker official recommendation about [USER](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user), we should use `groupadd` and `useradd` to address, rather than change `/etc/passwd` directly. If we specify the USER (useradd/groupadd) in Dockerfile in future, this change will be ignored.<br>2. (-0.5) In theory, application users should be decoupled from container users. Such as, spark docker image should use static user `spark` (just like we done in spark-docker), and other application respect the `spark` user, or don’t depends on the container user.<br>3. (+0.5) As per https://github.com/apache/spark/pull/23017 original 【design】, it was intend to switch user name 【dynamic】ally.<br>4. (+0.5) Consider the Spark case, there are many users want to migrate YARN to K8s easily, support user 【dynamic】 switch is a reasonable case.<br>5. (+0.5) It's a K8s only 【feature】, not for Docker image, so 1 / 2 could be balanced in some level.<br><br>So, I am +0.5 on this PR. : )
@pan3793 are we good to merge?
> Do you think you can add an integration 【test】 case in order to prevent a future regression, @pan3793 ?<br><br>Sure, IT is added.
@Yikun @dongjoon-hyun would you please take a look again?
@cloud-fan The failed GA is unrelated to this PR.
@cloud-fan Thank you!
@rangadi a general question: when I use `dev/connect-gen-protos.sh` to generate protobuf related 【c<font color=blue>hang】es】, it automatically added a lot of lines of `@typing_extensions.final` to different files, should I remove this in my PR?
> when I use dev/connect-gen-protos.sh to generate protobuf related 【c<font color=blue>hang】es】, it automatically added a lot of lines of @typing_extensions.final to different files, should I remove this in my PR?<br><br>No need. That is fine. It is a known problem.
@bogao007 what's your JIRA id? I need to assign you in the JIRA ticket.
> @bogao007 what's your JIRA id? I need to assign you in the JIRA ticket.<br><br>I think this might be my JIRA id `62cbecffa94a6f9c0efe1622`, let me know if it doesn't work.
CI passed w/ small 【c<font color=blue>hang】es】 in UT, kindly ping @cloud-fan
All related 【test】s passed.<br><br>Merged to master.
cc @yaooqinn @cloud-fan
OK, I won't notify you again about this release, but will get in touch when a new version is 【available】. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.<br><br>If you change your mind, just re-open this PR and I'll resolve any conflicts on it.
@mridulm @tgravescs @zhouyejoe @akpatnam25 @shuwang21 Please help 【review】
+CC @attilapiros
so the intention here is it still uses external shuffle service but doesn't store the secret so if node manager goes down it can't recover that, correct?<br>  <br>Personally I'm fine with adding this option, you also need to add 【documentation】 for this new configuration in .md file with explanation why you would want to disable it.  I'm wondering if we want something in the 【security】 doc about it.
> so the intention here is it still uses external shuffle service but doesn't store the secret so if node manager goes down it can't recover that, correct?<br><br>Yes. Since, the secret is not encrypted, it is a 【security】 violation for some applications. If the NM goes down, the metadata of these apps will not be recovered but that is acceptable. <br><br>> you also need to add 【documentation】 for this new configuration in .md file with explanation why you would want to disable it. I'm wondering if we want something in the 【security】 doc about it.<br><br>I updated the `configuration.md` and `【security】.md` with the config. Let me know if I should add more explanation in the 【security】 doc.
Exactly that usecase @tgravescs.<br>We have usecases where applications require heightened 【security】, and are fine with the recomputation cost due to lost shuffle data.
As we can run multiple external shuffle services on the same NM what about introducing just a new config for the whole external shuffle service controlling whether recovery is enabled.<br><br>Then those applications with higher 【security】 can use the external shuffle service where the recovery is disabled.<br><br>I think regarding code complexity and code size it would be a much better solution. WDYT?
> As we can run multiple external shuffle services on the same NM what about introducing just a new config for the whole external shuffle service controlling whether recovery is enabled.<br>> <br>> Then those applications with higher 【security】 can use the external shuffle service where the recovery is disabled.<br><br>It can be done but setting up and running  another shuffle service requires much more effort at least in our production environment. <br><br>> I think regarding code complexity and code size it would be a much better solution. WDYT?<br><br>The code change here is quite small. Agreed that disabling recovery for the whole ESS may need much smaller change but there is not much complexity with this change as well.<br>
【Thanks】 for fixing this @otterc !<br>【Thanks】 for the 【review】s @tgravescs, @zhouyejoe :-)
cc @HyukjinKwon FYI
Merged to master for Apache Spark 3.5.0.<br>Thank you, @panbingkun , @LuciferYang , @Hisoka-X .
> To @panbingkun , if you don't mind, please don't expose your internal fork info in Apache Spark repository. For example, the PR description will be a commit log in Apache Spark repository, so we had better remove those information from your PR description like your company's Spark 4-digit version name and company-name postfix.<br>> <br>> <br>> <br>> FYI, for this PR, I 【clean】 them up.<br><br> OK,【Thanks】.
cc. @HyukjinKwon <br>Could you please help looking into this, especially the point of backward 【compatibility】 on PySpark side? I don't see the way to do that in PySpark but I'm not an expert on Python so I might miss something. 【Thanks】 in advance!<br><br>cc. @zsxwing @viirya @rangadi @jerrypeng
cc @sunchao
~~Does it mean we drop support for building against vanilla Hadoop3 client?~~<br><br>https://github.com/apache/spark/blob/09a43531d30346bb7c8d213822513dc35c70f82e/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L118-L124<br><br>Update: leave it, https://github.com/apache/spark/pull/33160 didn't get in, Spark does not support for building against vanilla Hadoop3 client
> ~Does it mean we drop support for building against vanilla Hadoop3 client?~<br>> <br>> https://github.com/apache/spark/blob/09a43531d30346bb7c8d213822513dc35c70f82e/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L118-L124<br>> <br>> Update: leave it, #33160 didn't get in, Spark does not support for building against vanilla Hadoop3 client<br><br>friendly ping @sunchao
Some related JIRA: https://issues.apache.org/jira/browse/SPARK-37994
@sunchao so the current supported Hadoop version is 3.2.2+ and 3.3.1+? there is some code for Hadoop 3.0 and 3.1, should we remove it then?
yea, the shaded Hadoop client only work for Hadoop 3.2.2+ and 3.3.1+. I'm not sure if there're people that still use Hadoop 3.0/3.1 with Spark though.<br><br>I'm not aware of any code in Spark that 【specific】ally depend on Hadoop 3.0/3.1. Could you point to them for me?
> I'm not aware of any code in Spark that 【specific】ally depend on Hadoop 3.0/3.1. Could you point to them for me?<br><br>@sunchao I find two examples<br><br>- SPARK-13704 added a workaround for [YARN-9332](https://issues.apache.org/jira/browse/YARN-9332) (【<font color=blue>fix】ed】 in Hadoop 3.2.1/3.3.0)<br>- SPARK-32256 added a workaround for [HADOOP-14067](https://issues.apache.org/jira/browse/HADOOP-14067) (【<font color=blue>fix】ed】 in Hadoop 3.1.0)
No strong opinion on this, but we should make it clear that this PR is explicitly dropping support for Hadoop 3.0/3.1 and earlier versions of 3.2<br><br>cc @mridulm
@xkrogen @sunchao @pan3793 I would like to clarify, actually, no longer using Hadoop 3.0/3.1 support for build ant 【test】 is not the original intention of this PR.<br><br>So if there is an way to build and 【test】 Hadoop 3.0/3.1 【success】fully before this pr, but it loses after this pr,  I think we should stop this work because Apache Spark has not previously stated on any occasion that it no longer supports Hadoop 3.0/3.1, right ?<br><br><br>@xkrogen @sunchao @pan3793 Can you give a command that can be used for build & 【test】 with Hadoop 3.0/3.1? I want to manually 【check】 it, thanks ~<br>
> So if there is an way to build and 【test】 Hadoop 3.0/3.1 【success】fully before this pr, but it loses after this pr, I think we should stop this work because Apache Spark has not previously stated on any occasion that it no longer supports Hadoop 3.0/3.1, right ?<br><br>Yes, I think that's probably a sensible thing to do. <br><br>> @xkrogen @sunchao @pan3793 Can you give a command that can be used for build & 【test】 with Hadoop 3.0/3.1? I want to manually 【check】 it, thanks ~<br><br>You can 【check】 this JIRA for the command to build: https://issues.apache.org/jira/browse/SPARK-37994
> > So if there is an way to build and 【test】 Hadoop 3.0/3.1 【success】fully before this pr, but it loses after this pr, I think we should stop this work because Apache Spark has not previously stated on any occasion that it no longer supports Hadoop 3.0/3.1, right ?<br>> <br>> Yes, I think that's probably a sensible thing to do.<br>> <br>> > @xkrogen @sunchao @pan3793 Can you give a command that can be used for build & 【test】 with Hadoop 3.0/3.1? I want to manually 【check】 it, thanks ~<br>> <br>> You can 【check】 this JIRA for the command to build: https://issues.apache.org/jira/browse/SPARK-37994<br><br>I encountered the following error while compiling `hadoop-cloud` module during build with hadoop 3.1.x:<br><br>```<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:34: 【value】 hasPathCapability is not a member of org.apache.hadoop.fs.FileContext<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:34: not found: 【value】 CommonPathCapabilities<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:52: 【value】 abort is not a member of org.apache.hadoop.fs.FSDataOutputStream<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:66: 【value】 abort is not a member of org.apache.hadoop.fs.FSDataOutputStream<br>```<br><br>Due to the 【<font color=blue>fix】ed】 version of HADOOP-15691 being `<br>3.3.0, 3.2.2, 3.2.3` and the 【<font color=blue>fix】ed】 version of HADOOP-16906 being `3.3.1`, so it is definitely not possible to build hadoop-cloud` module using Hadoop 3.1. x. I would like to remove this module to continue my experiment<br><br>
convert to draft to avoid accidental merging
@xkrogen @sunchao @pan3793 Synchronize my experimental results<br><br>1. Before building, we need to add the following content to `resource-managers/yarn/pom.xml` refer to https://github.com/apache/spark/pull/33160/files: <br><br>```<br>    <profile><br>      <id>no-shaded-hadoop-client</id><br>      <dependencies><br>        <dependency><br>          <groupId>org.apache.hadoop</groupId><br>          <artifactId>hadoop-yarn-api</artifactId><br>        </dependency><br>        <dependency><br>          <groupId>org.apache.hadoop</groupId><br>          <artifactId>hadoop-yarn-common</artifactId><br>        </dependency><br>        <dependency><br>          <groupId>org.apache.hadoop</groupId><br>          <artifactId>hadoop-yarn-server-web-proxy</artifactId><br>        </dependency><br>        <dependency><br>          <groupId>org.apache.hadoop</groupId><br>          <artifactId>hadoop-yarn-client</artifactId><br>        </dependency><br>        <dependency><br>          <groupId>org.apache.hadoop</groupId><br>          <artifactId>hadoop-yarn-server-resourcemanager</artifactId><br>          <scope>【test】</scope><br>        </dependency><br>        <dependency><br>          <groupId>org.apache.hadoop</groupId><br>          <artifactId>hadoop-yarn-server-【test】s</artifactId><br>          <classifier>【test】s</classifier><br>          <scope>【test】</scope><br>        </dependency><br>      </dependencies><br>    </profile><br>```<br><br>otherwise, the following compilation error will occurred with `-Pyarn`:<br><br>```<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala:30: object MiniYARNCluster is not a member of package org.apache.hadoop.yarn.server<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala:65: not found: type MiniYARNCluster<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/BaseYarnClusterSuite.scala:111: not found: type MiniYARNCluster<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:38: object resourcemanager is not a member of package org.apache.hadoop.yarn.server<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:39: object resourcemanager is not a member of package org.apache.hadoop.yarn.server<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:40: object resourcemanager is not a member of package org.apache.hadoop.yarn.server<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:41: object resourcemanager is not a member of package org.apache.hadoop.yarn.server<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:249: not found: type RMContext<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:251: not found: type RMApp<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:260: not found: type RMApplicationHistoryWriter<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:262: not found: type SystemMetricsPublisher<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:266: not found: type RMAppManager<br>[ERROR] [Error] /${spark-source-dir}/resource-managers/yarn/src/【test】/scala/org/apache/spark/deploy/yarn/ClientSuite.scala:271: not found: type ClientRMService<br>```<br><br>2. With the above change， master can build 【success】 with hadoop 3.1.x as following<br><br>```<br>build/mvn 【clean】 install -Dhadoop.version=3.1.4 -Dhadoop-client-api.artifact=hadoop-client -Dhadoop-client-runtime.artifact=hadoop-yarn-api -Dhadoop-client-minicluster.artifact=hadoop-client -DskipTests -Pno-shaded-hadoop-client -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive<br><br>```<br><br>Otherwise, cannot build `yarn` module with hadoop 3.1.x.<br><br><br>3. `hadoop-cloud` can't build with hadoop 3.1.x due to https://github.com/apache/spark/pull/40847#issuecomment-1518931685<br><br><br>**Overall, the current master cannot compile `yarn` and `hadoop-cloud` modules using hadoop 3.1.x without any 【c<font color=blue>hang】es】, and all other modules are ok**<br><br>
More<br>1. The conclusion using hadoop 3.0.x and hadoop 3.1.x is the same<br>2. Use hadoop 3.2.x can't build `hadoop-cloud` module too<br>```<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:34: 【value】 ABORTABLE_STREAM is not a member of object org.apache.hadoop.fs.CommonPathCapabilities<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:52: 【value】 abort is not a member of org.apache.hadoop.fs.FSDataOutputStream<br>[ERROR] [Error] /${spark-source-dir}/hadoop-cloud/src/hadoop-3/main/scala/org/apache/spark/internal/io/cloud/AbortableStreamBasedCheckpointFileManager.scala:66: 【value】 abort is not a member of org.apache.hadoop.fs.FSDataOutputStream<br>[ERROR] three errors found<br>```<br><br>3. Currently, only hadoop 3.3.x can build all modules
Interesting, thanks for the detailed 【analysis】 @LuciferYang !<br><br>> Use hadoop 3.2.x can't build hadoop-cloud module too<br><br>This is Hadoop 3.2.2 ? I remember at some point we started to enable `hadoop-cloud` in Spark release, so I wonder why this didn't cause any error back in the time ..
> Interesting, thanks for the detailed 【analysis】 @LuciferYang !<br>> <br>> > Use hadoop 3.2.x can't build hadoop-cloud module too<br>> <br>> This is Hadoop 3.2.2 ? I remember at some point we started to enable `hadoop-cloud` in Spark release, so I wonder why this didn't cause any error back in the time ..<br><br>I 【test】 with hadoop 3.2.4. `AbortableStreamBasedCheckpointFileManager` was introduced in SPARK-40039, and it uses APIs that are only 【available】 in Hadoop 3.3.1+([HADOOP-16906](https://issues.apache.org/jira/browse/HADOOP-16906)  `FSDataOutputStream#abort()`)<br><br>
Just to be clear are we saying this is OK to merge or there are issues with hadoop-cloud?
> No strong opinion on this, but we should make it clear that this PR is explicitly dropping support for Hadoop 3.0/3.1 and earlier versions of 3.2<br><br>@srowen I'm also +1 that we should document clearly the Hadoop client version support strategy<br><br>
cc @sunchao @LuciferYang
@pan3793 I found an interesting thing, after this one merged, when I run the following commands:<br><br>```<br>build/mvn 【clean】 install -DskipTests<br>build/mvn 【test】 -pl connector/connect/client/jvm -D【test】=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite<br>```<br><br>there are 4 【test】 failed as following:<br><br>```<br>- read and write *** FAILED ***<br>  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated<br>  at io.grpc.Status.asRuntimeException(Status.java:535)<br>  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)<br>  at scala.collection.Iterator.foreach(Iterator.scala:943)<br>  at scala.collection.Iterator.foreach$(Iterator.scala:943)<br>  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)<br>  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:458)<br>  at org.apache.spark.sql.DataFrameWriter.executeWriteOperation(DataFrameWriter.scala:257)<br>  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:221)<br>  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:210)<br>  ...<br>- textFile *** FAILED ***<br>  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated<br>  at io.grpc.Status.asRuntimeException(Status.java:535)<br>  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>  at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)<br>  at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)<br>  at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)<br>  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2688)<br>  at org.apache.spark.sql.Dataset.withResult(Dataset.scala:3128)<br>  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2687)<br>  at org.apache.spark.sql.ClientE2ETestSuite.$anonfun$new$12(ClientE2ETestSuite.scala:169)<br>  at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>  ...<br>- write table *** FAILED ***<br>  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated<br>  at io.grpc.Status.asRuntimeException(Status.java:535)<br>  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)<br>  at scala.collection.Iterator.toStream(Iterator.scala:1417)<br>  at scala.collection.Iterator.toStream$(Iterator.scala:1416)<br>  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)<br>  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)<br>  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)<br>  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)<br>  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:471)<br>  ...<br>- write without table or path *** FAILED ***<br>  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated<br>  at io.grpc.Status.asRuntimeException(Status.java:535)<br>  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)<br>  at scala.collection.Iterator.toStream(Iterator.scala:1417)<br>  at scala.collection.Iterator.toStream$(Iterator.scala:1416)<br>  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)<br>  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)<br>  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)<br>  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)<br>  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:471)<br>  ...<br>```<br><br>but  if I revert this one, the the failure will disappear. `CatalogSuite` and `StreamingQuerySuite` also has some problems.Already created [SPARK-43647](https://issues.apache.org/jira/browse/SPARK-43647) to tracking this, Do you have time to investigate togethe?<br><br>
Tests pass if enable `-Phive`<br><br>```<br>build/mvn 【clean】 install -DskipTests -Phive<br>build/mvn 【test】 -pl connector/connect/client/jvm -D【test】=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite<br>```<br><br>Seems reasonable
> Tests pass if enable `-Phive`<br>> <br>> ```<br>> build/mvn 【clean】 install -DskipTests -Phive<br>> build/mvn 【test】 -pl connector/connect/client/jvm -D【test】=none -DwildcardSuites=org.apache.spark.sql.ClientE2ETestSuite<br>> ```<br>> <br>> Seems reasonable<br><br>No, these 【test】 can passed without -Phive without this pr
The error stack in server side as follows:<br><br>```<br>java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated<br>\tat java.util.ServiceLoader.fail(ServiceLoader.java:232)<br>\tat java.util.ServiceLoader.access$100(ServiceLoader.java:185)<br>\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)<br>\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)<br>\tat java.util.ServiceLoader$1.next(ServiceLoader.java:480)<br>\tat scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:46)<br>\tat scala.collection.Iterator.foreach(Iterator.scala:943)<br>\tat scala.collection.Iterator.foreach$(Iterator.scala:943)<br>\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)<br>\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)<br>\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)<br>\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)<br>\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)<br>\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)<br>\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)<br>\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)<br>\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)<br>\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)<br>\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)<br>\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)<br>\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:860)<br>\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)<br>\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)<br>\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2321)<br>\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2091)<br>\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handleCommand(SparkConnectStreamHandler.scala:120)<br>\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:86)<br>\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)<br>\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)<br>\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)<br>\tat org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)<br>\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)<br>\tat org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)<br>\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)<br>\tat org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)<br>\tat org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)<br>\tat org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)<br>\tat org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)<br>\tat org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)<br>\tat java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>\tat java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>\tat java.lang.Thread.run(Thread.java:750)<br>Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc<br>\tat java.lang.Class.getDeclaredConstructors0(Native Method)<br>\tat java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)<br>\tat java.lang.Class.getConstructor0(Class.java:3075)<br>\tat java.lang.Class.newInstance(Class.java:412)<br>\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)<br>\t... 41 more<br>Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc<br>\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)<br>\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)<br>\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)<br>\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)<br>\t... 46 more<br>```
cc @sunchao @yaooqinn
cc @zsxwing @HeartSaVioR @LuciferYang
also cc @sunchao
@pan3793 thanks for 【clean】ing up the code! What does `partially` mean in the PR description?
@zsxwing I mean this PR removes the code added in SPARK-19718 for pre-Hadoop 2.8, reserves the code for Hadoop 2.8+
@pan3793 Could you update the PR description to be more accurate? 【Thanks】!
@zsxwing updated, please take a look again, thanks.
Just curious <br><br>```<br>Remove this restriction from the client and let the service enforce<br>length limits.<br>```<br>Is the server already enforcing the length limits or you plan to have a follow-up to add that?
There is no server-side validation. The limit to 200 characters was mostly something that was done as a 【simple】 protection mechanism. We've looked into different 【specific】ations for what could be part of the user agent and the conclusion was to not have a limit at this point.
Merged to master.<br><br>(since the last change is basically no-op)
GA passed, kindly ping @sunchao
Merged to master, thanks!<br>
cc @sunchao @pan3793
【Thanks】 @sunchao
cc @cloud-fan, @maryannxue
@peter-toth can you briefly explain the idea of fixing it?
> @peter-toth can you briefly explain the idea of fixing it?<br><br>I've updated the PR recently, but the main change is that the CTE accumulator map argument of `buildCTEMap()` changed from <br>`mutable.HashMap.empty[Long, (CTERelationDef, Int)]`<br>to <br>`mutable.SortedMap.empty[Long, (CTERelationDef, Int, mutable.Map[Long, Int])]`.<br>The new `mutable.Map[Long, Int]` part tracks where the references are pointing to from a CTE. (The old `Int` part tracks the \"count of incoming references\".)<br><br>Once we have this extended outer map we can correct the \"count of incoming references\" in `【clean】CTEMap()`. We just need to iterate the CTEs in reverse order (that's why the outer map is now a `SortedMap`) and if we encounter a CTE whose \"count of incoming references\" is 0 then we decrease the referenced CTE's \"count of incoming references\".<br><br>To build the new inner map `buildCTEMap()` has a new `outerCTEId` optional argument.<br><br><br>
【Thanks】 @cloud-fan!
The PR title is broken.
@ueshin Can you please 【review】? 【Thanks】
also cc @tgravescs
@pan3793 jira id should be [SPARK-43202](https://issues.apache.org/jira/browse/SPARK-43202)
> @pan3793 jira id should be [SPARK-43202](https://issues.apache.org/jira/browse/SPARK-43202)<br><br>【<font color=blue>fix】ed】
Merged to master, thanks all!
PTAL! @amaliujia @HyukjinKwon Thank you!
Also verified null query names:<br>```<br>>>> q = spark.readStream.format(\"rate\").load().writeStream.format(\"console\").start()<br>>>> q1 = spark.streams.get(q.id)<br>>>> q1.name<br>''<br>>>> q.name<br>''<br>>>> q.name == q1.name<br>True<br>```
【Thanks】 for the PR. Let's wait and see.
> 【Thanks】 for the PR. Let's wait and see.<br><br>ok
```<br>[info] *** 4 TESTS FAILED ***<br>[error] Failed 【test】s:<br>[error] \torg.apache.spark.sql.UserDefinedFunctionE2ETestSuite<br>[error] (connect-client-jvm / Test / 【test】) sbt.TestsFailedException: Tests un【success】ful<br>[error] Total time: 114 s (01:54), completed Apr 20, 2023 4:32:20 AM<br>```<br><br>https://github.com/LuciferYang/spark/actions/runs/4749019479/jobs/8438173582<br><br>Rerunning these four cases still failed ....
Hmmmm ....
@HyukjinKwon It seems inevitable to fail, not random... <br><br>local run `build/sbt \"connect-client-jvm/【test】` 3 times, all four cases have failed.<br><br>I'll go have lunch first and  investigate in the afternoon ...<br><br>
Seems like it was because of https://github.com/apache/spark/commit/09a43531d30346bb7c8d213822513dc35c70f82e. Let's close this :-). 【Thanks】 for your effort though.
@HyukjinKwon Do you know the maximum memory that GA instances can use? <br><br>I 【check】ed the logs and found that with the same memory configuration, logs similar to `In the last 10 seconds, 5.524 (55.8%) were spent in GC` are printed more times in GA than locally<br><br>
Its 7GB / 2cores IIRC (https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources)
hmm... I suggest add more memory. I have seen new `GC overhead limit exceeded.` on GA today. If 7G is 【available】, we can directly make the mima 【check】 use 5G<br><br>
Can you help me to re open this one ? @HyukjinKwon
thanks @HyukjinKwon
Actually MiMa 【check】 seems fine, and doesn;t seem failing.
@pan3793 I remember you provided an OOM case this morning. Can you provide a link?<br><br>
> @pan3793 I remember you provided an OOM case this morning. Can you provide a link?<br><br>https://github.com/pan3793/spark/actions/runs/4746374013/jobs/8431906344
OK, Let's open this pr for a few days. If the master has  mima 【check】 OOM, there is no need to resubmit<br><br>
OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002
> OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002<br><br>with 5g?
> > OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002<br>> <br>> with 5g?<br><br>No, I didn't change anything, so it will be 4G?
> > > OOM too. https://github.com/Hisoka-X/spark/actions/runs/4751425402/jobs/8441304002<br>> > <br>> > <br>> > with 5g?<br>> <br>> No, I didn't change anything, so it will be 4G?<br><br>4196m now, It seems that 4196m is not 【stable】 enough now<br><br>
Test once more<br><br>
From the GA task logs, it can be seen that using 5g only 1 ~ 2 `for better 【performance】` related logs during mima 【check】(w/o this pr, it will print 30+ times and still a possibility of `GC overhead limit exceeded`)<br><br><br><br>
> Its 7GB / 2cores IIRC (https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners#supported-runners-and-hardware-resources)<br><br>After multiple verifications, I suggest change -Xmx to 5120. From the logs, it will significantly reduce the number of GCs and the amount of time consumed, and there will be no `GC overhead limit exceeded` issue again
@HyukjinKwon <br><br>https://github.com/LuciferYang/spark/actions/runs/4776155966/jobs/8491000795<br><br>See `GC overhead limit exceeded` again<br><br>
mima 【test】 OOM again https://github.com/apache/spark/actions/runs/4783257117/jobs/8503361722
Let's merge this one to avoid oom 【:)】
【Thanks】 @zhengruifeng @HyukjinKwon @pan3793 @Hisoka-X
failure of `UserDefinedFunctionE2ETestSuite ` should be unrelated, but let me take a look
cc @haiyangsun-db FYI
After double thoughts, we'd better not touch Pandas UDF to preserve backward 【compatibility】. Let me close the PR and have a new prototype.
also cc @jchen5 @allisonwang-db
> I think your fix will fix the COUNT(*) is null case, but break the false case<br><br>Hi @jchen5 , can you help add a 【test】 for it in a new PR? The current PR passes all 【test】s which means our 【test】 coverage is not good.
> > I think your fix will fix the COUNT(*) is null case, but break the false case<br>> <br>> Hi @jchen5 , can you help add a 【test】 for it in a new PR? The current PR passes all 【test】s which means our 【test】 coverage is not good.<br><br>If you don't mind, I can also add the required 【test】 cases to my PR. However, after I merged #40811, it seems that the current use case has been able to prove the correctness of this PR.😂
@Hisoka-X do you mean the bug is already 【<font color=blue>fix】ed】 now?
> @Hisoka-X do you mean the bug is already 【<font color=blue>fix】ed】 now?<br><br>Depends on what is correct results of `select *, (select any_【value】(false) as result from t1 where t0.a = t1.c) from t0)` ?<br><br>> if the subquery is something like select false from ... group by c then it will still actually return null on empty inputs.<br><br>The current state is as described by @jchen5  . `select false from ... group by c` will return null on empty inputs, not return `false`. I think this is what @jchen5 worry about. You can 【check】 by https://github.com/apache/spark/pull/40865#【discussion】_r1173210161 .<br><br>If you think `select false from ... group by c` in subquery should return `false` on empty inputs, this PR have bug now. If you think it should return `null` on empty inputs, this PR are correct.<br><br>
Before we make a decision, let's try the query with postgres, oracle, db2, etc.
> Before we make a decision, let's try the query with postgres, oracle, db2, etc.<br><br>Ok. By the way, I 【check】 with postgres, it return `null`<br>![image](https://user-images.githubusercontent.com/32387433/233600204-e254ca5c-daec-43cb-9490-0bf406d5d844.png)<br>
> Depends on what is correct results of select *, (select any_【value】(false) as result from t1 where t0.a = t1.c) from t0) ?<br><br>Yes, this should return null on empty data.<br><br>I will try to find another 【test】 case to 【check】 the potential issue I mentioned. Maybe something like `select *, (select false as result from t1 where t0.a = t1.c limit 1) from t0)`(correct answer for that is also null). (This 【specific】 case isn't currently supported though)
I 【check】ed the case of `any_【value】(false)` in a debugger and it works because resultWithZeroTups is NULL there, so that explains why it works - because there's an aggregation 【value】 around false, and not just constant false.<br><br>This fix seems to work with the cases I've thought of (results match SQL standard semantics and postgres).<br><br>I think this PR is mergeable. I'll plan to add some more 【test】 coverage for related cases in a future PR.
kindly ping @cloud-fan . All CI passed.
@Hisoka-X I think it's clearer to use my example to explain the bug. `NullPropagation` triggers the bug for `(count(1)) is null`, but the root cause is the wrong handling of literals when dealing with the count bug. The bug can be triggered when the scalar subquery has a global aggregate with a constant, which can be produced by `NullPropagation` or by the user query directly.
It's better to have this fix in 3.4, @Hisoka-X can you open a new PR for 3.4 to backport https://github.com/apache/spark/pull/40946 and this PR together?
> It's better to have this fix in 3.4, @Hisoka-X can you open a new PR for 3.4 to backport #40946 and this PR together?<br><br>Please 【check】 #40977
the python linter failed
Good catch
cc @HyukjinKwon @sunchao
@pan3793 good catch, have you seen any issue caused by this?
> @pan3793 good catch, have you seen any issue caused by this?<br><br>Not exactly. Since the current Hive 2.3.9 can talk w/ HMS 2.1+ smoothly (plus [HIVE-25500](https://issues.apache.org/jira/browse/HIVE-25500), the HMS versions expand to 1.2+), it's rare to use a custom Hive client version in our production cases.
the failed 【test】 should be unrelated, but would you mind re-runing it for double 【check】?
@itholic mind rebasing this please?
cc @HyukjinKwon @sunchao @LuciferYang
Actually ... we need a JIRA since this change is in 3.4.1 but the original change remains in 3.4.0 .. so difficult to set the 【<font color=blue>fix】ed】 JRIA version.
Too bad.<br><br>To @cloud-fan , as Hyukjin mentioned, we should not revert like this after the official release.<br><br>cc @sunchao , too.
I've created a new JIRA ticket
actually, `DataFrame.offset` is missing in vanilla pyspark, I will add it in another PR. Since this fix should be also backported to 3.4
except one run failed due to [something unrelated](https://github.com/apache/spark/runs/12992252712), in all other 10 runs the torch 【test】s passed 【success】fully.
GA passed after rerunning failed jobs.
cc @HyukjinKwon @cloud-fan
dup with https://github.com/apache/spark/pull/40860, closed
https://github.com/apache/spark/pull/40408 is related to this. <br>CC @LuciferYang
@LuciferYang Thank you 👍 <br>
@HyukjinKwon will you have a look at this?
> If you don't mind, could you update the PR description with a micro-benchmark like #35679 ?<br><br>#35679 is a PR for \"There is a 【performance】 【optimization】 ...\"<br>This PR is only to fix dependents that did have CVE issues. <br><br>I dont think that benchmarks for this PR is relevant.
I believe we need to see that this PR doesn't cause any perf regression, @bjornjorgensen .
Oh.. I think you have found this https://github.com/tink-crypto/tink-java/issues/6 <br>He is using 1.7.0 <br>https://github.com/ks-yim/tink-bm/blob/960a88d69934b40dcfdd9b2aeb94028c64393a84/build.gradle.kts#LL27C11-L27C11<br>Witch we are also using https://github.com/apache/spark/pull/37473
Here https://github.com/bjornjorgensen/GoogleThinkBenchmark is the code that I have used to 【test】 it.<br>With<br>\"com.google.crypto.tink\" % \"tink\" % \"1.6.1\"<br>```<br>[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...<br>[info] running (fork) org.openjdk.jmh.Main <br>[info] # JMH version: 1.31<br>[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2<br>[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java<br>[info] # VM options: <none><br>[info] # Blackhole mode: full + dont-inline hint<br>[info] # Warmup: 5 iterations, 10 s each<br>[info] # Measurement: 5 iterations, 10 s each<br>[info] # Timeout: 10 min per iteration<br>[info] # Threads: 1 thread, will synchronize iterations<br>[info] # Benchmark mode: Average time, time/op<br>[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt<br>[info] # Run 【progress】: 0.00% complete, ETA 00:08:20<br>[info] # Fork: 1 of 5<br>[info] # Warmup Iteration   1: 79207785.709 ns/op<br>[info] # Warmup Iteration   2: 75605599.346 ns/op<br>[info] # Warmup Iteration   3: 76214904.538 ns/op<br>[info] # Warmup Iteration   4: 76989327.792 ns/op<br>[info] # Warmup Iteration   5: 77099889.177 ns/op<br>[info] Iteration   1: 75762018.553 ns/op<br>[info] Iteration   2: 75379888.865 ns/op<br>[info] Iteration   3: 75498314.789 ns/op<br>[info] Iteration   4: 75024163.500 ns/op<br>[info] Iteration   5: 75680816.338 ns/op<br>[info] # Run 【progress】: 20.00% complete, ETA 00:06:43<br>[info] # Fork: 2 of 5<br>[info] # Warmup Iteration   1: 80245617.368 ns/op<br>[info] # Warmup Iteration   2: 75652339.774 ns/op<br>[info] # Warmup Iteration   3: 77061164.331 ns/op<br>[info] # Warmup Iteration   4: 76287806.765 ns/op<br>[info] # Warmup Iteration   5: 75317346.985 ns/op<br>[info] Iteration   1: 75686301.880 ns/op<br>[info] Iteration   2: 75330525.474 ns/op<br>[info] Iteration   3: 75734112.594 ns/op<br>[info] Iteration   4: 76288954.348 ns/op<br>[info] Iteration   5: 76606136.962 ns/op<br>[info] # Run 【progress】: 40.00% complete, ETA 00:05:02<br>[info] # Fork: 3 of 5<br>[info] # Warmup Iteration   1: 80973643.427 ns/op<br>[info] # Warmup Iteration   2: 77317109.354 ns/op<br>[info] # Warmup Iteration   3: 76322130.439 ns/op<br>[info] # Warmup Iteration   4: 76301836.621 ns/op<br>[info] # Warmup Iteration   5: 77046937.308 ns/op<br>[info] Iteration   1: 76474108.687 ns/op<br>[info] Iteration   2: 77131903.554 ns/op<br>[info] Iteration   3: 76728218.573 ns/op<br>[info] Iteration   4: 77324718.069 ns/op<br>[info] Iteration   5: 76930795.244 ns/op<br>[info] # Run 【progress】: 60.00% complete, ETA 00:03:21<br>[info] # Fork: 4 of 5<br>[info] # Warmup Iteration   1: 80497824.184 ns/op<br>[info] # Warmup Iteration   2: 76670359.000 ns/op<br>[info] # Warmup Iteration   3: 77410211.954 ns/op<br>[info] # Warmup Iteration   4: 76880883.221 ns/op<br>[info] # Warmup Iteration   5: 76903019.008 ns/op<br>[info] Iteration   1: 76878110.405 ns/op<br>[info] Iteration   2: 77073021.462 ns/op<br>[info] Iteration   3: 76818391.687 ns/op<br>[info] Iteration   4: 76985174.577 ns/op<br>[info] Iteration   5: 76645837.389 ns/op<br>[info] # Run 【progress】: 80.00% complete, ETA 00:01:40<br>[info] # Fork: 5 of 5<br>[info] # Warmup Iteration   1: 80435216.312 ns/op<br>[info] # Warmup Iteration   2: 76728526.878 ns/op<br>[info] # Warmup Iteration   3: 76168514.106 ns/op<br>[info] # Warmup Iteration   4: 75383839.180 ns/op<br>[info] # Warmup Iteration   5: 75929712.985 ns/op<br>[info] Iteration   1: 76427345.038 ns/op<br>[info] Iteration   2: 76718093.679 ns/op<br>[info] Iteration   3: 76872120.244 ns/op<br>[info] Iteration   4: 76085370.545 ns/op<br>[info] Iteration   5: 76203878.348 ns/op<br>[info] Result \"com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt\":<br>[info]   76331532.832 ±(99.9%) 488639.588 ns/op [Average]<br>[info]   (min, avg, max) = (75024163.500, 76331532.832, 77324718.069), stdev = 652319.870<br>[info]   CI (99.9%): [75842893.244, 76820172.420] (assumes normal distribution)<br>[info] # Run complete. Total time: 00:08:24<br>[info] REMEMBER: The numbers below are just data. To gain 【reusable】 insights, you need to follow up on<br>[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), 【design】 factorial<br>[info] experiments, perform baseline and negative 【test】s that provide experimental control, make sure<br>[info] the benchmarking environment is safe on JVM/OS/HW level, ask for 【review】s from the domain experts.<br>[info] Do not assume the numbers tell you what you want them to tell.<br>[info] Benchmark                                  Mode  Cnt         Score        Error  Units<br>[info] GoogleThinkBenchmark.【test】AesGcmJceEncrypt  avgt   25  76331532.832 ± 488639.588  ns/op<br>[【success】] Total time: 509 s (08:29), completed May 7, 2023, 7:19:58 PM<br>```<br>With<br>\"com.google.crypto.tink\" % \"tink\" % \"1.9.0\"<br>```<br>[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...<br>[info] running (fork) org.openjdk.jmh.Main <br>[info] # JMH version: 1.31<br>[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2<br>[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java<br>[info] # VM options: <none><br>[info] # Blackhole mode: full + dont-inline hint<br>[info] # Warmup: 5 iterations, 10 s each<br>[info] # Measurement: 5 iterations, 10 s each<br>[info] # Timeout: 10 min per iteration<br>[info] # Threads: 1 thread, will synchronize iterations<br>[info] # Benchmark mode: Average time, time/op<br>[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt<br>[info] # Run 【progress】: 0.00% complete, ETA 00:08:20<br>[info] # Fork: 1 of 5<br>[info] # Warmup Iteration   1: 81974464.393 ns/op<br>[info] # Warmup Iteration   2: 78344967.031 ns/op<br>[info] # Warmup Iteration   3: 77870422.233 ns/op<br>[info] # Warmup Iteration   4: 76366142.015 ns/op<br>[info] # Warmup Iteration   5: 76969131.346 ns/op<br>[info] Iteration   1: 77587424.279 ns/op<br>[info] Iteration   2: 77137913.315 ns/op<br>[info] Iteration   3: 76307498.705 ns/op<br>[info] Iteration   4: 76561784.282 ns/op<br>[info] Iteration   5: 77806261.512 ns/op<br>[info] # Run 【progress】: 20.00% complete, ETA 00:06:42<br>[info] # Fork: 2 of 5<br>[info] # Warmup Iteration   1: 80478869.944 ns/op<br>[info] # Warmup Iteration   2: 77009168.477 ns/op<br>[info] # Warmup Iteration   3: 77253208.562 ns/op<br>[info] # Warmup Iteration   4: 77522236.047 ns/op<br>[info] # Warmup Iteration   5: 77473338.262 ns/op<br>[info] Iteration   1: 77294234.708 ns/op<br>[info] Iteration   2: 77547845.969 ns/op<br>[info] Iteration   3: 77319105.646 ns/op<br>[info] Iteration   4: 77437665.792 ns/op<br>[info] Iteration   5: 76279051.841 ns/op<br>[info] # Run 【progress】: 40.00% complete, ETA 00:05:02<br>[info] # Fork: 3 of 5<br>[info] # Warmup Iteration   1: 81151850.952 ns/op<br>[info] # Warmup Iteration   2: 77319736.377 ns/op<br>[info] # Warmup Iteration   3: 77127278.746 ns/op<br>[info] # Warmup Iteration   4: 77566176.907 ns/op<br>[info] # Warmup Iteration   5: 77195342.962 ns/op<br>[info] Iteration   1: 77081135.108 ns/op<br>[info] Iteration   2: 77275550.108 ns/op<br>[info] Iteration   3: 76869123.466 ns/op<br>[info] Iteration   4: 77654387.488 ns/op<br>[info] Iteration   5: 77896922.171 ns/op<br>[info] # Run 【progress】: 60.00% complete, ETA 00:03:21<br>[info] # Fork: 4 of 5<br>[info] # Warmup Iteration   1: 82232457.566 ns/op<br>[info] # Warmup Iteration   2: 78139230.227 ns/op<br>[info] # Warmup Iteration   3: 78621229.438 ns/op<br>[info] # Warmup Iteration   4: 78189807.516 ns/op<br>[info] # Warmup Iteration   5: 78500250.758 ns/op<br>[info] Iteration   1: 78417104.656 ns/op<br>[info] Iteration   2: 77769043.891 ns/op<br>[info] Iteration   3: 78550508.242 ns/op<br>[info] Iteration   4: 78590966.453 ns/op<br>[info] Iteration   5: 78112666.589 ns/op<br>[info] # Run 【progress】: 80.00% complete, ETA 00:01:40<br>[info] # Fork: 5 of 5<br>[info] # Warmup Iteration   1: 80581520.376 ns/op<br>[info] # Warmup Iteration   2: 77263616.777 ns/op<br>[info] # Warmup Iteration   3: 76836909.809 ns/op<br>[info] # Warmup Iteration   4: 77991064.147 ns/op<br>[info] # Warmup Iteration   5: 76437476.748 ns/op<br>[info] Iteration   1: 77256809.777 ns/op<br>[info] Iteration   2: 78291977.688 ns/op<br>[info] Iteration   3: 77834381.442 ns/op<br>[info] Iteration   4: 77869096.798 ns/op<br>[info] Iteration   5: 77068233.792 ns/op<br>[info] Result \"com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt\":<br>[info]   77512667.749 ±(99.9%) 474041.907 ns/op [Average]<br>[info]   (min, avg, max) = (76279051.841, 77512667.749, 78590966.453), stdev = 632832.384<br>[info]   CI (99.9%): [77038625.841, 77986709.656] (assumes normal distribution)<br>[info] # Run complete. Total time: 00:08:23<br>[info] REMEMBER: The numbers below are just data. To gain 【reusable】 insights, you need to follow up on<br>[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), 【design】 factorial<br>[info] experiments, perform baseline and negative 【test】s that provide experimental control, make sure<br>[info] the benchmarking environment is safe on JVM/OS/HW level, ask for 【review】s from the domain experts.<br>[info] Do not assume the numbers tell you what you want them to tell.<br>[info] Benchmark                                  Mode  Cnt         Score        Error  Units<br>[info] GoogleThinkBenchmark.【test】AesGcmJceEncrypt  avgt   25  77512667.749 ± 474041.907  ns/op<br>[【success】] Total time: 510 s (08:30), completed May 7, 2023, 7:34:13 PM<br>```
Reboot and run with `sbt 'jmh:run -i 10 -wi 10 -f 1 -t 1'`<br>with<br>\"com.google.crypto.tink\" % \"tink\" % \"1.9.0\"<br>```<br>[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...<br>[info] running (fork) org.openjdk.jmh.Main -i 10 -wi 10 -f 1 -t 1<br>[info] # JMH version: 1.31<br>[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2<br>[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java<br>[info] # VM options: <none><br>[info] # Blackhole mode: full + dont-inline hint<br>[info] # Warmup: 10 iterations, 10 s each<br>[info] # Measurement: 10 iterations, 10 s each<br>[info] # Timeout: 10 min per iteration<br>[info] # Threads: 1 thread, will synchronize iterations<br>[info] # Benchmark mode: Average time, time/op<br>[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt<br>[info] # Run 【progress】: 0.00% complete, ETA 00:03:20<br>[info] # Fork: 1 of 1<br>[info] # Warmup Iteration   1: 83028380.132 ns/op<br>[info] # Warmup Iteration   2: 78802359.000 ns/op<br>[info] # Warmup Iteration   3: 79210527.969 ns/op<br>[info] # Warmup Iteration   4: 78695687.141 ns/op<br>[info] # Warmup Iteration   5: 78938831.701 ns/op<br>[info] # Warmup Iteration   6: 78575255.695 ns/op<br>[info] # Warmup Iteration   7: 79112203.724 ns/op<br>[info] # Warmup Iteration   8: 79323400.079 ns/op<br>[info] # Warmup Iteration   9: 78537590.867 ns/op<br>[info] # Warmup Iteration  10: 78413295.234 ns/op<br>[info] Iteration   1: 77933768.721 ns/op<br>[info] Iteration   2: 79192640.055 ns/op<br>[info] Iteration   3: 79182903.512 ns/op<br>[info] Iteration   4: 79111605.283 ns/op<br>[info] Iteration   5: 78859743.535 ns/op<br>[info] Iteration   6: 78318937.016 ns/op<br>[info] Iteration   7: 77986910.411 ns/op<br>[info] Iteration   8: 78648235.734 ns/op<br>[info] Iteration   9: 78609703.445 ns/op<br>[info] Iteration  10: 79423429.968 ns/op<br>[info] Result \"com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt\":<br>[info]   78726787.768 ±(99.9%) 786981.254 ns/op [Average]<br>[info]   (min, avg, max) = (77933768.721, 78726787.768, 79423429.968), stdev = 520539.373<br>[info]   CI (99.9%): [77939806.514, 79513769.022] (assumes normal distribution)<br>[info] # Run complete. Total time: 00:03:21<br>[info] REMEMBER: The numbers below are just data. To gain 【reusable】 insights, you need to follow up on<br>[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), 【design】 factorial<br>[info] experiments, perform baseline and negative 【test】s that provide experimental control, make sure<br>[info] the benchmarking environment is safe on JVM/OS/HW level, ask for 【review】s from the domain experts.<br>[info] Do not assume the numbers tell you what you want them to tell.<br>[info] Benchmark                                  Mode  Cnt         Score        Error  Units<br>[info] GoogleThinkBenchmark.【test】AesGcmJceEncrypt  avgt   10  78726787.768 ± 786981.254  ns/op<br>```<br>with <br>\"com.google.crypto.tink\" % \"tink\" % \"1.6.1\"<br>```<br>[info] compiling 5 Java sources to /home/bjorn/Documents/github/GoogleThinkBenchmark/target/scala-2.12/classes ...<br>[info] running (fork) org.openjdk.jmh.Main -i 10 -wi 10 -f 1 -t 1<br>[info] # JMH version: 1.31<br>[info] # VM version: JDK 17.0.6, OpenJDK 64-Bit Server VM, 17.0.6+10-Ubuntu-1ubuntu2<br>[info] # VM invoker: /usr/lib/jvm/java-17-openjdk-amd64/bin/java<br>[info] # VM options: <none><br>[info] # Blackhole mode: full + dont-inline hint<br>[info] # Warmup: 10 iterations, 10 s each<br>[info] # Measurement: 10 iterations, 10 s each<br>[info] # Timeout: 10 min per iteration<br>[info] # Threads: 1 thread, will synchronize iterations<br>[info] # Benchmark mode: Average time, time/op<br>[info] # Benchmark: com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt<br>[info] # Run 【progress】: 0.00% complete, ETA 00:03:20<br>[info] # Fork: 1 of 1<br>[info] # Warmup Iteration   1: 83260488.298 ns/op<br>[info] # Warmup Iteration   2: 77947987.496 ns/op<br>[info] # Warmup Iteration   3: 78086226.109 ns/op<br>[info] # Warmup Iteration   4: 76767415.580 ns/op<br>[info] # Warmup Iteration   5: 76449646.229 ns/op<br>[info] # Warmup Iteration   6: 78385935.750 ns/op<br>[info] # Warmup Iteration   7: 77999410.016 ns/op<br>[info] # Warmup Iteration   8: 76971635.577 ns/op<br>[info] # Warmup Iteration   9: 76934072.685 ns/op<br>[info] # Warmup Iteration  10: 77180919.354 ns/op<br>[info] Iteration   1: 76979372.431 ns/op<br>[info] Iteration   2: 76777254.260 ns/op<br>[info] Iteration   3: 76748232.038 ns/op<br>[info] Iteration   4: 77181769.492 ns/op<br>[info] Iteration   5: 78160841.664 ns/op<br>[info] Iteration   6: 77396535.208 ns/op<br>[info] Iteration   7: 77519909.492 ns/op<br>[info] Iteration   8: 77570303.233 ns/op<br>[info] Iteration   9: 77661022.992 ns/op<br>[info] Iteration  10: 78231386.313 ns/op<br>[info] Result \"com.googlethinkbenchmark.GoogleThinkBenchmark.【test】AesGcmJceEncrypt\":<br>[info]   77422662.712 ±(99.9%) 782935.090 ns/op [Average]<br>[info]   (min, avg, max) = (76748232.038, 77422662.712, 78231386.313), stdev = 517863.085<br>[info]   CI (99.9%): [76639727.623, 78205597.802] (assumes normal distribution)<br>[info] # Run complete. Total time: 00:03:21<br>[info] REMEMBER: The numbers below are just data. To gain 【reusable】 insights, you need to follow up on<br>[info] why the numbers are the way they are. Use profilers (see -prof, -lprof), 【design】 factorial<br>[info] experiments, perform baseline and negative 【test】s that provide experimental control, make sure<br>[info] the benchmarking environment is safe on JVM/OS/HW level, ask for 【review】s from the domain experts.<br>[info] Do not assume the numbers tell you what you want them to tell.<br>[info] Benchmark                                  Mode  Cnt         Score        Error  Units<br>[info] GoogleThinkBenchmark.【test】AesGcmJceEncrypt  avgt   10  77422662.712 ± 782935.090  ns/op<br>[【success】] Total time: 207 s (03:27), completed May 7, 2023, 9:13:15 PM<br>```<br><br>Almost the same.. Think 1.9.0 is perhaps a bit slower.
Could you put that into the PR description in order to make a commit log, @bjornjorgensen ?
ok, I have added <br>This have be benchmarks 【test】ed <br>With<br>\"com.google.crypto.tink\" % \"tink\" % \"1.6.1\"<br>(min, avg, max) = (75024163.500, 76331532.832, 77324718.069), stdev = 652319.870<br><br>With<br>\"com.google.crypto.tink\" % \"tink\" % \"1.9.0\"<br>(min, avg, max) = (76279051.841, 77512667.749, 78590966.453), stdev = 632832.384<br><br>Almost the same.. Think 1.9.0 is perhaps a bit slower.<br><br>Is it ok or?
Merged to master for Apache Spark 3.5.0.<br>Thank you, @bjornjorgensen and @LuciferYang .
late 【LGTM】
will leave it to @cloud-fan though.
Test first, will update pr  description later
wait https://github.com/apache/spark/pull/40870
【Thanks】 @HyukjinKwon @zhengruifeng @pan3793
+CC @attilapiros who last looked into this.<br>I will circle back to this PR later this week - thanks for detailed jira @yorksity !<br>Can you please fix the PR description as well ? 【Thanks】.
> +CC @attilapiros who last looked into this. I will circle back to this PR later this week - thanks for detailed jira @yorksity ! Can you please fix the PR description as well ? 【Thanks】.<br><br>I 【<font color=blue>fix】ed】 the PR description,<br>Next, I will try to fix the pyspark ut error and supplement the UT for this scenario<br>But it doesn't block the 【discussion】 of this issue<br>
This seems to me not a bug but a 【performance】 【improvement】, right?
If this is a bug please try to reproduce it with a unit 【test】 which fails with old code and passes with the new one!
> This seems to me not a bug but a 【performance】 【improvement】, right?<br><br>No, this is not an 【improvement】, this is a bug. I introduced the reason for this bug in Jira<br>https://issues.apache.org/jira/browse/SPARK-43221<br>
> If this is a bug please try to reproduce it with a unit 【test】 which fails with old code and passes with the new one!<br><br>Yes, I am working on resolving this UT error and adding new UT to protect it
@cloud-fan How are docs done? Attach them to the same PR?<br>The only central place I can see to add this 【feature】 is:<br>https://spark.apache.org/docs/la【test】/sql-ref-identifier.html<br><br>Or we could go into each supported section (e.g. create table) and spell out that identifier() is supported.
FYI the [【test】s that failed](https://github.com/ryan-johnson-databricks/spark/actions/runs/4765599580/jobs/8471553389) are broken upstream -- they also fail when I run them locally on the version of spark/master this PR is currently based on.
You probably also need to generate the golden file for `ProtoToParsedPlanTestSuite`. There is instructions documented in that suite.
@HyukjinKwon Can you merge this when you get a chance? Thank you!
@amaliujia @HyukjinKwon <br>I also changed `ProtoToParsedPlanTestSuite` a little to remove the memory addresses, before the change the 【test】 for streaming table would fail with:<br>```<br>- streaming_table_API_with_options *** FAILED *** (8 milliseconds)<br>[info]   Expected and actual plans do not match:<br>[info]   <br>[info]   === Expected Plan ===<br>[info]   SubqueryAlias primary.tempdb.myStreamingTable<br>[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@752725d9, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@347d8e2a, tempdb.myStreamingTable<br>[info]   <br>[info]   <br>[info]   === Actual Plan ===<br>[info]   SubqueryAlias primary.tempdb.myStreamingTable<br>[info]   +- StreamingRelationV2 primary.tempdb.myStreamingTable, org.apache.spark.sql.connector.catalog.InMemoryTable@a88a5db, [p1=v1, p2=v2], [id#0L], org.apache.spark.sql.connector.catalog.InMemoryCatalog@2c6b362e, tempdb.myStreamingTable<br>```<br>Because the memory address (`InMemoryTable@752725d9`) is different every time it runs. I removed these in the 【test】 suite.<br><br><br>And verified that memory addresses doesn't exist in existing explain files:<br>```<br>wei.liu:~/oss-spark$ cat connector/connect/common/src/【test】/resources/query-【test】s/explain-results/* | grep @<br>wei.liu:~/oss-spark$ <br>```<br><br>PTAL, thank you!
cc @cloud-fan @viirya @RussellSpitzer
Merged to branch-3.3. Thank you all for 【review】ing!
cc @cloud-fan @HyukjinKwon @MaxGekk
Merged to master, thanks for fixing this @pan3793 !<br>【Thanks】 for 【review】 @LuciferYang :-)
If this is not consistent with the initial idea of this ticket, please let me know<br><br>
cc @rangadi
rebased and resolved conflicts
friendly ping @HyukjinKwon can we merge this one?
I'll leave this to you for self-merging, so that you can 【test】 your new permission. Congrats again 【:)】
> I'll leave this to you for self-merging, so that you can 【test】 your new permission. Congrats again 【:)】<br><br>【Thanks】 @HeartSaVioR 【:)】
I'll just help merging this one as it has been here for multiple weeks and we don't want to require this PR to be rebased anymore.
【Thanks】 @LuciferYang , I merged this to master.
【Thanks】 @HeartSaVioR @HyukjinKwon @rangadi ~ <br><br>I have already 【test】ed my new permissions in other pr 【:)】<br><br>
It drops support for building w/ pre Hive 2.3.9, then SPARK-37446 can be reverted.
CC @srowen
Is this possible now that Hadoop 2 support is gone? just 【check】ing what the implications of this change are.<br>Are the Hive.get 【c<font color=blue>hang】es】 needed, or can we batch those 【c<font color=blue>hang】es】 with reverting the Hive <2.3.9 support? I also don't know what the implication of that is.
@srowen <br><br>> Are the `Hive.get` 【c<font color=blue>hang】es】 needed<br><br>Yes, `Hive.get(conf)` triggers the Hive built-in JSON functions initialization, which requires the Jackson 1.x classes.<br><br>@sunchao I suppose Spark does not officially support building against Hive other than 2.3.9, for cases listed in SPARK-37446, it's the vendor's responsibility to port HIVE-21563 into their maintained Hive 2.3.8-[vender-custom-version]
@sunchao can we expect a new release(focus on 【security】) for Hive 2.3? Considering Spark master and all maintained branches use Hive 2.3.9, which was reported some CVEs, from thrift, guava, log4j, jackson, etc.<br><br>Or, Spark should move forward to a new Hive version. (should take much effort and not sure of benefits other than getting rid of CVEs)
@pan3793 AFAIK the development efforts in Hive 【community】 are only in Hive 3.x/4.x at the moment, and the 2.x branch is barely maintained. I can try to start a conversation in the Hive 【community】 to have a new 2.3.10 release and see how it looks like.<br><br>From the long term perspective, it'd be better for Spark to move to Hive 3.x/4.x.<br>
OK, am I right that this does not make Spark any _less_ compatible with any version of Hive that is currently supported (>= 2.3.9)? If so then this is fine
> OK, am I right that this does not make Spark any _less_ compatible with any version of Hive that is currently supported (>= 2.3.9)? If so then this is fine<br><br>Yes.
Conflict with https://github.com/apache/spark/pull/40892, will close this PR
cc @WeichenXu123 @HyukjinKwon @cloud-fan
> This PR will not add a user-facing API or Parameter or Annotation, instead only a private function attribute will be added<br><br>Q: Where is related doc and example usage code for this attribute ? <br><br>Q2: Why it is not a user-facing attribute ? We have 3rd-party library xgboost-spark that uses \"barrier mode\" and we need to make xgboost-spark supports pyspark-connect mode.
> Q: Where is related doc and example usage code for this attribute ?<br><br>I will add it.<br><br><br>> Q2: Why it is not a user-facing attribute ? We have 3rd-party library xgboost-spark that uses \"barrier mode\" and we need to make xgboost-spark supports pyspark-connect mode.<br><br>we can add doc and 【test】 for it, to avoid break it in the future.<br><br>But it is kind of `DeveloperApi`, our usage pattern is pretty limited. Moreover, it's likely to lead to weird behavior/failure to end users, due to the underlying `RDDBarrier` limitation.
> But it is kind of DeveloperApi, our usage pattern is pretty limited. Moreover, it's likely to lead to weird behavior/failure to end users, due to the underlying RDDBarrier limitation.<br><br>Our xgboost users are already aware of the limitations, it is not an issue.<br>note that currently xgboost library( python package) already uses RDD barrier API,<br>in future we need to adapt xgboost with spark connect mode, <br>this means the SQL side barrier flag should also be an user-facing interface.
let me move the functions to sql.pandas.utils, will be less visible then
It seems to me that functions under `python/pyspark/sql/pandas/utils.py` are used internally - in the existing Spark source code only.<br><br>A developer API \"should\" still be called externally, by developers though,  e.g. `semanticHash`.<br><br>So defining a developer API in `utils.py` seems a little unclear to me.<br>
> It seems to me that functions under `python/pyspark/sql/pandas/utils.py` are used internally - in the existing Spark source code only.<br>> <br>> A developer API \"should\" still be called externally, by developers though, e.g. `semanticHash`.<br>> <br>> So defining a developer API in `utils.py` seems a little unclear to me.<br><br>Can we move the `barrier` API definition to proper place ?
> If we're going to have this standalone API, this should work together with other similar API like groupby().applyInPandas.<br><br>I think we won't support other Pandas API `groupby().applyInPandas`:<br><br>- it is non-trivial to support due to limitation of `RDDBarrier`;<br>- the ml side doesn't need them for now;<br><br><br>> It seems to me that functions under python/pyspark/sql/pandas/utils.py are used internally - in the existing Spark source code only.<br><br>> A developer API \"should\" still be called externally, by developers though, e.g. semanticHash.<br><br>That is a good point, what about keeping `barrier` in `pandas/utils.py` and only used it internally like other helper functions? <br><br>@HyukjinKwon @WeichenXu123 @xinrong-meng
Not sure why we want to keep `barrier` as a standalone API if this cannot work together with other similar API. Why don't we just add a param to `mapInPandas` and `mapInArrow`?
> It seems to me that functions under python/pyspark/sql/pandas/utils.py are used internally - in the existing Spark source code only.<br><br>This might be an issue, because the 3rd-party project \"xgboost-spark\" is going to use it. Currently \"xgboost-spark\" project already uses barrier RDD API, and in future we want to make it support spark connect.
I think we (@HyukjinKwon @WeichenXu123 and I) have reach to agreement that this PR (which introduces an `@barrier` annotation) is no longer, and we will keep current `barrier` parameter in `MapInPandas/Arrow`.<br><br>The reason is that barrier UDF is still too 【specific】 to `MapInPandas/Arrow`, due to the limitation of underlying `RDDBarrier`:<br><br>- can not use it within common dataframe operators like `select`, `withColumn`<br>- can not use it within other similar pandas function like `applyInPandas`;<br><br>I will close it in two days if no more comments.<br><br>also cc @mengxr <br>
+1 for the proposal, thanks @zhengruifeng !<br><br>> I think we (@HyukjinKwon @WeichenXu123 and I) have reach to agreement that this PR (which introduces an `@barrier` annotation) is no longer, and we will keep current `barrier` parameter in `MapInPandas/Arrow`.<br><br>
```<br>ERROR: Comparing client jar: /__w/spark/spark/connector/connect/client/jvm/target/scala-2.12/spark-connect-client-jvm-assembly-3.5.0-SNAPSHOT.jar and and sql jar: /__w/spark/spark/sql/core/target/scala-2.12/spark-sql_2.12-3.5.0-SNAPSHOT.jar <br>problems: <br>method fillValue(java.lang.Object,scala.Option)org.apache.spark.sql.Dataset in class org.apache.spark.sql.DataFrameNaFunctions does not have a correspondent in client version<br>Exceptions to binary 【compatibility】 can be added in 'CheckConnectJvmClientCompatibility#【check】MiMaCompatibility'<br>connect-client-jvm module mima 【check】 failed.<br>Error: Process completed with exit code 1.<br>```<br><br>seems Mima also 【check】 `private[sql]`
【Thanks】 for the 【review】s, merged to master
The original PR was merged to 3.4, so this bufgix should also go to branch-3.4.
Mind filing a JIRA please? If it is a bugfix, I think it deserve a JIRA ticket.
merged to master and branch-3.4, thanks
【Thanks】 for adding the JIRA!
@sunchao @LuciferYang
cc @pan3793 @sunchao @HyukjinKwon
@cloud-fan, 【test】s are passing (and running this time 😄). Let me know what you think.
@cloud-fan, updated for your suggestions. Tests ran and are green.
Closing this and will replace with SPARK-43286 and SPARK-43290.
@siying - you might need to enable github actions for the 【test】s to run
@HeartSaVioR - please take a look and merge after builds pass, thx !
@siying 【Thanks】 for your first 【contribution】 to Apache Spark project. Welcome!
@rangadi @pengzhon-db
Stack trace will be added in this ticket https://issues.apache.org/jira/browse/SPARK-43206, after this PR is merged
> CheckConnectJvmClientCompatibility<br><br>Ah thanks! Removed it!
@HyukjinKwon Can you merge this when get a change? Thank you!
The `optional` in command.proto is needed or it throws:<br>```<br>[error] /home/wei.liu/oss-spark/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala:210:19: 【value】 hasExceptionMessage is not a member of org.apache.spark.connect.proto.StreamingQueryCommandResult.ExceptionResult<br>[error]     if (exception.hasExceptionMessage) {<br>[error]                   ^<br>[error] one error found<br>```
The linter build only failed which linter for scala/python got passed. The failure does not seem to be related.<br>https://github.com/WweiL/oss-spark/actions/runs/4805404722/jobs/8551810603
Mind filing a JIRA please?
Please file a JIRA in ASF JIRA (at here https://issues.apache.org/jira/projects/SPARK/issues). See also https://spark.apache.org/contributing.html
> engine that uses `dir()` to generate autocomplete suggestions (e.g. IPython kernel, Databricks Notebooks) will suggest column names on the completion `df.|`<br><br>Sure, create one ASF JIRA: https://issues.apache.org/jira/browse/SPARK-43270
Have to mention, this solution is not perfect solution: <br><br>dir won't return private method, so if a column start with an _, it would be ignored in the suggestion
Looks fine to me.
cc @viirya @ueshin @holdenk
Looks okay to me.
Made a followup to implement this in Spark Connect: https://github.com/apache/spark/pull/41009
@cloud-fan @MaxGekk @hvanhovell Hi, PTAL. 【Thanks】!
Hi @holdenk could you please take a look, thanks!
@mridulm Help take a look?
Hey, I think we shouldn't just keep fixing it without knowing the cause. When does this happen?
@HyukjinKwon `message` is an optional parameter when creating an `Exception`/`Throwable` and can be `null`.<br>(The default`Throwable()`, `Throwable(String message)`. etc  can result in `null` message).<br><br>This change was identified during 【review】 of the earlier PR, and is to harden our codebase against accidental NPE's
Can you fix the conflict @warrenzhu25 ? We can merge it after that.
> Can you fix the conflict @warrenzhu25 ? We can merge it after that.<br><br>Done.
Merged to master.<br>【Thanks】 for fixing this @warrenzhu25 !<br>【Thanks】 for 【review】 @HyukjinKwon :-)
To me it seams like we can just add `show_counts` to this function. We already have this max row to calculate on.   <br><br>Or we can implement something like this..<br><br>```<br>from collections import Counter<br>from pyspark.sql.functions import col, count, when<br><br>def spark_info(df):<br>    # Print basic DataFrame information<br>    print(f\"<class '{df.__class__.__module__}.{df.__class__.__name__}'>\")<br>    print(f\"Number of rows: {df.count()}\")<br>    print(f\"Number of columns: {len(df.columns)}\")<br><br>    # Print column header for the detailed DataFrame information<br>    print(\"\<br>Column\" + \" \" * 110 + \"Non-Null Count\" + \" \" + \"Dtype\")<br>    print(\"-\" * 6, \" \" * 108, \"-\" * 14, \"-\" * 5)<br><br>    # Calculate non-null counts for each column<br>    non_null_counts = df.agg(*[count(when(col(f\"`{c}`\").isNotNull(), f\"`{c}`\")).alias(c) for c in df.columns]).collect()[0]<br><br>    # Initialize a counter to store data type counts<br>    dtype_counter = Counter()<br><br>    # Iterate through the schema fields and print detailed column information<br>    for i, field in enumerate(df.schema.fields):<br>        non_null_count = non_null_counts[field.name]<br>        dtype = field.dataType.【simple】String()<br>        print(f\"{field.name:<90} {non_null_count:>30} non-null {dtype}\")<br><br>        # Update the data type counter<br>        dtype_counter[dtype] += 1<br><br>    # Print data type 【summary】<br>    dtypes_【summary】 = \ \".join([f\"{dtype}({count})\" for dtype, count in dtype_counter.items()])<br>    print(f\"\<br>dtypes: {dtypes_【summary】}\")<br> ```<br><br>![image](https://user-images.githubusercontent.com/47577197/233838325-b1b7b5ef-b358-4c41-a20c-f841f3484d2c.png)<br>(...)<br><br>![image](https://user-images.githubusercontent.com/47577197/233838368-5599bfe9-2a05-44d6-b583-cd2bbb444127.png)<br>
add Counter to imports <br>```<br>from collections import defaultdict, namedtuple, Counter<br><br>def info(<br>        self,<br>        verbose: Optional[bool] = None,<br>        buf: Optional[IO[str]] = None,<br>        max_cols: Optional[int] = None,<br>    ) -> None:       <br>        # To avoid pandas' existing config affects pandas-on-Spark.<br>        # TODO: should we have corresponding pandas-on-Spark configs?<br>        #with pd.option_context(<br>        #    \"display.max_info_columns\ sys.maxsize, \"display.max_info_rows\ sys.maxsize<br>        #):<br>        if verbose is None or verbose:<br>            index_type: Type = type(self.index).__name__<br>            print(f\"<class '{self.__class__.__module__}.{self.__class__.__name__}'>\")<br>            print(f\"{index_type}: {len(self)} entries, {self.index.min()} to {self.index.max()}\")<br><br>            # Print column header for the detailed DataFrame information<br>            print(f\"Data columns (total {len(self.columns)} columns):\")<br>            print(f\" #   Column{' ' * 106}Non-Null Count  Dtype\")<br>            print(f\"---  ------{' ' * 106}--------------  -----\")<br><br>        # Calculate non-null counts for each column<br>        non_null_counts: Dict[str, int] = self.count().to_dict()<br><br>        # Initialize a counter to store data type counts<br>        dtype_counter: Counter = Counter()<br><br>        # Iterate through the schema fields and print detailed column information<br>        for idx, column in enumerate(self.columns):<br>            dtype: str = str(self[column].dtype)<br>            non_null_count: int = non_null_counts[column]<br>            if verbose is None or verbose:<br>                print(f\"{idx:<3} {column:<90} {non_null_count:>30} non-null {dtype}\")<br><br>            # Update the data type counter<br>            dtype_counter[dtype] += 1<br><br>        if verbose is None or verbose:<br>            # Print data type 【summary】<br>            dtypes_【summary】: str = \ \".join([f\"{dtype}({count})\" for dtype, count in dtype_counter.items()])<br>            print(f\"\<br>dtypes: {dtypes_【summary】}\")<br>        elif not verbose:<br>            print(f\"<class '{self.__class__.__module__}.{self.__class__.__name__}'>\")<br>            print(f\"Index: {len(self)} entries, {self.index.min()} to {self.index.max()}\")<br>            print(f\"Columns: {len(self.columns)} entries, {self.columns[0]} to {self.columns[-1]}\")<br>            dtypes_【summary】: str = \ \".join([f\"{dtype}({count})\" for dtype, count in dtype_counter.items()])<br>            print(f\"dtypes: {dtypes_【summary】}\")<br><br>```
@cloud-fan Please help to 【review】. 【Thanks】.
Do 3.3/3.4/master have the same issue?
> Do 3.3/3.4/master have the same issue?<br><br>Spark [3.3](https://github.com/apache/spark/blob/8f0c75cbbab0cb76a30272a157b4f4cc02cab444/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala#L291) have this issue. And [spark 3.4 and main](https://github.com/apache/spark/blob/b70407eb815ac97f5992b6cf961911e878ea5510/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala#L240) doesn't seem to have this issue. Because the [StatFunctions.scala](https://github.com/apache/spark/pull/38346) is reimplemented and doesn't call the rdd.collect() method.
Oh, you're fixing branch-3.2. It reached EOL, and there won't be more releases in 3.2.x.
I am fine if we can land this to branch-3.3 alone but would need to fix the JIRA's affected version.
> I am fine if we can land this to branch-3.3 alone but would need to fix the JIRA's affected version.<br><br>Sure. I have change the version to branch-3.3. Please help to 【review】 again. 【Thanks】.
【Thanks】 for your 【review】. @cloud-fan Can you help to merge?
thanks, merging to 3.3!
Merged to branch-3.3.
@Knorreman I am not sure how much new things we want to add to the RDD API. The SQL API should be the primary API.
Let us minimize the diff - currently it is fairly large, and it is difficult to reason about what is being changed: most of it is unrelated to the change you are trying to propose.
> Let us minimize the diff - currently it is fairly large, and it is difficult to reason about what is being changed: most of it is unrelated to the change you are trying to propose.<br><br>@mridulm Yes my bad. I ran the ./dev/scalafmt script before pushing. I guess that introduced more 【c<font color=blue>hang】es】 than I wanted.
> @Knorreman I am not sure how much new things we want to add to the RDD API. The SQL API should be the primary API.<br><br>@hvanhovell Every now and then there are stuff added to RDDs. I think it would be benefitial to RDD users to have this type of join 【available】 for some usecases.
@cloud-fan @sunchao @viirya @huaxingao @dongjoon-hyun @gengliangwang, this is a follow-up to PR #40308.
【Thanks】 for 【review】ing, @cloud-fan!
kindly ping @sunchao
Oops almost forgot. Merged to master, thanks!
cc @Yikf @sadikovi @gengliangwang
I think the PR still introduces user-facing 【c<font color=blue>hang】es】. <br>Also, would it be possible to not make any 【c<font color=blue>hang】es】 in Cast and do everything in `df.show` method?
> would it be possible to not make any 【c<font color=blue>hang】es】 in Cast and do everything in df.show method?<br><br>We can by duplicating the code of `Cast`, but I don't think that's a good idea.
I suppose it is fine to have 【c<font color=blue>hang】es】 in Cast. Would it be possible to 【check】 the example queries in my comment in https://github.com/apache/spark/pull/40699 and what results they return? Or let me know if this is ready for 【review】 and I can 【check】 out the PR and try those examples on my machine.
Does this PR need https://github.com/apache/spark/pull/40699? I was under the assumption that we had to revert the original patch and have another solution instead.
Most of the 【c<font color=blue>hang】es】 in https://github.com/apache/spark/pull/40699 are updating 【test】s, and we still need them as we don't revert the behavior change of `df.show`. The behavior change of Cast is reverted and we can tell it from 【test】s: [sql/catalyst/src/【test】/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala](https://github.com/apache/spark/pull/40922/files#diff-7c552f1d8b4654347032ec37209ecc518b82f9b48916c54a18fb4c9bd4aee2d6R787)<br><br>It's ready for 【review】 now.
cc @LuciferYang @AngersZhuuuu @yaooqinn
+1, late 【LGTM】
@LuciferYang can you update.<br><br>Overall this looks good to me. Can you make sure we don't exclude too many cases?
```<br>Error: Exception in thread \"main\" java.lang.IllegalArgumentException: Unsupported class file major version 61<br>\tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:195)<br>\tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:176)<br>\tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:162)<br>\tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:283)<br>\tat org.clapper.classutil.asm.ClassFile$.load(ClassFinderImpl.scala:222)<br>\tat org.clapper.classutil.ClassFinder.classData(ClassFinder.scala:404)<br>\tat org.clapper.classutil.ClassFinder.$anonfun$processOpenZip$2(ClassFinder.scala:359)<br>\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)<br>\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)<br>\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)<br>\tat scala.collection.Iterator.toStream(Iterator.scala:1417)<br>\tat scala.collection.Iterator.toStream$(Iterator.scala:1416)<br>\tat scala.collection.AbstractIterator.toStream(Iterator.scala:1431)<br>\tat scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1417)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>\tat scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>\tat scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>\tat scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>\tat scala.collection.immutable.Stream.filterImpl(Stream.scala:506)<br>\tat scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>\tat scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>\tat scala.collection.generic.Growable.loop$1(Growable.scala:57)<br>\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:61)<br>\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)<br>\tat scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:381)<br>\tat scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:329)<br>\tat scala.collection.TraversableLike.to(TraversableLike.scala:786)<br>\tat scala.collection.TraversableLike.to$(TraversableLike.scala:783)<br>\tat scala.collection.AbstractTraversable.to(Traversable.scala:108)<br>\tat scala.collection.TraversableOnce.toSet(TraversableOnce.scala:360)<br>\tat scala.collection.TraversableOnce.toSet$(TraversableOnce.scala:360)<br>\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:108)<br>\tat org.apache.spark.tools.GenerateMIMAIgnore$.getClasses(GenerateMIMAIgnore.scala:[15](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:16)6)<br>\tat org.apache.spark.tools.GenerateMIMAIgnore$.privateWithin(GenerateMIMAIgnore.scala:57)<br>\tat org.apache.spark.tools.GenerateMIMAIgnore$.main(GenerateMIMAIgnore.scala:1[22](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:23))<br>\tat org.apache.spark.tools.GenerateMIMAIgnore.main(GenerateMIMAIgnore.scala)<br>Error: Process completed with exit code 1.<br>```<br><br>After rebase the code, the `connect-jvm-client-mima-【check】` will run failed as above, it seems that master branch has a dependency compiled by Java 17, let me investigate first.<br><br>
> ```<br>> Error: Exception in thread \"main\" java.lang.IllegalArgumentException: Unsupported class file major version 61<br>> \tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:195)<br>> \tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:176)<br>> \tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:162)<br>> \tat org.objectweb.asm.ClassReader.<init>(ClassReader.java:283)<br>> \tat org.clapper.classutil.asm.ClassFile$.load(ClassFinderImpl.scala:222)<br>> \tat org.clapper.classutil.ClassFinder.classData(ClassFinder.scala:404)<br>> \tat org.clapper.classutil.ClassFinder.$anonfun$processOpenZip$2(ClassFinder.scala:359)<br>> \tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)<br>> \tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)<br>> \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)<br>> \tat scala.collection.Iterator.toStream(Iterator.scala:1417)<br>> \tat scala.collection.Iterator.toStream$(Iterator.scala:1416)<br>> \tat scala.collection.AbstractIterator.toStream(Iterator.scala:1431)<br>> \tat scala.collection.Iterator.$anonfun$toStream$1(Iterator.scala:1417)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>> \tat scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>> \tat scala.collection.immutable.Stream.$anonfun$$plus$plus$1(Stream.scala:372)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>> \tat scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>> \tat scala.collection.immutable.Stream.filterImpl(Stream.scala:506)<br>> \tat scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>> \tat scala.collection.immutable.Stream$.$anonfun$filteredTail$1(Stream.scala:1260)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)<br>> \tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)<br>> \tat scala.collection.generic.Growable.loop$1(Growable.scala:57)<br>> \tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:61)<br>> \tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)<br>> \tat scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:381)<br>> \tat scala.collection.immutable.Set$SetBuilderImpl.$plus$plus$eq(Set.scala:329)<br>> \tat scala.collection.TraversableLike.to(TraversableLike.scala:786)<br>> \tat scala.collection.TraversableLike.to$(TraversableLike.scala:783)<br>> \tat scala.collection.AbstractTraversable.to(Traversable.scala:108)<br>> \tat scala.collection.TraversableOnce.toSet(TraversableOnce.scala:360)<br>> \tat scala.collection.TraversableOnce.toSet$(TraversableOnce.scala:360)<br>> \tat scala.collection.AbstractTraversable.toSet(Traversable.scala:108)<br>> \tat org.apache.spark.tools.GenerateMIMAIgnore$.getClasses(GenerateMIMAIgnore.scala:[15](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:16)6)<br>> \tat org.apache.spark.tools.GenerateMIMAIgnore$.privateWithin(GenerateMIMAIgnore.scala:57)<br>> \tat org.apache.spark.tools.GenerateMIMAIgnore$.main(GenerateMIMAIgnore.scala:1[22](https://github.com/LuciferYang/spark/actions/runs/4998621687/jobs/8954833741#step:14:23))<br>> \tat org.apache.spark.tools.GenerateMIMAIgnore.main(GenerateMIMAIgnore.scala)<br>> Error: Process completed with exit code 1.<br>> ```<br>> <br>> After rebase the code, the `connect-jvm-client-mima-【check】` will run failed as above, it seems that master branch has a dependency compiled by Java 17, let me investigate first.<br><br>change of [9620361](https://github.com/apache/spark/pull/40925/commits/9620361a677013725befabaa262603f7a450ff32) will fix this issue, I will make a separate pr to 【upgrade】 the asm version of tools module, this submitted just to verify that `connect-jvm-client-mima-【check】` can execute 【success】fully with this one<br><br><br><br>
> Can you make sure we don't exclude too many cases?<br><br>Will double 【check】 this later
CI passed. @zhengruifeng Could you take an another look when you find some time?
to be more consistent with https://github.com/apache/spark/blob/a45affe3c8e7a724aea7dbbc1af08e36001c7540/python/pyspark/sql/column.py#L924-L932 <br><br>what about changing to?<br><br>```<br>        if isinstance(length, Column):<br>            length_expr = length._expr<br>            start_expr = startPos._expr<br>        elif isinstance(length, int):<br>            length_expr = LiteralExpression._from_【value】(length)<br>            start_expr = LiteralExpression._from_【value】(startPos)<br>        else:<br>            raise PySparkTypeError(<br>                error_class=\"NOT_COLUMN_OR_INT\<br>                message_parameters={\"arg_name\": \"length\ \"arg_type\": type(length).__name__},<br>            )          <br><br>```
> what about changing to?<br><br>@zhengruifeng Nice! Just applied the comment
oh, the linter fails
Fixed!
@majdyz Can you enable GA first refer to <br><img width=\"1347\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/234031906-ad7fa49e-209b-4369-888a-e81a1299943d.png\"><br>https://github.com/apache/spark/pull/40929/【check】s?【check】_run_id=12977948949
@LuciferYang 【Thanks】; It's already enabled now.
This PR fails on <br>```<br>Error:  running /home/runner/work/spark/spark/dev/mima -Phadoop-3 -Pdocker-integration-【test】s -Pyarn -Pkubernetes -Pspark-ganglia-lgpl -Pmesos -Pconnect -Phive -Phadoop-cloud -Pkinesis-asl -Phive-thriftserver -Pvolcano ; received return code 1<br>```<br><br>but there is no error messages about how to fix. I run this locally which output nothing as well.<br><br><br>Stuck on mima stuff now,
Trying to exclude the MIMA 【check】 for `common-utils`.
ok looks like we also have 【compatibility】 【check】s on Spark-core<br>```<br>[error] spark-core: Failed binary 【compatibility】 【check】 against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4036)<br>[error]  * interface org.apache.spark.QueryContext does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.QueryContext\")<br>[error]  * class org.apache.spark.SparkException does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.SparkException\")<br>[error]  * object org.apache.spark.SparkException does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.SparkException$\")<br>[error]  * interface org.apache.spark.SparkThrowable does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.SparkThrowable\")<br>```<br><br>So I choose to exclude these classes.
`【continuous】-integration/appveyor/pr` seems to be stuck.
Hi @LuciferYang , do you have any idea why mima fails for this PR? The error message says nothing. 【Thanks】!
Looking
@amaliujia @cloud-fan <br><br>https://github.com/apache/spark/blob/b461cdea92ea08ce39bb3c9d733f0af7c56abf8d/project/SparkBuild.scala#L404-L407<br><br>should add `commonUtils` to this Seq now due to 3.4.0 no `common-utils` module:<br><br>```<br>[error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0<br>[error]   Not found<br>[error]   Not found<br>[error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml<br>[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom<br>[error] \tat lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:344)<br>[error] \tat lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:313)<br>[error] \tat scala.util.Either$LeftProjection.map(Either.scala:573)<br>[error] \tat lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:313)<br>[error] \tat sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)<br>[error] \tat sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:59)<br>[error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:133)<br>[error] \tat sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)<br>[error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:146)<br>[error] \tat scala.util.control.Exception$Catch.apply(Exception.scala:228)<br>[error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:146)<br>[error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:127)<br>[error] \tat sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)<br>[error] \tat sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:160)<br>[error] \tat sbt.Classpaths$.$anonfun$updateTask0$1(Defaults.scala:3687)<br>[error] \tat scala.Function1.$anonfun$compose$1(Function1.scala:49)<br>[error] \tat sbt.internal.util.$tilde$【great】er.$anonfun$$u2219$1(TypeFunctions.scala:62)<br>[error] \tat sbt.std.Transform$$anon$4.work(Transform.scala:68)<br>[error] \tat sbt.Execute.$anonfun$submit$2(Execute.scala:282)<br>[error] \tat sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)<br>[error] \tat sbt.Execute.work(Execute.scala:291)<br>[error] \tat sbt.Execute.$anonfun$submit$1(Execute.scala:282)<br>[error] \tat sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)<br>[error] \tat sbt.CompletionService$$anon$2.call(CompletionService.scala:64)<br>[error] \tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>[error] \tat java.util.【concurrent】.Executors$RunnableAdapter.call(Executors.java:511)<br>[error] \tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>[error] \tat java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>[error] \tat java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>[error] \tat java.lang.Thread.run(Thread.java:750)<br>[error] (oldDeps / update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0<br>[error]   Not found<br>[error]   Not found<br>[error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml<br>[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom<br>```
> @amaliujia @cloud-fan<br>> <br>> https://github.com/apache/spark/blob/b461cdea92ea08ce39bb3c9d733f0af7c56abf8d/project/SparkBuild.scala#L404-L407<br>> <br>> should add `commonUtils` to this Seq now due to 3.4.0 no `common-utils` module:<br>> <br>> ```<br>> [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0<br>> [error]   Not found<br>> [error]   Not found<br>> [error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml<br>> [error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom<br>> [error] \tat lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:344)<br>> [error] \tat lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:313)<br>> [error] \tat scala.util.Either$LeftProjection.map(Either.scala:573)<br>> [error] \tat lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:313)<br>> [error] \tat sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)<br>> [error] \tat sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:59)<br>> [error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:133)<br>> [error] \tat sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)<br>> [error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:146)<br>> [error] \tat scala.util.control.Exception$Catch.apply(Exception.scala:228)<br>> [error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:146)<br>> [error] \tat sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:127)<br>> [error] \tat sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)<br>> [error] \tat sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:160)<br>> [error] \tat sbt.Classpaths$.$anonfun$updateTask0$1(Defaults.scala:3687)<br>> [error] \tat scala.Function1.$anonfun$compose$1(Function1.scala:49)<br>> [error] \tat sbt.internal.util.$tilde$【great】er.$anonfun$$u2219$1(TypeFunctions.scala:62)<br>> [error] \tat sbt.std.Transform$$anon$4.work(Transform.scala:68)<br>> [error] \tat sbt.Execute.$anonfun$submit$2(Execute.scala:282)<br>> [error] \tat sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)<br>> [error] \tat sbt.Execute.work(Execute.scala:291)<br>> [error] \tat sbt.Execute.$anonfun$submit$1(Execute.scala:282)<br>> [error] \tat sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)<br>> [error] \tat sbt.CompletionService$$anon$2.call(CompletionService.scala:64)<br>> [error] \tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>> [error] \tat java.util.【concurrent】.Executors$RunnableAdapter.call(Executors.java:511)<br>> [error] \tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>> [error] \tat java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>> [error] \tat java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>> [error] \tat java.lang.Thread.run(Thread.java:750)<br>> [error] (oldDeps / update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-common-utils_2.12:3.4.0<br>> [error]   Not found<br>> [error]   Not found<br>> [error]   not found: /Users/yangjie01/.ivy2/local/org.apache.spark/spark-common-utils_2.12/3.4.0/ivys/ivy.xml<br>> [error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-common-utils_2.12/3.4.0/spark-common-utils_2.12-3.4.0.pom<br>> ```<br><br>with this change and rebase the code, then run the mima 【check】:<br><br><br>```<br>[warn] multiple main classes detected: run 'show discoveredMainClasses' to see the list<br>[info] spark-examples: mimaPreviousArtifacts not set, not analyzing binary 【compatibility】<br>[error] spark-core: Failed binary 【compatibility】 【check】 against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4035)<br>[error]  * interface org.apache.spark.QueryContext does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.QueryContext\")<br>[error]  * class org.apache.spark.SparkException does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.SparkException\")<br>[error]  * object org.apache.spark.SparkException does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.SparkException$\")<br>[error]  * interface org.apache.spark.SparkThrowable does not have a correspondent in current version<br>[error]    filter with: ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.SparkThrowable\")<br>[error] java.lang.RuntimeException: Failed binary 【compatibility】 【check】 against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4035)<br>[error] \tat scala.sys.package$.error(package.scala:30)<br>[error] \tat com.typesafe.tools.mima.plugin.SbtMima$.reportModuleErrors(SbtMima.scala:89)<br>[error] \tat com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2(MimaPlugin.scala:36)<br>[error] \tat com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$2$adapted(MimaPlugin.scala:26)<br>[error] \tat scala.collection.Iterator.foreach(Iterator.scala:943)<br>[error] \tat scala.collection.Iterator.foreach$(Iterator.scala:943)<br>[error] \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)<br>[error] \tat com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$1(MimaPlugin.scala:26)<br>[error] \tat com.typesafe.tools.mima.plugin.MimaPlugin$.$anonfun$projectSettings$1$adapted(MimaPlugin.scala:25)<br>[error] \tat scala.Function1.$anonfun$compose$1(Function1.scala:49)<br>[error] \tat sbt.internal.util.$tilde$【great】er.$anonfun$$u2219$1(TypeFunctions.scala:62)<br>[error] \tat sbt.std.Transform$$anon$4.work(Transform.scala:68)<br>[error] \tat sbt.Execute.$anonfun$submit$2(Execute.scala:282)<br>[error] \tat sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)<br>[error] \tat sbt.Execute.work(Execute.scala:291)<br>[error] \tat sbt.Execute.$anonfun$submit$1(Execute.scala:282)<br>[error] \tat sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)<br>[error] \tat sbt.CompletionService$$anon$2.call(CompletionService.scala:64)<br>[error] \tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>[error] \tat java.util.【concurrent】.Executors$RunnableAdapter.call(Executors.java:511)<br>[error] \tat java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>[error] \tat java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>[error] \tat java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>[error] \tat java.lang.Thread.run(Thread.java:750)<br>[error] (core / mimaReportBinaryIssues) Failed binary 【compatibility】 【check】 against org.apache.spark:spark-core_2.12:3.4.0! Found 4 potential problems (filtered 4035)<br>[error] Total time: 92 s (01:32), completed 2023-4-26 13:46:13<br><br>```
@LuciferYang thank you!<br><br>@cloud-fan then probably it goes back to what I did last time. What do you think?
Yea we have to exclude it
CC @pjfanning
if you use jackson 2.14.2 - you can just 【upgrade】 snakeyaml to 2.x (there is a fix in jackson 2.14.2 that makes it easier to 【upgrade】 snakeyaml dependency)<br><br>jackson 2.15.0 has extra 【c<font color=blue>hang】es】 that you need to worry about - see https://issues.apache.org/jira/browse/SPARK-42854
Looks good, just resolve the conflict and we can 【check】 【test】s again
@srowen [CVE-2022-1471](https://nvd.nist.gov/vuln/detail/CVE-2022-1471) is a snakeyaml CVE and you can 【upgrade】 snakeyaml explicitly without upgrading jackson. Jackson 2.15 applies limits on the JSON inputs it parses. It is probably not a good idea to use Jackson 2.15 without supporting overrides for these limits. See [SPARK-42854](https://issues.apache.org/jira/browse/SPARK-42854).
Ah OK, @bjornjorgensen how about just doing snakeyaml here as an intermediate step?
Snakeyaml 1.32 introduced an approx 5mb default limit on input yaml. Spark trunk already has this change but it might be worth considering making this limit configurable using spark configs. The snakeyaml class to look for is called LoaderOptions.
<img width=\"1060\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1475305/234313784-5f6e54f0-5e2b-4725-a1c1-8f62e7cb6d41.png\"><br><br>https://github.com/FasterXML/jackson-core/blob/a2c0bdcfb9aae8fca555240e63e57c1d9e6f8079/src/main/java/com/【fast】erxml/jackson/core/StreamReadConstraints.java#L30-L51<br><br>It seems to have added 4 constraints, but `MAX_BIGINT_SCALE_MAGNITUDE ` is not configurable.<br><br>May be we should add corresponding configurable items to `org.apache.spark.sql.catalyst.json.JSONOptions` and  inject them in `JSONOptions#buildJsonFactory` by `JsonFactoryBuilder#streamReadConstraints(StreamReadConstraints)`.<br><br>This may handle most scenarios, but there are still some places in Spark that call `new ObjectMapper()`, these will using `StreamReadConstraints.defaults()`, but for them, using the default 【value】 may be OK(need to analyze it one by one). <br><br>If there is a problem with what I said, please correct me @pjfanning , thanks <br><br><br><br><br>
And will these new constraints make JSON parsing slower?<br><br>
@LuciferYang your 【analysis】 summarises the situation well
> And will these new constraints make JSON parsing slower?<br><br>there is no evidence of a significant 【performance】 drop
@bjornjorgensen can you engage with the comments? A few of us have basically asked for this PR to be modified or redone but not to proceed as it is.<br><br>* one option is to replace this PR with 1 that 【upgrade】s just snakeyaml and leaves jackson alone<br>* another option is to 【upgrade】 jackson but to also introduce new spark configs so a StreamReadConstraints can be created and set on the ObjectMapper based on the config 【value】s
@pjfanning yes, sorry <br>I have marked it as a draft now.
Please fix the compilation error first @bjornjorgensen thanks<br><br>
I found `Update jackson-module-scala to 2.15.0 in 2.12.x` in release plan of Scala 2.12.18:<br><br>https://github.com/scala/scala/milestone/99
> I found `Update jackson-module-scala to 2.15.0 in 2.12.x` in release plan of Scala 2.12.18:<br>> <br>> https://github.com/scala/scala/milestone/99<br><br>@LuciferYang Core Scala libs don't have a dependency on Jackson. This 【upgrade】 seems to affect a module called `compilerOptionsExporter`. I don't really know what that that is but I doubt whether it has much 【impact】 outside the Scala build.
anyway this is my \"hello world\" in Scala. <br>What is the best way to get this forward? @LuciferYang will you take over? or..
> Ah OK, @bjornjorgensen how about just doing snakeyaml here as an intermediate step?<br><br>Well, upgrading SnakeYAML might work as a temporary solution, but for Spark's long-term benefit, I believe it's best to 【upgrade】 Jackson now. This will make future 【upgrade】s of Jackson much easier. Additionally, Snyk has opened two PRs to 【upgrade】 `com.【fast】erxml.jackson.dataformat:jackson-dataformat-yaml` to version 2.15.0. By upgrading Jackson now, we'll address these issues and won't receive any further request about this.
we can 【upgrade】 it to <br>```<br> private def safeStringToInt(【value】: String, default: Int): Int = {<br>    try {<br>      val intValue = 【value】.toInt<br>      if (intValue >= 0) intValue else default<br>    } catch {<br>      case _: NumberFormatException => default<br>    }<br>  }<br><br><br>  private val maxNestingDepth: Int = parameters<br>    .get(\"maxNestingDepth\")<br>    .map(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_DEPTH))<br>    .getOrElse(StreamReadConstraints.DEFAULT_MAX_DEPTH)<br><br>  private val maxNumLen: Int = parameters<br>    .get(\"maxNumLen\")<br>    .map(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_NUM_LEN))<br>    .getOrElse(StreamReadConstraints.DEFAULT_MAX_NUM_LEN)<br><br>  private val maxStringLen: Int = parameters<br>    .get(\"maxStringLen\")<br>    .map(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_STRING_LEN))<br>    .getOrElse(StreamReadConstraints.DEFAULT_MAX_STRING_LEN)<br><br>```<br><br>Or chatGPT will have this <br><br>Cache frequently used 【value】s to reduce the number of map lookups. For example, in the JSONOptions class, you can create lazy vals for the frequently accessed parameters:<br>```<br>  lazy val samplingRatio: Double = parameters.get(SAMPLING_RATIO).map(_.toDouble).getOrElse(1.0)<br>  lazy val primitivesAsString: Boolean = parameters.get(PRIMITIVES_AS_STRING).map(_.toBoolean).getOrElse(false)<br>  lazy val prefersDecimal: Boolean = parameters.get(PREFERS_DECIMAL).map(_.toBoolean).getOrElse(false)<br> ```<br>In the safeStringToInt method, you can use scala.util.Try to handle the conversion from String to Int:<br>```<br>\tprivate def safeStringToInt(【value】: String, default: Int): Int = {<br>\t\tscala.util.Try(【value】.toInt).filter(_ >= 0).getOrElse(default)<br>\t  }<br>```\t  <br>Consider using getOrElse with a default 【value】 directly instead of map followed by getOrElse for some parameters:<br>```<br>  private val maxNestingDepth: Int = parameters<br>    .getOrElse(\"maxNestingDepth\ StreamReadConstraints.DEFAULT_MAX_DEPTH.toString)<br>    .pipe(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_DEPTH))<br><br>  private val maxNumLen: Int = parameters<br>    .getOrElse(\"maxNumLen\ StreamReadConstraints.DEFAULT_MAX_NUM_LEN.toString)<br>    .pipe(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_NUM_LEN))<br><br>  private val maxStringLen: Int = parameters<br>    .getOrElse(\"maxStringLen\ StreamReadConstraints.DEFAULT_MAX_STRING_LEN.toString)<br>    .pipe(safeStringToInt(_, StreamReadConstraints.DEFAULT_MAX_STRING_LEN))<br>```<br>but it seams to be to mach.. <br><br><br><br>
I think it's overkill to put too much processing into this arg parsing
Thank you all so much for your help and support for this update.
Note that this breaks downstream projects that want to read json:<br><br>```scala<br>spark.read.json(\"file.json\")<br>```<br>```<br>java.lang.NoClassDefFoundError: com/【fast】erxml/jackson/core/StreamReadConstraints<br>\tat org.apache.spark.sql.catalyst.json.JSONOptions.buildJsonFactory(JSONOptions.scala:195)<br>\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:83)<br>...<br>Caused by: java.lang.ClassNotFoundException: com.【fast】erxml.jackson.core.StreamReadConstraints<br>\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)<br>\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)<br>\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)<br>\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)<br>\t... 16 more<br>```<br><br>The reason is that spark-core depends on avro 1.11.1, which pulls in jackson-core 2.12.7:<br>```<br>[INFO] +- org.apache.spark:spark-core_2.12:jar:3.5.0-SNAPSHOT:provided<br>[INFO] |  +- org.apache.avro:avro:jar:1.11.1:provided<br>[INFO] |  |  \\- com.【fast】erxml.jackson.core:jackson-core:jar:2.12.7:provided<br>```<br><br>Project avro has 【upgrade】d to jackson 2.15.0 a few days ago: https://github.com/apache/avro/commit/3b6c6cc43d54ae56b51dacfd6d86d54a0733d57b<br><br>I think for this 【upgrade】 in Spark to work, the avro dependency of spark-core has to be 【upgrade】d to their next release as well.<br><br>My project depends on spark-core and spark-sql only.
@EnricoMi I read and write JSON files using pyspark. I haven't seen any problems so far. <br>
I doubt `spark-submit`, `spark-shell` or `pyspark` are affected. The Spark sources are also not affected:<br><br>```<br>[INFO] org.apache.spark:spark-core_2.12:jar:3.5.0-SNAPSHOT<br>[INFO] +- org.apache.avro:avro:jar:1.11.1:compile<br>[INFO] |  \\- com.【fast】erxml.jackson.core:jackson-core:jar:2.15.0:compile<br>```<br><br>I am talking about Java / Scala projects that depend on `spark-core`, like https://github.com/G-Research/spark-extension.
All wrappers would be affected if any are, as all of this goes through the JVM for JSON processing.<br>Yes, you show that Spark does _not_ pull in older Jackson versions via Avro. So aren't you talking about other projects that may somehow pull in older Jackson, not Spark? I don't quite understand. Spark JSON processing worsk with this change.
Spark pulls in Jackson 2.15.0 for compile scope. Projects depending on Spark do not transitively depend on Jackson 2.15.0.<br><br>Spark depends on Avro which depends on Jackson 2.12.7. Projects that depend on Spark transitively depend on Jackson 2.12.7.<br><br>Maybe spark-core should depend on Jackson 2.15.0 with runtime scope, rather than compile scope.<br><br>The following pom reproduces the issue:<br>```xml<br><project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"><br>  <modelVersion>4.0.0</modelVersion><br>  <groupId>uk.co.gresearch.spark</groupId><br>  <artifactId>spark-example_2.13</artifactId><br>  <version>1.0.0-SNAPSHOT</version><br><br>  <dependencies><br>    <dependency><br>      <groupId>org.apache.spark</groupId><br>      <artifactId>spark-core_2.13</artifactId><br>      <version>3.5.0-SNAPSHOT</version><br>      <scope>provided</scope><br>    </dependency><br>  </dependencies><br><br>  <repositories><br>    <!-- required to resolve Spark snapshot versions --><br>    <repository><br>      <id>apache snapshots</id><br>      <name>Apache Snapshots</name><br>      <url>https://repository.apache.org/snapshots/</url><br>    </repository><br>  </repositories><br></project><br>```<br>```bash<br>mvn dependency:tree<br>```<br>```<br>[INFO] uk.co.gresearch.spark:spark-example_2.13:jar:1.0.0-SNAPSHOT<br>[INFO] \\- org.apache.spark:spark-core_2.13:jar:3.5.0-SNAPSHOT:provided<br>[INFO]    +- org.scala-lang.modules:scala-【parallel】-collections_2.13:jar:1.0.4:provided<br>[INFO]    +- org.apache.avro:avro:jar:1.11.1:provided<br>[INFO]    |  \\- com.【fast】erxml.jackson.core:jackson-core:jar:2.12.7:provided<br>```<br>
They do transitively depend on Jackson, if it's compile scope. Lots of stuff would never work otherwise.<br>runtime scope does not work; we could not compile against Jackson code then, and Spark uses Jackson classes.<br><br>You are correct that there is a conflict that Maven resolves, and its rules may not give the desired effect. However Spark does \"all it can do\" by directly depending on Jackson. <br><br>The only thing I can think of is to also exclude the jackson dep from Avro explicitly in the Spark POM?
> The only thing I can think of is to also exclude the jackson dep from Avro explicitly in the Spark POM?<br><br>But then we will have to write a lot of 【test】es to 【test】 Avro with  jackson version 2.15.0. <br><br>Cant we open a issue in Avro and ask for a new version? There lasts release is from Aug 5, 2022.
That was my initial suggested solution. Once Avro releases a new version, and Spark is compatible with that, the problem should be resolved by upgrading to that new Avro version.
I'm a Jackson committer and 2.15.0 has some big 【c<font color=blue>hang】es】 in it. A number of 【c<font color=blue>hang】es】 have already been made based on some bugs and some tweaking of the 【c<font color=blue>hang】es】 (for a forthcoming v2.15.1 release). I would discourage anyone from making releases that are based on Jackson 2.15.0.
cc @gengliangwang @dongjoon-hyun @viirya @huaxingao @sunchao @cloud-fan @HyukjinKwon
The error messages don't seem related.
【Thanks】 for 【review】ing, @gengliangwang @sunchao @huaxingao @viirya!
【LGTM】2
thanks @LuciferYang for 【review】, merged into master
cc: @WweiL, @pengzhon-db, @LuciferYang, @grundprinzip, @amaliujia, @juliuszsompolski, @HyukjinKwon <br><br>(Fairly broad CC since it adds caching at the service level and is relevant for future life cycle management for queries)
> Solution: This PR adds SparkConnectStreamingQueryCache that does not the following:<br><br>does not the following?<br>
cc @HeartSaVioR too
@amaliujia, could you also take a quick look? I would like to get this merged today.
Test first, will update pr description laster
after adding `set -ex`, the output will be like:<br><br>```<br>~/spark$ ./dev/protobuf-breaking-【c<font color=blue>hang】es】-【check】.sh branch-3.4<br>+ [[ 1 -gt 1 ]]<br>+++ dirname ./dev/protobuf-breaking-【c<font color=blue>hang】es】-【check】.sh<br>++ cd ./dev/..<br>++ pwd<br>+ SPARK_HOME=/home/ruifeng.zheng/spark<br>+ cd /home/ruifeng.zheng/spark<br>+ BRANCH=master<br>+ [[ 1 -eq 1 ]]<br>+ BRANCH=branch-3.4<br>+ pushd connector/connect/common/src/main<br>~/spark/connector/connect/common/src/main ~/spark<br>+ echo 'Start protobuf breaking 【c<font color=blue>hang】es】 【check】ing against branch-3.4'<br>Start protobuf breaking 【c<font color=blue>hang】es】 【check】ing against branch-3.4<br>+ buf breaking --against https://github.com/apache/spark.git#branch=branch-3.4,subdir=connector/connect/common/src/main<br>+ echo 'Finsh protobuf breaking 【c<font color=blue>hang】es】 【check】ing: SUCCESS'<br>Finsh protobuf breaking 【c<font color=blue>hang】es】 【check】ing: SUCCESS<br>+ [[ 0 -ne -0 ]]<br>```
cc @HyukjinKwon  @grundprinzip
Great 【feature】. For example, based on statistics, users can determine whether to merge small files.<br>Support non-partitioned table statistics?<br>
> Support non-partitioned table statistics?<br><br>Yeap, it is supported in this pr.<br>
【Thanks】, merged to master. Test failure unrelated.
【Thanks】 @sunchao @mridulm @pan3793
Don't need to regenerate the golden file?<br><br>
No need, 【test】 results are the same and the config statements don't get output to the golden file. I ran the 【test】 and it passed.
`FileMetadataStructSuite.metadata struct (json): read partial/all metadata struct fields` fails, @databricks-david-lewis
Oof. This is because of the following:<br>```<br>val p1 = new Path(\"file:///a\")<br>val p2 = new Path(\"file:/a\")<br>p1 == p2<br>p1.toString == p2.toString<br>p1.toUri == p2.toUri<br>// but...<br>p1.toUri.toString != p2.toUri.toString  // \"file:///a\" != \"file:/a\"<br>```<br><br>Before any of my 【c<font color=blue>hang】es】, we were doing `new Path(uriString).toString` which would \"normalize\" the uri string.<br>Should we do the same here? Seems silly...<br>I could also update the  【test】 to explicitly pass in `file:/...` ...<br><br>Thoughts @cloud-fan ?
@databricks-david-lewis let's just update the 【test】
Updated @cloud-fan. How do we merge this to the 3.4 branch as well?
Is there a way to reproduce the failure like GA? I always run 【success】fully using Java 17 locally w/o this pr.
@LuciferYang To repro, I set `JAVA_HOME` to `/usr/lib/jvm/java-17-openjdk-amd64/` and made sure that sbt is picking this settings up (these lines should be in the log `Using /usr/lib/jvm/java-17-openjdk-amd64/ as default JAVA_HOME.<br>Note, this will be overridden by -java-home if it is set.`)<br>This was enough to trigger the failure:<br><img width=\"1124\" alt=\"Screenshot 2023-04-25 at 21 42 59\" src=\"https://user-images.githubusercontent.com/21010250/234358778-5de172ae-d8fe-4d54-bf2e-8d76dff1555b.png\"><br>
> @LuciferYang To repro, I set `JAVA_HOME` to `/usr/lib/jvm/java-17-openjdk-amd64/` and made sure that sbt is picking this settings up (these lines should be in the log `Using /usr/lib/jvm/java-17-openjdk-amd64/ as default JAVA_HOME. Note, this will be overridden by -java-home if it is set.`) This was enough to trigger the failure: <img alt=\"Screenshot 2023-04-25 at 21 42 59\" width=\"1124\" src=\"https://user-images.githubusercontent.com/21010250/234358778-5de172ae-d8fe-4d54-bf2e-8d76dff1555b.png\"><br><br>Let me give it a try ~ thanks ~
Merging this unblock JDK 17 build.
@MaxGekk could you help 【review】 this?
+1, 【LGTM】. Merging to master.<br>Thank you, @amousavigourabi.
@amousavigourabi Congratulations with your first 【contribution】 to Apache Spark!
@cloud-fan , it happened since https://github.com/apache/spark/pull/32198 and with 【concurrent】 writer on.
@cloud-fan any comments ?
cc @mridulm , too.
also cc @cloud-fan @hvanhovell @ueshin
Waiting for CI. @liang3zy22 Could you re-trigger GitHub actions:<br>```<br>$ git commit --allow-empty -m \"Trigger build\"<br>```
@LuciferYang can we just move the RemoteClassLoader to spark/core?
SGTM,  Let GA 【test】 it first
@hvanhovell Is it better to keep `ExecutorClassLoader` in `org.apache.spark.repl` or move it to other package, like `org.apache.spark.executor`?<br><br><br><br>
will update pr title and description later if all 【test】 pass<br><br>
Yeah let's put it in `org.apache.spark.executor`.
@LuciferYang can you also get rid of the reflection in Executor.scala that used to be needed?
> @LuciferYang can you also get rid of the reflection in Executor.scala that used to be needed?<br><br>I'll look this today, and I think there should be no need to use reflection anymore.<br><br>
@LuciferYang is this one good to go?
I think this one is ok
Alrighty
Merged!
【Thanks】 @hvanhovell ~
@MaxGekk hi, could you help to 【review】 this?
@MaxGekk Because scala 2.13 mandates handling default case, I replaced this error class with internal error. But `commonNaturalJoinProcessing` is a private method and I can't trigger the default case, so I didn't add any ut. Could you please help me to 【review】 this?
@JinHelin404 Do you have an account the JIRA: https://issues.apache.org/jira/browse/SPARK-43257. If not, could you leave a request, or leave a comment if you have one.
@MaxGekk Yes, and I've just left a comment.
+1, 【LGTM】. Merged to master.<br>Thank you, @JinHelin404 and congratulations with your first 【contribution】 to Apache Spark!
@MaxGekk 【Thanks】!
Could you please file a new JIRA ticket, or add JIRA ticket number as the prefix of PR title? You can see examples https://github.com/apache/spark/pulls.
@bogao007 Can you add some 【test】s and update PR description ?
> Not 【test】ed yet, will perform the 【test】 when I'm back.<br><br>Is this 【test】ed yet? Could you update the PR description?
thanks for the 【review】, merging to master/3.4/3.3!
【Thanks】 for your 【review】 @srowen. I am on vacation and will continue to complete these tasks after May 8th.<br><br>________________________________<br>发件人: Sean Owen ***@***.***><br>发送时间: 2023年5月4日 22:57:27<br>收件人: apache/spark<br>抄送: yangjie01; Author<br>主题: Re: [apache/spark] [SPARK-43294][BUILD] Upgrade zstd-jni to 1.5.5-2 (PR #40962)<br><br><br>@srowen approved this pull request.<br><br>Looks OK, just add context about what the update does<br><br>―<br>Reply to this email directly, view it on GitHub<https://mailshield.baidu.com/【check】?q=4%2fgpZ9TUEilYD9%2fXl%2f%2fdZ08xybHnY39K5GNnRidojzsXOcQrY4G74%2ftGPWda%2f0bG4rBPHY0ERfSeAWNQe0IWpFuPmjDRr01b0B39Jw%3d%3d>, or unsubscribe<https://mailshield.baidu.com/【check】?q=TboBl7gBc9ssIw%2fYtfvVtyVmwybmo1xowOHIEtt99hxrTkRQwwlxhTAqm6PKe8JFikz38E%2brBzEwW4wnNg2bfgrGkPmhNQuOrjnb8SXet2ZBamJzhIHTAlf6q8NZ5jsCpAi%2f6Nn5zu0%3d>.<br>You are receiving this because you authored the thread.Message ID: ***@***.***><br>
The 【c<font color=blue>hang】es】 are minor:<br>https://github.com/luben/zstd-jni/compare/v1.5.5-1...v1.5.5-2<br>I think it's fine.
【Thanks】 @dongjoon-hyun @srowen , I am 【check】ing the microbenchmark results and updating this pr later<br><br>
Since the one failure seems to be flaky one, we can ignore it.<br>```<br>[info] *** 1 TEST FAILED ***<br>[error] Failed: Total 3638, Failed 1, Errors 0, Passed 3637, Ignored 10, Canceled 2<br>[error] Failed 【test】s:<br>[error] \torg.apache.spark.scheduler.HealthTrackerIntegrationSuite<br>```
Please make a 【clean】 PR.
> Please make a 【clean】 PR.<br><br>OK, Let me make a new one.<br><br>
@dongjoon-hyun I open a new one : https://github.com/apache/spark/pull/41135
Fixing 【test】 failures in HiveDDLSuite, HiveCatalogedDDLSuite, SQLQueryTestSuite
【Thanks】 @Hisoka-X for comments. Looking ...
@rangadi @pengzhon-db @amaliujia Can you guys take a look? TY!
@allisonwang-db Could you also take a look? 【Thanks】!
cc @HyukjinKwon @mengxr @WeichenXu123
Mind taking a look at https://github.com/apache/spark/pull/40967/【check】s?【check】_run_id=13051337101?
Hmm, looks like I might be stuck with [this issue](https://github.com/docker/build-push-action/issues/687), any ideas?  I've configured my \"actions permissions\" to match [this comment](https://github.com/docker/build-push-action/issues/687#issuecomment-1238980158), but I'm still getting this error:<br>```<br>Error: buildx failed with: ERROR: failed to solve: failed to push ghcr.io/leewyang/apache-spark-ci-image:master-4815979720: failed commit on ref \"manifest-sha256:80eca005ea656d063e07f9059619043cd701e0c0d17029523fc167e4915405b4\": unexpected status: 403 Forbidden<br>```
Nm, got past this issue... waiting on the rest of the build results (but already failed the [Kubernetes Integration Test](https://github.com/leewyang/spark/actions/runs/4815979720/jobs/8587955821)).
oops, I missed that the linter failed. Reverting, and reopening the PR.
@rangadi @pengzhon-db @zhenlineo Can you guys take a look? Thank you!
@HyukjinKwon Can you merge this? 【Thanks】!
> @sweisdb Could you remove the error class `AES_SALTED_MAGIC` from `error-classes.json`, and related function `aesInvalidSalt()` + the 【test】 \"INVALID_PARAMETER_VALUE.AES_SALTED_MAGIC: AES decrypt failure - invalid salt\"<br><br>Updated to address this.
@sweisdb Could you re-trigger GitHub actions, please. Highly likely the failure of the 【test】 org.apache.spark.storage.BlockManagerBasicStrategyReplicationSuite is not related to your 【c<font color=blue>hang】es】 but just in case let's re-run all 【test】s.
@MaxGekk Sure, that worked and it's all green now.
+1, 【LGTM】. Merging to master.<br>Thank you, @sweisdb.
@sweisdb Congratulations with your first 【contribution】 to Apache Spark!
@sweisdb Could you fix the code style issues:<br>```<br>Checkstyle 【check】s failed at following occurrences:<br>Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[56] (sizes) LineLength: Line is longer than 100 characters (found 114).<br>Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[117] (sizes) LineLength: Line is longer than 100 characters (found 121).<br>Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[118] (sizes) LineLength: Line is longer than 100 characters (found 102).<br>Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[121] (sizes) LineLength: Line is longer than 100 characters (found 110).<br>Error:  src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionImplUtils.java:[122] (sizes) LineLength: Line is longer than 100 characters (found 104).<br>```
> This change adds support for optional IV and AAD fields to aes_encrypt and aes_decrypt<br><br>@sweisdb Looking at the constructors of the `AesEncrypt` and `AesDecrypt` expressions, they still don't support new parameters. Are you going to update the expressions, correct?
@MaxGekk I am planning to doing the user-facing SQL expression 【c<font color=blue>hang】es】 in a followup to make each change more 【simple】. I want to land this first.
> I want to land this first.<br><br>ok. Let's modify PR's title and its description according to your actual 【c<font color=blue>hang】es】.
I updated the description to clarify that this change is just for `ExpressionImplUtilsSuite` and doesn't expose the user-facing 【c<font color=blue>hang】es】 in `aes_encrypt` and `aes_decrypt` yet.
> this change is just for ExpressionImplUtilsSuite <br>> Adds AES IV and AAD support to ExpressionImplUtilsSuite<br><br>@sweisdb Please, replace ExpressionImplUtilsSuite by ExpressionImplUtils. *Suite is the suffix of 【test】 suites but you changed not only 【test】s.
> > this change is just for ExpressionImplUtilsSuite<br>> > Adds AES IV and AAD support to ExpressionImplUtilsSuite<br>> <br>> @sweisdb Please, replace ExpressionImplUtilsSuite by ExpressionImplUtils. *Suite is the suffix of 【test】 suites but you changed not only 【test】s.<br><br>Doh. Done. Thank you.
@Ngone51 @attilapiros <br>Please help to 【review】, thanks.
@cloud-fan  @dongjoon-hyun  @HyukjinKwon <br><br>Please help to 【review】, thanks.
I 【<font color=blue>fix】ed】 the PR title.<br>```<br>- [SPARK-43156][SQL][3.3] Fix COUNT(*) is null bug in correlated scalar subquery<br>+ [SPARK-43156][SQL][3.4] Fix COUNT(*) is null bug in correlated scalar subquery<br>```
cc @sunchao and @parthchandra
The 【test】 failure in GitHub Actions \"track allocated resources by taskId\" seems to be unrelated to this change.
@cloud-fan, @srielau, could you take a look at this?
@MaxGekk sub error class can't have dedicated sql state.
> sub error class can't have dedicated sql state.<br><br>ok, but `INTERNAL_ERROR_BROADCAST` and `INTERNAL_ERROR` have the same sql state in the PR. Just in case, can't we override the sql state in sub-classes (are there any principal issues of implementing this?).
Another advantage is that having `INTERNAL_ERROR_BROADCAST` will make it possible to have subclasses for the broadcast package.
cc @cloud-fan @hvanhovell This is how keyAs is impl at this moment.
【Thanks】 @HeartSaVioR . @siying and @Kimahriman - could you folks please take a look ? Thx
PR description seems a bit vague, given that actual code comments give pretty clear reasoning for why this change is needed?<br><br>> Wrap cache in `CodeGenerator` as an example. Feel free to use this in other places where we used Guava cache and don't want fate-sharing behavior.<br><br>It's not just an \"example\" -- the code comments detail a 【specific】 bad behavior we're trying to avoid with this change.<br><br>> This fate sharing behavior might lead to unexpected results in some situation.<br><br>Again, we can be 【specific】 -- we know it _does_ lead to unexpected results (query canceled during codegen)<br>
Hi @ryan-johnson-databricks would you mind triggering the merge for this PR?
> Hi @ryan-johnson-databricks would you mind triggering the merge for this PR?<br><br>Sorry, I'm not a spark committer.
Hi @JoshRosen would you mind merging this PR? Or maybe @cloud-fan could help?
I've merged this to master (Spark 3.5.0). 【Thanks】 @liuzqt!
cc: @gengliangwang, @SandishKumarHN
【Thanks】, merging to master.
【Thanks】 @gengliangwang!
I'll move the conversion function `PandasConversionMixin._create_converter` to `pyspark/sql/pandas/types.py`.
Merged to master, thank you!
seems scala style 【check】 failed
there is a 【test】 failure in cte.sql
@gengliangwang could you help merge it? 【Thanks】
cc @cloud-fan @viirya if you have time to 【review】, thank you
ping @dongjoon-hyun
Please update the title of the PR to <br><br>`[SPARK-43332][CONNECT][PYTHON] Make it possible to extend ChannelBuilder for SparkConnectClient`
Can I ask when you need a customized channel?
You need it you want to pass specialized authentication handlers for GRPC in the form of credentials plugins. In addition it allows you to configure the GRPC channel with additional deployment 【specific】 options.
@nfx Mind filing a JIRA please? See also https://spark.apache.org/contributing.html
@HyukjinKwon @zhengruifeng please 【review】! 【Thanks】!
cc @dongjoon-hyun and @Yikun
Thank you for pinging me, @pan3793 .
> Since #41034 is merged, could you rebase this PR to the master branch, @pan3793 ?<br><br>Sure, rebased
cc @srowen @sunchao @AngersZhuuuu
Late 【LGTM】. Was trying to +1 and merge this but @srowen beat me on it.
@dtenedor BTW could you mention the new trait in the PR description?
@dtenedor FYI there are 【test】 failures in the la【test】 code.
cc @cloud-fan since this is DSV2 related.
> @dtenedor FYI there are 【test】 failures in the la【test】 code.<br><br>@gengliangwang thanks, 【<font color=blue>fix】ed】<br>
【Thanks】, merging to master/branch-3.4
Thank you for reverting.
@EnricoMi @cloud-fan @dongjoon-hyun can you take a look , thanks ~
What is the fallout of `committer.setupJob(job)` not being executed in presence of an error?
I think Spark 3.2 is EOL, the final patch release was 3.2.4. a month ago. So this should target branch-3.3.<br><br>Note that a similar fix went into master and branch-3.4: #39431
Is this fixing https://github.com/apache/spark/pull/38358#issuecomment-1455371977?
> Is this fixing [#38358 (comment)](https://github.com/apache/spark/pull/38358#issuecomment-1455371977)?<br><br>yes
Hi, @zzzzming95 .<br>According to Apache Spark versioning policy, Apache Spark 3.2 reached EOL already and 3.2.4 was the last one. As a result, we close all PRs against `branch-3.2`.<br>- https://spark.apache.org/versioning-policy.html
> Hi, @zzzzming95 . According to Apache Spark versioning policy, Apache Spark 3.2 reached EOL already and 3.2.4 was the last one. As a result, we close all PRs against `branch-3.2`.<br>> <br>> * https://spark.apache.org/versioning-policy.html<br><br>OK , I see a similar implementation for Spark3.3, and I will submit it to Spark3.3.
@zzzzming95 . I'm not sure you are aware of Apache Spark backporting policy. To prevent a future regression, we start to 【review】 from `master` branch first. Then, backport it from `master` to `branch-3.4` to `branch-3.3`.<br>> OK , I see a similar implementation for Spark3.3, and I will submit it to Spark3.3.<br><br>
cc. @cloud-fan @HyukjinKwon @zsxwing @viirya
cc @hvanhovell @HyukjinKwon @grundprinzip
Hi, the jira ticket should be [SPARK-43331](https://issues.apache.org/jira/browse/SPARK-43331), not [SPARK-43267](https://issues.apache.org/jira/browse/SPARK-43267)
【Thanks】 @Hisoka-X ! I have no idea how I so randomly mistyped that number.
Merging to master thanks! Please address remaining comments in a couple of follow-ups.
Merged to `master`.<br>Thank you for the quick fix, @WweiL , @hvanhovell , @amaliujia .
@cloud-fan @gengliangwang @dtenedor Aside from my atrocious Scala skills, the code still needs comments. But It think it's ready for a 【review】.<br>
@srielau I am rethinking the requirement after reading the related docs (especially [the doc from snowflake](https://docs.snowflake.com/en/sql-reference/identifier-literal))<br>So how important it is to support all kinds of expressions(e.g string concats) within the `IDENTIFIER()` clause? It would be much easier if we limit the requirement only accepts the following:<br>* quoted identifier<br>* session variable<br><br>So, instead of <br>```<br>identifierReference<br>    : IDENTIFIER_KW LEFT_PAREN expression RIGHT_PAREN<br>    | multipartIdentifier<br>    ;<br>```<br>We can make it<br>```<br>identifierReference<br>    : IDENTIFIER_KW LEFT_PAREN '\\'' multipartIdentifier  '\\'' RIGHT_PAREN    #singleQuotedIdentifier<br>    | IDENTIFIER_KW LEFT_PAREN '\"' multipartIdentifier  '\"' RIGHT_PAREN      #doubleQuotedIdentifier<br>    | IDENTIFIER_KW LEFT_PAREN '$' multipartIdentifier RIGHT_PAREN           #sessionvariableIdentifier<br>    | multipartIdentifier                                                    #【simple】MultipartIdentifier<br>    ;<br>```<br>And this requires much less 【c<font color=blue>hang】es】 in the parser and analyzer.<br>
> @srielau I am rethinking the requirement after reading the related docs (especially [the doc from snowflake](https://docs.snowflake.com/en/sql-reference/identifier-literal)) So how important it is to support all kinds of expressions(e.g string concats) within the `IDENTIFIER()` clause? It would be much easier if we limit the requirement only accepts the following:<br>> <br>> * quoted identifier<br>> * session variable<br>> <br>> So, instead of<br>> <br>> ```<br>> identifierReference<br>>     : IDENTIFIER_KW LEFT_PAREN expression RIGHT_PAREN<br>>     | multipartIdentifier<br>>     ;<br>> ```<br>> <br>> We can make it<br>> <br>> ```<br>> identifierReference<br>>     : IDENTIFIER_KW LEFT_PAREN '\\'' multipartIdentifier  '\\'' RIGHT_PAREN    #singleQuotedIdentifier<br>>     | IDENTIFIER_KW LEFT_PAREN '\"' multipartIdentifier  '\"' RIGHT_PAREN      #doubleQuotedIdentifier<br>>     | IDENTIFIER_KW LEFT_PAREN '$' multipartIdentifier RIGHT_PAREN           #sessionvariableIdentifier<br>>     | multipartIdentifier                                                    #【simple】MultipartIdentifier<br>>     ;<br>> ```<br>> <br>> And this requires much less 【c<font color=blue>hang】es】 in the parser and analyzer.<br>This is similar to what I tried in my previous attempt: https://github.com/apache/spark/pull/40884<br>We need to support parameter markers as well as \"proper\" (session) variables which would not have a leading '$'.<br>They would just be identifiers which need to be resolved.<br>Also we need the ability for users to pre-fix and post-fix. For example a schema-name or a field name.<br>Without solid support for parameter markers (and variables) we are still open to SQL injection.<br>I was told that I cannot call eval from within the parser.<br><br>  <br>
@gengliangwang @dtenedor @gengliangwang <br>It's camera ready now. Please have at it.<br>
@srielau @gengliangwang @cloud-fan <br><br>The general structure of the PR looks OK.<br><br>This PR proposes to add new unresolved nodes for several different locations in the parser. This could be useful in the future if we want to add more custom 【analysis】 support for these different areas.<br><br>Alternatively, we could leave the multipartIdentifier references where they are, and just update its definition instead:<br><br>```<br>multipartIdentifier<br>    : IDENTIFIER_KW LEFT_PAREN expression RIGHT_PAREN<br>    | parts+=errorCapturingIdentifier (DOT parts+=errorCapturingIdentifier)*<br>    ;<br>```<br><br>This would reduce the number of 【c<font color=blue>hang】es】 to the .g4 file, since the proposed namespaceReference and functionNameReference and relationReference are all the same syntax. Then the PR would be easier to merge into different Spark forks out there. But this approach grants us future 【flexibility】, if we ever anticipate the syntax to diverge for these different cases. I am OK with the proposed approach here.
@dtenedor <br>There are still 30 multipartIdentifier usages that do NOT support IDENTIFIER() notation.<br>So we would trade mechanical churn in the grammar for code 【c<font color=blue>hang】es】 in AstBuilder et al.<br>
> @dtenedor<br>There are still 30 multipartIdentifier usages that do NOT support IDENTIFIER() notation.<br>So we would trade mechanical churn in the grammar for code 【c<font color=blue>hang】es】 in AstBuilder et al.<br><br>This is true, we can keep the grammar 【c<font color=blue>hang】es】 then.
@gengliangwang @cloud-fan  Can you take a peek with your error-context hat on?<br>As you see my adding withOrigin() into the wrapper has focused many of the contexts around the table names.<br>Sometimes that is quite right, sometimes that is debatable.<br>Did you add all those createUnresolved*() methods for the context? I suppose I can add them back.
> @gengliangwang @cloud-fan Can you take a peek with your error-context hat on?<br>As you see my adding withOrigin() into the wrapper has focused many of the contexts around the table names.<br>Sometimes that is quite right, sometimes that is debatable.<br><br><br>So I am seeing code like <br>```<br>        withIdentClause(relation, ident => {<br>          InsertIntoStatement(<br>            UnresolvedRelation(ident),<br>            partition,<br>            cols,<br>            query,<br>            overwrite = false,<br>            ifPartitionNotExists)<br>        })<br>```<br>The context `relation` will be applied on the whole `InsertIntoStatement`. I think what you want is on `UnresolvedRelation(ident)` only. <br>
> > @gengliangwang @cloud-fan Can you take a peek with your error-context hat on?<br>> > As you see my adding withOrigin() into the wrapper has focused many of the contexts around the table names.<br>> > Sometimes that is quite right, sometimes that is debatable.<br>> <br>> So I am seeing code like<br>> <br>> ```<br>>         withIdentClause(relation, ident => {<br>>           InsertIntoStatement(<br>>             UnresolvedRelation(ident),<br>>             partition,<br>>             cols,<br>>             query,<br>>             overwrite = false,<br>>             ifPartitionNotExists)<br>>         })<br>> ```<br>> <br>> The context `relation` will be applied on the whole `InsertIntoStatement`. I think what you want is on `UnresolvedRelation(ident)` only.<br><br>Yes.
Yeah, so we can have code like<br>```<br>val unresolvedRelation = withOrigin(ctx) {<br>  UnresolvedRelation(ident)<br>}<br> InsertIntoStatement(<br>            unresolvedRelation,<br>            partition,<br>            cols,<br>            query,<br>            overwrite = false,<br>            ifPartitionNotExists)<br>```
【Thanks】 for the 【great】 work! Merging to master!
cc @amaliujia @cloud-fan @hvanhovell FYI
@MaxGekk 【Thanks】 for the reivew. Merging to master/branch-3.4
@srowen @gengliangwang <br>Address issue mentioned in https://github.com/apache/spark/pull/36226#issuecomment-1530789474
> Is this the only way to fix the issue, @maytasm ?<br><br>No. We can also keep the current DataTables version 1.10.25 and modify the jquery.dataTables.1.10.25.min.css file by removing t`!important` i.e. <br>`\"../images/sort_asc.png\") !important` to `\"../images/sort_asc.png\")`
> Please remove the following from the PR description, @maytasm .<br>> <br>> > The 1.10.25 version of DataTables, that Spark currently uses, seems vulerable: https://【security】.snyk.io/package/npm/datatables.net/1.10.25. The vulnerability may not affect Spark but it is safer to update.<br><br>Done
@dongjoon-hyun WDYT? I'm neutral on one change vs the other. I suppose modifying the CSS is a smaller change
I prefer the minimal change instead of this PR because there is no easy way for us to guarantee that new version has no regression somewhere. To be honest, actually, it's difficult for me to make it sure.<br><br>Anyway, I will not be against this PR too if people are sure about this new version.
@srowen @dongjoon-hyun I don't have strong opinion either way too. However, upon thinking about it more, I am leaning toward minimal CSS change without upgrading DataTables library for the following reason:<br>- DataTables version 【upgrade】d to 1.13.2 is only in master branch and has not been part of any release (i.e. 3.3, 3.4) yet. Making it risky<br>- I am not familiar with DataTables library and have no idea about the 【c<font color=blue>hang】es】 between the current version and 1.13.2<br>- The CSS change is very 【simple】, easy to understand, and has a very minimal surface area.  <br><br>I will put up a PR for the CSS change but will leave this PR open for a bit longer in case anyone feel strongly for upgrading DataTables to 1.13.2
@srowen @dongjoon-hyun <br>Here is a 【simple】 CSS fix instead: https://github.com/apache/spark/pull/41061
Yeah let's go with your alternative
I agree with back-porting. We just need to get 【test】s to pass. Did the UI seem OK in a quick manual 【test】? just want to verify the arrows work.
> I agree with back-porting. We just need to get 【test】s to pass. Did the UI seem OK in a quick manual 【test】? just want to verify the arrows work.<br><br>Do you know why the `On pull request update / Notify 【test】 workflow` failed? Looks like all the 【test】s passed on https://github.com/apache/spark/pull/41011
> BTW, please remove this from the PR description, @maytasm .<br>> <br>> > The 1.10.25 version of DataTables, that Spark currently uses, seems vulerable: https://【security】.snyk.io/package/npm/datatables.net/1.10.25. The vulnerability may not affect Spark but it is safer to update.<br><br>done
> Is this the only way to fix the issue, @maytasm ?<br><br>https://github.com/apache/spark/pull/41011#issuecomment-1531983883
@srowen @dongjoon-hyun <br>Here is a 【simple】 CSS fix instead: https://github.com/apache/spark/pull/41060
Hi, `./bin/pyspark --remote local` shows the following error after this commit.<br><br>```py<br>% ./bin/pyspark --remote local<br>...<br>Traceback (most recent call last):<br>  File \"/.../python/pyspark/shell.py\ line 78, in <module><br>    sc = spark.sparkContext<br>  File \"/.../python/pyspark/sql/connect/session.py\ line 567, in sparkContext<br>    raise PySparkNotImplementedError(<br>pyspark.errors.exceptions.base.PySparkNotImplementedError: [NOT_IMPLEMENTED] sparkContext() is not implemented.<br>```
creating a followup now
https://github.com/apache/spark/pull/41206
+1, 【LGTM】. Merging to master.<br>Thank you, @juliuszsompolski and @grundprinzip @nija-at for 【review】.
cc @MaxGekk FYI
@thejdeep, can you fix the conflicts please ?
+CC @AngersZhuuuu who last worked on this.<br>Also +CC @srowen who 【review】ed the previous 【c<font color=blue>hang】es】.
@srowen 【Thanks】 for the 【review】! Updated the PR after addressing the comments, can you please take another pass ?
+1, 【LGTM】. Merging to master.<br>Thank you, @NarekDW.
Thank you, @bjornjorgensen and @HyukjinKwon .<br>Merged to master for Apache Spark 3.5.0.
Thank you @bjornjorgensen @dongjoon-hyun !
cc: @MaxGekk do I need to separate this into multiple PRs (one for each error message)?
+1, 【LGTM】. Merging to master.<br>Thank you, @imback82.
@HeartSaVioR will you have time to take a look?
> Nice finding! Looks 【great】 in overall - just to see whether we can still read a single file containing special character via escaping it. I'd OK with it if it does not work in any way (Hadoop side bug or something) but better to confirm.<br><br> 【Thanks】 for your 【review】 and suggestion. I gave it a try ant it looks like \"\\[\" works. I updated the PR.<br>
Not sure why one 【test】 ran for 4+ hours and it still didn't succeed or fail.
It seems like the actual 【test】 run is only 2 hours ago - 【check】 out the full log via `view raw logs`.
Failed 【test】s don't seem to be related at all:<br>```<br>2023-05-08T23:33:54.8173143Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0mFailed 【test】s:\u001B[0m<br>2023-05-08T23:33:54.8173854Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0m\torg.apache.spark.sql.connect.client.SparkConnectClientSuite\u001B[0m<br>2023-05-08T23:33:54.8805941Z \u001B[0m[\u001B[0m\u001B[31merror\u001B[0m] \u001B[0m\u001B[0m(connect-client-jvm / Test / \u001B[31m【test】\u001B[0m) sbt.TestsFailedException: Tests un【success】ful\u001B[0m<br>```
I see all 【test】s passed; https://github.com/siying/spark/actions/runs/4920444751<br><br>【Thanks】! Merging to master.
Thank you, @ueshin .
Rebased to master since https://github.com/apache/spark/pull/41024 is merged.
All 【test】s passed and there is no change in code during rebasing. Merged to master for Apache Spark 3.5.0.<br>Thank you again, @HyukjinKwon and @ueshin .
Thank you, @HyukjinKwon .
Thank you, @Yikun .
Could you 【review】 once more, @HyukjinKwon and @Yikun . <br>I found that we didn't support Python 3.8+ in PyPy3 environment. <br>I converted this PR from `INFRA` to `PYTHON` module in both the JIRA and here.
Merged to master. Thank you all!<br>The last commit was disabling a single 【test】 case.
@HyukjinKwon @HeartSaVioR @xinrong-meng @rangadi @pengzhon-db @amaliujia Can you guys take a look? 【Thanks】!
cc @dongjoon-hyun @viirya @huaxingao @cloud-fan @sunchao
Thank you for pinging me, @aokolnychyi .
Thank you, @dongjoon-hyun @cloud-fan!
Looks good, if you can resolve the conflicts and 【test】s pass. File a little JIRA and update the title if you please too https://spark.apache.org/contributing.html
cc @dongjoon-hyun @Yikun
Thank you, @pan3793 .
This change is not backward compatible, I'm not sure if this change is necessary, if necessary, also need parameters to control whether to open.
@stijndehaes Do you have the error message if do not quote date?
@wangyum @Hisoka-X <br><br>There is more info in the issue on JIRA: https://issues.apache.org/jira/browse/SPARK-43357<br><br>So basically when integrating we glue we are getting issues:<br><br>```<br>org.apache.hadoop.hive.metastore.api.InvalidObjectException: Unsupported expression '2023 - 05 - 03' (Service: AWSGlue; Status Code: 400; Error Code: InvalidInputException; Request ID: beed68c6-b228-442e-8783-52c25b9d2243; Proxy: null)<br>```<br><br>Now this issue might be on the side of glue, but glue is supposed to be hive compatible. So I think this should also work with a hive catalog. However I do not have access to one, and would net to se one up.<br>So maybe someone with more experience with that could help me to 【test】 【compatibility】.
@wangyum does all hive versions support it?
> @wangyum does all hive versions support it?<br><br>Hive 0.13.0 and later support it: https://issues.apache.org/jira/browse/HIVE-5679.
> 【Thanks】. Please file a JIRA and update the PR title, @hiboyang .<br>> <br>> cc @HyukjinKwon , @cloud-fan , @hvanhovell , @LuciferYang , @grundprinzip<br><br>【Thanks】 for the suggestion! Added JIRA.
> This is 【awesome】! 【Thanks】 for starting the work. I think the next step would be to have a quick 【discussion】 in a readme or the PR on the rough 【design】 of the objects and methods so that we get an idea of what might work or not.<br><br>【Thanks】 @grundprinzip for the feedback 【:)】 Updated PR description and README just now!<br>
Need to add unit 【test】 / integration 【test】.
If you don't mind, my first suggestion is that we make a quick prototype of the build system so that we don't have to 【check】 in the generated code. <br><br>I can help with that as well.
> If you don't mind, my first suggestion is that we make a quick prototype of the build system so that we don't have to 【check】 in the generated code.<br>> <br>> I can help with that as well.<br><br>Yes, one thing to consider is how Spark Connect Go application could reference those code as a Go library. Please go ahead with the prototype, thanks for helping here!<br>
I created a draft PR that contains the necessary build code and moves the files around to fit the approach, feel free to just use the code in your PR.<br><br>https://github.com/apache/spark/pull/41080
> I created a draft PR that contains the necessary build code and moves the files around to fit the approach, feel free to just use the code in your PR.<br>> <br>> #41080<br><br>This is 【great】, thanks @grundprinzip! I copied code from your PR to this PR. Those `make` commands are very convenient!<br><br>By the way, we need to discuss where to publish those generated Go proto files. People will need to reference those files as Go module/package, when they write Spark Connect Go application. Any thoughts on this?<br>
<br>> By the way, we need to discuss where to publish those generated Go proto files. People will need to reference those files as Go module/package, when they write Spark Connect Go application. Any thoughts on this?<br><br>I think we probably need to figure out how `go get` will work  as well. Unfortunately, the release distribution will require us to 【check】-in the generated code. <br><br><br><br>
It would be 【great】 if you could just merge the change from my PR (just add the remote tree to your branch and merge the 【c<font color=blue>hang】es】) and then push to your PR again. Right now, the Makefile is semi broken because you didn't mv the examples from examples to `cmd`.<br><br>I took the project layout from: https://github.com/golang-standards/project-layout<br><br>Now, for the package name we will have to refactor this a bit to make go get work. I think it should become: `https://github.com/apache/spark/connector/connect/client/go`<br><br>We can use a versioning scheme similar to what arrow does.
@hvanhovell @HyukjinKwon @cloud-fan @gatorsmile How do you propose we make 【progress】 here?<br><br>I'm worried that we will accumulate a large number of 【c<font color=blue>hang】es】 in particular with the generated code that needs to be 【check】ed in and changed to make the prototype working. In the worst case it will make it harder to 【review】.<br><br>What would be a good skeleton to submit?
Do we have a dev guideline for this go client? And will Github Action run 【test】s for it? I think we can merge this PR once the infra is ready, the API coverage can be 【improve】d later.
I can help run the 【test】s and build as part of the CI that shouldn't be too hard when the make build runs locally. <br><br>@hiboyang let me know when make and make full【test】 pass locally and then we can try to get a first version merged before we add a lot more coverage.
> I can help run the 【test】s and build as part of the CI that shouldn't be too hard when the make build runs locally.<br>> <br>> @hiboyang let me know when make and make full【test】 pass locally and then we can try to get a first version merged before we add a lot more coverage.<br><br>Yes, thanks @grundprinzip for helping CI here!<br><br>`make full【test】` is passing now. Need to run `make internal/generated.out` first to generate Go proto files.<br><br>```<br>make internal/generated.out<br><br>make full【test】<br>>> TEST, \"coverage\"<br>     ?   \tgithub.com/apache/spark/go/v_3_4/examples/spark-connect-example-raw-grpc-client\t[no 【test】 files]<br>     ?   \tgithub.com/apache/spark/go/v_3_4/examples/spark-connect-example-spark-session\t[no 【test】 files]<br>     ?   \tgithub.com/apache/spark/go/v_3_4/internal/generated\t[no 【test】 files]<br>     ok  \tgithub.com/apache/spark/go/v_3_4/spark/sql\t0.358s\tcoverage: 22.6% of statements<br>```<br>
> It would be 【great】 if you could just merge the change from my PR (just add the remote tree to your branch and merge the 【c<font color=blue>hang】es】) and then push to your PR again. Right now, the Makefile is semi broken because you didn't mv the examples from examples to `cmd`.<br>> <br>> I took the project layout from: https://github.com/golang-standards/project-layout<br>> <br>> Now, for the package name we will have to refactor this a bit to make go get work. I think it should become: `https://github.com/apache/spark/connector/connect/client/go`<br>> <br>> We can use a versioning scheme similar to what arrow does.<br><br>Got it, I am not super familiar with git commands to merge between different PRs, sorry for breaking here. I think now Makefile is good.<br><br>I renamed to `https://github.com/apache/spark/connector/connect/client/go` in the code.<br><br>
@hiboyang I will need some time to integrate a build into the CI (github worklows). I hope I can get it done quickly<br>
Build link :https://github.com/hiboyang/spark/actions/runs/4933558573/jobs/8817627400
@hiboyang I've updated the github workflows file to build the Go 【test】s as well. Please pick the 【c<font color=blue>hang】es】 from my 【test】 PR https://github.com/apache/spark/pull/41117<br><br>* .github/workflows/build_and_【test】.yml<br><br>In addition, please make sure to pick the 【c<font color=blue>hang】es】 on the `.gitignore` and please remove the `internal/generated.out` file from the git history it's needed to trigger the code generation.
【Thanks】 @grundprinzip for adding github workflow! I merged your whole PR branch to my PR just now.
Hi @grundprinzip, I see some error like following in PR 【check】 (https://github.com/hiboyang/spark/actions/runs/4955561542/jobs/8865088163). I already run `./dev/connect-gen-protos.sh`, but still see this error. Do you have any suggestion on how to deal with this?<br><br>```<br>Different files: ['base_pb2.pyi', 'catalog_pb2.pyi', 'commands_pb2.pyi', 'common_pb2.pyi', 'example_plugins_pb2.pyi', 'expressions_pb2.pyi', 'relations_pb2.pyi', 'types_pb2.pyi']<br>Generated files for pyspark-connect are out of sync! If you have touched files under connector/connect/common/src/main/protobuf/, please run ./dev/connect-gen-protos.sh. If you haven't touched any file above, please rebase your PR against main branch.<br>Error: Process completed with exit code 255.<br>```
@HyukjinKwon This is relatively small but 【c<font color=blue>hang】es】 something, can you take a look?<br>Also CC @rangadi @pengzhon-db
What is this?
@srowen 【<font color=blue>fix】ed】 description, didn't expect you to be so quick
+1, 【LGTM】. Merging to master.<br>Thank you, @vitaliili-db.
@WweiL can you add `StreamingManager` to the `CheckConnectJvmClientCompatibility` 【test】?
@cloud-fan Can you help me merge this? Thx!
@HeartSaVioR @siying - please take a look. 【Thanks】
cc @HeartSaVioR and @xuanyuanking FYI
thanks @dongjoon-hyun
I think I added old error class back when resolving merge conflict. So create this PR to delete it.<br>@MaxGekk , please help 【review】 it.
Can we mention it in the PR description that this is not a real perf issue, but just make the plan clearer about subquery reuse?
thank you @cloud-fan @peter-toth @dongjoon-hyun
The CI has passed with<br>https://github.com/ulysses-you/spark/actions/runs/4882406778
thank you @dongjoon-hyun @HyukjinKwon @juliuszsompolski
+1, 【LGTM】. Merging to master.<br>Thank you, @bozhang2820.
@JoshRosen and @dongjoon-hyun Can you please 【check】 the 【c<font color=blue>hang】es】?
Thank u for pinging me. Will take a look tonight.
> Thank u for pinging me. Will take a look tonight.<br><br>【Thanks】 @dongjoon-hyun <br>I am not sure I can find the reason behind the build errors though<br>```<br>#35 ERROR: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4886449337: unexpected status: 403 Forbidden<br><br>#36 [auth] amahussein/apache-spark-ci-image:pull,push apache/spark/apache-spark-github-action-image-cache:pull token for ghcr.io<br>#36 DONE 0.0s<br>------<br> > exporting to image:<br>------<br>ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4886449337: unexpected status: 403 Forbidden<br>Error: buildx failed with: ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4886449337: unexpected status: 403 Forbidden<br>```
@dongjoon-hyun  do you know about what might be missing for permissions for his build error here? <br><br> (ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4897157804: unexpected status: 403 Forbidden)<br><br>I haven't seen that before.
To @tgravescs , I also hit that issue before when I re-forked Spark repository. Like 403 error means, it was a permission issue in GitHub setting. But, I forgot the details. At that time, @Yikun helped me because he did setup the pipeline in the 【community】~<br>
Welcome to the Apache Spark 【community】, @amahussein .<br>I added you to the Apache Spark contributor group and assigned SPARK-43340 to you.
【LGTM】 as well. 【Thanks】 for fixing this!
> ERROR: failed to solve: failed to push ghcr.io/amahussein/apache-spark-ci-image:master-4897157804: unexpected status: 403 Forbidden<br><br>There are two possible issue we met before:<br>1. Github Action flaky issue: https://github.com/orgs/【community】/【discussion】s/32184<br>2. Permisssion seeting issue. By default, write permission already included to your GITHUB_TOKEN, but if you set it manually (see also [1], [2]), it will failed, current CI will first build infra and push to your ghcr so write permission is required.<br>[1] https://github.blog/changelog/2021-04-20-github-actions-control-permissions-for-github_token/<br>[2] https://github.com/users/amahussein/packages/container/apache-spark-ci-image/settings<br><br>>  Like 403 error means, it was a permission issue in GitHub setting. But, I forgot the details. At that time, @Yikun helped me because he did setup the pipeline in the 【community】~<br><br>Yes, we do some configure on https://github.com/apache/spark/pull/37745#issuecomment-1234063012 , but after that I found it was a flaky issue of github aciton.
Thank you, @srowen !
> Interval types can be read as date or timestamp types that would lead to wildly different results<br><br><br>@zeruibao could you update the PR description about what's the behavior will be?
> Interval types can be read as date or timestamp types that would lead to wildly different results<br><br><br>@zeruibao could you update the PR description about what the behavior will be?
@zeruibao shall we only block the cases mentioned in the PR description?<br>```<br> prevent reading interval types as date or timestamp types to avoid getting corrupt dates as well as reading decimal types with incorrect precision.<br>```
Yeah, ok. I can only block <br>- read DayTimeIntervalType as TimestampType<br>- read DayTimeIntervalType as TimestampNTZType<br>- read DayTimeIntervalType as DateType<br>- read YearMonthIntervalType as TimestampType<br>- read YearMonthIntervalType as TimestampNTZType<br>- read YearMonthIntervalType as DateType<br><br>Does it look good to you now? @gengliangwang
Yes
> > Interval types can be read as date or timestamp types that would lead to wildly different results<br>> <br>> @zeruibao could you update the PR description about what the behavior will be?<br><br>Sure, just add it.
@zeruibao please also update `sql-migration-guide.md` and mention this behavior change.
Please feel free to leave comments if any, I'll adjust them in a follow-up.
@JinHelin404 Sorry, this is a duplicate of https://github.com/apache/spark/pull/41020. cc @imback82
@MaxGekk Ok, i will close this pr.
@JinHelin404 Thank you.
@dongjoon-hyun @cometta would be good if you have time to 【review】 this PR
I wrote a 【clean】-revert PR with your authorship. Reverting PR should follow this style instead of claiming a new 【contribution】 like `Support ...`.<br>- https://github.com/apache/spark/pull/41069
Replaced this with https://github.com/apache/spark/pull/41069
Don't know why CI \"【continuous】-integration/appveyor/pr\" is triggered. Checked other PR, this CI 【test】 all not triggered.
Merged to 3.3
Merged to 3.4
@dtenedor @gengliangwang
kindly ping @cloud-fan ~
Close as not required
cc @AngersZhuuuu
First, pls write a jira and assign this pr to jira ticket<br>These code is used to format sql from LogicalPlan, seems copied from Hive, ok to remove.
> First, pls write a jira and assign this pr to jira ticket These code is used to format sql from LogicalPlan, seems copied from Hive, ok to remove.<br><br>Ok, I created [SPARK-43405](https://issues.apache.org/jira/browse/SPARK-43405)
> 1. Could you revise the PR title and description, @Hisoka-X ?<br><br>Done<br><br>> 2. Do you happen to know when these methods become useless?<br><br>It unused when https://github.com/apache/spark/pull/16869 .<br>
Thank you for the info, @Hisoka-X .<br><br>cc @jiangxb1987 and @hvanhovell from #16869
@dongjoon-hyun that is looonnnnggg time ago 【:)】<br><br>Let me take a look.
Thank you, @Hisoka-X and @hvanhovell . 【:)】
Thank you @dongjoon-hyun @HyukjinKwon ~
cc @sadikovi FYI
【Thanks】 @dongjoon-hyun for your point : )<br><br>The intent of this PR is to change the way `df.show` represents the map data type, which is currently different from some mainstream databases. We also want `df.show` and `spark-sql `CLI to be as consistent as possible, since they are both spark CLI. So we think the representation after PR might be a better representation.<br><br>Adding configuration is also a good idea.
Kindly ping @cloud-fan , Could you please take a look if you find a time :-)
Mind filing a JIRA? See also https://spark.apache.org/contributing.html
Filed: <br>https://issues.apache.org/jira/browse/SPARK-43496<br>
+1, 【LGTM】. Merging to master.<br>Thank you, @eogren.
@eogren Congratulations with your first 【contribution】 to Apache Spark!
cc @dcoliversun This will be merged with your authorship.
After merging this 【clean】-revert, you may want to make a new and proper 【test】 PR for https://github.com/apache/spark/pull/41057/files#r1186469992 if you want.
@dongjoon-hyun 【Thanks】 for your PR. I'm +1
Thank you, @dcoliversun .
May I ask why this is reverted?
Sorry for missing context, @viirya .<br><br>SPARK-43329 and SPARK-43342 were filed as a regression at Apache Spark 3.4.0 and those issues have the error message, `IllegalArgumentException: PVC ClaimName: a1pvc should contain OnDemand or SPARK_EXECUTOR_ID when requiring multiple executors`.<br><br>I linked all the reported issues to the original JIRA which caused this regression.<br>- https://issues.apache.org/jira/browse/SPARK-39006 Show a directional error message for PVC Dynamic Allocation Failure<br><br>![Screenshot 2023-05-06 at 5 13 51 PM](https://user-images.githubusercontent.com/9700541/236651391-dcb09a64-3384-4dea-8bc9-de5a886b19a3.png)<br><br><br><br>And, the following was the reverting PR from the author of SPARK-39006, @dcoliversun.<br>- https://github.com/apache/spark/pull/41057<br><br><br>In short, while adding a log message, we missed a corner case which one NFS-backed PVC is mounted by multiple executors. So, we decide to revert SPARK-39006 because it doesn't give actual benefit (except logging).
Thank you @dongjoon-hyun
cc @cloud-fan @sunchao @beliefer
@dongjoon-hyun @mridulm help take a look?
+CC @otterc
Thank you for the fix.
@dongjoon-hyun any more comments on this?
It's probably OK (re-run the 【test】s?)
GA passed, the `【continuous】-integration/appveyor/pr` randomly appears, and it [never succeeded](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/history), where does the appveyor come from?
cc @srowen @LuciferYang
Actually there is another tar command in release-build.sh ; do we need this change there too?
We don't need to apply this change to `release-build.sh` because it's only called inside the `spark-rm` container during the release procedure. Unless you want it to work on macOS too.
To @srowen , we released Apache Spark 3.2.4 as EOL release and `branch-3.2` is not considered maintained now.  It would be 【great】 if we don't touch EOL branches because it will cause confusion to the contributors.
Yeah it won't matter, likely. On the off chance we make one more release, this is a low-risk 'fix' to the release process
cc @rangadi @HyukjinKwon @pang-wu
@rangadi is it good to go?
@HyukjinKwon Yep, this looks good to go.
Just wondering why this is relevant to Spark Connect (seeing `[CONNECT]` in the PR title)?
> Just wondering why this is relevant to Spark Connect (seeing `[CONNECT]` in the PR title)?<br><br>ah sorry about that. i think i confused that this is under the `connector/` folder with `CONNECT` (which im guessing now refers to Spark Connect?) which i saw in some other PR titles. In the future i'll add only `PROTOBUF` 👍
@dongjoon-hyun @mridulm Help take a look?
> Thank you for pinging me. Could you 【improve】 this proposal more, @warrenzhu25 ?<br>> <br>> * This PR claims that data migration can hurt 【performance】, but the code is applied in all cases including the case data migration (shuffle/rdd) is disabled. Moreover, in general, when the migration data size is small, the claim is invalid.<br>> * In the same way, I can imagine this PR introduces another regression in terms of resource utilizations. For example, Spark Thrift Server can decommission all executors at the same time, but this configuration (< 0.1) may hurt the 【speed】 of scaling down.<br>> * It would be 【great】 if we can have a clear 【documentation】 about the relationship between this and other decommission configs.<br><br>`maxRatio` has default 【value】 1, so it's same as current behavior. Users has full control of this ratio based on their judgement of storage migration size and 【impact】.
No, I'm not saying the default 【value】. The default 【value】 should be 1.<br>> maxRatio has default 【value】 1, so it's same as current behavior. <br><br>When the new configuration `maxRatio` itself is controversial in terms of the benefits, it's just a dark-magic to confuse the users to put into the mud holes. Why do we need to add this dark-magic configuration, @warrenzhu25 ?<br>> Users has full control of this ratio based on their judgement of storage migration size and 【impact】.
> No, I'm not saying the default 【value】. The default 【value】 should be 1.<br>> <br>> > maxRatio has default 【value】 1, so it's same as current behavior.<br>> <br>> When the new configuration `maxRatio` itself is controversial in terms of the benefits, it's just a dark-magic to confuse the users to put into the mud holes. Why do we need to add this dark-magic configuration, @warrenzhu25 ?<br>> <br>> > Users has full control of this ratio based on their judgement of storage migration size and 【impact】.<br><br>I agree this config is not a perfect solution to solve this issue of migration competing with shuffle fetch. But it provides a feasible workaround to limit the 【impact】 of data migration. In our prod, using `maxRatio` of 0.3 could have similar perf as without shuffle migration while `maxRatio` of 0.5 or large has significant perf regression.
@dongjoon-hyun helpt take a look?
Merged to master for Apache Spark 3.5.0.<br>Thank you, @warrenzhu25 and @pan3793
cc @gengliangwang @HyukjinKwon @srowen
Can you fill out the JIRA, and add a little bit of explanation here?<br>It seems like you just avoid trying to extract more fields if basic parsing fails?
> Can you fill out the JIRA, and add a little bit of explanation here? <br><br>Done<br><br>> It seems like you just avoid trying to extract more fields if basic parsing fails?<br><br>Another formatter parsing method is used to prevent the formatter from throwing exceptions, which is similar to https://github.com/apache/spark/pull/36562 .
cc @sadikovi too
The same question as for https://github.com/apache/spark/pull/41091:<br><br>_Do the benchmarks `CSVBenchmark` and `JsonBenchmark` show any 【improvement】s? Could you regenerate the results `JsonBenchmark.*.txt` and `CSVBenchmark.*.txt`, please._
@Hisoka-X Could you highlight in PR description how much does it become 【fast】er. Please, put some numbers to the section `Why are the 【c<font color=blue>hang】es】 needed?`.
> @Hisoka-X Could you highlight in PR description how much does it become 【fast】er. Please, put some numbers to the section `Why are the 【c<font color=blue>hang】es】 needed?`.<br><br>Done
@MaxGekk I adjusted the code by your suggestion. By the way, maybe we should use spotless to avoid problem like this.
+1, 【LGTM】. Merging to master.<br>Thank you, @Hisoka-X and @srowen @HyukjinKwon for 【review】.
【Thanks】 @MaxGekk for your patience and @srowen @HyukjinKwon.
@HyukjinKwon
@dongjoon-hyun help take a look?
> The removed code seems to be originated from Apache Spark 3.0.0. Could you give a correct `Affected Versions` to [SPARK-43398](https://issues.apache.org/jira/browse/SPARK-43398) if you think this is a bug, @warrenzhu25 ?<br>> <br>> ![Screenshot 2023-05-07 at 1 22 51 PM](https://user-images.githubusercontent.com/9700541/236700961-c5886940-1c55-40ff-9555-6980447f751d.png)<br><br>Done.
@dongjoon-hyun any comments on this?
How are you observing recoverable fetch failures ?
> How are you observing recoverable fetch failures ?<br><br>I have seen 2 cases when target executor has busy shuffle fetch and upload due to shuffle migration:<br>1. All Netty request handler threads are exhausted, it'll throw `TimeoutException` when creating connection to target executor took longer than `connectTimeout`.<br>2. `IdleStateHandler` will close connection when fetch request took longer than `requestTimeout` to receive response.<br><br>Both cases are recoverable.
These looks like things which can be handled by appropriate configuration tuning ?<br>The PR itself requires a bit more work if that is not a feasible direction (【efficient】 【clean】up, handling corner cases, etc).
> These looks like things which can be handled by appropriate configuration tuning ? The PR itself requires a bit more work if that is not a feasible direction (【efficient】 【clean】up, handling corner cases, etc).<br><br>Case 2 can't be handled by existing config, there'll be other similar recoverable cases. Generally speaking, I think unregister all map output when fetch failed is too aggressive. So it's better to have config to control or disable such behavior. If the executor is really dead, the map output will be unregistered <br>when removing executor, if executor is just experiencing temporary and recoverable hiccup, then unregister is too expensive.
> Case 2 can't be handled by existing config, there'll be other similar recoverable cases<br><br>Did you try increasing idle timeout ?<br>The behavior is 【specific】 to the environment application is running in - where executors are unable to respond to shuffle requests for more than 2 minutes: this looks more like a tuning or deployment issue.<br><br>> Generally speaking, I think unregister all map output when fetch failed is too aggressive.<br><br>As described, this is a case of not appropriately configuring spark for the load/cluster characteristics.<br>For example, in our internal env, the network timeout is set to a significantly higher 【value】 than the default 120s due to a variety of factors - the default 2mins would result in failures (including this 【specific】 shuffle issue mentioned).<br><br>This proposed change would complicate the way we reason about when shuffle data is lost - and I am not inclined towards it if it is something that can be mitigated with appropriate tuning.
@hauntsaninja mind creating a JIRA, and keeping the PR description template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE)? See also https://spark.apache.org/contributing.html
【Thanks】, I updated the PR to keep the template. Do I need to have a JIRA? The issue here is trivial / I don't have an account / my previous PR to Spark didn't need a JIRA (https://github.com/apache/spark/pull/29264)
Could you re-trigger the failed linter 【test】s? It looks irrelevant.<br>```<br>python/pyspark/broadcast.py:106: error: Overloaded function implementation does not accept all possible arguments of signature 3  [misc]<br>Found 1 error in 1 file (【check】ed 511 source files)<br>```
^ That should be 【<font color=blue>fix】ed】 by https://github.com/apache/spark/pull/41086.
But let me retrigger anyway .. seems another 【test】 got stuck ..
Not sure why this fails from a cursory look. MyPy version looks the same. Should be caused by either Python version or dependency of MyPy.
@cloud-fan @viirya @wangyum  do you have other thought ?
> Do the benchmarks `CSVBenchmark` and `JsonBenchmark` show any 【improvement】s? Could you regenerate the results `JsonBenchmark.*.txt` and `CSVBenchmark.*.txt`, please.<br><br>I'm doing benchmarks, but I found a problem, like @gengliangwang said in https://github.com/apache/spark/pull/36562#issuecomment-1127600354 .The benchmarks no case for `the string inputs are not valid timestamps`. The 【speed】 up only work when `string input are not valid timestamps`. I'm worry about the benchmarks can't prove anything. Can I create a PR for add benchmarks for `type inference when string input are not valid timestamps`
> Can I create a PR for add benchmarks for type inference when string input are not valid timestamps<br><br>Yep. Let's do that.
@Hisoka-X Please, resolve the conflicts and rebase on the recent master.
> @Hisoka-X Please, resolve the conflicts and rebase on the recent master.<br><br>Ok, I will add benchmarks for this too. Please wait. 【Thanks】!
【Thanks】 @MaxGekk
cc @sunchao FYI
Merged to master, thanks @clownxc !
Welcome to the Apache Spark 【community】, @clownxc .<br>@sunchao added you to the Apache Spark contributor group and assigned SPARK-43410 to you.
@cloud-fan @allisonwang-db
Thank you, @jchen5 and @cloud-fan .
@HeartSaVioR
@chaoqin-li1123 - is the 【test】 failure related to the change ?<br><br>```<br> - Handle unsupported input of message type *** FAILED *** (12 milliseconds)<br> ```
I have addressed most pending comments. Could you take a look? @HeartSaVioR
Hi @HyukjinKwon, the 【test】s I add in this pr cause the sql 【test】 ci to timeout, is it possible to move streaming 【test】s to a separate 【test】 runnner?<br><br>`2023-05-18T21:02:41.4826861Z ##[error]The runner has received a shutdown signal. This can happen when the runner service is stopped, or a manually started runner is canceled.<br>2023-05-18T21:02:41.6228000Z Session terminated, killing shell...<br>2023-05-18T21:02:41.7441894Z ##[error]The operation was canceled.`
I can verify that all the streaming query integration 【test】s we override inherits StateStoreMetricsTest, which use a shared spark session. And they also use SQL conf to override state format version, so they should also respect the changelog 【check】pointing config we set.<br>RocksDBStateStoreIntegrationSuite  extends StreamTest, which have a shared spark session and use sql conf.<br>I do find that RocksDBStateStore suite ignore the sqlconf, have 【<font color=blue>fix】ed】 that.<br>
> If a bucket scan has no interesting partition or contains shuffle exchange, then we would disable it<br><br>qq: what should we disable?
@HyukjinKwon sorry for the confusion. It means bucket scan, the 【functionality】 of rule `DisableUnnecessaryBucketedScan` is auto disable bucket scan when the bucket scan is unnecessary, and this pr takes table cache into account. I have replaced it in pr description.
thanks @dongjoon-hyun and @cloud-fan
@MaxGekk 【Thanks】 for the pointer, I wasn't sure where to add the 【test】. I've added one (and that one fails if I revert the change).
@Fokko Rename PR title from `[SPARK-43425][CORE]` to `[SPARK-43425][SQL]`?
Thank you, @Fokko , @MaxGekk , @wangyum , @aokolnychyi .<br><br>Also, cc @cloud-fan too.<br><br>If the JIRA issue is categorized as `Bug`, we may want to bring this to branch-3.4.
【Thanks】 @dongjoon-hyun for 【review】ing. 【Thanks】 @MaxGekk and @aokolnychyi for the 【review】. I've changed it to a bug in Jira and I'll create a backport in a second.
cc @ryan-johnson-databricks @cloud-fan
@cloud-fan 【test】s passed after I 【<font color=blue>fix】ed】 scalastyle
cc @srowen
Here are some key code segments:<br><br>1. Old SparkUI  goes into `com.google.common.cache.LocalCache#removalNotificationQueue` when it is removed from `appCache`.<br><br>**com.google.common.cache.LocalCache.Segment**<br>```java<br>    @GuardedBy(\"Segment.this\")<br>    @Nullable<br>    ReferenceEntry<K, V> removeValueFromChain(ReferenceEntry<K, V> first,<br>        ReferenceEntry<K, V> entry, @Nullable K key, int hash, ValueReference<K, V> 【value】Reference,<br>        RemovalCause cause) {<br>      // Put removed 【value】 into removalNotificationQueue<br>      enqueueNotification(key, hash, 【value】Reference, cause);<br>      writeQueue.remove(entry);<br>      accessQueue.remove(entry);<br><br>      if (【value】Reference.isLoading()) {<br>        【value】Reference.notifyNewValue(null);<br>        return first;<br>      } else {<br>        return removeEntryFromChain(first, entry);<br>      }<br>    }<br>```<br><br>2. Execution of `removalListener#onRemoval` is not guarded by `appCache`'s lock<br><br>**com.google.common.cache.LocalCache.Segment**<br>```java<br>    V lockedGetOrLoad(K key, int hash, CacheLoader<? super K, V> loader)<br>        throws ExecutionException {<br>      ReferenceEntry<K, V> e;<br>      ValueReference<K, V> 【value】Reference = null;<br>      LoadingValueReference<K, V> loadingValueReference = null;<br>      boolean createNewEntry = true;<br><br>      lock();<br>      try {<br>         ...<br>      } finally {<br>        unlock();<br>        // `removalListener#onRemoval` executes inside postWriteCleanup()<br>        postWriteCleanup();<br>      }<br>      ...<br>    }<br>```<br><br>3. New SparkUI is detached unexpectedly during detaching of old SparkUI<br><br>**FsHistoryProvider**<br>```scala<br>  override def onUIDetached(appId: String, attemptId: Option[String], ui: SparkUI): Unit = {<br>    val uiOption = synchronized {<br>      // After new SparkUI is loaded, `activeUIs` contains new SparkUI<br>      activeUIs.remove((appId, attemptId))<br>    }<br>    uiOption.foreach { loadedUI =><br>      loadedUI.lock.writeLock().lock()<br>      try {<br>        loadedUI.ui.store.close()<br>      } finally {<br>        loadedUI.lock.writeLock().unlock()<br>      }<br><br>      diskManager.foreach { dm =><br>        // If the UI is not valid, delete its files from disk, if any. This relies on the fact that<br>        // ApplicationCache will never call this method 【concurrent】ly with getAppUI() for the same<br>        // appId / attemptId.<br>        dm.release(appId, attemptId, delete = !loadedUI.valid)<br>      }<br>    }<br>  }<br>```
@srowen @LuciferYang Added some code segments to help to understand this issue.
don't remember the cache stuff, sorry. I know we have a problem in hadoop where cache lookup can trigger >1 fs creation and the same time, and if that is slow then at best: needless work, at worst: conflict and sometimes failures. so we use a semaphore to limit the #of threads which can create a new FS at the same time (HADOOP-17313). spark/tez workers and cloud stores doing network IO in initialize() are the troublespot here, FWIW.
> don't remember the cache stuff, sorry. I know we have a problem in hadoop where cache lookup can trigger >1 fs creation and the same time, and if that is slow then at best: needless work, at worst: conflict and sometimes failures. so we use a semaphore to limit the #of threads which can create a new FS at the same time (HADOOP-17313). spark/tez workers and cloud stores doing network IO in initialize() are the troublespot here, FWIW.<br><br>@steveloughran 【Thanks】 for your inputs.<br><br>In SparkHistoryServer, SparkUIs are tracked by <appId, attemptId> in many places:<br>1. `ApplicationCache#appCache`<br>2. `FsHistoryProvider#activeUIs`<br>3. `HistoryServerDiskManager#active`<br>4. Disk-based KVStore backend local path: appStoreDir/\\<appId>_\\<attemptId>/<br>5. Disk-based KVStore backend local file lock: appStoreDir/\\<appId>_\\<attemptId>/LOCK<br><br>All the above places assumes there is only one SparkUI loaded for one <appId, attemptId> pair.<br><br>Guava LoadingCache can ensure this when SparkUIs are guarded by its locks.<br>But SparkUI's detaching is not guarded by LoadingCache's lock.  <br><br>This PR is aimed to ensure this from SparkUI's loading to detaching.
@LuciferYang Exception \"java.lang.IllegalStateException: DB is closed.\" can be reproduced in 【test】 now.<br><br><img width=\"1138\" alt=\"image\" src=\"https://github.com/apache/spark/assets/88070094/cea0d752-4323-40ef-b4d1-5ec457fc267e\"><br>
+CC @thejdeep
cc @gengliangwang and @sarutak too FYI
The GA [Run / Build modules: pyspark-sql, pyspark-mllib, pyspark-resource](https://github.com/MaxGekk/spark/actions/runs/4934033324/jobs/8818623610#logs) frozen in the run https://github.com/MaxGekk/spark/runs/13365446543 but it passed in https://github.com/MaxGekk/spark/runs/13370236086
Merging to master. Thank you, @HyukjinKwon @vicennial @amaliujia for 【review】.
cc @rangadi i've made a draft implementation here but just wanted to get your thoughts quickly on:<br><br>1. does the problem / solution make sense?<br>2. should we make this behavior the default or should we add an option to turn it on? <br><br>thanks 🙏
also cc @HyukjinKwon as you 【review】ed https://github.com/apache/spark/pull/31921 and have 【review】ed many proto 【c<font color=blue>hang】es】 too 😅
Where is the information loss or overflow? Java code generated by Protobuf for a uint32 field also returns an `int`, not `long`.
> Where is the information loss or overflow? Java code generated by Protobuf for a uint32 field also returns an `int`, not `long`.<br><br>sorry i didn't get a chance to reply to this until now. There is no information loss, technically, as uint32 is 4 bytes and uint64 is 8 bytes, same as int and long respectively. However, there is overflow in the representation.<br><br>Here's an example:<br><br>Consider a protobuf message like:<br>```<br>syntax = \"proto3\";<br><br>message Test {<br>  uint64 val = 1;<br>}<br>```<br><br>Generate a protobuf with a 【value】 above 2^63. I did this in python with a small script like:<br><br>```<br>import 【test】_pb2<br><br>s = 【test】_pb2.Test()<br>s.val = 9223372036854775809 # 2**63 + 1<br>serialized = s.SerializeToString()<br>print(serialized)<br>```<br><br>This generates the binary representation:<br><br>```<br>b'\\x08\\x81\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x01'<br>```<br><br>Then, deserialize this using `from_protobuf`. I did this in a notebook so its easier to see, but could reproduce in a scala 【test】 as well:<br><br><img width=\"597\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1002986/a6c58c19-b9d3-44d4-8c2a-605991d3d5de\"><br><br><br>This is exactly what we'd expect when you take a 64 bit number with the highest bit as `1` and then try to interpret it as a signed number (long), it becomes negative.<br><br>So this PR proposes some 【c<font color=blue>hang】es】 to the deserialization behavior. However, I don't know if its right to change the default or have an option to allow unpacking as a larger type.<br><br>
>  So this PR proposes some 【c<font color=blue>hang】es】 to the deserialization behavior. However, I don't know if its right to change the default or have an option to allow unpacking as a larger type.<br><br>What if you have a UDF that converts this to BigDecimal? Will you get the 【value】 back?<br>I guess that is the intention behind why protobuf-java casts unsiged to signed in its Java methods. <br>I think it 【simple】r to go this way. Given these kinds of issues, I guess it is not a good practice to use unsiged in protobuf. It can be intepreted correctly at application level when they are infact used this way.
> What if you have a UDF that converts this to BigDecimal? Will you get the 【value】 back? I guess that is the intention behind why protobuf-java casts unsiged to signed in its Java methods. I think it 【simple】r to go this way. <br><br>Yeah, there is no information loss so you can get the right 【value】 the way I did in this PR (Integer.toUnsignedLong, Long.toUnsignedString). I think, though, it's useful if the `spark-protobuf` library can do this; the burden of taking a struct and trying to do this transformation is cumbersome, in my opinion.<br><br>However, one additional piece of information is that **for unsigned types in parquet, the default behavior is to represent them in larger types**. I put this in the PR description but see this ticket https://issues.apache.org/jira/browse/SPARK-34817 implemented in this PR: https://github.com/apache/spark/pull/31921. Or the existing code today https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala#L243-L247 which shows that **by default** parquet unsigned 【value】s are actually expanded to larger types in spark.<br><br>So, since this same problem/solution exists in another storage format, I think its useful to implement this behavior here as well. I also think that it actually _does_ make sense to do it by default, as parquet already does this. However, i'm open also to doing this transformation behind an option so that no existing usages are broken. Mainly, I want to just make sure we do  what is the most correct and broadly consistent thing to do (and i'm not really sure exactly what that is, and would love some other inputs). cc @HyukjinKwon as well here since you 【review】ed the original PR doing this for parquet!<br><br>
A late question: Should `error/error-classes.json` and `SparkThrowableHelper` be moved together to the `common-utils` module?<br><br>
cc @MaxGekk @cloud-fan @hvanhovell
Hi @Hisoka-X <br><br>Good day. I would like to know whether this pull request to for spark sql query with delta lake delta table v2.<br>I simply encounter this issue with spark 3.2 (delta lake 1.2) and spark 3.3 (delta lake 2.2), which I think this pull request could help resolve this issue.<br><br>![image](https://github.com/apache/spark/assets/51392682/591ada27-5525-402e-995d-14f5c3c3f11f)<br><br>![image](https://github.com/apache/spark/assets/51392682/0908e173-325e-4057-be6b-c9ae5e990f8f)<br><br>I saw this ANALYZE TABLE command is originated from spark : https://spark.apache.org/docs/3.3.1/sql-ref-syntax-aux-analyze-table.html#analyze-table. But delta lake 【documentation】 do not mention ANALYZE TABLE usage https://docs.delta.io/2.2.0/delta-intro.html.<br><br>Since this pull request will merge to master branch and want to know **whether it will backfill to spark 3.2, spark 3.3 also**. <br><br>Thank you<br><br>Best regards,<br>Jerry
> I would like to know whether this pull request to for spark sql query with delta lake delta table v2.<br><br>This PR suite for all DataSource V2 which support `SupportsReportStatistics` interface.<br><br>> Since this pull request will merge to master branch and want to know whether it will backfill to spark 3.2, spark 3.3 also.<br><br>I'm not sure for this, seem like only merge into master.  @HyukjinKwon Can you help to answer this question? 【Thanks】.
Hi @HyukjinKwon ,<br><br>I would like to know this 【feature】 will be backfilled to spark 3.2, spark 3.3 also? It is important for us to 【review】 our OSS spark version.<br><br>Best regards,<br>Jerry Lin
@HyukjinKwon thanks for updating the title 👍🏻
Also, cc @MaxGekk
Thank you, @Fokko , @MaxGekk , @gengliangwang , @HyukjinKwon !
@HyukjinKwon @rangadi I am going to disable this 【test】 for now, also create https://issues.apache.org/jira/browse/SPARK-43435 to track this issue.
Hi, @cloud-fan @ulysses-you @wangyum  could you help to 【review】 this PR ? 【Thanks】
cc @rednaxelafx
and @peter-toth
Hi, @rednaxelafx @peter-toth could you help to 【review】 this PR ? 【Thanks】
> Hi, @rednaxelafx @peter-toth could you help to 【review】 this PR ? 【Thanks】<br><br>Hi @wankunde, thanks for pinging me. I can take a look at this PR sometime next week or the week after...
will update result of `StateStoreBasicOperationsBenchmark` later
When I tried this 【upgrade】, I found that the result of 'StateStoreBasicOperationsBenchmark' was unexpected. For 【check】 if it was a new version(8.1.1.1) issue, I also ran the 'StateStoreBasicOperationsBenchmark' on the master branch(with 8.0.0), and there were significant differences between the 【test】 result and the previous records and 'StateStoreBasicOperationsBenchmark' also run timeout（more than 6hours, It should have been completed in 3 hours before）:<br><br>https://github.com/LuciferYang/spark/actions/runs/4949396450/jobs/8856766625<br><br>```<br>[【success】] Total time: 791 s (13:11), completed May 11, 2023 7:33:07 PM<br>23/05/11 19:33:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable<br>Running org.apache.spark.sql.execution.benchmark.StateStoreBasicOperationsBenchmark:<br>23/05/11 19:33:15 WARN SparkContext: The JAR file:/home/runner/work/spark/spark/core/target/scala-2.12/spark-core_2.12-3.5.0-SNAPSHOT-【test】s.jar at spark://localhost:44659/jars/spark-core_2.12-3.5.0-SNAPSHOT-【test】s.jar has been added already. Overwriting of added jar is not supported in the current version.<br>Running benchmark: putting 10000 rows (10000 rows to overwrite - rate 100)<br>  Running case: In-memory<br>  Stopped after 10000 iterations, 82351 ms<br>  Running case: RocksDB (trackTotalNumberOfRows: true)<br>  Stopped after 10000 iterations, 599839 ms<br>  Running case: RocksDB (trackTotalNumberOfRows: false)<br>  Stopped after 10000 iterations, 210482 ms<br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (10000 rows to overwrite - rate 100):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>---------------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                            7              8           1          1.4         739.9       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                              58             60           1          0.2        5828.2       0.1X<br>RocksDB (trackTotalNumberOfRows: false)                             20             21           0          0.5        2033.4       0.4X<br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (7500 rows to overwrite - rate 75):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>-------------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                          8              9           1          1.3         772.9       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                            56             58           1          0.2        5567.7       0.1X<br>RocksDB (trackTotalNumberOfRows: false)                           21             23           4          0.5        2126.2       0.4X<br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (5000 rows to overwrite - rate 50):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>-------------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                          8              9           1          1.3         760.7       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                            51             53           1          0.2        5089.3       0.1X<br>RocksDB (trackTotalNumberOfRows: false)                           21             22           1          0.5        2133.7       0.4X<br><br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (2500 rows to overwrite - rate 25):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>-------------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                          7              8           1          1.3         747.6       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                            46             47           1          0.2        4603.3       0.2X<br>RocksDB (trackTotalNumberOfRows: false)                           21             22           1          0.5        2141.8       0.3X<br><br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (1000 rows to overwrite - rate 10):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>-------------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                          7              8           1          1.4         732.3       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                            43             44           1          0.2        4283.7       0.2X<br>RocksDB (trackTotalNumberOfRows: false)                           21             22           1          0.5        2132.0       0.3X<br><br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (500 rows to overwrite - rate 5):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>-----------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                        7              8           1          1.4         732.3       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                          42             43           1          0.2        4169.4       0.2X<br>RocksDB (trackTotalNumberOfRows: false)                         21             22           1          0.5        2124.4       0.3X<br><br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>putting 10000 rows (0 rows to overwrite - rate 0):  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>---------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                      7              8           1          1.4         727.9       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                        40             42           1          0.2        4038.6       0.2X<br>RocksDB (trackTotalNumberOfRows: false)                       21             22           1          0.5        2129.9       0.3X<br><br>.......<br><br>OpenJDK 64-Bit Server VM 1.8.0_362-b09 on Linux 5.15.0-1037-azure<br>Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz<br>evicting 1000 rows (maxTimestampToEvictInMillis: 999) from 10000 rows:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative<br>-----------------------------------------------------------------------------------------------------------------------------------------------------<br>In-memory                                                                          5              5           0          2.2         458.3       1.0X<br>RocksDB (trackTotalNumberOfRows: true)                                             9              9           0          1.1         871.2       0.5X<br>RocksDB (trackTotalNumberOfRows: false)                                            5              6           0          1.9         518.3       0.9X<br><br>Running benchmark: evicting 500 rows (maxTimestampToEvictInMillis: 499) from 10000 rows<br>  Running case: In-memory<br>  Stopped after 10000 iterations, 50617 ms<br>  Running case: RocksDB (trackTotalNumberOfRows: true)<br>Error: The operation was canceled.<br>```<br><br>I will try to investigate this issue, cc @dongjoon-hyun @HeartSaVioR FYI<br><br><br>EDIT:  The `RocksDB (trackTotalNumberOfRows: false) ` scene looks twice as slow as before
Thank you for your 【investigation】, @LuciferYang !
That's because we no longer use writebatch which has been problematic on memory usage. We should have probably run the benchmark and updated the result...<br><br>The overall 【performance】 won't be significantly reduced as we pay the cost in each operation without writebatch (which leads slowdown in benchmark), whereas we are going to pay the cost at once in commit phase when we use writebatch & flush in commit phase. (We benchmarked by ourselves.)<br><br>cc. @anishshri-db Would you mind adding more context here? We probably need to update the benchmark, with reduce of the number of operations.
If this is expected, optimizing benchmark 【test】ing should be more reasonable, as current benchmark 【test】ing will fail due to GA timeout<br><br>
@LuciferYang - As @HeartSaVioR mentioned, this is part of our broader effort to reduce memory usage for rocksdb. As part of that change, we don't use writeBatch any more and have also moved to disable the write ahead log. As part of this, puts/deletes will be written to memtables and gets will also be served from memtables/block cache which might lead to some difference in perf for those options. We can cover up for most of that since the db write during commit is now eliminated and we can also do periodic flushing with a related upcoming change. That overall portion, I believe is not covered by the b/mark 【test】 that we have today.<br><br>Could you let me know which b/mark 【test】 will fail and how we run that ?<br><br>@HeartSaVioR - do you proposing changing the b/mark itself ? or rerunning and updating the results page ?
@anishshri-db I need to update the results of `StateStoreBasicOperationsBenchmark` when 【upgrade】 rocksdbjni, we can run `StateStoreBasicOperationsBenchmark` with GA as following:<br><br><img width=\"355\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/b5ce0ad4-b3e8-48a1-bc52-3db3b0dfa5ad\"><br> <br>It did not fail and was killed by GA due to timeout. Previously, `StateStoreBasicOperationsBenchmark` was executed for 3 hours(The last time I updated the rocksdbjni version), but now it has been executed for more than 6 hours and has not been completed yet.<br>
@LuciferYang - ok cool thanks. I'll update the b/mark and try running this workflow.
【Thanks】 @anishshri-db
Merged to master for Apache Spark 3.5.0. Thank you, @LuciferYang and all!
Welcome to the Apache Spark 【community】, @TQJADE .<br>I added you to the Apache Spark contributor group and assigned SPARK-43441 to you.
cc @rednaxelafx @dongjoon-hyun , Could you take a look if you find a time, thanks : )
Thank you! Merged to master.
It seems that `【test】_ops_on_diff_frames_*` is much slower atop Connect<br><br>```<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/e0c7d560-dd29-408d-9f7e-834cd2cc683a/python3.9__pyspark.pandas.【test】s.【test】_ops_on_diff_frames__j1jfu3kt.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames (177s)<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/bff4fd9b-cf0b-4847-bc19-08114b9b106c/python3.9__pyspark.pandas.【test】s.【test】_ops_on_diff_frames_groupby__0fb3fdq5.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_groupby (116s)<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/89a994de-9f04-4ed8-aa77-16d95b79956e/python3.9__pyspark.pandas.【test】s.【test】_ops_on_diff_frames_slow__zjjj9b5a.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_slow (163s)<br>```<br><br>```<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/80844664-a5e0-4685-804e-8eff938c6681/python3.9__pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames__mdlsorlh.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames (1191s)<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/bb5e908e-9185-46ac-826f-a2055f247f43/python3.9__pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_groupby__50hudnbv.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_groupby (443s) ... 8 【test】s were skipped<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/a20e748b-8b2d-493c-b926-ade588f0cb7a/python3.9__pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_slow__c78c3kwt.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_slow (911s) ... 7 【test】s were skipped<br>```<br><br>cc @itholic @HyukjinKwon
> It seems that `【test】_ops_on_diff_frames_*` is much slower atop Connect<br><br>Interesting.. May I ask if CI jobs run those Connect 【test】s on a single node?
> > It seems that `【test】_ops_on_diff_frames_*` is much slower atop Connect<br>> <br>> Interesting.. May I ask if CI jobs run those Connect 【test】s on a single node?<br><br>yes, all 【test】s are on single node. I am afraid there maybe some 【performance】 issue in pandas on spark on connect
In the [la【test】 run](https://github.com/zhengruifeng/spark/actions/runs/4975618218/jobs/8902974576):<br><br>the `pyspark-pandas-connnect` took 1h 25m 21s, and `pyspark-pandas-slow-connnect` took 1h 45m 22s, so I think the split itself should be fine.<br><br>but still see :<br><br>```<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/bcf91e64-da92-456c-a65f-2acbb5a57c5e/python3.9__pyspark.pandas.【test】s.【test】_ops_on_diff_frames__vhzam369.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames (183s)<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/21464771-1bb4-4e2e-8a76-cc5424bc3eee/python3.9__pyspark.pandas.【test】s.【test】_ops_on_diff_frames_groupby__3jskgr8q.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_groupby (121s)<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/4ec97d5c-ff3a-474f-aaa4-a3e08b503667/python3.9__pyspark.pandas.【test】s.【test】_ops_on_diff_frames_slow__bfsd6ob8.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.【test】_ops_on_diff_frames_slow (167s)<br>```<br><br><br>```<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames (temp output: /__w/spark/spark/python/target/7d09fc80-b92b-4872-ae33-62f61aa4d2d9/python3.9__pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames__3d49xqjs.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames (1018s)<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_groupby (temp output: /__w/spark/spark/python/target/6542f9a4-39fb-4605-9e7b-6d9a3e7b5a63/python3.9__pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_groupby__swoyoc14.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_groupby (350s) ... 8 【test】s were skipped<br>Starting 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_slow (temp output: /__w/spark/spark/python/target/7880eb8c-9299-4e1c-af32-daa48aa10715/python3.9__pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_slow__y24aawsj.log)<br>Finished 【test】(python3.9): pyspark.pandas.【test】s.connect.【test】_parity_ops_on_diff_frames_slow (732s) ... 7 【test】s were skipped<br>```<br><br><br>I file https://issues.apache.org/jira/browse/SPARK-43501 to track the 【test】s' 【performance】, would you mind taking a look? @itholic
Thank you so much, @zhengruifeng , @HyukjinKwon , @xinrong-meng , @itholic !
thank you all for the 【review】s, @dongjoon-hyun @HyukjinKwon @xinrong-meng @itholic
I verified manually. Merged to master.<br>```<br>$ build/sbt \"catalyst/【test】Only org.apache.spark.sql.catalyst.expressions.ExpressionImplUtilsSuite\"<br>...<br>[info] ExpressionImplUtilsSuite:<br>[info] - AesDecrypt Only (43 milliseconds)<br>[info] - AesEncrypt and AesDecrypt (3 milliseconds)<br>[info] Run completed in 993 milliseconds.<br>[info] Total number of 【test】s run: 2<br>[info] Suites: completed 1, aborted 0<br>[info] Tests: succeeded 2, failed 0, canceled 0, ignored 0, pending 0<br>[info] All 【test】s passed.<br>[【success】] Total time: 17 s, completed May 10, 2023, 11:10:40 PM<br>```
This code rn is messy because it's based on unmerged PR: https://github.com/apache/spark/pull/41026<br><br>Only need to look at the part I commented
@WweiL I can run your 【test】 succesfully with SBT commands:<br>```<br>./build/sbt package -Phive -Pconnect<br>sbt \"【test】Only org.apache.spark.sql.streaming.StreamingQuerySuite\"<br>```<br><br>You got the `ClassNotFound` exception probably because you were trying to run via the shell?<br>It would work with Ammonite shell. If you define the user defined writer via the shell, it should also work.
If you hit `stream classdesc serialVersionUID = -2719662620125650908, local class serialVersionUID = 6534627183855972490`<br><br>It means the client has a version and server has another, when trying to mapping them java failed to match the UID.<br>The solution is to let the client use the server version and move the server side version into a `sql-util` package.
Getting a scala 【test】 related error, the code works fine in REPL<br>```<br> - foreach Row *** FAILED *** (15 milliseconds)<br>[info]   java.io.NotSerializableException: org.scala【test】.Engine<br>[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)<br>[info]   at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)<br>[info]   at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)<br>[info]   at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)<br>[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)<br>[info]   at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)<br>[info]   at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)<br>[info]   at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)<br>[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)<br>[info]   at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)<br>[info]   at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)<br>[info]   at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)<br>[info]   at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)<br>[info]   at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)<br>[info]   at org.apache.spark.util.Utils$.serialize(Utils.scala:126)<br>[info]   at org.apache.spark.sql.streaming.DataStreamWriter.foreach(DataStreamWriter.scala:226)<br>[info]   at org.apache.spark.sql.streaming.StreamingQuerySuite.$anonfun$new$12(StreamingQuerySuite.scala:204)<br>[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[info]   at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[info]   at org.scala【test】.TestSuite.withFixture(TestSuite.scala:196)<br>[info]   at org.scala【test】.TestSuite.withFixture$(TestSuite.scala:195)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>[info]   at scala.collection.immutable.List.foreach(List.scala:431)<br>[info]   at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[info]   at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[info]   at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.Suite.run(Suite.scala:1114)<br>[info]   at org.scala【test】.Suite.run$(Suite.scala:1096)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[info]   at org.apache.spark.sql.streaming.StreamingQuerySuite.org$scala【test】$BeforeAndAfterAll$$super$run(StreamingQuerySuite.scala:35)<br>[info]   at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[info]   at org.apache.spark.sql.streaming.StreamingQuerySuite.run(StreamingQuerySuite.scala:35)<br>[info]   at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[info]   at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[info]   at java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>[info]   at java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>[info]   at java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>[info]   at java.lang.Thread.run(Thread.java:750)<br>```
Thank you, @HyukjinKwon .<br>All 【test】 passed. Merged to master.
Thank you, @HyukjinKwon . <br>AppVeyor passed. Merged to master.
cc @dongjoon-hyun @sunchao @steveloughran
we emptied the jar but left the stub artifact there so that things which did explicitly pull it in wouldn't start breaking.<br><br>Now that spark is 3.3.5+ only most of the hadoop-cloud-storage dependencies can be reworked down to just<br>* import hadoop-cloud-storage<br>* cut alliyun-sdk if you don't want it (the 3.3.5 version isn't breaking s3 any more, FWIW)<br>* add google gcs
@steveloughran does Hadoop 3.3.5 guarantee 【compatibility】 w/ previous versions? e.g. is it OK to use Hadoop 3.3.5 client to access Hadoop 3.3.0~3.3.4 server?
SPARK-42537 covers the full 【clean】up.<br><br>w.r.t this patch: 【LGTM】.
>  is it OK to use Hadoop 3.3.5 client to access Hadoop 3.3.0~3.3.4 server?<br><br>should be. IPC is all based on protobuf and we try not to remove things to avoid breaking existing code. HDFS 【compatibility】 across major versions is something which mattersd a lot, I believe webhdfs has the strongest guarantees.<br><br>what does break, guaranteed, is mixing hadoop libraries from different versions on the classpath. Avoid that. on and for cloudstuff openssl/wildfly is a source of extreme brittleness, even though when it works it's often 【fast】er than JVM ssl<br><br>
Got it, thanks @steveloughran
Thank you, @HyukjinKwon , @wangyum , @zhengruifeng , @LuciferYang .<br>Every jobs are triggered 【success】fully and running.<br>Since this is irrelevant to the 【test】 results, I'll merge to master for Apache Spark 3.5.0.
cc @dongjoon-hyun @srowen FYI<br>
> How many PRs do you want to make<br><br>No overall estimation, I have been working on this area recently, and would like to propose change when I find something has room for 【improvement】.
Then, let's hold on this PR for a while because we don't need to take any risk.
Please change the subject: `[SPARK-43457][CONNECT][PYTHON]`
gental ping @dongjoon-hyun  @pan3793
Please follow the https://spark.apache.org/contributing.html to<br><br>> Go to “Actions” tab on your forked repository and enable “Build and 【test】” and “Report 【test】 results” workflows
Gentle ping @dongjoon-hyun
gentle ping @holdenk
thanks for the comments, I will 【check】 it.
Waiting for CI.
CI 【test】 `single listener, 【check】 trigger events are generated correctly` failed, which should be irrelevant to this change?
@bozhang2820 Could you rebase on the recent master and re-trigger GAs, please.
Looking at the two last commit, seems like just flaky 【test】s.<br><br>Merging to master. Thank you, @bozhang2820.
I found some downloading log when run `dev/make-distribution.sh --tgz` with this pr<br><br><img width=\"1481\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/7d12d1a0-fb62-41ee-a7a8-63efc5fac0ee\"><br><br><img width=\"1535\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/5bc7ac2e-7f43-4a79-b770-637b66a2846c\"><br><br><br>is this expected
@wangyum I think we should not skip building the 【test】 jar. I 【test】 the following commands with this pr:<br><br>```<br>build/mvn versions:set -DnewVersion=3.5.0.1<br>dev/make-distribution.sh --tgz <br>```<br><br>then the build will failed as follows:<br><br>```<br>[INFO] -----------------< org.apache.spark:spark-sketch_2.12 >-----------------<br>[INFO] Building Spark Project Sketch 3.5.0.1                             [3/30]<br>[INFO]   from common/sketch/pom.xml<br>[INFO] --------------------------------[ jar ]---------------------------------<br>Downloading from gcs-maven-central-mirror: https://maven-central.storage-download.googleapis.com/maven2/org/apache/spark/spark-tags_2.12/3.5.0.1/spark-tags_2.12-3.5.0.1-【test】s.jar<br>Downloading from central: https://repo.maven.apache.org/maven2/org/apache/spark/spark-tags_2.12/3.5.0.1/spark-tags_2.12-3.5.0.1-【test】s.jar<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Reactor Summary for Spark Project Parent POM 3.5.0.1:<br>[INFO] <br>[INFO] Spark Project Parent POM ........................... SUCCESS [  3.746 s]<br>[INFO] Spark Project Tags ................................. SUCCESS [  4.028 s]<br>[INFO] Spark Project Sketch ............................... FAILURE [01:16 min]<br>[INFO] Spark Project Local DB ............................. SKIPPED<br>[INFO] Spark Project Networking ........................... SKIPPED<br>[INFO] Spark Project Shuffle Streaming Service ............ SKIPPED<br>[INFO] Spark Project Unsafe ............................... SKIPPED<br>[INFO] Spark Project Common Utils ......................... SKIPPED<br>[INFO] Spark Project Launcher ............................. SKIPPED<br>[INFO] Spark Project Core ................................. SKIPPED<br>[INFO] Spark Project ML Local Library ..................... SKIPPED<br>[INFO] Spark Project GraphX ............................... SKIPPED<br>[INFO] Spark Project Streaming ............................ SKIPPED<br>[INFO] Spark Project Catalyst ............................. SKIPPED<br>[INFO] Spark Project SQL .................................. SKIPPED<br>[INFO] Spark Project ML Library ........................... SKIPPED<br>[INFO] Spark Project Tools ................................ SKIPPED<br>[INFO] Spark Project Hive ................................. SKIPPED<br>[INFO] Spark Project REPL ................................. SKIPPED<br>[INFO] Spark Project Assembly ............................. SKIPPED<br>[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED<br>[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED<br>[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED<br>[INFO] Spark Project Examples ............................. SKIPPED<br>[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED<br>[INFO] Spark Avro ......................................... SKIPPED<br>[INFO] Spark Project Connect Common ....................... SKIPPED<br>[INFO] Spark Project Connect Server ....................... SKIPPED<br>[INFO] Spark Project Connect Client ....................... SKIPPED<br>[INFO] Spark Protobuf ..................................... SKIPPED<br>[INFO] ------------------------------------------------------------------------<br>[INFO] BUILD FAILURE<br>[INFO] ------------------------------------------------------------------------<br>[INFO] Total time:  01:24 min<br>[INFO] Finished at: 2023-05-12T00:15:34+08:00<br>[INFO] ------------------------------------------------------------------------<br>[ERROR] Failed to execute goal on project spark-sketch_2.12: Could not resolve dependencies for project org.apache.spark:spark-sketch_2.12:jar:3.5.0.1: Could not transfer artifact org.apache.spark:spark-tags_2.12:jar:【test】s:3.5.0.1 from/to central (https://repo.maven.apache.org/maven2): transfer failed for https://repo.maven.apache.org/maven2/org/apache/spark/spark-tags_2.12/3.5.0.1/spark-tags_2.12-3.5.0.1-【test】s.jar: Connect to repo.maven.apache.org:443 [repo.maven.apache.org/151.101.76.215, repo.maven.apache.org/2a04:4e42:12:0:0:0:0:215] failed: No route to host (connect failed) -> [Help 1]<br>```<br><br>the 【test】 jar `spark-tags_2.12-3.5.0.1-【test】s.jar` cannot be downloaded<br><br>
Thank you @LuciferYang. It's not expected.
【Thanks】 @wangyum @dongjoon-hyun @pan3793 . Merged to master.
I do believe the failed 【test】 suite `HealthTrackerIntegrationSuite` is not related to my 【c<font color=blue>hang】es】. Though, I have 【check】ed it locally.<br><br>Merging to master. Thank you, @HyukjinKwon and @cloud-fan for 【review】.
These are part of spark env tab, right ? Why do we need to log them ?<br>... oh wait, you want to log this in each node as well ?<br><br>Agree with @HyukjinKwon, we should do it when driver/executor starts up.
@ueshin @HyukjinKwon @zhengruifeng would you please 【review】?
Please free to leave comments if any, I'll adjust them in follow-ups.
Merged to master. Thank you, @ueshin and @allisonwang-db .
Does that refactoring still conform to UNSUPPORTED_DATA_TYPE_FOR_ARROW_VERSION?
@xinrong-meng <br><br>> Does that refactoring still conform to UNSUPPORTED_DATA_TYPE_FOR_ARROW_VERSION?<br><br>This PR doesn't change anything related to pyarrow version.
Sorry I meant `UNSUPPORTED_DATA_TYPE_FOR_ARROW_CONVERSION`. Do we have plans to remove the constraints?<br>@ueshin
Specifically, nested StructType, and MapType with keys/【value】s in StructType/TimestampType?
> Do we have plans to remove the constraints?<br><br>I'm not sure if it's planned, but now we can remove the constraints with a bit more work.
This looks 【great】, thanks for doing it @ueshin !
cc. @zsxwing @viirya @HyukjinKwon Please take a look. 【Thanks】!
@cloud-fan @HyukjinKwon Thank you for all!
cc @srowen @sunchao
There are other unexpected jars that were wrongly included in the assembly jar, would like to address them in separate PRs, since they were caused by different tickets. But I'm fine to fix all issues in one PR if the 【review】ers prefer.
cc @dongjoon-hyun @srowen @sunchao @LuciferYang
https://github.com/apache/spark/blob/46332d985e52f76887c139f67d1471b82e85d5ca/connector/kafka-0-10-assembly/pom.xml#L62-L66<br><br>This one should be `2.5.0` before, I think we should manually specify its version to keep the behavior unchanged ?<br><br>
> Thank you for doing this with nice 【analysis】. Ya, definitely, this was the goal. BTW, could you send an email to dev@spark because this is an important removal of dependency, @pan3793 ?<br><br>+1, Agree
@dongjoon-hyun @LuciferYang FYI, I have sent a mail to the mailing list<br>https://lists.apache.org/thread/xzrov348c3dq0d8jwcxf4j7fk7lb3r92
> https://github.com/apache/spark/blob/46332d985e52f76887c139f67d1471b82e85d5ca/connector/kafka-0-10-assembly/pom.xml#L62-L66<br>> <br>> This one should be `2.5.0` before, I think we should manually specify its version to keep the behavior unchanged ?<br><br>@LuciferYang The version here does not matter. What's matter is the `<scope>provided</scope>`, which is used to exclude deps from the assembly jar.
@zzzzming95 do you know in which PR 【<font color=blue>fix】ed】 the issue in master branch?
> do you know in which PR 【<font color=blue>fix】ed】 the issue in master branch?<br>@HyukjinKwon <br><br><br>https://github.com/apache/spark/pull/36637<br><br>It looks like this issue introduces `eagerlyExecuteCommands` so that AQE execute() is triggered before `InsertIntoHadoopFsRelationCommand`.
@johanl-db Are you working on the PR? Could you, please, address the comments above.
> @johanl-db Are you working on the PR? Could you, please, address the comments above.<br><br>I was away the past few days, I'm picking this up now. I addressed your comments, please take another look.
+1, 【LGTM】. Merging to master.<br>Thank you, @johanl-db.
Late 【LGTM】, thank you!
@panbingkun Isn't this a duplicate of https://github.com/apache/spark/pull/41155?
> @panbingkun Isn't this a duplicate of #41155?<br><br>It seems like it's duplicated. Should I turn it off?
> It seems like it's duplicated. Should I turn it off?<br><br>Please, keep it alive so far.
@panbingkun Could you close this PR, please.
> @panbingkun Could you close this PR, please.<br><br>OK
@cloud-fan @wzhfy , please help 【review】 this pr, thanks.
gentle ping @cloud-fan
I also think that the different results between 0 in ('00') and 0 = '00' are confusing, and seems hive already 【<font color=blue>fix】ed】 this problem.<br>Could you also take a look? @cloud-fan @MaxGekk
I think this is indeed an issue, but it seems a bit weird to special-case the 1-element-in-list case. Thoughts? @gengliangwang @srielau
BTW can we also 【check】 the behavior in other databases like mysql, postgres, oracle, etc.?
quickly 【check】 behavior in mysql, and `in ('00')` has same query result with `= '00'` . @cloud-fan <br>![image](https://github.com/apache/spark/assets/132866841/715bf24b-f700-4110-bc10-27c15a27fc0d)<br>and also postgres behavior is consistent.<br>![image](https://github.com/apache/spark/assets/132866841/826134c9-8c3c-431f-9f83-1f895fdfcee7)<br><br><br>
@cloud-fan Could you 【review】 the 【c<font color=blue>hang】es】?
cc @sunchao @mridulm @pan3793 FYI
【Thanks】 @sunchao @dongjoon-hyun @pan3793
Will update bench result and pr description later
quick question, do you have any reference of this expression in other DBMSes?
> quick question, do you have any reference of this expression in other DBMSes?<br><br>Just presto. https://prestodb.io/docs/current/functions/json.html#json_array_get
<img width=\"1007\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/342d7c37-95cb-4760-9829-5cdc4a4ebfc9\"><br><br>It seems that presto does not recommend using this function, and it may be removed in future presto version, so is it really necessary for Spark to support it?<br><br>
> <img alt=\"image\" width=\"1007\" src=\"https://user-images.githubusercontent.com/1475305/238357206-342d7c37-95cb-4760-9829-5cdc4a4ebfc9.png\"><br>> It seems that presto does not recommend using this function, and it may be removed in future presto version, so is it really necessary for Spark to support it?<br><br>The reason as presto say, the semantics of this function are broken. If we will not have the same issue, I think we can support it.
By the way, does Spark have any way to get json array 【value】 use specified index. This is a useful function, if we can achieve this requirement without this function, this function maybe unnecessary.
> By the way, does Spark have any way to get json array 【value】 use specified index. This is a useful function, if we can achieve this requirement without this function, this function maybe unnecessary.<br><br>I believe you cloud use get_json_object to archive the same result for `json_array_get`<br>```<br>scala> spark.sql(\"\"\"select get_json_object('[{\"a\":123},{\"b\":\"hello\"}]', \"$[0]\");\"\"\").show<br>+------------------------------------------------+<br>|get_json_object([{\"a\":123},{\"b\":\"hello\"}], $[0])|<br>+------------------------------------------------+<br>|                                       {\"a\":123}|<br>+------------------------------------------------+<br><br><br>scala> spark.sql(\"\"\"select get_json_object('[{\"a\":123},{\"b\":\"hello\"}]', \"$[1]\");\"\"\").show<br>+------------------------------------------------+<br>|get_json_object([{\"a\":123},{\"b\":\"hello\"}], $[1])|<br>+------------------------------------------------+<br>|                                   {\"b\":\"hello\"}|<br>+------------------------------------------------+<br>```
Seem like this function are unnecessary, I close the PR. 【Thanks】 @HyukjinKwon @LuciferYang @【advanced】xy
thanks @HyukjinKwon for 【review】s, merged to master
The CI should pass. Would you mind to take a look at this @cloud-fan @vanzin?  【Thanks】.
gently ping @cloud-fan @vanzin.
@LuciferYang would you mind to take a look at this?
Maybe we should keep connect client synchronized with this change, or at least add an `exclude` entry in `CheckConnectJvmClientCompatibility`
> Maybe we should keep connect client synchronized with this change, or at least add an `exclude` entry in `CheckConnectJvmClientCompatibility`<br><br>I will add `exclude` entry in `CheckConnectJvmClientCompatibility`.<br>I will implements it at `connect` later.<br>
@panbingkun run `ProtoToParsedPlanTestSuite:` with this pr, `function_levenshtein` failed as follows:<br>```<br>[info] - function_levenshtein *** FAILED *** (4 milliseconds)<br>[info]   Expected and actual plans do not match:<br>[info]   <br>[info]   === Expected Plan ===<br>[info]   Project [levenshtein(g#0, bob) AS levenshtein(g, bob)#0]<br>[info]   +- LocalRelation <empty>, [id#0L, a#0, b#0, d#0, e#0, f#0, g#0]<br>[info]   <br>[info]   <br>[info]   === Actual Plan ===<br>[info]   Project [levenshtein(g#0, bob, None) AS levenshtein(g, bob)#0]<br>[info]   +- LocalRelation <empty>, [id#0L, a#0, b#0, d#0, e#0, f#0, g#0] (ProtoToParsedPlanTestSuite.scala:177)<br>```<br><br>We should re-generate the golden files.
I am wondering whether it is better to follow PostgreSQL's semantics:<br><br>> If the actual distance is less than or equal to max_d, then levenshtein_less_equal returns the correct distance; otherwise it returns some 【value】 【great】er than max_d.<br><br>or to follow `org.apache.commons.text.similarity.LevenshteinDistance.limitedCompare`'s semantics to return -1 when the distance is 【great】er than the threshold (the current code).<br><br>I think the former is probably better: the optimizer can safely convert `levenshtein(s1, s2) < c` into `levenshtein(s1, s2, c) < c`, which I believe should be a quite common use case of `levenshtein`.<br><br>---<br><br>Update: never mind, I just realized that `levenshtein(s1, s2) < c` can also be converted into `levenshtein(s1, s2, c) != -1`  in the current semantics.
also cc @wangyum @beliefer FYI
+1, 【LGTM】. Merging to master.<br>Thank you, @panbingkun.
Are there any other similar cases<br><br>
> Are there any other similar cases<br><br>No further cases have been found so far.
cc @srielau @MaxGekk
cc @tgravescs @jerryshao Please help 【review】.
re【test】 please
+CC @zhouyejoe
Have you only seen this once?  Do you have any repro case or tried to make a 【test】 to simulate?<br><br>I'm a bit unclear exactly how this happens, from taking a quick look, both those functions that add and remove are synchronized.  Am I missing how these can be called and actually run at the same time?<br><br>You are essentially implying that the completed container is called after we launched the executor but before we could add it containerIdToExecutorIdAndResourceProfileId.  I do see from your logs, all that happened with the same second.  Maybe not crucial but do you know why that container was completed so quickly?<br><br>As from my comment, I don't see you 【clean】ing up this new datastructure, I would much rather look at either preventing this from running or perhaps having another launching state that you would 【check】 before removing.  But would like to understand above first.
@tgravescs we have seen it quite often when YARN queues were full and the containers were **immediately preempted after launch**.  I've updated the PR keeping track of containers launching executors which can be 【clean】ed up.
@tgravescs could you take another look?
+1 looks good
@HeartSaVioR @LuciferYang - pls take a look. Thx<br><br>Runs now complete ~40-50 mins.
Thank you, @anishshri-db , @LuciferYang , @HeartSaVioR . Merged to master for Apache Spark 3.5.0.
I think you may need to add `torcheval ` to https://github.com/apache/spark/blob/f55fdca10b1d9df2d8126cde07a26f89a75ae1d2/dev/infra/Dockerfile#L74
cc @sadikovi and @ueshin for a look.
@pan3793 Could you update the pr description?
+1 for @wangyum 's comment.
@wangyum @dongjoon-hyun, the PR description is updated, please take a look again.
Thank you for updating. Merged to master for Apache Spark 3.5.0.
friendly cc @MaxGekk
I have 【check】ed the last commit locally.<br><br>Merging to master. Thank you, @panbingkun.
I encountered the same issues w/ Spark 3.3.1. This sounds like a regression (I suppose it works before SPARK-25815, I don't have experience w/ running such an old version of Spark on K8s).<br><br>The key point is that the executor needs to download artifacts during the bootstrap phase, so the assumption in SPARK-25815 is not always true.<br><br>> adding the Hadoop config to the executor pods: this is not needed<br>> since the Spark driver will serialize the Hadoop config and send<br>> it to executors when running tasks.<br><br>Given the executor use `SparkHadoopUtil.get.newConfiguration(conf)` to construct Hadoop conf,  we can put the related hdfs/s3 configurations into `spark-defaults.conf` w/ `spark.hadoop.` prefix as a workaround.<br><br>https://github.com/apache/spark/blob/0df4c01b7c4d4476fe0de9dccb3425cc1295fc85/core/src/main/scala/org/apache/spark/executor/Executor.scala#L1006-L1012<br><br>This PR definitely fixes some use cases, @turboFei would you mind updating \"Does this PR introduce any user-facing change?\"
> would you mind updating \"Does this PR introduce any user-facing change?\"<br><br>updated
seems the k8s integration 【test】ing is stuck, will 【check】 this pr in our dev hadoop cluster tomorrow.
thanks for the comments, I will 【check】 it
> Thank you for making a PR, @turboFei .<br>> <br>> However, this PR might cause a outage because the number of configMap is controlled by quota.<br>> <br>> ```<br>> $ kubectl describe quota | grep configmaps<br>> count/configmaps                                                  4     150<br>> ```<br>> <br>> To avoid the production outage, this should be under a new configuration with `false` by default at least.<br><br>150 is a bit small for serious production usage, we may add this note in the `running_on_k8s.md`  【documentation】.<br><br>And BTW, this PR doesn't create new ConfigMaps, it either uses a user pre-set config map (no creation) or just reuse the config map created by driver which is created if necessary.
> this PR doesn't create new ConfigMaps, it either uses a user pre-set config map (no creation) or just reuse the config map created by driver which is created if necessary.<br><br>yes, this PR doesn't create new ConfigMap.
Oh, got it. Thank you for correcting me.
the UT has passed, gentle ping @dongjoon-hyun
thanks @【advanced】xy<br>added more UT to 【check】 `additionalPodSystemProperties` and executor pod.
gentle ping @dongjoon-hyun would you like to 【review】 again? thanks
The Hadoop configurations can be propagated after https://github.com/apache/spark/pull/27735. And putting and locating extra configuration files in SPARK_HOME/conf is also a suggested way from our docs, so is this step necessary?<br><br>Alternatively, if both exist, what is the precedence between them? Is it idempotent?
> And putting and locating extra configuration files in SPARK_HOME/conf is also a suggested way from our docs, so is this step necessary?<br><br>I think it is necessary.<br><br>Hadoop and spark are different components, it is better to maintain them separately.<br><br>In our company, we have conf version for hadoop conf, so we do not put hadoop config files under SPARK_HOME/conf, we use soft link to manage the hadoop conf.<br><br><br>> Alternatively, if both exist, what is the precedence between them? Is it idempotent?<br><br>In this pr, it just mounts the hadoop config map in the executor side(mounts HADOOP_CONF_DIR env) and the hadoop conf mounted is absolute same with that in driver pod.<br><br>As shown below, the SPARK_CONF_DIR has higher precedence. I think it is idempotent.<br>https://github.com/apache/spark/blob/e096bce604ed6fab37713437ac0d985673910537/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L68-L76<br><br>
gentle ping @yaooqinn @dongjoon-hyun
> Hadoop and spark are different components, it is better to maintain them separately.<br><br>I do not fully agree. In the early days, Hadoop may be special. We have a 【specific】 code path to read HADOOP_CONF_DIR. But now Hadoop is an option, as we have other options for storage and scheduling, especially on the cloud or Kubernetes.<br><br>Maybe we shall treat it like hive configurations or other third-party components to reduce the maintenance burden and complexity of the deployment.
> Maybe we shall treat it like hive configurations or other third-party components to reduce the maintenance burden and complexity of the deployment.<br><br>I believe different companies treat hadoop conf differently.<br><br>For ebay, we add conf version for hadoop conf, because it is used by<br>- public  hadoop client nodes<br>- private hadoop client nodes<br>- hadoop service nodes(nn, rm, hms, kyuubi)<br>- hadoop slave nodes(nm, dn)<br><br>and between different conf versions, there might be incompatibilities. <br><br><img width=\"974\" alt=\"image\" src=\"https://github.com/apache/spark/assets/6757692/83d503bf-bbfb-4ede-80e9-120ad8a24b38\"><br><br>and we have an RESTful service to download the hdaoop conf and we use soft link to manage them locally.<br><br>Recently, we are making spark migration, from spark3 + hadoop2 to spark3 + hadoop3.<br><br>For hadoop2 and hadoop3, the hadoop confs are even different.<br><br>So to manage the hadoop conf well and due to the current situation,  in ebay, we do not want to put the hadoop conf files and spark conf files together.<br><br><br>> treat it like hive configurations or other third-party components to reduce the maintenance burden and complexity of the deployment.<br><br>yes, I agree, it makes it easy.
I can also help with a use case for this, usually the submission client is on a single environment (Lets say we have it on cloud), and with spark on k8s, we can easily run jobs in different envs like in private Cloud Clusters being submitted from public Cloud. Where we would need diff properties to be passed for the submission client as well as for drivers and executors. This is also a use case where mounting the hadoopConfMap in executors would help in making the task easy to maintain the configs.
> I can also help with a use case for this, usually the submission client is on a single environment (Lets say we have it on cloud), and with spark on k8s, we can easily run jobs in different envs like in private Cloud Clusters being submitted from public Cloud. Where we would need diff properties to be passed for the submission client as well as for drivers and executors. This is also a use case where mounting the hadoopConfMap in executors would help in making the task easy to maintain the configs.<br><br><br>Yes, I think this pr is general for hadoop conf use case, and it does not create more resource because it just use the existing config map.<br><br>@yaooqinn @dongjoon-hyun could you help to take another look? Appreciated for your help.<br>
Please use text instead of pictures to cite in the PR description, in case of users may want to search commit history for the issue caused by third-party dependencies
> Please use text instead of pictures to cite in the PR description, in case of users may want to search commit history for the issue caused by third-party dependencies<br><br>Ok, fix it.
Looks OK if 【test】s can pass
Thank you, @panbingkun , @pan3793 , @srowen . Merged to master.
I have 【check】ed all the codes of the `connect` module. (not include UT's code)
Can we fix https://github.com/apache/spark/blob/master/scalastyle-config.xml to enforce this?
Through the debug plugin scalastyle, I found that the logic of the ImportOrderChecker rule is as follows, for example, in the `ConnectRepl` file, the import order of reference classes is as follows：<br><img width=\"541\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/1ba23a98-9f88-4b67-a41a-0862d590eaf0\"><br><br>Firstly, when it iterates through the first import - `import ammonite.compiler.CodeClassWrapper`, it believes that there are no imports related to group `group.java` and `group.scala` at this time. The logic enters the 【check】 of the third group (`group.3rdParty`), and the rule we configured at this time only does not allow starting with `org.apache.spark.`. This condition is met, but it is not what we want.<br><br>By adjusting the regular matching rules of `group.3rdParty` group, the above situation can be avoided. We do not allow starting with `javax?.`, `scala.`, or `org.apache.spark.`.<br><br>PS: The logic of `org.scalastyle.scalariform.ImportOrderChecker` is as follows:<br><img width=\"588\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/0b47f217-19c0-4288-8537-c266731ec868\"><br>https://github.com/scalastyle/scalastyle/blob/ec14399543d2d5ccf93c3713aa5df21793844791/src/main/scala/org/scalastyle/scalariform/ImportsChecker.scala#L238-L276<br><br><br><br>
friendly ping @HyukjinKwon @dongjoon-hyun @srowen @LuciferYang
@LuciferYang would you mind trying to merge this into `master` branch by `./dev/merge_spark_pr.py` when the 【test】s pass? It would require you to set several environment variables like `JIRA_USERNAME`, and set the `remote` (please also read `./dev/merge_spark_pr.py` script). You should also install `pip install jira` (https://pypi.org/project/jira/) before running that script.<br><br>For example, this is my remote:<br><br>```<br>git remote -v<br>...<br>apache\thttps://github.com/apache/spark.git (fetch)<br>apache\thttps://github.com/apache/spark.git (push)<br>apache-github\thttps://github.com/apache/spark.git (fetch)<br>apache-github\thttps://github.com/apache/spark.git (push)<br>...<br>origin\thttps://github.com/HyukjinKwon/spark.git (fetch)<br>origin\thttps://github.com/HyukjinKwon/spark.git (push)<br>...<br>upstream\thttps://github.com/apache/spark.git (fetch)<br>upstream\thttps://github.com/apache/spark.git (push)<br>...<br>```
> @LuciferYang would you mind trying to merge this into `master` branch by `./dev/merge_spark_pr.py` when the 【test】s pass? It would require you to set several environment variables like `JIRA_USERNAME`, and set the `remote` (please also read `./dev/merge_spark_pr.py` script). You should also install `pip install jira` (https://pypi.org/project/jira/) before running that script.<br>> <br>> For example, this is my remote:<br>> <br>> ```<br>> git remote -v<br>> ...<br>> apache\thttps://github.com/apache/spark.git (fetch)<br>> apache\thttps://github.com/apache/spark.git (push)<br>> apache-github\thttps://github.com/apache/spark.git (fetch)<br>> apache-github\thttps://github.com/apache/spark.git (push)<br>> ...<br>> origin\thttps://github.com/HyukjinKwon/spark.git (fetch)<br>> origin\thttps://github.com/HyukjinKwon/spark.git (push)<br>> ...<br>> upstream\thttps://github.com/apache/spark.git (fetch)<br>> upstream\thttps://github.com/apache/spark.git (push)<br>> ...<br>> ```<br><br>OK
I hope to be qualified to do this one day in the future. Let me continue to strive for it! Congratulations again to @LuciferYang! haha
> I hope to be qualified to do this one day in the future. Let me continue to strive for it! Congratulations again to @LuciferYang! haha<br><br>【Thanks】 @panbingkun 【:)】
@HyukjinKwon @dongjoon-hyun @LuciferYang <br>I have found issues similar to the above in the Java code. Do we need to make restrictions on this? Because the 【check】Style plugin uses a similar configuration.<br>https://【check】style.sourceforge.io/config_imports.html#ImportOrder<br>https://github.com/apache/spark/blob/eeab2e701330f7bc24e9b09ce48925c2c3265aa8/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java#L18-L36<br>
In fact, Spark does not perform 【check】s on the import order of Java code<br><br>
@panbingkun Checking the import order of Java code involves a lot of code 【c<font color=blue>hang】es】, which may cause conflicts with previous versions. I think this is not worth because it is just `import order`.<br><br>If we could define a rule that 【check】s the import group without 【check】ing the order within the group, it might be easier to accept<br><br>
> If we could define a rule that 【check】s the import group without 【check】ing the order within the group, it might be easier to accept<br><br>@LuciferYang <br>CheckStyle supports this 【feature】<br><img width=\"1141\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/3b2b0830-13fd-449b-ad87-1e5352d7117d\"><br>
Oh, I think you can give it a try
Could you re-trigger the failed pipeline, @panbingkun ?
this PR should be backported to branch-3.4
cc @cloud-fan @sadikovi
do you know which commit broke this?
> do you know which commit broke this?<br><br>#37965
【Thanks】 @cloud-fan @sadikovi
@justaparth could you take a look at the comments? Would like to merge asap.
@rangadi just updated to respond to your comments!
cc @HyukjinKwon  would you mind taking a look and merging this one? thanks 🙏
@pengzhon-db Are you still interested in this? Do you have time to resolve the conflict?<br><br>
@LuciferYang This is a POC PR. I've converted it to Draft for now. Will look into it later.
【LGTM】, thanks!
late 【LGTM】！
Note @MaxGekk @gengliangwang it says the GitHub actions CI failed, but actually this was just an unrelated PySpark flake and they actually passed 【:)】
Update on this: I thought of a 【simple】r way to break apart this work into smaller pieces so we can make gradual 【improvement】s. Let me make a commit and then I can ping this thread again.
This is done, I have deferred the constant folding logic to the analyzer. This is ready for another look.
cc: @SandishKumarHN, @justaparth, @gengliangwang
I believe one main problem of carrying the byte buffer is that it's serialized and deserialized when scheduling tasks. <br><br>When the `FileDescritptorSet` size is large enough or many protobuf functions are used, task size would be larger and cause some scheduling overhead. It would be much lightweighter to just carrying the file path name.<br><br>To address that problem, we would normally broadcast the byte buffer, however that may not work well with spark connect?<br><br>Do you think it's necessary to give users the option to pass by descriptor file a.k.a the current behavior ?
@【advanced】xy broadcast is an interesting idea. Lets continue the 【discussion】 in a code comment here: https://github.com/apache/spark/pull/41192#【discussion】_r1197264386
@gengliangwang PTAL when you get chance.
@SandishKumarHN Please approve if this looks good. Would like to merge this soon if there are no 【outstanding】 issues.
thanks for this change, definitely nice to read the descriptor once and pass it around to avoid drift if the descriptor file 【c<font color=blue>hang】es】 over time 🙏
@LuciferYang do you need any more 【c<font color=blue>hang】es】 for this? I 【<font color=blue>fix】ed】 a 【test】 that fails only github CI. <br>I would like to get this merged asap.
@LuciferYang please let me know if you need any more 【c<font color=blue>hang】es】. If none, I can ask someone else to take a look and merge as well.
@rangadi <br><br><img width=\"1150\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1475305/70d708cf-a019-4ff6-a22f-4d7fea72aa02\"><br>
Let me take a look again<br><br>
@rangadi Please fix the compilation error first. For the rest, I only have this one question: https://github.com/apache/spark/pull/41192#【discussion】_r1206557463<br><br>
@LuciferYang all the 【test】 pass could you merge? I have multiple PRs waiting for this.
Merged to master. 【Thanks】 @rangadi
@xinrong-meng Could you triage and take a look at the 【test】s still disabled?
Thank you @ueshin !
Can you explain this more - your steps to reproduce require you to remove files from the build, and that causes the problem? but the JARs were there before you removed them. How does removing the scope change the result?
Thank you @srowen. I have updated the PR description.
It seems weird that log4j 2 config works, if you add log4j 1.x. Maybe so, just trying to figure out if this is really what's going on and if we have to let log4j 1.x back in? because then we have both log4j in the distribution
Maybe similar reason I made https://github.com/apache/spark/pull/37694 a while ago? Basically Spark logging setup assumes log4j2, but with hadoop provided you get 1.x from Hadoop. So you get weird logging behavior. We just have both providers in the class path and slf seems to pick the 2.x one
Seems reasonable then. Let's just get the 【test】s to run again.
Actually hit a new issue related to this after finally being able to 【test】 out Spark 3.4 from the Delta release. Because of the bump to slf4j 2, it seems `log4j-slf4j2-impl` doesn't get recognized, so it always falls back to the log4j/reload4j 1.x for logging. I had to also include slf4j-api 2.0.6 to get Spark's built-in log handling to work correctly (i.e. `SparkContext.setLogLevel`)
@Kimahriman Do you have a way to reproduce?
Nevermind it looks like including `log4j-1.2-api` actually fixes the issue, I wasn't including that before. In fact, just including `log4j-1.2-api` fixes the issue I made for https://issues.apache.org/jira/browse/SPARK-40246. That might not fix custom log4j2 configs, but at least makes `SparkContext.setLogLevel` work with a hadoop-provided build
cc @dongjoon-hyun @HyukjinKwon @srowen @hvanhovell FYI<br><br>I want to use `GenerateMIMAIgnore` to generate default exclude filters file base on master branch to solve SPARK-43246 for connect module and encountered this problem. <br><br>In the long run, we may also encounter this issue when we need run dev/mima with `previousSparkVersion = 3.5.0`<br>
To be clear this particular issue is not related to MiMa right? this ASM change fixes Jackson + Java bytecode version issues? they are related?
Yes, this is not related to MiMa 【check】 itself.<br><br>1. I hope to reuse `GenerateMIMAIgnore` base on master to generate `.generated-mima-class-excludes` and `.generated-mima-member-excludes` during  `dev/connect-jvm-client-mima-【check】` 【check】 process(https://github.com/apache/spark/pull/40925), then I encountered this problem.<br><br>2. In the long run, when we need to performing mima 【check】 between Spark 3.6.0 and Spark 3.5.0, we will also need to solve this problem if jackson-core always with Java 11/19 code(due to the `OLD_DEPS_CLASSPATH` will also contain jackson-core 2.15.0+), but this can indeed be 【<font color=blue>fix】ed】 when things happen<br><br><br><br>I have give an independent pr to solve this issue. If this is not appropriate, I can directly place this change in https://github.com/apache/spark/pull/40925 for workaround<br><br><br>
And I think the key issue solved by this PR is that the classpath processed by GenerateMIMAIgnore cannot contain Java 17 compiled code now due to the ASM version is too low
Updated the pr description.
Oh shoot, I was looking at the wrong PR - I'm not sure that 【test】s passed before I merged. Let me watch the result.
No worry, @srowen ~ I'll monitor together.
If there are any issues, please revert and I will resubmit one 【:)】<br><br>
OK yeah it was fine, false alarm. Oops.
【Thanks】 @srowen @dongjoon-hyun ~
See https://spark.apache.org/contributing.html and please fix up this PR. Needs more explanation too
@dongjoon-hyun could you please 【review】 this. 【Thanks】!
This is done.
cc @pralabhkumar and @holdenk from #37417
+1 looks reasonable module the existing suggestions (【clean】 up the logging + tighten the 【test】). 【Thanks】 for making this PR 【:)】
【LGTM】 .
gentle ping @dongjoon-hyun  @holdenk @pralabhkumar @pan3793 for the la【test】 change, thanks a lot.
【Thanks】 for comments, updated
Merged to master~
@bersprockets here are the 【c<font color=blue>hang】es】 to handle non-foldable input args, based on our conversation in https://github.com/apache/spark/pull/40615. cc @dtenedor @mkaravel
@RyanBerti thanks for the update!
@dtenedor I just pushed a commit that tries to generalize the foldable 【check】, as I'm seeing duplicate code in the datasketches functions as well as others (see ApproxCountDistinctForIntervals, ApproximatePercentile, CountMinSketchAgg, etc). I'm open to modifying the trait as needed, or reverting the commit and implementing your suggestion.
The new trait looks good. In the future we can think about reusing it.
@HyukjinKwon could you help us merge this one?
@ericm-db Could you allow GitHub actions in your fork and re-trigger GAs, please.
【Thanks】 for the 【review】! I've made the 【c<font color=blue>hang】es】, and I think it's ready to merge now @MaxGekk @HeartSaVioR
+1, 【LGTM】. Merging to master.<br>Thank you, @ericm-db.
@ericm-db Congratulations with your first 【contribution】 to Apache Spark!<br>
cc @attilapiros @viirya @sunchao @pan3793 FYI
<img width=\"893\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/6da74b5d-4e71-440e-bb47-d17ba7f7de1e\"><br>
Hmm .. does that mean Hadoop 3.2.0 won't work with this?
Hm, we currently build Spark w/ Hadoop 3.3.0 by default it might be fine but I would also ask some more looks e.g., @srowen @mridulm @tgravescs @dongjoon-hyun
Spark only works with Hadoop 3.3.1+ at the moment, as we discovered in https://github.com/apache/spark/pull/40847. We can potentially make it work with Hadoop 3.2.2+ if there's a workaround for https://issues.apache.org/jira/browse/SPARK-40039 which uses Hadoop API that only exist in Hadoop 3.3.1+. It definitely won't work with Hadoop 3.2.0 due to some shaded client related issues.
【Thanks】 for clarification. Lgtm2
1.https://issues.apache.org/jira/browse/HADOOP-14067<br><img width=\"816\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/3e59dc12-75de-457e-bd19-2fc75294f0ce\"><br><br>2.https://issues.apache.org/jira/browse/SPARK-32256<br><img width=\"652\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/efe72ef7-8e95-4cf1-b1ae-3d8358798ec5\"><br>
Merged to master. Thank you, @panbingkun .
@HyukjinKwon @dongjoon-hyun CI passed.<br>Also cc @zhengruifeng @ueshin @xinrong-meng FYI.
ping @dongjoon-hyun @Yikun
> Do you think you can make this new 【test】 environment variable works for both Maven and SBT, @beliefer ?<br><br>AFAIK, `SparkBuilder` only used for SBT.
@dongjoon-hyun Could you take a look again ? cc @Yikun
> > Do you think you can make this new 【test】 environment variable works for both Maven and SBT, @beliefer ?<br>> <br>> AFAIK, `SparkBuilder` only used for SBT.<br><br>What I asked was to support `HEAP_SIZE` in Maven, too.
@dongjoon-hyun I think no need to support `HEAP_SIZE` in Maven, just like the `METASPACE_SIZE` .
Got it.
cc @viirya @srowen
@srowen @dongjoon-hyun @viirya Thank you!
thanks @wangyum
cc @Yikf @dongjoon-hyun @HyukjinKwon
@panbingkun how did you find them?<br><br>FWIW, I thought we added a compilation 【feature】 to disallow unused variables IIRC, @LuciferYang .
> @panbingkun how did you find them?<br>> <br>> FWIW, I thought we added a compilation 【feature】 to disallow unused variables IIRC, @LuciferYang .<br><br>@HyukjinKwon <br><br>1.When I 【review】 the HadoopRDD code, I noticed some inspection prompts, eg:<br><img width=\"808\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/4dc7c9b9-a549-4e32-a0cf-679a236f13b1\"><br><br>2.So I wonder if there are other cases in the core module, then I export `unused declaration` rule to single file, name as `unused_declaration.xml`<br><img width=\"443\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/be2d73d6-a900-4a33-9927-91f9e191703c\"><br><br>3.execute command:<br>\"/Users/panbingkun/Library/Application Support/JetBrains/Toolbox/apps/IDEA-C/ch-0/231.9011.34/IntelliJ IDEA CE.app/Contents/bin/inspect.sh\" /Users/panbingkun/Developer/spark/spark-【community】 /Users/panbingkun/Developer/spark/spark-【community】/unused_declaration.xml /Users/panbingkun/Developer/spark/spark-【community】/unused_declaration_results -v2 -d /Users/panbingkun/Developer/spark/spark-【community】/core/src<br><br>4.The declared unused variable results in all `core` modules will be saved in the `unused_declaration_results` file.<br><br>5.Open it by IDE.<br><img width=\"695\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/74096512-4f09-4fab-8678-97e6a59cd85c\"><br><br>6.Some inspect results may be inaccurate and ultimately require manual reconfirmation.<br><br>
> IIRC<br><br>Currently, we have only added compilation 【check】s on `-Ywarn-unused:imports` for Scala 2.12 (the behavior of Scala 2.13 is slightly different). <br><br>https://github.com/apache/spark/blob/411bcd2f18b2275466edc752c8cdcdfcaab1cb9c/pom.xml#L2864<br><br>For the current version of Scala 2.12, we can further try to add more 【check】s, such as `patvars`, `privates`, and `locals`. For codes that cannot be changed, we can use `@nowarn` to suppress them.
+1, 【LGTM】. Merging to master.<br>Thank you, @panbingkun and @LuciferYang @HyukjinKwon for 【review】.
I 【test】ed it through Java 20.<br>Before: https://github.com/wangyum/spark/actions/runs/5015734585/jobs/8991701615<br>After: https://github.com/wangyum/spark/actions/runs/5015807076/jobs/8992120361
If you don't mind, let's hold on this PR because Java 20 is not our real target.
I'd like to find a failed 【test】 case to add to https://github.com/Homebrew/homebrew-core/pull/131189.
Well, `cyclonedx-maven-plugin` is not a 【test】 case. Also, it's used at release process, not a runtime.<br>> I'd like to find a failed 【test】 case to add to https://github.com/Homebrew/homebrew-core/pull/131189.
Maybe, you want to hide `cyclonedx-maven-plugin` under a profile?
2.7.9 also contains other bug fixes and 【improvement】s:<br>https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.9<br>https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.8<br>https://github.com/CycloneDX/cyclonedx-maven-plugin/releases/tag/cyclonedx-maven-plugin-2.7.7
Do you think Apache Spark's SBOM artifacts are affected by one of them?<br>> 2.7.9 also contains other bug fixes and 【improvement】s:
OK. so
Thank you for closing. We can reopen this when `master` branch is ready for `Java 21` and we are able to verify this patch.
@dongjoon-hyun @cloud-fan Could you 【review】 this backport, please.
Merged to branch-3.4 for Apache Spark 3.4.1.
Thank you, @HyukjinKwon !<br>K8s (Unit and Integration) 【test】s passed at the first commit and dependency 【test】 passed at the second commit already.<br>Let me merge this.
> Oh, could you take a look at the build error? This looks like a breaking change.<br>> <br>> ```<br>> [error] /home/runner/work/spark/spark/project/SparkBuild.scala:34:21: object sbt is not a member of package com.typesafe<br>> [error] import com.typesafe.sbt.pom.{PomBuild, SbtPomKeys}<br>> ```<br><br>Ok, let me 【check】 it.
Thank you for update!
@mridulm @shuwang21 @zhouyejoe Please 【review】
> I'm wondering if this can unblock [SPARK-36744](https://issues.apache.org/jira/browse/SPARK-36744) (Support IO encryption for push-based shuffle). If not, do you happen to know what are the remaining blockers?<br><br>This doesn't unblock [SPARK-36744](https://issues.apache.org/jira/browse/SPARK-36744). To be able to read merged data, clients only know the map outputs belonging to that merged blocks. If the map outputs are encrypted, the client will need to know the size as well to decrypt it which is a protocol change. <br><br>Here, the server supports encryption but the application which is doing push-based shuffle doesn't have encryption enabled. I will add this to the 【summary】.
Thank you for sharing the rich context, @otterc .
【LGTM】. 【Thanks】 for your efforts!<br><br>Do you think when `spark.network.crypto.saslFallback=true` and L95 from `AuthRpcHandler.java`. <br>```<br>saslHandler = new SaslRpcHandler(conf, channel, null, secretKeyHolder);<br>```<br>which will set `delegate=null`. Will this cause potential NPE? <br><br>
> Do you think when `spark.network.crypto.saslFallback=true` and L95 from `AuthRpcHandler.java`.<br>> <br>> ```<br>> saslHandler = new SaslRpcHandler(conf, channel, null, secretKeyHolder);<br>> ```<br>> <br>> which will set `delegate=null`. Will this cause potential NPE?<br><br>No, with this modification, we obtain the reference to the `MergedBlockMetaReqHandler` from the `delegate (ExternalBlockHandler)` that the `AuthRpcHandler` is initialized with. The `saslHandler` instance within the `AuthRpcHandler` is 【specific】ally created for fallback to SASL, and we should not have used it to retrieve the MergedBlockMetaReqHandler. It is insignificant whether the saslHandler is initialized with a null delegate or not.
> > Do you think when `spark.network.crypto.saslFallback=true` and L95 from `AuthRpcHandler.java`.<br>> > ```<br>> > saslHandler = new SaslRpcHandler(conf, channel, null, secretKeyHolder);<br>> > ```<br>> > <br>> > <br>> >     <br>> >       <br>> >     <br>> > <br>> >       <br>> >     <br>> > <br>> >     <br>> >   <br>> > which will set `delegate=null`. Will this cause potential NPE?<br>> <br>> No, with this modification, we obtain the reference to the `MergedBlockMetaReqHandler` from the `delegate (ExternalBlockHandler)` that the `AuthRpcHandler` is initialized with. The `saslHandler` instance within the `AuthRpcHandler` is 【specific】ally created for fallback to SASL, and we should not have used it to retrieve the MergedBlockMetaReqHandler. It is insignificant whether the saslHandler is initialized with a null delegate or not.<br><br>I see. Make sense to me. 【Thanks】!
@dongjoon-hyun @mridulm @tgravescs @Ngone51 Could you please help 【review】
Thank you @dongjoon-hyun and @shuwang21!
cc @zhenlineo I remember you mentioned a bug in mima 1.1.1: `the MiMa will not be able to 【check】 the class methods if the object is marked private`, so Spark have been using 1.1.0 before, do you have time to help confirm if 1.1.2 has 【<font color=blue>fix】ed】 this issue?<br><br><br>
Oh, +1 for @LuciferYang 's comment.
I 【check】ed locally. MiMa 1.1.2 can find errors about missing private classes e.g. `private[sql] object Dataset`<br>```<br>object org.apache.spark.sql.Dataset does not have a correspondent in client version<br>```<br>So it is good to 【upgrade】 to 1.1.2!
Thank you, @panbingkun , @LuciferYang , @zhenlineo !<br>Merged to master.
Expected NPE occurred:<br><br>```<br>Warning: Unable to serialize throwable of type io.grpc.StatusRuntimeException for TestFailed(Ordinal(0, 271),INTERNAL: Job aborted due to stage failure: Task 0 in stage 150.0 failed 1 times, most recent failure: Lost task 0.0 in stage 150.0 (TID 316) (localhost executor driver): java.lang.NullPointerException<br>\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:53)<br>\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.serialize(TypedAggregateExpression.scala:267)<br>\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:620)<br>\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:280)<br>\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:107)<br>\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:117)<br>\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)<br>\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)<br>\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)<br>\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)<br>\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)<br>\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)<br>\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)<br>\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)<br>\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)<br>\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)<br>\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)<br>\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)<br>\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)<br>\tat org.apache.spark.scheduler.Task.run(Task.sca...,UserDefinedFunctionE2ETestSuite,org.apache.spark.sql.UserDefinedFunctionE2ETestSuite,Some(org.apache.spark.sql.UserDefinedFunctionE2ETestSuite),Dataset reduce,Dataset reduce,Vector(),Vector(),Some(io.grpc.StatusRuntimeException: INTERNAL: Job aborted due to stage failure: Task 0 in stage 150.0 failed 1 times, most recent failure: Lost task 0.0 in stage 150.0 (TID 316) (localhost executor driver): java.lang.NullPointerException<br>\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:53)<br>\tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.serialize(TypedAggregateExpression.scala:267)<br>\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:620)<br>\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:280)<br>\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:107)<br>\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:117)<br>\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)<br>\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)<br>\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)<br>\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)<br>\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)<br>\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)<br>\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)<br>\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)<br>\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)<br>\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)<br>\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)<br>\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)<br>\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)<br>\tat org.apache.spark.scheduler.Task.run(Task.sca...),Some(290),Some(IndentedText(- Dataset reduce,Dataset reduce,0)),Some(SeeStackDepthException),Some(org.apache.spark.sql.UserDefinedFunctionE2ETestSuite),None,pool-1-thread-1-ScalaTest-running-UserDefinedFunctionE2ETestSuite,1684472246665), setting it as NotSerializableWrapperException.<br>[info] - Dataset reduce *** FAILED *** (290 milliseconds)<br>[info]   io.grpc.StatusRuntimeException: INTERNAL: Job aborted due to stage failure: Task 0 in stage 150.0 failed 1 times, most recent failure: Lost task 0.0 in stage 150.0 (TID 316) (localhost executor driver): java.lang.NullPointerException<br>[info] \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:53)<br>[info] \tat org.apache.spark.sql.execution.aggregate.ComplexTypedAggregateExpression.serialize(TypedAggregateExpression.scala:267)<br>[info] \tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:620)<br>[info] \tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:280)<br>[info] \tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.outputForEmptyGroupingKeyWithoutInput(ObjectAggregationIterator.scala:107)<br>[info] \tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:117)<br>[info] \tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)<br>[info] \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)<br>[info] \tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)<br>[info] \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)<br>[info] \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)<br>[info] \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)<br>[info] \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)<br>[info] \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)<br>[info] \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)<br>[info] \tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)<br>[info] \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)<br>[info] \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)<br>[info] \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)<br>[info] \tat org.apache.spark.scheduler.Task.run(Task.sca...<br>[info]   at io.grpc.Status.asRuntimeException(Status.java:535)<br>[info]   at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)<br>[info]   at org.apache.spark.sql.connect.client.SparkResult.org$apache$spark$sql$connect$client$SparkResult$$processResponses(SparkResult.scala:62)<br>[info]   at org.apache.spark.sql.connect.client.SparkResult.length(SparkResult.scala:114)<br>[info]   at org.apache.spark.sql.connect.client.SparkResult.toArray(SparkResult.scala:131)<br>[info]   at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2744)<br>[info]   at org.apache.spark.sql.Dataset.withResult(Dataset.scala:3184)<br>[info]   at org.apache.spark.sql.Dataset.collect(Dataset.scala:2743)<br>[info]   at org.apache.spark.sql.Dataset.reduce(Dataset.scala:1292)<br>[info]   at org.apache.spark.sql.UserDefinedFunctionE2ETestSuite.$anonfun$new$34(UserDefinedFunctionE2ETestSuite.scala:212)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf(OutcomeOf.scala:85)<br>[info]   at org.scala【test】.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)<br>[info]   at org.scala【test】.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:22)<br>[info]   at org.scala【test】.Transformer.apply(Transformer.scala:20)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)<br>[info]   at org.scala【test】.TestSuite.withFixture(TestSuite.scala:196)<br>[info]   at org.scala【test】.TestSuite.withFixture$(TestSuite.scala:195)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.SuperEngine.runTestImpl(Engine.scala:306)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)<br>[info]   at scala.collection.immutable.List.foreach(List.scala:431)<br>[info]   at org.scala【test】.SuperEngine.traverseSubNodes$1(Engine.scala:401)<br>[info]   at org.scala【test】.SuperEngine.runTestsInBranch(Engine.scala:396)<br>[info]   at org.scala【test】.SuperEngine.runTestsImpl(Engine.scala:475)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.Suite.run(Suite.scala:1114)<br>[info]   at org.scala【test】.Suite.run$(Suite.scala:1096)<br>[info]   at org.scala【test】.funsuite.AnyFunSuite.org$scala【test】$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.SuperEngine.runImpl(Engine.scala:535)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)<br>[info]   at org.scala【test】.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)<br>[info]   at org.apache.spark.sql.UserDefinedFunctionE2ETestSuite.org$scala【test】$BeforeAndAfterAll$$super$run(UserDefinedFunctionE2ETestSuite.scala:35)<br>[info]   at org.scala【test】.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)<br>[info]   at org.scala【test】.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)<br>[info]   at org.apache.spark.sql.UserDefinedFunctionE2ETestSuite.run(UserDefinedFunctionE2ETestSuite.scala:35)<br>[info]   at org.scala【test】.tools.Framework.org$scala【test】$tools$Framework$$runSuite(Framework.scala:321)<br>[info]   at org.scala【test】.tools.Framework$ScalaTestTask.execute(Framework.scala:517)<br>[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)<br>[info]   at java.util.【concurrent】.FutureTask.run(FutureTask.java:266)<br>[info]   at java.util.【concurrent】.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>[info]   at java.util.【concurrent】.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>[info]   at java.lang.Thread.run(Thread.java:750)<br>```
Using ./build/mvn with openjdk 17 I get this error with scala-maven-plugin.version 4.8.1 <br>```<br>#14 238.4 [INFO] --- scala-maven-plugin:4.8.1:compile (scala-compile-first) @ spark-core_2.12 ---<br>#14 238.4 [INFO] Compiler bridge file: /root/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.8.0-bin_2.12.17__61.0-1.8.0_20221110T195421.jar<br>#14 238.4 [INFO] compiler plugin: BasicArtifact(com.github.ghik,silencer-plugin_2.12.17,1.7.10,null)<br>#14 238.4 [INFO] compiling 602 Scala sources and 101 Java sources to /tmp/spark/core/target/scala-2.12/classes ...<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala:71: not found: 【value】 sun<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:26: not found: object sun<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:27: not found: object sun<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:206: not found: type DirectBuffer<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:210: not found: type Unsafe<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:212: not found: type Unsafe<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:213: not found: type DirectBuffer<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:216: not found: type DirectBuffer<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:236: not found: type DirectBuffer<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala:452: not found: 【value】 sun<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:26: not found: object sun<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type SignalHandler<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:99: not found: type Signal<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:83: not found: type Signal<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: type SignalHandler<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:108: not found: 【value】 Signal<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:114: not found: type Signal<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:116: not found: 【value】 Signal<br>Error: .4 [ERROR] [Error] /tmp/spark/core/src/main/scala/org/apache/spark/util/SignalUtils.scala:128: not found: 【value】 Signal<br>#14 238.4 [ERROR] 19 errors found<br>#14 238.4 [INFO] ------------------------------------------------------------------------<br>#14 238.4 [INFO] Reactor Summary for Spark Project Parent POM 3.5.0-SNAPSHOT:<br>#14 238.4 [INFO] <br>#14 238.4 [INFO] Spark Project Parent POM ........................... SUCCESS [ 34.963 s]<br>#14 238.4 [INFO] Spark Project Tags ................................. SUCCESS [ 23.148 s]<br>#14 238.4 [INFO] Spark Project Sketch ............................... SUCCESS [ 12.645 s]<br>#14 238.4 [INFO] Spark Project Local DB ............................. SUCCESS [ 17.202 s]<br>#14 238.4 [INFO] Spark Project Networking ........................... SUCCESS [ 20.815 s]<br>#14 238.4 [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 17.392 s]<br>#14 238.4 [INFO] Spark Project Unsafe ............................... SUCCESS [ 16.924 s]<br>#14 238.4 [INFO] Spark Project Common Utils ......................... SUCCESS [ 11.795 s]<br>#14 238.4 [INFO] Spark Project Launcher ............................. SUCCESS [ 13.309 s]<br>#14 238.4 [INFO] Spark Project Core ................................. FAILURE [ 40.546 s]<br>#14 238.4 [INFO] Spark Project ML Local Library ..................... SKIPPED<br>#14 238.4 [INFO] Spark Project GraphX ............................... SKIPPED<br>#14 238.4 [INFO] Spark Project Streaming ............................ SKIPPED<br>#14 238.4 [INFO] Spark Project Catalyst ............................. SKIPPED<br>#14 238.4 [INFO] Spark Project SQL .................................. SKIPPED<br>#14 238.4 [INFO] Spark Project ML Library ........................... SKIPPED<br>#14 238.4 [INFO] Spark Project Tools ................................ SKIPPED<br>#14 238.4 [INFO] Spark Project Hive ................................. SKIPPED<br>#14 238.4 [INFO] Spark Project REPL ................................. SKIPPED<br>#14 238.4 [INFO] Spark Project Assembly ............................. SKIPPED<br>#14 238.4 [INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED<br>#14 238.4 [INFO] Spark Integration for Kafka 0.10 ................... SKIPPED<br>#14 238.4 [INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED<br>#14 238.4 [INFO] Spark Project Examples ............................. SKIPPED<br>#14 238.4 [INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED<br>#14 238.4 [INFO] Spark Avro ......................................... SKIPPED<br>#14 238.4 [INFO] Spark Project Connect Common ....................... SKIPPED<br>#14 238.4 [INFO] Spark Protobuf ..................................... SKIPPED<br>#14 238.4 [INFO] Spark Project Connect Server ....................... SKIPPED<br>#14 238.4 [INFO] Spark Project Connect Client ....................... SKIPPED<br>#14 238.4 [INFO] ------------------------------------------------------------------------<br>#14 238.4 [INFO] BUILD FAILURE<br>#14 238.4 [INFO] ------------------------------------------------------------------------<br>#14 238.4 [INFO] Total time:  03:29 min<br>#14 238.4 [INFO] Finished at: 2023-05-22T10:27:14Z<br>#14 238.4 [INFO] ------------------------------------------------------------------------<br>#14 238.4 [ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile (scala-compile-first) on project spark-core_2.12: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.8.1:compile failed: org.apache.commons.exec.ExecuteException: Process exited with an error: 255 (Exit 【value】: 255) -> [Help 1]<br>#14 238.4 [ERROR] <br>#14 238.4 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.<br>#14 238.4 [ERROR] Re-run Maven using the -X switch to enable full debug logging.<br>#14 238.4 [ERROR] <br>#14 238.4 [ERROR] For more information about the errors and possible solutions, please read the following articles:<br>#14 238.4 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException<br>```<br><br>When I change scala-maven-plugin.version to 4.8.0 it works
@panbingkun dident you 【upgrade】 scala-maven-plugin from 4.8.0 to 4.8.1 in https://github.com/apache/spark/pull/40442<br>and @LuciferYang did have to revere it in https://github.com/apache/spark/commit/1530e8dc44d7d963791ede03d54b1c2ad5e91c99 <br>CC @srowen
Let's revert that change and leave a note that this plugin cannot be updated further. It seems to be missing the JDK 'internal' classes, maybe for good reasons but I don't see why
Could you 【review】 this please, @HyukjinKwon?
Could you 【review】 this, @LuciferYang ?
Thank you, @LuciferYang ! 😄
cc @wangyum @cloud-fan
I feel it's better to always respect the user-specified num slice parameter. If the num slice is not specified, I agree that we can make it not larger than the num elements.
> I feel it's better to always respect the user-specified num slice parameter. If the num slice is not specified, I agree that we can make it not larger than the num elements.<br><br>This may require adding a status to record whether it is a user-specified slice or use default? Sounds like it will increase the complexity of the code. I don't think it's worth because this just a minor case, maybe keep it as it is is better<br>
+1, 【LGTM】
Thank you all! Merged to master for Apache Spark 3.5.0. All 【test】s passed.
【Thanks】 @dongjoon-hyun @yaooqinn @panbingkun
Could you 【review】 this PR, @LuciferYang ?
Thank you so much, @LuciferYang !
This was small but much needed as it confuses developers. 【Thanks】 @dongjoon-hyun .
Thank you, @anigos .
【Thanks】 @HyukjinKwon ~ Merged to master
Thank you again, @LuciferYang ! Merged to branch-3.3.
【Thanks】 @HyukjinKwon ~ Merged to master.
cc @dongjoon-hyun Any thoughts on this small change?
This would actually allow:<br><br>```scala<br>  def createSerializerForLocalDateTime(inputObject: Expression): Expression = {<br>    StaticInvoke(<br>      DateTimeUtils.getClass,<br>      TimestampNTZType,<br>      \"localDateTimeToMicros\<br>      inputObject :: Nil,<br>      returnNullable = false)<br>  }<br>```<br><br>to change to:<br><br>```scala<br>  def createSerializerForLocalDateTime(inputObject: Expression): Expression = {<br>    StaticInvoke(<br>      DateTimeUtils.getClass,<br>      TimestampNTZType,<br>      \"anyToMicros\<br>      inputObject :: Nil,<br>      returnNullable = false)<br>  }<br>```<br><br>This would also make some code on the Iceberg side 【simple】r:<br><br>```java<br>        // This if/else can be removed once https://github.com/apache/spark/pull/41238 is in<br>        if (ts.shouldAdjustToUTC()) {<br>          // if spark.sql.datetime.java8API.enabled is set to true, java.time.Instant<br>          // for Spark SQL TIMESTAMP type is returned otherwise java.sql.Timestamp is returned.<br>          return DateTimeUtils.anyToMicros(object);<br>        } else {<br>          LocalDateTime ldt = (LocalDateTime) object;<br>          return DateTimeUtils.localDateTimeToMicros(ldt);<br>        }<br>```<br><br>https://github.com/apache/iceberg/blob/b2e99b5b35df8ced7366ad32e2c4fdf26ab3c331/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java#L75-L86
@Fokko no biggie but would be 【great】 to fill the PR description.
nit: the TODO [here](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/types.py#LL393C5-L393C74) seems to be removable
> nit: the TODO [here](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/types.py?rgh-link-date=2023-05-24T18%3A50%3A58Z#LL393C5-L393C74) seems to be removable<br><br>I don't think so. The function only supports top level timestamp type. I'm not sure we will support the nested timestamp types in the function, though.
ping @MaxGekk
+1, 【LGTM】. Merging to master.<br>Thank you, @beliefer.
@MaxGekk Thank you!
Could you 【review】 this PR, @zhengruifeng ?
Thank you so much, @zhengruifeng ! <br>Merged to master.
> Hi, @zhengruifeng . Does it mean we need to add a lower-bound instead?<br>> <br>> ><br><br>I guess we don't need a lower-bound:<br>1, we don't use a lower-bound in [CI](https://github.com/apache/spark/blob/master/dev/infra/Dockerfile#L68);<br>2, the `matplotlib` is only used in Pandas API on Spark, and `pandas` already has a [lower-bound](https://github.com/pandas-dev/pandas/blob/main/requirements-dev.txt#L29) `matplotlib>=3.6.1` for `matplotlib`.
Then, Pefect! Thank you!
@dongjoon-hyun thank you for the 【review】s. <br>merged to master
Test first, will update pr description later<br><br>
@zhengruifeng @MaxGekk @vicennial @hvanhovell PTAL
we would document the migration `sc.addArchive` to `session.addArchive` somewhere
> we would document the migration sc.addArchive to session.addArchive somewhere<br><br>I will document them in the next PR
cc @cloud-fan @MaxGekk @hvanhovell
> <br><br>Got it.
cc @HyukjinKwon @zhengruifeng @ueshin @xinrong-meng <br>Could you please take a look at this when you have time? This is essentially a foundational task for achieving 100% 【test】 coverage of the pandas API on Spark, built on top of Spark Connect.
eg: https://github.com/apache/spark/pull/41236/files/dcda3fb8673f639d410ad6e3f2d7e6b5286208ae#diff-d41e24da75af19647fadd76ad0b63ecb22b08c0004b07091e4603a30ec0fe013R5630<br><br><img width=\"788\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/49d7a7e8-140f-41ac-985f-8213bad69ad1\"><br>
> > which does not comply with code 【specific】ations<br>> <br>> Which 【specific】ations do you mean BTW. Please, point them out in the PR description.<br><br>1.I think it's a universal underlying rule, eg: <br>python: https://www.flake8rules.com/rules/W391.html<br><br>2.Why?<br>https://stackoverflow.com/questions/2287967/why-is-it-recommended-to-have-empty-line-in-the-end-of-a-source-file<br><br>3.All Spark source codes follow this rule<br><br>4.BTW, Perhaps we need to clarify this rule in `https://github.com/databricks/scala-style-guide` or other docs? @MaxGekk
Hi @dongjoon-hyun and @ScrapCodes, would you mind to take a look at this pr, and let's discuss and sync on the same pages before I'm going any further.<br><br>I wrote an email to the dev list but got no replies. Thought I should go ahead and create this PR first.
Gently ping @dongjoon-hyun and @ScrapCodes .
cc @HyukjinKwon @ueshin @xinrong-meng @itholic
I wanna think again about this, so convert to draft for now
closing this in favor of https://github.com/apache/spark/pull/41330
cc @xinrong-meng @hvanhovell
【LGTM】, thanks for improving the error handling!
【Thanks】! @xinrong-meng
Hold with merging, I may make some more small 【c<font color=blue>hang】es】 to print better errors here, as I try to reproduce and debug https://github.com/apache/spark/pull/41005#【discussion】_r1200551487
@dongjoon-hyun yes, it's ready as is.<br>I was trying to debug https://github.com/apache/spark/pull/41005#【discussion】_r1200924867 and maybe do something to fix it, but I'm stumped there. It seems to be some manifestation of https://issues.apache.org/jira/browse/SPARK-43227 and may need a separate 【investigation】, and is orthogonal of query interrupt.<br>It also isn't failing in CI, so seems 【specific】 to some environment...
> It also isn't failing in CI, so seems 【specific】 to some environment...<br><br>GA passed because it uses sbt for 【test】ing, and the connect server fork by sbt will with classpath of connect client, which will masks some issues
> GA passed because it uses sbt for 【test】ing, and the connect server fork by sbt will with classpath of connect client, which will masks some issues<br><br>I see.<br>But it's still interesting that the same 【test】 copied to `UserDefinedFunctionE2ETestSuite`, which should have the same classpath, works...
~Ur? Do you mean that this PR causes failures?~ Sorry, I overlooked the comment, @LuciferYang .
For the Maven failure issue, shall we file a dedicated JIRA issue, @LuciferYang ?<br>- https://github.com/apache/spark/pull/41005#【discussion】_r1200551487
I will create a JIRA issue.
Created https://issues.apache.org/jira/browse/SPARK-43744
> Created https://issues.apache.org/jira/browse/SPARK-43744<br><br>Actually I have created https://issues.apache.org/jira/browse/SPARK-43648 yesterday 【:)】
【Thanks】. Then, let's track that in SPARK-43648 instead of here.<br>And, sorry again for missing your comment, @LuciferYang .
@dongjoon-hyun No problem 【:)】
@panbingkun  @LuciferYang  @srowen<br>Do we need a new JIRA?
I tend to create a new jira and add some comments for this<br><br>
【LGTM】, pending on 【test】s.<br>Awesome refactoring!
cc @dongjoon-hyun @sunchao @gengliangwang
Thank you for pinging me, @sadikovi .
@dongjoon-hyun Could you take another look when you have time? Thank you.
Move the knob from SQLConf to AvroOptions, after the 【discussion】 with @sadikovi and @rangadi
@dongjoon-hyun I addressed the comments and the CI appears to pass now. Can you help take a look?
@dongjoon-hyun the 【test】s are all pass now. Can you help take a look?
also cc @cloud-fan
What is the 【value】 that allows `numSlices` to be 【great】er than `numElements`? I haven't figured it out yet<br><br>
@LuciferYang https://github.com/apache/spark/blob/5f325ec917ced819a19911b472ebf7eb52010203/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala#L317<br><br>It is a parameter.
【Thanks】 @amaliujia , I know this
friendly ping @MaxGekk Do you have time to take a look at this one? 【Thanks】 ~
Looks good, thanks!<br><br>CC @cloud-fan
WDTY about this PR, @sarutak and @gengliangwang ?
Thank you, @HyukjinKwon and @sarutak .
Thank you all! All 【test】s passed. Merged to master/3.4/3.3.<br><br>![Screenshot 2023-05-22 at 10 16 27 PM](https://github.com/apache/spark/assets/9700541/4d95cb45-05ce-40ea-8177-bcaa4125c3a1)<br>
cc @rednaxelafx @cloud-fan FYI
Let me append the results of ZStandardBenchmark.
cc @dongjoon-hyun  @srowen @LuciferYang FYI
Merged to master. Thank you, @panbingkun and all.
> Hi, @panbingkun . Could you elaborate a little more about what was the release notes and patches we are going to 【test】?<br>> <br>> > Routine 【upgrade】.<br><br>This is done.<br>【Thanks】！
Please don't 【upgrade】 `commons-crypto`, there is an existing issue for Apple Silicon. <br><br>https://github.com/apache/spark/pull/40082#issuecomment-1443129166
> #40082 (comment)<br><br>Ok, Let me revert it.
> Could you 【check】 the avro failures and re-trigger the failed pipeline?<br>> <br>> ```<br>> <br>> [info] *** 2 TESTS FAILED ***<br>> <br>> [error] Failed: Total 294, Failed 2, Errors 0, Passed 292, Ignored 2<br>> <br>> [error] Failed 【test】s:<br>> <br>> [error] \torg.apache.spark.sql.avro.AvroV1Suite<br>> <br>> [error] \torg.apache.spark.sql.avro.AvroV2Suite<br>> <br>> ```<br><br>OK，let me 【check】 it
- 2.11.0 FileUtils.touch, Call stack as follows<br>1.FileUtils.touch<br>2.FileUtils.setLastModified<br>3.File.setLastModified(timeMillis)<br>4.FileSystem.setLastModifiedTime(this, time)<br>5.UnixFileSystem.setLastModifiedTime --> **Found that the file does not exist, throw FileNotFoundException**<br><img width=\"517\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/120783ee-6606-414f-9f78-9fe9326807c6\"><br><br>- 2.12.0 FileUtils.touch, Call stack as follows<br>1.FileUtils.touch<br>2.PathUtils.touch<br>3.java.nio.file.Files.createFile<br>4.java.nio.file.Files.newByteChannel<br>5.sun.nio.fs.UnixFileSystemProvider.newByteChannel<br>6.sun.nio.fs.UnixChannelFactory.newFileChannel<br>7.sun.nio.fs.UnixChannelFactory.open<br>8.UnixNativeDispatcher.open<br>9.(Native)UnixNativeDispatcher.open0 -> **throw UnixException**<br>10.UnixException.rethrowAsIOException<br>10.UnixException.translateToIOException --> **Convert to java.nio.file.NoSuchFileException throw it**<br><img width=\"545\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/1d81d5e5-08a0-4d98-8349-5a81b74b9737\"><br><img width=\"768\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/63216355-6d4a-4670-bbdc-aefe7d1366e2\">
1. Please put the 【analysis】 to the PR description.<br>2. +1 for `NoSuchFileException` because it's better than a general `IOException`.
> 1. Please put the 【analysis】 to the PR description.<br>> 2. +1 for `NoSuchFileException` because it's better than a general `IOException`.<br><br>This is done.
@dongjoon-hyun @HyukjinKwon @LuciferYang <br>In fact, there is a problem with this UT. Its original intention was to 【test】 if there is no file with `.avro` extensions in the directory, and the read should fail. However, this UT triggered the error as `FileUtils.touch` instead of `spark.read.format(\"avro\").load(dir.toString)`. <br><br>After this PR is completed, I will submit a new PR to fix this issue.<br><br>The root cause for the failure of this case is that the parent directory was not created. When FileUtils.touch is called in version 1.11.0, it just throws `java.io.FileNotFoundException`, which covers the error.<br>
> I merged #41289 . Please rebase this PR to the master branch.<br><br>This is done.
```<br>2.Note: The error exception of the FileUtils.touch method has been changed from java.io.FileNotFoundException to java.nio.file.NoSuchFileException, 【c<font color=blue>hang】es】 from PR: [Add PathUtils.touch(Path)](https://github.com/apache/commons-io/commit/fd7c8182d2117d01f43ccc9fe939105f834ba672). The 【analysis】 process is as follows:<br>```<br><br>still need this part in description? @panbingkun
Right, @LuciferYang . We can remove that from this PR description.
> Right, @LuciferYang . We can remove that from this PR description.<br><br>Ok, Let me update it.
Thank you, @panbingkun and all! Merged to master.
thank you @dongjoon-hyun @HyukjinKwon
Merged to master/3.4/3.3.
./build/mvn -Pkinesis-asl -Pmesos -Pkubernetes -Pyarn -Phive -Phive-thriftserver -am 【check】style:【check】style<br><img width=\"1106\" alt=\"image\" src=\"https://github.com/apache/spark/assets/15246973/d458c5a2-47ac-4b45-a9e6-455a3d8e181a\"><br><br>
@panbingkun why we need 【check】 `dev/sbt-【check】style`, it is not using Maven, we should 【check】 `build/mvn 【check】style: 【check】style -P...`<br><br>
> @panbingkun why we need 【check】 `dev/sbt-【check】style`, it is not using Maven, we should 【check】 `build/mvn 【check】style: 【check】style -P...`<br><br>Sorry, I updated the wrong information. I thought it was another PR.<br>Updated!
Please update the pr description, it still `Manual 【test】ing by: sh dev/sbt-【check】style<br>`
> Please update the pr description, it still `Manual 【test】ing by: sh dev/sbt-【check】style `<br><br>Updated
Merged to master. 【Thanks】 @HyukjinKwon and @panbingkun ~
According to the `Affected Version` of JIRA, I merged to master only. But, I'm also +1 for backporting this if you want.
@MaxGekk The failure 【test】s seems unrelated to this PR.
thanks, merged to master(3.5.0), 3.4.1 and 3.3.3
How is this \"update to 【secure】 version of 【fast】erxml\" ? <br><br>\"this is to fix the vulnerability sonatype-2022-6438\" <br>that was 【<font color=blue>fix】ed】 in 2.15.0 witch we updated in https://github.com/apache/spark/commit/a4a274c4e4f709765e7a8c687347816d8951a681 <br><br><br>This is the change log for 2.15.1 https://github.com/FasterXML/jackson-databind/blob/77789abaecd2e42a3765af5231e252ee62578b18/release-notes/VERSION-2.x#LL17C21-L17C21
Ok my scans were giving a spurious result sorry
@ronandoolan2 you can update it to 2.15.1 if you will?
ok sure
we need a JiRA for this <br>and you must run `./dev/【test】-dependencies.sh --replace-manifest` to update the rest of pom.xml file.
This is how I would have done it. <br>Feel free to change what you want to change 【:)】<br><br>Change tittle to something like [SPARK-XXXX][BUILD] Upgrade FasterXML jackson to 2.15.1<br><br><br>What 【c<font color=blue>hang】es】 were proposed in this pull request?<br>Upgrade FasterXML jackson from 2.15.0 to 2.15.1<br><br>Why are the 【c<font color=blue>hang】es】 needed?<br>New version that fix some bugs see release-notes https://github.com/FasterXML/jackson-databind/blob/77789abaecd2e42a3765af5231e252ee62578b18/release-notes/VERSION-2.x#LL17C21-L17C21 for more info.<br><br>Does this PR introduce any user-facing change?<br>No.<br><br>How was this patch 【test】ed?<br>Pass GA
@bjornjorgensen do you have the link to Jira? I assume I need to create a ticket with the number SPARK-43774
Let's update the release notes to https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1<br><br>And if upgrading to this version has benefits for Spark, it can also be listed in the PR description. @ronandoolan2 <br><br>
@ronandoolan2 you need to have a JIRA account https://selfserve.apache.org/jira-account.html<br>for this one you can use https://issues.apache.org/jira/browse/SPARK-43774 witch is what you are using.
> Let's update the release notes to https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1<br>> <br>> And if upgrading to this version has benefits for Spark, it can also be listed in the PR description. @ronandoolan2<br><br>Where are the release notes that I need to update, I couldn't find them in the repo
@ronandoolan2 It looks like @LuciferYang  have changed it. <br><br>\"New version that fix some bugs, the full release-notes as follows:<br><br>https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1\"
> @ronandoolan2 It looks like @LuciferYang have changed it.<br>> <br>> \"New version that fix some bugs, the full release-notes as follows:<br>> <br>> https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.15.1\"<br><br>Ok 【great】 thanks, do you know when the next version of pyspark will be released to pypi?
| July 16th 2023 | Code freeze. Release branch cut.|<br>| Late July 2023 | QA period. Focus on bug fixes, 【test】s, 【stability】 and docs. Generally, no new 【feature】s merged.|<br>| August 2023    | Release candidates (RC), voting, etc. until final release passes<br><br><br>https://github.com/apache/spark-website/commit/18ca078b23f826c24bed32df1dc89854a91cb580
also cc @srowen FYI
Merged to master. 【Thanks】 @ronandoolan2 @srowen @bjornjorgensen  ~
Welcome, @ronandoolan2 <br><br>We are thrilled to have you join the Apache Spark 【community】 as a new contributor. It's always exciting to see fresh minds and ideas. We're confident that your skills and expertise will be an asset to our project, enhancing our 【collaborative】 efforts to 【improve】 Apache Spark.<br><br>Just as a reminder, the Apache Spark 【community】 【value】s open communication, respect, and inclusiveness. Don't hesitate to ask questions, share your thoughts, or request help when needed. Whether it's through bug fixes, adding new 【feature】s, or improving 【documentation】, every 【contribution】 is highly appreciated.<br><br>We hope that you'll find this experience rewarding. Together, we can make Apache Spark even better. Again, welcome aboard, @ronandoolan2
Test first, will add some comments and update pr description later<br><br>
【Thanks】 @dongjoon-hyun This is a problem that was exposed after https://github.com/apache/spark/pull/40848 was merged, I will explain why this change was made tomorrow, and we can discuss whether there is a better solution. <br><br>It's too late in my time zone, I'm going to bed. Good night ~<br><br>
Sorry, needs to be postponed, some works that have to be completed first ...
@dongjoon-hyun I have updated the PR description, is it clear? Do you have any better suggestions?<br><br>Of course, there are other ways to work around even without making any code 【c<font color=blue>hang】es】:<br><br>Maven: manual 【clean】 hive module before 【test】<br><br>```<br>build/mvn 【clean】 install -DskipTests<br>build/mvn 【clean】 -pl sql/hive<br>build/mvn 【test】 -pl connector/connect/client/jvm<br>```<br><br>SBT: 【test】 `connect-client-jvm` module always with `【clean】`<br><br>`build/sbt \"connect-client-jvm/【test】\"`
Another possible way is to find a way to remove `$sparkHome/sql/hive/target/scala-2.12/classes/` from SimpleSparkConnectService classpath with 【test】 without `-Phive` profile, but this requires further 【investigation】.<br><br>
Feel free to merge~
【Thanks】 @dongjoon-hyun ~
It would be really nice to have https://github.com/apache/spark/pull/40925 to filter package private methods. Looking at MiMa issues. It seems they have been trying to solve the same issue. e.g. https://github.com/lightbend/mima/issues/53 But when using the MiMa 【check】 in our 【test】s, we still have to skip the private[sql] classes manually.
Thank you, @sunchao and all. Merged to master/3.4.<br><br>Most 【test】s (except dependency 【test】) passed in the first commit. And, second commit passed dependency 【test】 too.<br>![Screenshot 2023-05-23 at 7 49 03 PM](https://github.com/apache/spark/assets/9700541/ccbee96f-497f-4414-8708-ce628e5add2e)<br>
Sorry folks. I missed that `branch-3.4` has Hadoop 2 profile still. I made a follow-up for branch-3.4.<br>- https://github.com/apache/spark/pull/41291
@ueshin thanks for the fix, merged to master/branch-3.4
Is there some setting needed to materialize the issue? Using the queries you added as 【test】s (in `scalar-subquery-predicate.sql` and `scalar-subquery-select.sql`), I am getting your expected results on the master branch:<br>```<br>spark-sql (default)> select version();<br>3.5.0 5f325ec917ced819a19911b472ebf7eb52010203<br>Time taken: 2.69 seconds, Fetched 1 row(s)<br>spark-sql (default)> select *<br>                   > from range(1, 3) t1<br>                   > where (select sum(c) from (<br>                   >         select t2.id * t2.id c<br>                   >         from range (1, 2) t2 where t1.id = t2.id<br>                   >         group by t2.id<br>                   >        )<br>                   > ) is not null;<br>1<br>Time taken: 1.141 seconds, Fetched 1 row(s)<br>spark-sql (default)> select *<br>                   > from<br>                   > (<br>                   >  select t1.id c1, (<br>                   >                     select sum(c)<br>                   >                     from (<br>                   >                       select t2.id * t2.id c<br>                   >                       from range (1, 2) t2 where t1.id = t2.id<br>                   >                       group by t2.id<br>                   >                     )<br>                   >                    ) c2<br>                   >  from range (1, 3) t1<br>                   > ) t<br>                   > where t.c2 is not null;<br>1\t1<br>Time taken: 0.404 seconds, Fetched 1 row(s)<br>spark-sql (default)> <br>```
> Is there some setting needed to materialize the issue? Using the queries you added as 【test】s (in `scalar-subquery-predicate.sql` and `scalar-subquery-select.sql`), I am getting your expected results on the master branch:<br>> <br>> ```<br>> spark-sql (default)> select version();<br>> 3.5.0 5f325ec917ced819a19911b472ebf7eb52010203<br>> Time taken: 2.69 seconds, Fetched 1 row(s)<br>> spark-sql (default)> select *<br>>                    > from range(1, 3) t1<br>>                    > where (select sum(c) from (<br>>                    >         select t2.id * t2.id c<br>>                    >         from range (1, 2) t2 where t1.id = t2.id<br>>                    >         group by t2.id<br>>                    >        )<br>>                    > ) is not null;<br>> 1<br>> Time taken: 1.141 seconds, Fetched 1 row(s)<br>> spark-sql (default)> select *<br>>                    > from<br>>                    > (<br>>                    >  select t1.id c1, (<br>>                    >                     select sum(c)<br>>                    >                     from (<br>>                    >                       select t2.id * t2.id c<br>>                    >                       from range (1, 2) t2 where t1.id = t2.id<br>>                    >                       group by t2.id<br>>                    >                     )<br>>                    >                    ) c2<br>>                    >  from range (1, 3) t1<br>>                    > ) t<br>>                    > where t.c2 is not null;<br>> 1\t1<br>> Time taken: 0.404 seconds, Fetched 1 row(s)<br>> spark-sql (default)> <br>> ```<br><br>Great catch! I used the wrong query 【test】s, updated them now. In both cases the results on the master branch are wrong:<br>```<br>select * from (<br> select t1.id c1, (<br>  select t2.id c from range (1, 2) t2<br>  where t1.id = t2.id  ) c2<br> from range (1, 3) t1 ) t<br>where t.c2 is not null<br>-- !query schema<br>struct<c1:bigint,c2:bigint><br>-- !query output<br>1\t1<br>2\tNULL<br><br>```<br>and <br><br>```<br>-- !query<br>select *<br>from range(1, 3) t1<br>where (select t2.id c<br>       from range (1, 2) t2 where t1.id = t2.id<br>      ) is not null<br>-- !query schema<br>struct<id:bigint><br>-- !query output<br>1<br>2<br>```<br><br>-- in both queries the second row should not be present.
@gengliangwang  @cloud-fan
Makes more sense now. Can you update the Jira as well?
Updated Jira ticket too
【LGTM】, can we fix the conflicts?
Conflicts are resolved.
cc @dongjoon-hyun @HyukjinKwon @LuciferYang
> withTempDir<br><br>Yep, Perhaps using `withTempDir` code here is more 【concise】,<br>Let me do it?<br>Updated.
Could you 【review】 this, @sunchao and @viirya ?
Could you 【review】 this, @HyukjinKwon and @wangyum ?
Thank you so much, @wangyum .
I verified this manually. Merged to branch-3.4.<br>```<br>$ dev/【test】-dependencies.sh --replace-manifest<br>```
cc @hvanhovell @zhengruifeng
> 【Thanks】 for doing this! Are you also taking care of python?<br><br>I want to try it. 😄
The implementation of `Python` & `connect Python` is: https://github.com/apache/spark/pull/41296,<br>After it, I will continue to work on above.<br>@hvanhovell
@panbingkun <br>Before submitting your change, please make sure to format your code using the following command:<br>./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm<br>Error: Process completed with exit code 1.<br>
is this one ready to go? @panbingkun
> ./build/mvn -Pscala-2.12 scalafmt:format -Dscalafmt.skip=false -Dscalafmt.validateOnly=false -Dscalafmt.changedOnly=false -pl connector/connect/common -pl connector/connect/server -pl connector/connect/client/jvm<br><br>OK
> is this one ready to go<br><br>Yes
Merged to master. 【Thanks】 @dongjoon-hyun and @panbingkun
cc @zhenlineo FYI
Waiting for https://github.com/apache/spark/pull/41293
@panbingkun you would need `dev/reformat-python` to fix python linter issue
> @panbingkun you would need `dev/reformat-python` to fix python linter issue<br><br>Ok, let me try. 【Thanks】!
> @panbingkun you would need `dev/reformat-python` to fix python linter issue<br><br>This is done.
cc @MaxGekk <br>Sorry, The previous PR was damaged by me, this is a new one.<br>
BTW, if `identifier` can be `NOSCAN` or nothing, we should specify this in rules:<br>```<br>    | ANALYZE TABLE multipartIdentifier partitionSpec? COMPUTE STATISTICS<br>        (NOSCAN | FOR COLUMNS identifierSeq | FOR ALL COLUMNS)?    #analyze<br>    | ANALYZE TABLES ((FROM | IN) multipartIdentifier)? COMPUTE STATISTICS<br>        (NOSCAN)?                                                  #analyzeTables<br>```
I needed to re-generate the golden files.
It seems to fail to re-trigger, @aokolnychyi . Do you have a GitHub Action link on your commit?
cc @grundprinzip @cloud-fan, please take a look.
@jwang0306 Sorry I may miss the point, can you give an example to illustrate how we use this configuration in the Spark Connect scenario?
It's not meant to be used by spark Connect but simply a convenience option that folks can provide on cluster startup without manipulating the whole log4j.conf which can be quite cumbersome.
But the PR description mentions `This is particularly helpful in the Spark Connect scenario` ...<br><br>
Correct, with Spark Connect does not have access to the spark context. For users that want to transition this config it becomes bow very easy. Before they would use set log level as it provides the easiest way to increase or decrease the level. With this option the user has the same surface but using a spark conf.
@LuciferYang thanks for the question, given the context provided by @grundprinzip, do you have any suggestions on how to update the PR description to better help understand?<br><br>> For example launching a cluster with the following command: `./bin/pyspark --conf spark.log.level ERROR`<br>> In this setup, the Spark Connect client doesn't have access to `SparkContext`, so we can't configure the log level using `sc.setLogLevel()`. But now with this --conf, the launcher is able to set it.<br><br>
cc @rednaxelafx @dongjoon-hyun FYI
@dongjoon-hyun thanks for the 【review】,<br>1. This config statically overrides the log level, but that doesn't  prevent users from 【dynamic】ally setting new overrides with `setLogLevel`. If you have a strong preference, we can change it to a name without the override word in it, what would you suggest?<br>2. Sounds good, I will add it after (3).<br>3. That's a very good point. To solve this inconsistency, I think we have multiple alternatives:<br>    - That line of log happens when `isInterpreter` = true and `silent` = false, however, these are not fields in the Logging trait. So we can't read them in `SparkContext` to warn users about the override under the exact same condition. Maybe we can made the `isInterpreter` and `silent` fields in the Logging trait, or add a new filed to tell whether or not the REPL warning is printed. If we don't care about the original condition (i.e., we don't care about the `silent` mode), then we can print the warning log unconditionally at line 399 as suggested.<br>    - Alternatively, the Logging trait itself might be a better place to implement the override, but `SparkConf` is not 【available】 when the trait is initializing. So if we choose to configure the log level override there, we'll have to change the conf to use something else that's already 【available】 at that point, like environment variable or Java System Properties. WDYT? cc. @grundprinzip @HyukjinKwon @cloud-fan @LuciferYang @rednaxelafx
Given the config name, `spark.log.level`, simply `LOG_LEVEL` or `SPARK_LOG_LEVEL`?<br><br>> This config statically overrides the log level, but that doesn't prevent users from 【dynamic】ally setting new overrides with setLogLevel. If you have a strong preference, we can change it to a name without the override word in it, what would you suggest?<br>
cc @zhengruifeng
ping @srowen @viirya cc @cloud-fan
> Any other related 【c<font color=blue>hang】es】?<br><br>I only found OOM when using `build/sbt `to 【test】 `GenTPCDSData` now.
@srowen @viirya Thank you!
ping @HyukjinKwon @zhengruifeng cc @LuciferYang @amaliujia
cc @cloud-fan since this PR aims to add a new catalog API
https://github.com/apache/spark/blob/3a6d2153b93c759b68e5827905d1867ba93ec9cf/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala#L2700-L2702<br><br>https://github.com/apache/spark/blob/3a6d2153b93c759b68e5827905d1867ba93ec9cf/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala#L2946-L2948<br><br>two similar cases in `SparkConnectPlanner`
@LuciferYang I have another PR to add parameters into `proto.ListDatabases` and `proto.ListCatalogs`, so this PR keep them not changed.
ping @HyukjinKwon @zhengruifeng cc @amaliujia
@zhengruifeng @HyukjinKwon Thank you!
@hvanhovell @grundprinzip @LuciferYang
```<br>[info] *** 1 TEST FAILED ***<br>[error] Failed: Total 3649, Failed 1, Errors 0, Passed 3648, Ignored 10, Canceled 2<br>[error] Failed 【test】s:<br>[error] \torg.apache.spark.storage.BlockManagerProactiveReplicationSuite<br>```<br>unrelated flake.<br>Will wait with retriggering CI for 【review】 comments.
+1, 【LGTM】. Merging to master/3.4.<br>Thank you, @Kimahriman.
@Kimahriman The 【c<font color=blue>hang】es】 cause conflicts in branch-3.4. Could you backport your 【c<font color=blue>hang】es】 to 3.4, please.<br><br>Just in case, does branch-3.3 have the same issue? If so, please, backport the 【c<font color=blue>hang】es】 to 3.3 too.
> Just in case, does branch-3.3 has the same issue? If so, please, backport the 【c<font color=blue>hang】es】 to 3.3 too.<br><br>No this was added in 3.4, though there was talks of potentially backporting the original PR with the issue https://github.com/apache/spark/pull/37483. I'll make the 3.4 backport PR.
@dtenedor Once I am finished implementing the other functions, you can 【review】. Of course, if there is something which you noticed I should fix earlier on, let me know!
@learningchess2003 Allow GitHub action in your fork, please: https://github.com/apache/spark/pull/41319/【check】s?【check】_run_id=13791306580
@dtenedor @MaxGekk Addressed the comments. Let me know what you think!
@MaxGekk Comments addressed.
@MaxGekk Alright, shortened it and just had it call some standalone examples.
thank you @itholic , merged to master
> @asl3 Can you reproduce the error from user space? If so, please, add a 【test】 otherwise we should convert this to an internal error.<br><br>marked as internal error since error is difficult to reproduce from user space
@HyukjinKwon i'm splitting some 【test】s a bit further, since they are much slower than others
some groups are too slow in this PR, I am closing it in favor of https://github.com/apache/spark/pull/41330
ping @HyukjinKwon @hvanhovell @zhengruifeng cc @amaliujia @LuciferYang <br>If we agree this refactor, I will continue to complete it.
I personally like this direction which is separating validation and transforming. The main benefit is code 【readability】 and also structure the code in a more logical way. <br><br><br>cc @grundprinzip
ping @hvanhovell @HyukjinKwon @zhengruifeng cc @RyanBerti @LuciferYang
cc @HyukjinKwon @itholic
@wangyum @sunchao @cloud-fan Hello, I wonder if you can take a look? 【Thanks】. Previously there was an attempt already but closed later on https://github.com/apache/spark/pull/40294, now I am trying to continue the work.
@MaxGekk
> @Kimahriman Could you add to PR's description: This is a backport of #41317.<br>> <br>> Besides of this, 【LGTM】. Waiting for CI.<br><br>Updated!
@gengliangwang how do we 【test】 doc 【c<font color=blue>hang】es】?
@srielau We 【test】 doc 【c<font color=blue>hang】es】 by 【check】ing the webpage. Please follow https://github.com/apache/spark/blob/master/docs/README.md and p【review】 your doc change in the browser.<br>(Note that the Scala/Python API Prerequisites are not required for this one.)
BTW, this seems to happen frequently. Do you think there is a way to prevent this automatically, @WeichenXu123 , @zhengruifeng ? Also, cc @HyukjinKwon .
Let me see if there's a linter rule to detect this
@panbingkun Can you explain why these two dependencies are no longer useful?<br><br>
Just need to get the 【test】 to re-run and pass but looking OK
cc: @LuciferYang, @pengzhon-db
All 【test】s passed.<br>![Screenshot 2023-05-28 at 1 38 42 AM](https://github.com/apache/spark/assets/9700541/89d2b7de-3c7d-4fbe-9843-9db8c07fe27e)<br>
Thank you, @MaxGekk . Merged to master.
Thank you @dongjoon-hyun  <br>This is quite a big change for Spark. Perhaps we should inform everyone with an email to @dev. After all, it's holidays now, so I don't think everyone knows about this PR.
Yes, I agree with you, @bjornjorgensen . I'm also planning to send out this as a part of the followings `Apache Spark 3.5.0 Expectations` soon.<br><br>(Sorted by ID)<br>- SPARK-40497 Upgrade `Scala 2.13.11`<br>- SPARK-42452 Remove `hadoop-2` profile from Apache Spark 3.5.0<br>- SPARK-42913 Upgrade to `Hadoop 3.3.5` (`aws-java-sdk-bundle`: `1.12.262` -> `1.12.316`)<br>- SPARK-43024 Upgrade Pandas to 2.0.0<br>- SPARK-43200 Remove `Hadoop 2` reference in docs<br>- SPARK-43347 Remove `Python 3.7` Support<br>- SPARK-43348 Support `Python 3.8` in PyPy3<br>- SPARK-43379 Deprecate old Java 8 versions prior to 8u371<br>- SPARK-43394 Upgrade to `Maven 3.8.8`<br>- SPARK-43436 Upgrade to `RocksDbjni 8.1.1.1`<br>- SPARK-43446 Upgrade to `Apache Arrow 12.0.0`<br>- SPARK-43447 Support `R 4.3.0`<br>- SPARK-43489 Remove `protobuf 2.5.0`<br>- SPARK-43519 Bump Parquet to 1.13.1<br>- SPARK-43581 Upgrade `kubernetes-client` to 6.6.2<br>- SPARK-43588 Upgrade to `ASM 9.5`<br>- SPARK-43600 Update K8s doc to recommend `K8s 1.24+`<br>- SPARK-43738 Upgrade to `DropWizard Metrics 4.2.18`<br>- SPARK-43831 Build and Run Spark on `Java 21`<br>- SPARK-43832 Upgrade to `Scala 2.12.18`<br>- SPARK-43836 Make `Scala 2.13 as default` in Spark 3.5<br>- SPARK-43842 Upgrade `gcs-connector` to 2.2.14<br>- SPARK-43844 Update to `ORC 1.9.0`<br>- UMBRELLA: Add SQL functions into Scala, Python and R API<br>
I'm very happy to see this change @dongjoon-hyun 【:)】<br>
Thank you always in many ways, @LuciferYang ! 【:)】
For the record, I sent an email to dev@spark here.<br>- https://lists.apache.org/thread/3x6dh17bmy20n3frtt3crgxjydnxh2o0 `Apache Spark 3.5.0 Expectations (?)`
cc @cloud-fan @jchen5
cc @aokolnychyi @cloud-fan @viirya
@dependabot ignore this dependency
OK, I won't notify you about org.scala-lang:scala-library again, unless you re-open this PR. 😢
Could you 【review】 this, @bjornjorgensen ?<br><br>![Screenshot 2023-05-28 at 2 07 45 PM](https://github.com/apache/spark/assets/9700541/9ac837d1-5b44-4768-a0e2-55fdedf9cd2d)<br>
I'm not sure what's going wrong here, except that I see there are some problems with Parquet and an error message indicating \"Too much memory used.\" <br>The first thing I will do is to retry the failed 【test】.
To @bjornjorgensen , this PR doesn't touch anything in code base. What you are referring is `sql - other ...` pipeline, isn't it? This PR is irrelevant to that flaky 【test】. 【:)】 <br><br>The PR is only replacing `Scala 2.13 build with SBT` pipeline with `Scala 2.12 build with SBT` pipeline. And, it's replaced and passed correctly.
Anyway, I re-triggered the pipeline as you recommended.<br><br><img width=\"248\" alt=\"Screenshot 2023-05-28 at 2 51 35 PM\" src=\"https://github.com/apache/spark/assets/9700541/5818879e-42f2-4a0f-911f-7b8eca58501c\"><br>
> To @bjornjorgensen , this PR doesn't touch anything in code base. What you are referring is `sql - other ...` pipeline, isn't it? This PR is irrelevant to that flaky 【test】. 【:)】<br>> <br>yes. 【:)】 <br>When it pass I think its OK. <br><br><br>
All 【test】s passed.<br><br><img width=\"739\" alt=\"Screenshot 2023-05-28 at 5 14 30 PM\" src=\"https://github.com/apache/spark/assets/9700541/c9e880d4-8fbc-4f81-8263-d6106749c21e\"><br>
Could you 【review】 this PR too, @bjornjorgensen ? I added the manual 【test】 result in the PR description.
Its ok but \"This PR aims to 【upgrade】 gas-connector to 2.2.14.\" -> gcs-connector ;)
Nice catch. Thank you! I 【<font color=blue>fix】ed】 the typo.
I have never used gcs, but @michTalebzadeh you are using it? can you help us her?
Oh, never mind. I can ping other committers. 【Thanks】.<br>> I have never used gcs, ...
The current valid gcs-connector is gcs-connector-hadoop3-2.2.0-shaded.jar that works OK with spark 3.4.0<br><br>I tend to place it under `$SPARK_HOME/jars`<br><br>ls -l gcs*<br>-rwxr--r--. 1 hduser hadoop 17663298 Apr 20 09:37 gcs-connector-hadoop3-2.2.0-shaded.jar<br>lrwxrwxrwx. 1 hduser hadoop       38 Apr 20 09:39 gcs-connector.jar -> gcs-connector-hadoop3-2.2.0-shaded.jar<br>```<br>What is gas-connector to 2.2.14 a typo?
Also 【check】 this<br><br>https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/
Could you 【review】 this when you have some time please, @HyukjinKwon and @LuciferYang ?
Thank you so much, @HyukjinKwon !
Thank you, @LuciferYang !
Could you 【review】 this too, @HyukjinKwon ?
Thank you, @yaooqinn and @LuciferYang .<br>Merged to master.