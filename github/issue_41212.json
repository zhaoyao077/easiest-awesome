{
  "url": "https://api.github.com/repos/apache/spark/issues/41212",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/41212/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/41212/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/41212/events",
  "html_url": "https://github.com/apache/spark/pull/41212",
  "id": 1715214376,
  "node_id": "PR_kwDOAQXtWs5QyMlZ",
  "number": 41212,
  "title": "[SPARK-43573][BUILD] Make SparkBuilder could config the heap size of test JVM.",
  "user": {
    "login": "beliefer",
    "id": 8486025,
    "node_id": "MDQ6VXNlcjg0ODYwMjU=",
    "avatar_url": "https://avatars.githubusercontent.com/u/8486025?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/beliefer",
    "html_url": "https://github.com/beliefer",
    "followers_url": "https://api.github.com/users/beliefer/followers",
    "following_url": "https://api.github.com/users/beliefer/following{/other_user}",
    "gists_url": "https://api.github.com/users/beliefer/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/beliefer/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/beliefer/subscriptions",
    "organizations_url": "https://api.github.com/users/beliefer/orgs",
    "repos_url": "https://api.github.com/users/beliefer/repos",
    "events_url": "https://api.github.com/users/beliefer/events{/privacy}",
    "received_events_url": "https://api.github.com/users/beliefer/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1406627200,
      "node_id": "MDU6TGFiZWwxNDA2NjI3MjAw",
      "url": "https://api.github.com/repos/apache/spark/labels/BUILD",
      "name": "BUILD",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 9,
  "created_at": "2023-05-18T08:12:21Z",
  "updated_at": "2023-05-25T01:35:29Z",
  "closed_at": "2023-05-24T16:54:39Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/41212",
    "html_url": "https://github.com/apache/spark/pull/41212",
    "diff_url": "https://github.com/apache/spark/pull/41212.diff",
    "patch_url": "https://github.com/apache/spark/pull/41212.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nCurrently, Spark provides the `GenTPCDSData` to generates TPCDS table data by using tpcds-kit: https://github.com/databricks/tpcds-kit.\r\n\r\nIf users want generates TPCDS table data, we can run:\r\n`build/sbt \"sql/Test/runMain <this class> --dsdgenDir <path> --location <path> --scaleFactor 1\"`\r\n\r\nIf the scale factor is smaller, such as: scaleFactor < 100, `GenTPCDSData` works good. otherwise, OOM issues prevent generating data.\r\n\r\n```\r\n[info] 16:43:41.618 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.\r\n[info] 16:43:41.627 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.\r\n[info] 16:43:41.646 WARN org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete file:/home/ubuntu/tpcdsdata/test/catalog_sales/_tempo\r\nrary/0/_temporary/attempt_202305181633205732292221634890857_0006_m_000010_610\r\n[info] 16:43:41.647 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.\r\n[info] 16:43:41.647 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.\r\n[info] 16:43:41.656 WARN org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete file:/home/ubuntu/tpcdsdata/test/catalog_sales/_tempo\r\nrary/0/_temporary/attempt_202305181633205732292221634890857_0006_m_000014_614\r\n[info] 16:43:41.656 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.\r\n[info] 16:43:41.668 WARN org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete file:/home/ubuntu/tpcdsdata/test/catalog_sales/_tempo\r\nrary/0/_temporary/attempt_202305181633205732292221634890857_0006_m_000002_602\r\n[info] 16:43:41.668 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Job job_202305181633205732292221634890857_0006 aborted.\r\n[error] Exception in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 6.0 failed 1 times, most recent fail\r\nure: Lost task 13.0 in stage 6.0 (TID 613) (ip-172-31-27-53.cn-northwest-1.compute.internal executor driver): org.apache.spark.SparkException: [TASK_WRITE_\r\nFAILED] Task failed while writing rows to file:/home/ubuntu/tpcdsdata/test/catalog_sales.\r\n[error]         at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n[error]         at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n[error]         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n[error]         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n[error]         at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n[error]         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n[error]         at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n[error]         at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n[error]         at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n[error]         at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1487)\r\n[error]         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[error]         at java.lang.Thread.run(Thread.java:750)\r\n[error] Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n[error] Driver stacktrace:\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2815)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2751)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2750)\r\n[error]         at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n[error]         at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n[error]         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2750)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1218)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1218)\r\n[error]         at scala.Option.foreach(Option.scala:407)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1218)\r\n[error]         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3014)\r\n[error]         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2953)\r\n[error]         at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2942)\r\n[error]         at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n[error]         at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:983)\r\n[error]         at org.apache.spark.SparkContext.runJob(SparkContext.scala:2285)\r\n[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n[error]         at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n[error]         at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n[error]         at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n[error]         at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n[error]         at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n[error]         at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n[error]         at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n[error]         at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)\r\n[error]         at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n[error]         at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n[error]         at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n[error]         at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n[error]         at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n[error]         at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n[error]         at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n[error]         at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n[error]         at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n[error]         at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n[error]         at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n[error]         at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n[error]         at org.apache.spark.sql.TPCDSTables$Table.genData(GenTPCDSData.scala:246)\r\n[error]         at org.apache.spark.sql.TPCDSTables.$anonfun$genData$10(GenTPCDSData.scala:276)\r\n[error]         at org.apache.spark.sql.TPCDSTables.$anonfun$genData$10$adapted(GenTPCDSData.scala:273)\r\n[error]         at scala.collection.immutable.List.foreach(List.scala:431)\r\n[error]         at org.apache.spark.sql.TPCDSTables.genData(GenTPCDSData.scala:273)\r\n[error]         at org.apache.spark.sql.GenTPCDSData$.main(GenTPCDSData.scala:440)\r\n[error]         at org.apache.spark.sql.GenTPCDSData.main(GenTPCDSData.scala)\r\n[error] Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/ubuntu/tpcdsdata/test/catalog_sales.\r\n[error]         at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n[error]         at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n[error]         at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n[error]         at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n[error]         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n[error]         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n[error]         at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n[error]         at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n[error]         at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n[error]         at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n[error]         at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n[error]         at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1487)\r\n[error]         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n[error]         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n[error]         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n[error]         at java.lang.Thread.run(Thread.java:750)\r\n[error] Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n```\r\n\r\nThe process tree show below:\r\n```\r\nubuntu   3272634 3206726  0 18:30 pts/1    00:00:00 bash ./build/sbt sql/Test/runMain org.apache.spark.sql.GenTPCDSData --dsdgenDir /home/ubuntu/github-fork/tpcds-kit/tools --location /home/ubuntu/tpcdsdata/1000 --scaleFactor 1000 --partitionTables\r\nubuntu   3272647 3272634  7 18:30 pts/1    00:02:20 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xms4096m -Xmx4096m -XX:ReservedCodeCacheSize=512m -Xmx1024m -Xss2m -Xmx10G -Xms10G -jar build/sbt-launch-1.8.3.jar sql/Test/runMain org.apache.spark.sql.GenTPCDSData --dsdgenDir /home/ubuntu/github-fork/tpcds-kit/tools --location /home/ubuntu/tpcdsdata/1000 --scaleFactor 1000 --partitionTables\r\nubuntu   3272844 3272647 99 18:30 pts/1    05:01:56 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xmx4g -XX:MaxMetaspaceSize=1300m -Djava.io.tmpdir=/home/ubuntu/github-fork/spark/target/tmp -Dspark.test.home=/home/ubuntu/github-fork/spark -Dspark.testing=1 -Dspark.port.maxRetries=100 -Dspark.master.rest.enabled=false -Dspark.memory.debugFill=true -Dspark.ui.enabled=false -Dspark.ui.showConsoleProgress=false -Dspark.unsafe.exceptionOnMemoryLeak=true -Dspark.hadoop.hadoop.security.key.provider.path=test:/// -Dsun.io.serialization.extendedDebugInfo=false -Dderby.system.durability=test -Dio.netty.tryReflectionSetAccessible=true -ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=1300m -XX:ReservedCodeCacheSize=128m -Dfile.encoding=UTF-8 -XX:+IgnoreUnrecognizedVMOptions\r\n```\r\n\r\nAfter my investigation, **-Xmx4g** is fixed in `project/SparkBuilder.scala`.\r\nThis PR want adds a new environment variable to control the JVM heap size for test process.\r\n\r\n### Why are the changes needed?\r\nAvoid OOM for generates TPC-DS data.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n'No'.\r\nThis change is relationed to developers.\r\n\r\n\r\n### How was this patch tested?\r\nManual test.\r\n",
  "closed_by": {
    "login": "srowen",
    "id": 822522,
    "node_id": "MDQ6VXNlcjgyMjUyMg==",
    "avatar_url": "https://avatars.githubusercontent.com/u/822522?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/srowen",
    "html_url": "https://github.com/srowen",
    "followers_url": "https://api.github.com/users/srowen/followers",
    "following_url": "https://api.github.com/users/srowen/following{/other_user}",
    "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/srowen/subscriptions",
    "organizations_url": "https://api.github.com/users/srowen/orgs",
    "repos_url": "https://api.github.com/users/srowen/repos",
    "events_url": "https://api.github.com/users/srowen/events{/privacy}",
    "received_events_url": "https://api.github.com/users/srowen/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/41212/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/41212/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
