{
  "url": "https://api.github.com/repos/apache/spark/issues/40372",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40372/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40372/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40372/events",
  "html_url": "https://github.com/apache/spark/pull/40372",
  "id": 1619593298,
  "node_id": "PR_kwDOAQXtWs5LzK0x",
  "number": 40372,
  "title": "[SPARK-42752][PYSPARK][SQL] Make PySpark exceptions printable during initialization",
  "user": {
    "login": "gerashegalov",
    "id": 3187938,
    "node_id": "MDQ6VXNlcjMxODc5Mzg=",
    "avatar_url": "https://avatars.githubusercontent.com/u/3187938?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/gerashegalov",
    "html_url": "https://github.com/gerashegalov",
    "followers_url": "https://api.github.com/users/gerashegalov/followers",
    "following_url": "https://api.github.com/users/gerashegalov/following{/other_user}",
    "gists_url": "https://api.github.com/users/gerashegalov/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/gerashegalov/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/gerashegalov/subscriptions",
    "organizations_url": "https://api.github.com/users/gerashegalov/orgs",
    "repos_url": "https://api.github.com/users/gerashegalov/repos",
    "events_url": "https://api.github.com/users/gerashegalov/events{/privacy}",
    "received_events_url": "https://api.github.com/users/gerashegalov/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1981527456,
      "node_id": "MDU6TGFiZWwxOTgxNTI3NDU2",
      "url": "https://api.github.com/repos/apache/spark/labels/CORE",
      "name": "CORE",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 1982260031,
      "node_id": "MDU6TGFiZWwxOTgyMjYwMDMx",
      "url": "https://api.github.com/repos/apache/spark/labels/PYTHON",
      "name": "PYTHON",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2023-03-10T20:39:37Z",
  "updated_at": "2023-03-17T22:32:44Z",
  "closed_at": "2023-03-14T13:30:46Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40372",
    "html_url": "https://github.com/apache/spark/pull/40372",
    "diff_url": "https://github.com/apache/spark/pull/40372.diff",
    "patch_url": "https://github.com/apache/spark/pull/40372.patch",
    "merged_at": null
  },
  "body": "Ignore SQLConf initialization exceptions during Python exception creation.\r\n\r\nOtherwise there is no diagnostics for the issue in the following scenario:\r\n\r\n1. download a standard \"Hadoop Free\" build\r\n2. Start PySpark REPL with Hive support\r\n```bash\r\nSPARK_DIST_CLASSPATH=$(~/dist/hadoop-3.4.0-SNAPSHOT/bin/hadoop classpath) \\\r\n  ~/dist/spark-3.2.3-bin-without-hadoop/bin/pyspark --conf spark.sql.catalogImplementation=hive\r\n```\r\n3. Execute any simple dataframe operation\r\n```Python\r\n>>> spark.range(100).show()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/session.py\", line 416, in range\r\n    jdf = self._jsparkSession.range(0, int(start), int(step), int(numPartitions))\r\n  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\r\n  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/utils.py\", line 117, in deco\r\n    raise converted from None\r\npyspark.sql.utils.IllegalArgumentException: <exception str() failed>\r\n```\r\n4. In fact just spark.conf already exhibits the issue\r\n```Python\r\n>>> spark.conf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/session.py\", line 347, in conf\r\n    self._conf = RuntimeConfig(self._jsparkSession.conf())\r\n  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\r\n  File \"/home/user/dist/spark-3.2.3-bin-without-hadoop/python/pyspark/sql/utils.py\", line 117, in deco\r\n    raise converted from None\r\npyspark.sql.utils.IllegalArgumentException: <exception str() failed>\r\n```\r\n\r\nThere are probably two issues here:\r\n1) that Hive support should be gracefully disabled if it the dependency not on the classpath as claimed by https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html\r\n2) but at the very least the user should be able to see the exception to understand the issue, and take an action\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nIgnore exceptions during `CapturedException` creation\r\n\r\n### Why are the changes needed?\r\nTo make the cause visible to the user\r\n\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/gits/apache/spark/python/pyspark/sql/session.py\", line 679, in conf\r\n    self._conf = RuntimeConfig(self._jsparkSession.conf())\r\n  File \"/home/user/gits/apache/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\r\n  File \"/home/user/gits/apache/spark/python/pyspark/errors/exceptions/captured.py\", line 166, in deco\r\n    raise converted from None\r\npyspark.errors.exceptions.captured.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\r\n\r\nJVM stacktrace:\r\njava.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\r\n        at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1237)\r\n        at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\r\n        at scala.Option.getOrElse(Option.scala:189)\r\n        at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\r\n        at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\r\n        at org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\r\n        at org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n        at py4j.Gateway.invoke(Gateway.java:282)\r\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n        at java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.hive.HiveSessionStateBuilder\r\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n        at java.lang.Class.forName0(Native Method)\r\n        at java.lang.Class.forName(Class.java:348)\r\n        at org.apache.spark.util.Utils$.classForName(Utils.scala:225)\r\n        at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1232)\r\n        ... 18 more\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nThe only semantic change is that the conf `spark.sql.pyspark.jvmStacktrace.enabled` is ignored if the SQLConf is broken. \r\n\r\n### How was this patch tested?\r\nManual testing using the repro steps above",
  "closed_by": {
    "login": "srowen",
    "id": 822522,
    "node_id": "MDQ6VXNlcjgyMjUyMg==",
    "avatar_url": "https://avatars.githubusercontent.com/u/822522?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/srowen",
    "html_url": "https://github.com/srowen",
    "followers_url": "https://api.github.com/users/srowen/followers",
    "following_url": "https://api.github.com/users/srowen/following{/other_user}",
    "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/srowen/subscriptions",
    "organizations_url": "https://api.github.com/users/srowen/orgs",
    "repos_url": "https://api.github.com/users/srowen/repos",
    "events_url": "https://api.github.com/users/srowen/events{/privacy}",
    "received_events_url": "https://api.github.com/users/srowen/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40372/reactions",
    "total_count": 1,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40372/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
