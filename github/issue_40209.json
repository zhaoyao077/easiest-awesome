{
  "url": "https://api.github.com/repos/apache/spark/issues/40209",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40209/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40209/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40209/events",
  "html_url": "https://github.com/apache/spark/pull/40209",
  "id": 1602344012,
  "node_id": "PR_kwDOAQXtWs5K5H7R",
  "number": 40209,
  "title": "[SPARK-42427][SQL][TESTS][FOLLOW-UP] Disable ANSI for one more conv test case in MathFunctionsSuite",
  "user": {
    "login": "HyukjinKwon",
    "id": 6477701,
    "node_id": "MDQ6VXNlcjY0Nzc3MDE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/6477701?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/HyukjinKwon",
    "html_url": "https://github.com/HyukjinKwon",
    "followers_url": "https://api.github.com/users/HyukjinKwon/followers",
    "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}",
    "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions",
    "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs",
    "repos_url": "https://api.github.com/users/HyukjinKwon/repos",
    "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}",
    "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2023-02-28T04:19:21Z",
  "updated_at": "2023-02-28T08:20:35Z",
  "closed_at": "2023-02-28T08:20:35Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40209",
    "html_url": "https://github.com/apache/spark/pull/40209",
    "diff_url": "https://github.com/apache/spark/pull/40209.diff",
    "patch_url": "https://github.com/apache/spark/pull/40209.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to disable ANSI for one more conv test cases in `MathFunctionsSuite`. They are intentionally testing the behaviours when ANSI is disabled. This is another followup of https://github.com/apache/spark/pull/40117.\r\n\r\n### Why are the changes needed?\r\n\r\nTo make the ANSI tests pass. It currently fails (https://github.com/apache/spark/actions/runs/4277973597/jobs/7447263656):\r\n\r\n```\r\n2023-02-28T02:34:53.0298317Z \u001B[0m[\u001B[0m\u001B[0minfo\u001B[0m] \u001B[0m\u001B[0m\u001B[32m- SPARK-36229 inconsistently behaviour where returned value is above the 64 char threshold (105 milliseconds)\u001B[0m\u001B[0m\r\n2023-02-28T02:34:53.0631723Z 02:34:53.062 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 146.0 (TID 268)\r\n2023-02-28T02:34:53.0632672Z org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\r\n2023-02-28T02:34:53.0633557Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.arithmeticOverflowError(QueryExecutionErrors.scala:643)\r\n2023-02-28T02:34:53.0634361Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.overflowInConvError(QueryExecutionErrors.scala:315)\r\n2023-02-28T02:34:53.0635124Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.encode(NumberConverter.scala:68)\r\n2023-02-28T02:34:53.0711747Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:158)\r\n2023-02-28T02:34:53.0712298Z \tat org.apache.spark.sql.catalyst.util.NumberConverter.convert(NumberConverter.scala)\r\n2023-02-28T02:34:53.0712925Z \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:38)\r\n2023-02-28T02:34:53.0713547Z \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n2023-02-28T02:34:53.0714098Z \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n2023-02-28T02:34:53.0714552Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n2023-02-28T02:34:53.0715094Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n2023-02-28T02:34:53.0715466Z \tat org.apache.spark.util.Iterators$.size(Iterators.scala:29)\r\n2023-02-28T02:34:53.0715829Z \tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1944)\r\n2023-02-28T02:34:53.0716195Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1266)\r\n2023-02-28T02:34:53.0716555Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1266)\r\n2023-02-28T02:34:53.0716963Z \tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n2023-02-28T02:34:53.0717400Z \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n2023-02-28T02:34:53.0717857Z \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n2023-02-28T02:34:53.0718332Z \tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n2023-02-28T02:34:53.0718743Z \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n2023-02-28T02:34:53.0719152Z \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)\r\n2023-02-28T02:34:53.0719548Z \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n2023-02-28T02:34:53.0720001Z \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n2023-02-28T02:34:53.0720481Z \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2023-02-28T02:34:53.0720848Z \tat java.lang.Thread.run(Thread.java:750)\r\n2023-02-28T02:34:53.0721492Z 02:34:53.065 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 146.0 (TID 268) (localhost executor driver): org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] Overflow in function conv(). If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\r\n2023-02-28T02:34:53.0722264Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.arithmeticOverflowError(QueryExecutionErrors.scala:643)\r\n2023-02-28T02:34:53.0722821Z \tat org.apache.spark.sql.errors.QueryExecutionErrors$.overflowInConvError(QueryExecutionErrors.scala:315)\r\n2023-02-28T02:34:53.0723337Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.encode(NumberConverter.scala:68)\r\n2023-02-28T02:34:53.0723963Z \tat org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:158)\r\n2023-02-28T02:34:53.0724474Z \tat org.apache.spark.sql.catalyst.util.NumberConverter.convert(NumberConverter.scala)\r\n2023-02-28T02:34:53.0725128Z \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:38)\r\n2023-02-28T02:34:53.0725826Z \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n2023-02-28T02:34:53.0726376Z \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n2023-02-28T02:34:53.0726827Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n2023-02-28T02:34:53.0727189Z \tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n2023-02-28T02:34:53.0727556Z \tat org.apache.spark.util.Iterators$.size(Iterators.scala:29)\r\n2023-02-28T02:34:53.0727931Z \tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1944)\r\n2023-02-28T02:34:53.0728346Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1266)\r\n2023-02-28T02:34:53.0728701Z \tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1266)\r\n2023-02-28T02:34:53.0729096Z \tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n2023-02-28T02:34:53.0729523Z \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n2023-02-28T02:34:53.0729966Z \tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo, test-only.\r\n\r\n### How was this patch tested?\r\n\r\nFixed unittests.",
  "closed_by": {
    "login": "HyukjinKwon",
    "id": 6477701,
    "node_id": "MDQ6VXNlcjY0Nzc3MDE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/6477701?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/HyukjinKwon",
    "html_url": "https://github.com/HyukjinKwon",
    "followers_url": "https://api.github.com/users/HyukjinKwon/followers",
    "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}",
    "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions",
    "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs",
    "repos_url": "https://api.github.com/users/HyukjinKwon/repos",
    "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}",
    "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40209/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40209/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
