{
  "url": "https://api.github.com/repos/apache/spark/issues/40321",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40321/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40321/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40321/events",
  "html_url": "https://github.com/apache/spark/pull/40321",
  "id": 1613959324,
  "node_id": "PR_kwDOAQXtWs5LgL8Y",
  "number": 40321,
  "title": "[SPARK-42704] SubqueryAlias propagates metadata columns that child outputs",
  "user": {
    "login": "ryan-johnson-databricks",
    "id": 79601771,
    "node_id": "MDQ6VXNlcjc5NjAxNzcx",
    "avatar_url": "https://avatars.githubusercontent.com/u/79601771?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ryan-johnson-databricks",
    "html_url": "https://github.com/ryan-johnson-databricks",
    "followers_url": "https://api.github.com/users/ryan-johnson-databricks/followers",
    "following_url": "https://api.github.com/users/ryan-johnson-databricks/following{/other_user}",
    "gists_url": "https://api.github.com/users/ryan-johnson-databricks/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ryan-johnson-databricks/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ryan-johnson-databricks/subscriptions",
    "organizations_url": "https://api.github.com/users/ryan-johnson-databricks/orgs",
    "repos_url": "https://api.github.com/users/ryan-johnson-databricks/repos",
    "events_url": "https://api.github.com/users/ryan-johnson-databricks/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ryan-johnson-databricks/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2023-03-07T18:06:56Z",
  "updated_at": "2023-03-13T19:16:16Z",
  "closed_at": null,
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40321",
    "html_url": "https://github.com/apache/spark/pull/40321",
    "diff_url": "https://github.com/apache/spark/pull/40321.diff",
    "patch_url": "https://github.com/apache/spark/pull/40321.patch",
    "merged_at": null
  },
  "body": "<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nThe `AddMetadataColumns` analyzer rule is designed to resolve metadata columns using `LogicalPlan.metadataOutput` -- even if the plan already contains projections whose output does not specifically include a requested metadata column.\r\n\r\nMeanwhile, the `SubqueryAlias` plan node intentionally does _NOT_ propagate metadata columns automatically from a non-leaf/non-subquery child node, because the following should _NOT_ work:\r\n```scala\r\nspark.read.table(\"t\").select(\"a\", \"b\").as(\"s\").select(\"_metadata\")\r\n```\r\n\r\nHowever, the current implementation is too strict in breaking the metadata chain, in case the child node's output already includes the metadata column:\r\n```scala\r\n// expected to work (and does)\r\nspark.read.table(\"t\")\r\n  .select(\"a\", \"b\").select(\"_metadata\")\r\n\r\n// by extension, this should also work (but does not)\r\nspark.read.table(\"t\").select(\"a\", \"b\", \"_metadata\").as(\"s\")\r\n  .select(\"a\", \"b\").select(\"_metadata\")\r\n```\r\n\r\nThe solution is for `SubqueryAlias` to propagate metadata columns that are already in the child's output, thus preserving the `metadataOutput` chain for such columns.\r\n\r\n### Why are the changes needed?\r\n\r\nThe current implementation of `SubqueryAlias` breaks the intended behavior of metadata column propagation. \r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes. The following now works, where previously it did not:\r\n```scala\r\nspark.read.table(\"t\").select(\"a\", \"b\", \"_metadata\").as(\"s\")\r\n  .select(\"a\", \"b\").select(\"_metadata\")\r\n```\r\n\r\n### How was this patch tested?\r\n\r\nNew unit tests verify the expected behavior holds, with and without subqueries in the plan.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40321/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40321/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
