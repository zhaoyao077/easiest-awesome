{
  "url": "https://api.github.com/repos/apache/spark/issues/40434",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40434/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40434/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40434/events",
  "html_url": "https://github.com/apache/spark/pull/40434",
  "id": 1624803971,
  "node_id": "PR_kwDOAQXtWs5MEXBh",
  "number": 40434,
  "title": "[SPARK-42801][CONNECT][TESTS] Ignore flaky `write jdbc` test of `ClientE2ETestSuite` on Java 8",
  "user": {
    "login": "dongjoon-hyun",
    "id": 9700541,
    "node_id": "MDQ6VXNlcjk3MDA1NDE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/9700541?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/dongjoon-hyun",
    "html_url": "https://github.com/dongjoon-hyun",
    "followers_url": "https://api.github.com/users/dongjoon-hyun/followers",
    "following_url": "https://api.github.com/users/dongjoon-hyun/following{/other_user}",
    "gists_url": "https://api.github.com/users/dongjoon-hyun/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/dongjoon-hyun/subscriptions",
    "organizations_url": "https://api.github.com/users/dongjoon-hyun/orgs",
    "repos_url": "https://api.github.com/users/dongjoon-hyun/repos",
    "events_url": "https://api.github.com/users/dongjoon-hyun/events{/privacy}",
    "received_events_url": "https://api.github.com/users/dongjoon-hyun/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 4556440342,
      "node_id": "LA_kwDOAQXtWs8AAAABD5XDFg",
      "url": "https://api.github.com/repos/apache/spark/labels/CONNECT",
      "name": "CONNECT",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2023-03-15T05:53:15Z",
  "updated_at": "2023-03-15T06:28:36Z",
  "closed_at": "2023-03-15T06:28:04Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40434",
    "html_url": "https://github.com/apache/spark/pull/40434",
    "diff_url": "https://github.com/apache/spark/pull/40434.diff",
    "patch_url": "https://github.com/apache/spark/pull/40434.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR aims to ignore the flaky `write jdbc` test of `ClientE2ETestSuite` on Java 8 \r\n\r\n![Screenshot 2023-03-14 at 10 56 34 PM](https://user-images.githubusercontent.com/9700541/225219845-94eaea79-ade6-435d-9d03-19fc73cb8617.png)\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently, this happens on `branch-3.4` with Java 8 only.\r\n\r\n**BRANCH-3.4**\r\nhttps://github.com/apache/spark/commits/branch-3.4\r\n\r\n![Screenshot 2023-03-14 at 10 55 29 PM](https://user-images.githubusercontent.com/9700541/225219670-f8a68dc0-5aa6-428f-9c02-ae41580a38bc.png)\r\n\r\n**JAVA 8**\r\n\r\n1. Currently, `Connect` server is using `Hive` catalog during testing and uses `Derby` with disk store when it creates a table\r\n2. `Connect Client` is trying to use `Derby` with `mem` store and it fails with `No suitable driver` at the first attempt.\r\n```\r\n$ bin/spark-shell -c spark.sql.catalogImplementation=hive\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n23/03/14 21:50:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nSpark context available as 'sc' (master = local[64], app id = local-1678855843831).\r\nSpark session available as 'spark'.\r\nWelcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1-SNAPSHOT\r\n      /_/\r\n\r\nUsing Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)\r\nType in expressions to have them evaluated.\r\nType :help for more information.\r\n\r\nscala> sc.setLogLevel(\"INFO\")\r\n\r\nscala> sql(\"CREATE TABLE t(a int)\")\r\n23/03/14 21:51:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\r\n23/03/14 21:51:08 INFO SharedState: Warehouse path is 'file:/Users/dongjoon/APACHE/spark-merge/spark-warehouse'.\r\n23/03/14 21:51:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\r\n23/03/14 21:51:10 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\r\n23/03/14 21:51:11 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/Users/dongjoon/APACHE/spark-merge/spark-warehouse\r\nres1: org.apache.spark.sql.DataFrame = []\r\n\r\nscala> java.sql.DriverManager.getConnection(\"jdbc:derby:memory:1234;create=true\").createStatement().execute(\"CREATE TABLE s(a int)\");\r\njava.sql.SQLException: No suitable driver found for jdbc:derby:memory:1234;create=true\r\n  at java.sql.DriverManager.getConnection(DriverManager.java:689)\r\n  at java.sql.DriverManager.getConnection(DriverManager.java:270)\r\n  ... 47 elided\r\n```\r\n\r\n**JAVA 11**\r\n```\r\n$ bin/spark-shell -c spark.sql.catalogImplementation=hive\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n23/03/14 21:57:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nSpark context Web UI available at http://localhost:4040\r\nSpark context available as 'sc' (master = local[*], app id = local-1678856279685).\r\nSpark session available as 'spark'.\r\nWelcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1-SNAPSHOT\r\n      /_/\r\n\r\nUsing Scala version 2.12.17 (OpenJDK 64-Bit Server VM, Java 11.0.18)\r\nType in expressions to have them evaluated.\r\nType :help for more information.\r\n\r\nscala> sql(\"CREATE TABLE hive_t2(a int)\")\r\n23/03/14 21:58:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\r\n23/03/14 21:58:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\r\n23/03/14 21:58:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\r\n23/03/14 21:58:07 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\r\n23/03/14 21:58:07 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore dongjoon@127.0.0.1\r\n23/03/14 21:58:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\r\n23/03/14 21:58:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\r\n23/03/14 21:58:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\r\n23/03/14 21:58:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\r\n23/03/14 21:58:07 WARN HiveMetaStore: Location: file:/Users/dongjoon/APACHE/spark-merge/spark-warehouse/hive_t2 specified for non-external table:hive_t2\r\nres0: org.apache.spark.sql.DataFrame = []\r\n\r\nscala> java.sql.DriverManager.getConnection(\"jdbc:derby:memory:1234;create=true\").createStatement().execute(\"CREATE TABLE derby_t2(a int)\");\r\nres1: Boolean = false\r\n\r\nscala> :quit\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo. This is a test only PR.\r\n\r\n### How was this patch tested?\r\n\r\nPass the CIs.",
  "closed_by": {
    "login": "dongjoon-hyun",
    "id": 9700541,
    "node_id": "MDQ6VXNlcjk3MDA1NDE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/9700541?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/dongjoon-hyun",
    "html_url": "https://github.com/dongjoon-hyun",
    "followers_url": "https://api.github.com/users/dongjoon-hyun/followers",
    "following_url": "https://api.github.com/users/dongjoon-hyun/following{/other_user}",
    "gists_url": "https://api.github.com/users/dongjoon-hyun/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/dongjoon-hyun/subscriptions",
    "organizations_url": "https://api.github.com/users/dongjoon-hyun/orgs",
    "repos_url": "https://api.github.com/users/dongjoon-hyun/repos",
    "events_url": "https://api.github.com/users/dongjoon-hyun/events{/privacy}",
    "received_events_url": "https://api.github.com/users/dongjoon-hyun/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40434/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40434/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
