{
  "url": "https://api.github.com/repos/apache/spark/issues/41240",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/41240/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/41240/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/41240/events",
  "html_url": "https://github.com/apache/spark/pull/41240",
  "id": 1717952118,
  "node_id": "PR_kwDOAQXtWs5Q7hA2",
  "number": 41240,
  "title": "[SPARK-43545][SQL][PYTHON] Support nested timestamp type",
  "user": {
    "login": "ueshin",
    "id": 506656,
    "node_id": "MDQ6VXNlcjUwNjY1Ng==",
    "avatar_url": "https://avatars.githubusercontent.com/u/506656?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ueshin",
    "html_url": "https://github.com/ueshin",
    "followers_url": "https://api.github.com/users/ueshin/followers",
    "following_url": "https://api.github.com/users/ueshin/following{/other_user}",
    "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions",
    "organizations_url": "https://api.github.com/users/ueshin/orgs",
    "repos_url": "https://api.github.com/users/ueshin/repos",
    "events_url": "https://api.github.com/users/ueshin/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ueshin/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 1981527456,
      "node_id": "MDU6TGFiZWwxOTgxNTI3NDU2",
      "url": "https://api.github.com/repos/apache/spark/labels/CORE",
      "name": "CORE",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 1982260031,
      "node_id": "MDU6TGFiZWwxOTgyMjYwMDMx",
      "url": "https://api.github.com/repos/apache/spark/labels/PYTHON",
      "name": "PYTHON",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 4138679613,
      "node_id": "LA_kwDOAQXtWs72rz09",
      "url": "https://api.github.com/repos/apache/spark/labels/PANDAS%20API%20ON%20SPARK",
      "name": "PANDAS API ON SPARK",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 4556440342,
      "node_id": "LA_kwDOAQXtWs8AAAABD5XDFg",
      "url": "https://api.github.com/repos/apache/spark/labels/CONNECT",
      "name": "CONNECT",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2023-05-20T00:08:11Z",
  "updated_at": "2023-05-25T17:16:41Z",
  "closed_at": "2023-05-25T02:40:24Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/41240",
    "html_url": "https://github.com/apache/spark/pull/41240",
    "diff_url": "https://github.com/apache/spark/pull/41240.diff",
    "patch_url": "https://github.com/apache/spark/pull/41240.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nSupports nested timestamp type in `spark.createDataFrame()` with pandas DataFrame and `df.toPandas()`, and makes them return correct results.\r\n\r\nFor the following schema and pandas DataFrame:\r\n\r\n```py\r\nschema = (\r\n    StructType()\r\n    .add(\"ts\", TimestampType())\r\n    .add(\"ts_ntz\", TimestampNTZType())\r\n    .add(\r\n        \"struct\", StructType().add(\"ts\", TimestampType()).add(\"ts_ntz\", TimestampNTZType())\r\n    )\r\n    .add(\"array\", ArrayType(TimestampType()))\r\n    .add(\"array_ntz\", ArrayType(TimestampNTZType()))\r\n    .add(\"map\", MapType(StringType(), TimestampType()))\r\n    .add(\"map_ntz\", MapType(StringType(), TimestampNTZType()))\r\n)\r\n\r\ndata = [\r\n    Row(\r\n        datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n        datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n        Row(\r\n            datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n            datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n        ),\r\n        [datetime.datetime(2023, 1, 1, 0, 0, 0)],\r\n        [datetime.datetime(2023, 1, 1, 0, 0, 0)],\r\n        dict(ts=datetime.datetime(2023, 1, 1, 0, 0, 0)),\r\n        dict(ts_ntz=datetime.datetime(2023, 1, 1, 0, 0, 0)),\r\n    )\r\n]\r\n\r\npdf = pd.DataFrame.from_records(data, columns=schema.names)\r\n```\r\n\r\n##### `spark.createDataFrame()`\r\n\r\nFor all, return the same results:\r\n\r\n```py\r\n>>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\r\n>>> spark.createDataFrame(pdf, schema).show(truncate=False)\r\n+-------------------+-------------------+------------------------------------------+---------------------+---------------------+---------------------------+-------------------------------+\r\n|ts                 |ts_ntz             |struct                                    |array                |array_ntz            |map                        |map_ntz                        |\r\n+-------------------+-------------------+------------------------------------------+---------------------+---------------------+---------------------------+-------------------------------+\r\n|2023-01-01 00:00:00|2023-01-01 00:00:00|{2023-01-01 00:00:00, 2023-01-01 00:00:00}|[2023-01-01 00:00:00]|[2023-01-01 00:00:00]|{ts -> 2023-01-01 00:00:00}|{ts_ntz -> 2023-01-01 00:00:00}|\r\n+-------------------+-------------------+------------------------------------------+---------------------+---------------------+---------------------------+-------------------------------+\r\n```\r\n\r\n##### `df.toPandas()`\r\n\r\n```py\r\n>>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\r\n>>> df.toPandas()\r\n                   ts     ts_ntz                                      struct                  array              array_ntz                          map                          map_ntz\r\n0 2023-01-01 03:00:00 2023-01-01  (2023-01-01 03:00:00, 2023-01-01 00:00:00)  [2023-01-01 03:00:00]  [2023-01-01 00:00:00]  {'ts': 2023-01-01 03:00:00}  {'ts_ntz': 2023-01-01 00:00:00}\r\n```\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently nested timestamps in `spark.createDataFrame()` with pandas DataFrame and `df.toPandas()` are not supported with `ArrayType` and `MapType`, or return different results from the top-level timestamps with `StructType`.\r\n\r\nFor the following schema and pandas DataFrame:\r\n\r\n```py\r\nschema = (\r\n    StructType()\r\n    .add(\"ts\", TimestampType())\r\n    .add(\"ts_ntz\", TimestampNTZType())\r\n    .add(\r\n        \"struct\", StructType().add(\"ts\", TimestampType()).add(\"ts_ntz\", TimestampNTZType())\r\n    )\r\n)\r\n\r\ndata = [\r\n    Row(\r\n        datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n        datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n        Row(\r\n            datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n            datetime.datetime(2023, 1, 1, 0, 0, 0),\r\n        ),\r\n    )\r\n]\r\n\r\npdf = pd.DataFrame.from_records(data, columns=schema.names)\r\n```\r\n\r\n##### `spark.createDataFrame()`\r\n\r\n- Without Arrow\r\n\r\n```py\r\n>>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\r\n>>> spark.createDataFrame(pdf, schema).show(truncate=False)\r\n+-------------------+-------------------+------------------------------------------+\r\n|ts                 |ts_ntz             |struct                                    |\r\n+-------------------+-------------------+------------------------------------------+\r\n|2023-01-01 00:00:00|2023-01-01 00:00:00|{2023-01-01 03:00:00, 2023-01-01 00:00:00}|\r\n+-------------------+-------------------+------------------------------------------+\r\n```\r\n\r\n- With Arrow or Spark Connect:\r\n\r\n```py\r\n>>> spark.createDataFrame(pdf, schema).show(truncate=False)\r\n+-------------------+-------------------+------------------------------------------+\r\n|ts                 |ts_ntz             |struct                                    |\r\n+-------------------+-------------------+------------------------------------------+\r\n|2023-01-01 00:00:00|2023-01-01 00:00:00|{2022-12-31 19:00:00, 2023-01-01 00:00:00}|\r\n+-------------------+-------------------+------------------------------------------+\r\n```\r\n\r\n##### `df.toPandas()`\r\n\r\nFor the following DataFrame:\r\n\r\n```py\r\n>>> spark.conf.unset(\"spark.sql.session.timeZone\")\r\n>>> df = spark.createDataFrame(data, schema)\r\n>>>\r\n>>> df.show(truncate=False)\r\n+-------------------+-------------------+------------------------------------------+\r\n|ts                 |ts_ntz             |struct                                    |\r\n+-------------------+-------------------+------------------------------------------+\r\n|2023-01-01 00:00:00|2023-01-01 00:00:00|{2023-01-01 00:00:00, 2023-01-01 00:00:00}|\r\n+-------------------+-------------------+------------------------------------------+\r\n\r\n>>> spark.conf.set('spark.sql.execution.pandas.structHandlingMode', 'row')\r\n```\r\n\r\n- Without Arrow\r\n\r\n```py\r\n>>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\r\n>>> df.toPandas()\r\n                   ts     ts_ntz                                      struct\r\n0 2023-01-01 03:00:00 2023-01-01  (2023-01-01 00:00:00, 2023-01-01 00:00:00)\r\n```\r\n\r\n- With Arrow or Spark Connect:\r\n\r\n```py\r\n>>> df.toPandas()\r\n                   ts     ts_ntz                                      struct\r\n0 2023-01-01 03:00:00 2023-01-01  (2023-01-01 08:00:00, 2023-01-01 00:00:00)\r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nUsers will be able to use nested timestamps.\r\n\r\n### How was this patch tested?\r\n\r\nAdded/updated the related tests.",
  "closed_by": {
    "login": "zhengruifeng",
    "id": 7322292,
    "node_id": "MDQ6VXNlcjczMjIyOTI=",
    "avatar_url": "https://avatars.githubusercontent.com/u/7322292?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/zhengruifeng",
    "html_url": "https://github.com/zhengruifeng",
    "followers_url": "https://api.github.com/users/zhengruifeng/followers",
    "following_url": "https://api.github.com/users/zhengruifeng/following{/other_user}",
    "gists_url": "https://api.github.com/users/zhengruifeng/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/zhengruifeng/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/zhengruifeng/subscriptions",
    "organizations_url": "https://api.github.com/users/zhengruifeng/orgs",
    "repos_url": "https://api.github.com/users/zhengruifeng/repos",
    "events_url": "https://api.github.com/users/zhengruifeng/events{/privacy}",
    "received_events_url": "https://api.github.com/users/zhengruifeng/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/41240/reactions",
    "total_count": 3,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 3
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/41240/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
