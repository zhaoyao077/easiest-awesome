{
  "url": "https://api.github.com/repos/apache/spark/issues/40742",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40742/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40742/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40742/events",
  "html_url": "https://github.com/apache/spark/pull/40742",
  "id": 1662563777,
  "node_id": "PR_kwDOAQXtWs5OBzHT",
  "number": 40742,
  "title": "[SPARK-43095][SQL] Avoid Once strategy's idempotence is broken for batch: `Infer Filters`",
  "user": {
    "login": "wangyum",
    "id": 5399861,
    "node_id": "MDQ6VXNlcjUzOTk4NjE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/5399861?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/wangyum",
    "html_url": "https://github.com/wangyum",
    "followers_url": "https://api.github.com/users/wangyum/followers",
    "following_url": "https://api.github.com/users/wangyum/following{/other_user}",
    "gists_url": "https://api.github.com/users/wangyum/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/wangyum/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/wangyum/subscriptions",
    "organizations_url": "https://api.github.com/users/wangyum/orgs",
    "repos_url": "https://api.github.com/users/wangyum/repos",
    "events_url": "https://api.github.com/users/wangyum/events{/privacy}",
    "received_events_url": "https://api.github.com/users/wangyum/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2023-04-11T14:04:03Z",
  "updated_at": "2023-04-15T01:25:39Z",
  "closed_at": "2023-04-15T01:24:51Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40742",
    "html_url": "https://github.com/apache/spark/pull/40742",
    "diff_url": "https://github.com/apache/spark/pull/40742.diff",
    "patch_url": "https://github.com/apache/spark/pull/40742.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR makes it also remove `EqualNullSafe` when removing `EqualTo` if their children are same when constructing candidate constraints in `ConstraintHelper.inferAdditionalConstraints`.\r\nFor example: `l = r and l <=> r`. Before this PR, `l = r and l <=> r` can infer `l <=> l and r <=> r` which is useless. After This PR, it can't infer anything.\r\n\r\n### Why are the changes needed?\r\n\r\nAvoid Once strategy's idempotence is broken for batch: `Infer Filters`:\r\n```sql\r\nexport SPARK_TESTING=1\r\n\r\nCREATE TABLE t1 (i INT, j INT, k STRING) USING parquet;\r\nCREATE TABLE t2 (i INT, j INT, k STRING) USING parquet;\r\nCREATE TABLE t3 (i INT, j INT, k STRING) USING parquet;\r\n\r\nSELECT *\r\nFROM   (SELECT t1.i, t1.i as t1i\r\n        FROM t1 JOIN t3 ON t1.i = t3.i) t\r\n       JOIN t2 ON t.i = t2.i;\r\n```\r\n\r\nBefore this PR:\r\n```\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===\r\n Join Inner, (i#72 = i#78)                                               Join Inner, (i#72 = i#78)\r\n!:- Project [i#72, i#72 AS t1i#71]                                       :- Filter ((i#72 <=> i#72) AND (t1i#71 <=> t1i#71))\r\n!:  +- Join Inner, (i#72 = i#75)                                         :  +- Project [i#72, i#72 AS t1i#71]\r\n!:     :- Project [i#72]                                                 :     +- Join Inner, (i#72 = i#75)\r\n!:     :  +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet   :        :- Filter isnotnull(i#72)\r\n!:     +- Project [i#75]                                                 :        :  +- Project [i#72]\r\n!:        +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet   :        :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet\r\n!+- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet            :        +- Filter isnotnull(i#75)\r\n!                                                                        :           +- Project [i#75]\r\n!                                                                        :              +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet\r\n!                                                                        +- Filter isnotnull(i#78)\r\n!                                                                           +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet\r\n\r\n\r\norg.apache.spark.SparkRuntimeException: Once strategy's idempotence is broken for batch Infer Filters\r\n Join Inner, (i#72 = i#78)                                                     Join Inner, (i#72 = i#78)\r\n :- Filter ((i#72 <=> i#72) AND (t1i#71 <=> t1i#71))                           :- Filter ((i#72 <=> i#72) AND (t1i#71 <=> t1i#71))\r\n :  +- Project [i#72, i#72 AS t1i#71]                                          :  +- Project [i#72, i#72 AS t1i#71]\r\n :     +- Join Inner, (i#72 = i#75)                                            :     +- Join Inner, (i#72 = i#75)\r\n :        :- Filter isnotnull(i#72)                                            :        :- Filter isnotnull(i#72)\r\n :        :  +- Project [i#72]                                                 :        :  +- Project [i#72]\r\n :        :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet   :        :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet\r\n :        +- Filter isnotnull(i#75)                                            :        +- Filter isnotnull(i#75)\r\n :           +- Project [i#75]                                                 :           +- Project [i#75]\r\n :              +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet   :              +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet\r\n!+- Filter isnotnull(i#78)                                                     +- Filter (i#78 <=> i#78)\r\n!   +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet                  +- Filter isnotnull(i#78)\r\n!                                                                                    +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet.\r\n```\r\n\r\nAfter this PR:\r\n```\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===\r\n Join Inner, (i#72 = i#78)                                               Join Inner, (i#72 = i#78)\r\n :- Project [i#72, i#72 AS t1i#71]                                       :- Project [i#72, i#72 AS t1i#71]\r\n :  +- Join Inner, (i#72 = i#75)                                         :  +- Join Inner, (i#72 = i#75)\r\n!:     :- Project [i#72]                                                 :     :- Filter isnotnull(i#72)\r\n!:     :  +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet   :     :  +- Project [i#72]\r\n!:     +- Project [i#75]                                                 :     :     +- Relation spark_catalog.default.t1[i#72,j#73,k#74] parquet\r\n!:        +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet   :     +- Filter isnotnull(i#75)\r\n!+- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet            :        +- Project [i#75]\r\n!                                                                        :           +- Relation spark_catalog.default.t3[i#75,j#76,k#77] parquet\r\n!                                                                        +- Filter isnotnull(i#78)\r\n!                                                                           +- Relation spark_catalog.default.t2[i#78,j#79,k#80] parquet\r\n \r\n```\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nUnit test.",
  "closed_by": {
    "login": "wangyum",
    "id": 5399861,
    "node_id": "MDQ6VXNlcjUzOTk4NjE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/5399861?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/wangyum",
    "html_url": "https://github.com/wangyum",
    "followers_url": "https://api.github.com/users/wangyum/followers",
    "following_url": "https://api.github.com/users/wangyum/following{/other_user}",
    "gists_url": "https://api.github.com/users/wangyum/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/wangyum/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/wangyum/subscriptions",
    "organizations_url": "https://api.github.com/users/wangyum/orgs",
    "repos_url": "https://api.github.com/users/wangyum/repos",
    "events_url": "https://api.github.com/users/wangyum/events{/privacy}",
    "received_events_url": "https://api.github.com/users/wangyum/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40742/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40742/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
