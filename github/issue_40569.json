{
  "url": "https://api.github.com/repos/apache/spark/issues/40569",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40569/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40569/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40569/events",
  "html_url": "https://github.com/apache/spark/pull/40569",
  "id": 1642880812,
  "node_id": "PR_kwDOAQXtWs5NA0PZ",
  "number": 40569,
  "title": "[SPARK-42937][SQL] `PlanSubqueries` should set `InSubqueryExec#shouldBroadcast` to true",
  "user": {
    "login": "bersprockets",
    "id": 21131848,
    "node_id": "MDQ6VXNlcjIxMTMxODQ4",
    "avatar_url": "https://avatars.githubusercontent.com/u/21131848?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/bersprockets",
    "html_url": "https://github.com/bersprockets",
    "followers_url": "https://api.github.com/users/bersprockets/followers",
    "following_url": "https://api.github.com/users/bersprockets/following{/other_user}",
    "gists_url": "https://api.github.com/users/bersprockets/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/bersprockets/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/bersprockets/subscriptions",
    "organizations_url": "https://api.github.com/users/bersprockets/orgs",
    "repos_url": "https://api.github.com/users/bersprockets/repos",
    "events_url": "https://api.github.com/users/bersprockets/events{/privacy}",
    "received_events_url": "https://api.github.com/users/bersprockets/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 4,
  "created_at": "2023-03-27T22:02:18Z",
  "updated_at": "2023-04-27T22:24:27Z",
  "closed_at": "2023-03-28T12:32:38Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40569",
    "html_url": "https://github.com/apache/spark/pull/40569",
    "diff_url": "https://github.com/apache/spark/pull/40569.diff",
    "patch_url": "https://github.com/apache/spark/pull/40569.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nChange `PlanSubqueries` to set `shouldBroadcast` to true when instantiating an `InSubqueryExec` instance.\r\n\r\n### Why are the changes needed?\r\n\r\nThe below left outer join gets an error:\r\n```\r\ncreate or replace temp view v1 as\r\nselect * from values\r\n(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1),\r\n(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2),\r\n(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\r\nas v1(key, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10);\r\n\r\ncreate or replace temp view v2 as\r\nselect * from values\r\n(1, 2),\r\n(3, 8),\r\n(7, 9)\r\nas v2(a, b);\r\n\r\ncreate or replace temp view v3 as\r\nselect * from values\r\n(3),\r\n(8)\r\nas v3(col1);\r\n\r\nset spark.sql.codegen.maxFields=10; -- let's make maxFields 10 instead of 100\r\nset spark.sql.adaptive.enabled=false;\r\n\r\nselect *\r\nfrom v1\r\nleft outer join v2\r\non key = a\r\nand key in (select col1 from v3);\r\n```\r\nThe join fails during predicate codegen:\r\n```\r\n23/03/27 12:24:12 WARN Predicate: Expr codegen error and falling back to interpreter mode\r\njava.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)\r\n\tat org.apache.spark.sql.execution.InSubqueryExec.doGenCode(subquery.scala:156)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:201)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:196)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$2(CodeGenerator.scala:1278)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1278)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:41)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.generate(GeneratePredicate.scala:33)\r\n\tat org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:73)\r\n\tat org.apache.spark.sql.catalyst.expressions.Predicate$.createCodeGeneratedObject(predicates.scala:70)\r\n\tat org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:51)\r\n\tat org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:86)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.boundCondition(HashJoin.scala:146)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.boundCondition$(HashJoin.scala:140)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition$lzycompute(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.boundCondition(BroadcastHashJoinExec.scala:40)\r\n```\r\nIt fails again after fallback to interpreter mode:\r\n```\r\n23/03/27 12:24:12 ERROR Executor: Exception in task 2.0 in stage 2.0 (TID 7)\r\njava.lang.IllegalArgumentException: requirement failed: input[0, int, false] IN subquery#34 has not finished\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.sql.execution.InSubqueryExec.prepareResult(subquery.scala:144)\r\n\tat org.apache.spark.sql.execution.InSubqueryExec.eval(subquery.scala:151)\r\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedPredicate.eval(predicates.scala:52)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2(HashJoin.scala:146)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.$anonfun$boundCondition$2$adapted(HashJoin.scala:146)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.$anonfun$outerJoin$1(HashJoin.scala:205)\r\n```\r\nBoth the predicate codegen and the evaluation fail for the same reason: `PlanSubqueries` creates `InSubqueryExec` with `shouldBroadcast=false`. The driver waits for the subquery to finish, but it's the executor that uses the results of the subquery (for predicate codegen or evaluation). Because `shouldBroadcast` is set to false, the result is stored in a transient field (`InSubqueryExec#result`), so the result of the subquery is not serialized when the `InSubqueryExec` instance is sent to the executor.\r\n\r\nThe issue occurs, as far as I can tell, only when both whole stage codegen is disabled and adaptive execution is disabled. When wholestage codegen is enabled, the predicate codegen happens on the driver, so the subquery's result is available. When adaptive execution is enabled, `PlanAdaptiveSubqueries` always sets `shouldBroadcast=true`, so the subquery's result is available on the executor, if needed.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nNo.\r\n\r\n### How was this patch tested?\r\n\r\nNew unit test.\r\n",
  "closed_by": {
    "login": "dongjoon-hyun",
    "id": 9700541,
    "node_id": "MDQ6VXNlcjk3MDA1NDE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/9700541?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/dongjoon-hyun",
    "html_url": "https://github.com/dongjoon-hyun",
    "followers_url": "https://api.github.com/users/dongjoon-hyun/followers",
    "following_url": "https://api.github.com/users/dongjoon-hyun/following{/other_user}",
    "gists_url": "https://api.github.com/users/dongjoon-hyun/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/dongjoon-hyun/subscriptions",
    "organizations_url": "https://api.github.com/users/dongjoon-hyun/orgs",
    "repos_url": "https://api.github.com/users/dongjoon-hyun/repos",
    "events_url": "https://api.github.com/users/dongjoon-hyun/events{/privacy}",
    "received_events_url": "https://api.github.com/users/dongjoon-hyun/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40569/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40569/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
