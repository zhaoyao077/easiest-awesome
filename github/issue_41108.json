{
  "url": "https://api.github.com/repos/apache/spark/issues/41108",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/41108/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/41108/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/41108/events",
  "html_url": "https://github.com/apache/spark/pull/41108",
  "id": 1702424172,
  "node_id": "PR_kwDOAQXtWs5QHVNj",
  "number": 41108,
  "title": "[SPARK-43427] spark protobuf: modify serde behavior of unsigned integer types",
  "user": {
    "login": "justaparth",
    "id": 1002986,
    "node_id": "MDQ6VXNlcjEwMDI5ODY=",
    "avatar_url": "https://avatars.githubusercontent.com/u/1002986?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/justaparth",
    "html_url": "https://github.com/justaparth",
    "followers_url": "https://api.github.com/users/justaparth/followers",
    "following_url": "https://api.github.com/users/justaparth/following{/other_user}",
    "gists_url": "https://api.github.com/users/justaparth/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/justaparth/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/justaparth/subscriptions",
    "organizations_url": "https://api.github.com/users/justaparth/orgs",
    "repos_url": "https://api.github.com/users/justaparth/repos",
    "events_url": "https://api.github.com/users/justaparth/events{/privacy}",
    "received_events_url": "https://api.github.com/users/justaparth/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 4654282764,
      "node_id": "LA_kwDOAQXtWs8AAAABFWq4DA",
      "url": "https://api.github.com/repos/apache/spark/labels/PROTOBUF",
      "name": "PROTOBUF",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 6,
  "created_at": "2023-05-09T17:12:54Z",
  "updated_at": "2023-05-22T18:45:10Z",
  "closed_at": null,
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/41108",
    "html_url": "https://github.com/apache/spark/pull/41108",
    "diff_url": "https://github.com/apache/spark/pull/41108.diff",
    "patch_url": "https://github.com/apache/spark/pull/41108.patch",
    "merged_at": null
  },
  "body": "https://issues.apache.org/jira/browse/SPARK-43427\r\n\r\n<!--\r\nThanks for sending a pull request!  Here are some tips for you:\r\n  1. If this is your first time, please read our contributor guidelines: https://spark.apache.org/contributing.html\r\n  2. Ensure you have added or run the appropriate tests for your PR: https://spark.apache.org/developer-tools.html\r\n  3. If the PR is unfinished, add '[WIP]' in your PR title, e.g., '[WIP][SPARK-XXXX] Your PR title ...'.\r\n  4. Be sure to keep the PR description updated to reflect all changes.\r\n  5. Please write your PR title to summarize what this PR proposes.\r\n  6. If possible, provide a concise example to reproduce the issue for a faster review.\r\n  7. If you want to add a new configuration, please read the guideline first for naming configurations in\r\n     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.\r\n  8. If you want to add or modify an error type or message, please read the guideline first in\r\n     'core/src/main/resources/error/README.md'.\r\n-->\r\n\r\n### What changes were proposed in this pull request?\r\n\r\n**Explanation**\r\nProtobuf supports unsigned integer types, including `uint32` and `uint64`. When deserializing protobuf values with fields of these types, the `from_protobuf` library currently transforms them to the spark type of:\r\n\r\nuint32 => `IntegerType`\r\nuint64 => `LongType`\r\n\r\n`IntegerType` and `LongType` are [signed](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) integer types, so this can lead to confusing results. Namely, if a uint32 value in a stored proto is above 2^31 or a uint64 value is above 2^63, their representation in binary will contain a 1 in the highest bit, which when interpreted as a signed integer will come out as negative (I.e. overflow).\r\n\r\nI propose that we deserialize unsigned integer types into a type that can contain them correctly, e.g.\r\nuint32 => `LongType`\r\nuint64 => `Decimal(20, 0)`\r\n\r\n**Precedent**\r\nI believe that unsigned integer types in **parquet** are deserialized in a similar manner, i.e. put into a larger type so that the unsigned representation natively fits. https://issues.apache.org/jira/browse/SPARK-34817 and https://github.com/apache/spark/pull/31921\r\n\r\n** Example to reproduce **\r\n\r\nConsider a protobuf message like:\r\n```\r\nsyntax = \"proto3\";\r\n\r\nmessage Test {\r\n  uint64 val = 1;\r\n}\r\n```\r\n\r\nGenerate a protobuf with a value above 2^63. I did this in python with a small script like:\r\n\r\n```\r\nimport test_pb2\r\n\r\ns = test_pb2.Test()\r\ns.val = 9223372036854775809 # 2**63 + 1\r\nserialized = s.SerializeToString()\r\nprint(serialized)\r\n```\r\n\r\nThis generates the binary representation:\r\n\r\n```\r\nb'\\x08\\x81\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x80\\x01'\r\n```\r\n\r\nThen, deserialize this using `from_protobuf`. I did this in a notebook so its easier to see, but could reproduce in a scala test as well:\r\n\r\n<img width=\"597\" alt=\"image\" src=\"https://github.com/apache/spark/assets/1002986/a6c58c19-b9d3-44d4-8c2a-605991d3d5de\">\r\n\r\n\r\n\r\n**Backwards Compatibility / Default Behavior**\r\n**Should we maintain backwards compatibility and add an option that allows deserializing these types differently? OR should we change change the default behavior (with an option to go back to the old way)? Would love some thoughts here!**\r\n\r\nI think by default it makes more sense to deserialize them as the larger types so that it's semantically more correct. However, there may be existing users of this library that would be affected by this behavior change. Though, maybe we can justify the change since the function is tagged as `Experimental` (and spark 3.4.0 was only released very recently).\r\n\r\n\r\n\r\n### Why are the changes needed?\r\nImprove unsigned integer deserialization behavior.\r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, as written it would change the deserialization behavior of unsigned integer field types. However, please see the above section about whether we should or should not maintain backwards compatibility.\r\n\r\n\r\n### How was this patch tested?\r\nUnit Testing\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/41108/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/41108/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
