{
  "url": "https://api.github.com/repos/apache/spark/issues/40364",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/40364/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/40364/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/40364/events",
  "html_url": "https://github.com/apache/spark/pull/40364",
  "id": 1618674479,
  "node_id": "PR_kwDOAQXtWs5LwFT9",
  "number": 40364,
  "title": "[SPARK-42745][SQL] Improved AliasAwareOutputExpression works with DSv2",
  "user": {
    "login": "peter-toth",
    "id": 7253827,
    "node_id": "MDQ6VXNlcjcyNTM4Mjc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/7253827?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/peter-toth",
    "html_url": "https://github.com/peter-toth",
    "followers_url": "https://api.github.com/users/peter-toth/followers",
    "following_url": "https://api.github.com/users/peter-toth/following{/other_user}",
    "gists_url": "https://api.github.com/users/peter-toth/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/peter-toth/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/peter-toth/subscriptions",
    "organizations_url": "https://api.github.com/users/peter-toth/orgs",
    "repos_url": "https://api.github.com/users/peter-toth/repos",
    "events_url": "https://api.github.com/users/peter-toth/events{/privacy}",
    "received_events_url": "https://api.github.com/users/peter-toth/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 7,
  "created_at": "2023-03-10T09:53:52Z",
  "updated_at": "2023-03-10T15:15:22Z",
  "closed_at": "2023-03-10T12:58:50Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/40364",
    "html_url": "https://github.com/apache/spark/pull/40364",
    "diff_url": "https://github.com/apache/spark/pull/40364.diff",
    "patch_url": "https://github.com/apache/spark/pull/40364.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nAfter https://github.com/apache/spark/pull/37525 (SPARK-40086 / SPARK-42049) the following, simple subselect expression containing query:\r\n```\r\nselect (select sum(id) from t1)\r\n```\r\nfails with:\r\n```\r\n09:48:57.645 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 3.0 (TID 3)\r\njava.lang.NullPointerException\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch$lzycompute(BatchScanExec.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.batch(BatchScanExec.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.hashCode(BatchScanExec.scala:60)\r\n\tat scala.runtime.Statics.anyHash(Statics.java:122)\r\n        ...\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.hashCode(TreeNode.scala:249)\r\n\tat scala.runtime.Statics.anyHash(Statics.java:122)\r\n\tat scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)\r\n\tat scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)\r\n\tat scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)\r\n\tat scala.collection.mutable.HashTable.addEntry(HashTable.scala:149)\r\n\tat scala.collection.mutable.HashTable.addEntry$(HashTable.scala:148)\r\n\tat scala.collection.mutable.HashMap.addEntry(HashMap.scala:44)\r\n\tat scala.collection.mutable.HashTable.init(HashTable.scala:110)\r\n\tat scala.collection.mutable.HashTable.init$(HashTable.scala:89)\r\n\tat scala.collection.mutable.HashMap.init(HashMap.scala:44)\r\n\tat scala.collection.mutable.HashMap.readObject(HashMap.scala:195)\r\n        ...\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\r\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:85)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1520)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n```\r\nwhen DSv2 is enabled.\r\n\r\nThis PR proposes to fix `BatchScanExec` as its `equals()` and `hashCode()` as those shouldn't throw NPE in any circumstances.\r\n\r\nBut if we dig deeper we realize that the NPE orrurs since https://github.com/apache/spark/pull/37525 and the root cause of the problem is changing `AliasAwareOutputExpression.aliasMap` from immutable to mutable. The mutable map deserialization invokes the `hashCode()` of the keys while that is not the case with immutable maps. In this case the key is a subquery expression whose plan contains the `BatchScanExec`.\r\nPlease note that the mutability of `aliasMap` shouldn't be an issue as it is a `private` field of `AliasAwareOutputExpression` (though adding a simple `.toMap` would also help to avoid the NPE).\r\nBased on the above findings this PR also proposes making `aliasMap` to transient as it isn't needed on executors.\r\n\r\nA side quiestion is if adding any subqery expressions to `AliasAwareOutputExpression.aliasMap` makes any sense because `AliasAwareOutputExpression.projectExpression()` mainly projects `child.outputPartitioning` and `child.outputOrdering` that can't contain subquery expressions. But there are a few exceptions (`SortAggregateExec`, `TakeOrderedAndProjectExec`) where `AliasAwareQueryOutputOrdering.orderingExpressions` doesn't come from the `child` and actually leaving those expressions in the map doesn't do any harm.\r\n\r\n### Why are the changes needed?\r\nTo fix regression introduced with https://github.com/apache/spark/pull/37525.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nYes, the query works again.\r\n\r\n### How was this patch tested?\r\nAdded new UT.\r\n",
  "closed_by": {
    "login": "cloud-fan",
    "id": 3182036,
    "node_id": "MDQ6VXNlcjMxODIwMzY=",
    "avatar_url": "https://avatars.githubusercontent.com/u/3182036?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/cloud-fan",
    "html_url": "https://github.com/cloud-fan",
    "followers_url": "https://api.github.com/users/cloud-fan/followers",
    "following_url": "https://api.github.com/users/cloud-fan/following{/other_user}",
    "gists_url": "https://api.github.com/users/cloud-fan/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/cloud-fan/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/cloud-fan/subscriptions",
    "organizations_url": "https://api.github.com/users/cloud-fan/orgs",
    "repos_url": "https://api.github.com/users/cloud-fan/repos",
    "events_url": "https://api.github.com/users/cloud-fan/events{/privacy}",
    "received_events_url": "https://api.github.com/users/cloud-fan/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/40364/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/40364/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
