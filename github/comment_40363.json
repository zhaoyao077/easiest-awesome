[
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1467439787",
    "html_url": "https://github.com/apache/spark/pull/40363#issuecomment-1467439787",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40363",
    "id": 1467439787,
    "node_id": "IC_kwDOAQXtWs5Xd1qr",
    "user": {
      "login": "thousandhu",
      "id": 5370302,
      "node_id": "MDQ6VXNlcjUzNzAzMDI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/5370302?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/thousandhu",
      "html_url": "https://github.com/thousandhu",
      "followers_url": "https://api.github.com/users/thousandhu/followers",
      "following_url": "https://api.github.com/users/thousandhu/following{/other_user}",
      "gists_url": "https://api.github.com/users/thousandhu/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/thousandhu/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/thousandhu/subscriptions",
      "organizations_url": "https://api.github.com/users/thousandhu/orgs",
      "repos_url": "https://api.github.com/users/thousandhu/repos",
      "events_url": "https://api.github.com/users/thousandhu/events{/privacy}",
      "received_events_url": "https://api.github.com/users/thousandhu/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-14T06:27:56Z",
    "updated_at": "2023-03-14T06:28:17Z",
    "author_association": "NONE",
    "body": "> BTW, you can prevent the leak very easily by using TTL like S3/MinIO lifecycle rules.\r\n\r\n\r\n\r\nWe are using HDFS as the storage. \r\nAnd the ttl is not enough to all apps, such as streaming and AI training job which runs for several days.\r\nIf TTL is too short, it will affect the failover for spark apps. If TTL is very long, it cases storage waste.",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1467439787/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1490831779",
    "html_url": "https://github.com/apache/spark/pull/40363#issuecomment-1490831779",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40363",
    "id": 1490831779,
    "node_id": "IC_kwDOAQXtWs5Y3Emj",
    "user": {
      "login": "shrprasa",
      "id": 109815907,
      "node_id": "U_kgDOBouoYw",
      "avatar_url": "https://avatars.githubusercontent.com/u/109815907?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/shrprasa",
      "html_url": "https://github.com/shrprasa",
      "followers_url": "https://api.github.com/users/shrprasa/followers",
      "following_url": "https://api.github.com/users/shrprasa/following{/other_user}",
      "gists_url": "https://api.github.com/users/shrprasa/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/shrprasa/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/shrprasa/subscriptions",
      "organizations_url": "https://api.github.com/users/shrprasa/orgs",
      "repos_url": "https://api.github.com/users/shrprasa/repos",
      "events_url": "https://api.github.com/users/shrprasa/events{/privacy}",
      "received_events_url": "https://api.github.com/users/shrprasa/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-30T19:34:15Z",
    "updated_at": "2023-03-30T19:34:15Z",
    "author_association": "CONTRIBUTOR",
    "body": "@thousandhu @dongjoon-hyun  @holdenk \r\nThe approach in this PR only handles the cleanup on driver side. It won't clean up the files if files were uploaded during job submission but then submission fails due to some reason. As in this driver won't be launched and clean up will not happen. \r\n\r\nThis Jira SPARK-42744 is duplicate of SPARK-42466. I had already created PR for this issue which handles cleanup on both driver as well client side in case of app submission failure. \r\nOther than this it also optimises the file upload by creating just one upload sub directory rather than created several sub - directories for each file getting uploaded. \r\n\r\nhttps://github.com/apache/spark/pull/40128\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1490831779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
