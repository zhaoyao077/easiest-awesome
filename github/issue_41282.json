{
  "url": "https://api.github.com/repos/apache/spark/issues/41282",
  "repository_url": "https://api.github.com/repos/apache/spark",
  "labels_url": "https://api.github.com/repos/apache/spark/issues/41282/labels{/name}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/41282/comments",
  "events_url": "https://api.github.com/repos/apache/spark/issues/41282/events",
  "html_url": "https://github.com/apache/spark/pull/41282",
  "id": 1722379782,
  "node_id": "PR_kwDOAQXtWs5RKbll",
  "number": 41282,
  "title": "[SPARK-43647][CONNECT][TESTS] Clean up hive classes dir when test `connect-client-jvm` without -Phive",
  "user": {
    "login": "LuciferYang",
    "id": 1475305,
    "node_id": "MDQ6VXNlcjE0NzUzMDU=",
    "avatar_url": "https://avatars.githubusercontent.com/u/1475305?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/LuciferYang",
    "html_url": "https://github.com/LuciferYang",
    "followers_url": "https://api.github.com/users/LuciferYang/followers",
    "following_url": "https://api.github.com/users/LuciferYang/following{/other_user}",
    "gists_url": "https://api.github.com/users/LuciferYang/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/LuciferYang/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/LuciferYang/subscriptions",
    "organizations_url": "https://api.github.com/users/LuciferYang/orgs",
    "repos_url": "https://api.github.com/users/LuciferYang/repos",
    "events_url": "https://api.github.com/users/LuciferYang/events{/privacy}",
    "received_events_url": "https://api.github.com/users/LuciferYang/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 4556440342,
      "node_id": "LA_kwDOAQXtWs8AAAABD5XDFg",
      "url": "https://api.github.com/repos/apache/spark/labels/CONNECT",
      "name": "CONNECT",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 8,
  "created_at": "2023-05-23T16:02:01Z",
  "updated_at": "2023-05-26T01:31:52Z",
  "closed_at": "2023-05-26T01:31:24Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "draft": false,
  "pull_request": {
    "url": "https://api.github.com/repos/apache/spark/pulls/41282",
    "html_url": "https://github.com/apache/spark/pull/41282",
    "diff_url": "https://github.com/apache/spark/pull/41282.diff",
    "patch_url": "https://github.com/apache/spark/pull/41282.patch",
    "merged_at": null
  },
  "body": "### What changes were proposed in this pull request?\r\nThis pr aims to added a cleaning action for the `$sparkHome/sql/hive/target/$scalaDir/classes` and `$sparkHome/sql/hive/target/$scalaDir/test-classes` directories before `SimpleSparkConnectService` starts when running test cases that inherit `RemoteSparkSession` without `-Phive` to avoid to unexpected loading of `sql/hive/target/scala-2.12/classes/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` by `ServiceLoader`.\r\n \r\n\r\n### Why are the changes needed?\r\nWhen we run the test cases that inherit `RemoteSparkSession`, the classpath used to launch `SimpleSparkConnectService` will at least include the following directory, both maven and sbt:\r\n\r\n```\r\n$sparkHome/conf/\r\n$sparkHome/common/kvstore/target/scala-2.12/classes/\r\n$sparkHome/common/network-common/target/scala-2.12/classes/\r\n$sparkHome/common/network-shuffle/target/scala-2.12/classes/\r\n$sparkHome/common/network-yarn/target/scala-2.12/classes\r\n$sparkHome/common/sketch/target/scala-2.12/classes/\r\n$sparkHome/common/tags/target/scala-2.12/classes/\r\n$sparkHome/common/unsafe/target/scala-2.12/classes/\r\n$sparkHome/core/target/scala-2.12/classes/\r\n$sparkHome/examples/target/scala-2.12/classes/\r\n$sparkHome/graphx/target/scala-2.12/classes/\r\n$sparkHome/launcher/target/scala-2.12/classes/\r\n$sparkHome/mllib/target/scala-2.12/classes/\r\n$sparkHome/repl/target/scala-2.12/classes/\r\n$sparkHome/resource-managers/mesos/target/scala-2.12/classes\r\n$sparkHome/resource-managers/yarn/target/scala-2.12/classes\r\n$sparkHome/sql/catalyst/target/scala-2.12/classes/\r\n$sparkHome/sql/core/target/scala-2.12/classes/\r\n$sparkHome/sql/hive/target/scala-2.12/classes/\r\n$sparkHome/sql/hive-thriftserver/target/scala-2.12/classes/\r\n$sparkHome/streaming/target/scala-2.12/classes/\r\n$sparkHome/common/kvstore/target/scala-2.12/test-classes\r\n$sparkHome/common/network-common/target/scala-2.12/test-classes/\r\n$sparkHome/common/network-shuffle/target/scala-2.12/test-classes/\r\n$sparkHome/common/network-yarn/target/scala-2.12/test-classes\r\n$sparkHome/common/sketch/target/scala-2.12/test-classes\r\n$sparkHome/common/tags/target/scala-2.12/test-classes/\r\n$sparkHome/common/unsafe/target/scala-2.12/test-classes\r\n$sparkHome/core/target/scala-2.12/test-classes/\r\n$sparkHome/examples/target/scala-2.12/test-classes\r\n$sparkHome/graphx/target/scala-2.12/test-classes\r\n$sparkHome/launcher/target/scala-2.12/test-classes/\r\n$sparkHome/mllib/target/scala-2.12/test-classes\r\n$sparkHome/repl/target/scala-2.12/test-classes\r\n$sparkHome/resource-managers/mesos/target/scala-2.12/test-classes\r\n$sparkHome/resource-managers/yarn/target/scala-2.12/test-classes\r\n$sparkHome/sql/catalyst/target/scala-2.12/test-classes/\r\n$sparkHome/sql/core/target/scala-2.12/test-classes\r\n$sparkHome/sql/hive/target/scala-2.12/test-classes\r\n$sparkHome/sql/hive-thriftserver/target/scala-2.12/test-classes\r\n$sparkHome/streaming/target/scala-2.12/test-classes\r\n$sparkHome/connector/connect/client/jvm/target/scala-2.12/test-classes/\r\n$sparkHome/connector/connect/common/target/scala-2.12/test-classes/\r\n\r\n...\r\n\r\n```\r\n\r\nSo if the test case need calls `DataSource#lookupDataSource` and the `hive` module is compiled, `sql/hive/target/scala-2.12/classes/META-INF/services/org.apache.spark.sql.sources.DataSourceRegister` will be loaded by `ServiceLoader`.\r\n\r\n\r\nAfter SPARK-43186 | https://github.com/apache/spark/pull/40848 merged, `org.apache.spark.sql.hive.execution.HiveFileFormat` changed to use `org.apache.hadoop.hive.ql.plan.FileSinkDesc` instead of `org.apache.spark.sql.hive.HiveShim.ShimFileSinkDesc`, it has a strong dependence on `hive-exec`. But when there is no hive related jars under `assembly/target/$scalaDir/jars/`, it will cause initialization fail of `org.apache.spark.sql.hive.execution.HiveFileFormat` and test fail.\r\n\r\nFor example, when we run the following commands to test `connect-client-jvm` without `-Phive`:\r\n\r\n```\r\nbuild/mvn clean install -DskipTests\r\nbuild/mvn test -pl connector/connect/client/jvm\r\n```\r\n\r\nThen hive related jars will not be copied to `assembly/target/$scalaDir/jars/`, there will be test error as:\r\n\r\n**Client side**\r\n\r\n```\r\n- read and write *** FAILED ***\r\n  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated\r\n  at io.grpc.Status.asRuntimeException(Status.java:535)\r\n  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)\r\n  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\r\n  at scala.collection.Iterator.toStream(Iterator.scala:1417)\r\n  at scala.collection.Iterator.toStream$(Iterator.scala:1416)\r\n  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)\r\n  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)\r\n  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)\r\n  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)\r\n  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)\r\n  ... \r\n```\r\n\r\n**Server side**\r\n\r\n```\r\njava.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated\r\n\tat java.util.ServiceLoader.fail(ServiceLoader.java:232)\r\n\tat java.util.ServiceLoader.access$100(ServiceLoader.java:185)\r\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)\r\n\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)\r\n\tat java.util.ServiceLoader$1.next(ServiceLoader.java:480)\r\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:46)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r\n\tat scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)\r\n\tat scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)\r\n\tat scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)\r\n\tat scala.collection.TraversableLike.filter(TraversableLike.scala:395)\r\n\tat scala.collection.TraversableLike.filter$(TraversableLike.scala:395)\r\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:629)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:559)\r\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:2326)\r\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2091)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handleCommand(SparkConnectStreamHandler.scala:120)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$2(SparkConnectStreamHandler.scala:86)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:825)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.$anonfun$handle$1(SparkConnectStreamHandler.scala:53)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:209)\r\n\tat org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager$.withArtifactClassLoader(SparkConnectArtifactManager.scala:178)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectStreamHandler.handle(SparkConnectStreamHandler.scala:48)\r\n\tat org.apache.spark.sql.connect.service.SparkConnectService.executePlan(SparkConnectService.scala:166)\r\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:611)\r\n\tat org.sparkproject.connect.grpc.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:352)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:866)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n\tat org.sparkproject.connect.grpc.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/plan/FileSinkDesc\r\n\tat java.lang.Class.getDeclaredConstructors0(Native Method)\r\n\tat java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)\r\n\tat java.lang.Class.getConstructor0(Class.java:3075)\r\n\tat java.lang.Class.newInstance(Class.java:412)\r\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)\r\n\t... 40 more\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.plan.FileSinkDesc\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\r\n\t... 45 more\r\n```\r\n\r\nSo this PR proposal takes the initiative to clean up the `$sparkHome/sql/hive/target/$scalaDir/classes` and `$sparkHome/sql/hive/target/$scalaDir/test-classes` directories when `IntegrationTestUtils#isSparkHiveJarAvailable` is false to protect the above scenario.\r\n \r\n\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\nNo, just for test.\r\n\r\n\r\n### How was this patch tested?\r\n- Pass Github Actions\r\n- Manual test:\r\n\r\nThe following command can reproduce the problem without this pr\r\n\r\nMaven\r\n\r\n```\r\nbuild/mvn clean install -DskipTests\r\nbuild/mvn test -pl connector/connect/client/jvm\r\n```\r\n\r\nSBT\r\n\r\n```\r\nbuild/sbt package\r\nbuild/sbt \"connect-client-jvm/test\"\r\n```\r\n\r\n**Before**\r\n\r\nMaven \r\n\r\nThere are 13 test cases with similar failures\r\n\r\n```\r\n- recoverPartitions *** FAILED ***\r\n  io.grpc.StatusRuntimeException: INTERNAL: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.hive.execution.HiveFileFormat could not be instantiated\r\n  at io.grpc.Status.asRuntimeException(Status.java:535)\r\n  at io.grpc.stub.ClientCalls$BlockingResponseStream.hasNext(ClientCalls.java:660)\r\n  at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:45)\r\n  at scala.collection.Iterator.toStream(Iterator.scala:1417)\r\n  at scala.collection.Iterator.toStream$(Iterator.scala:1416)\r\n  at scala.collection.AbstractIterator.toStream(Iterator.scala:1431)\r\n  at scala.collection.TraversableOnce.toSeq(TraversableOnce.scala:354)\r\n  at scala.collection.TraversableOnce.toSeq$(TraversableOnce.scala:354)\r\n  at scala.collection.AbstractIterator.toSeq(Iterator.scala:1431)\r\n  at org.apache.spark.sql.SparkSession.execute(SparkSession.scala:489)\r\n  ...\r\n\r\n```\r\n\r\nSBT \r\nThere are similar errors of sbt, and the test will unexpectedly aborted.\r\n\r\n**After**\r\nBoth Maven and SBT no longer have similar test failures",
  "closed_by": {
    "login": "LuciferYang",
    "id": 1475305,
    "node_id": "MDQ6VXNlcjE0NzUzMDU=",
    "avatar_url": "https://avatars.githubusercontent.com/u/1475305?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/LuciferYang",
    "html_url": "https://github.com/LuciferYang",
    "followers_url": "https://api.github.com/users/LuciferYang/followers",
    "following_url": "https://api.github.com/users/LuciferYang/following{/other_user}",
    "gists_url": "https://api.github.com/users/LuciferYang/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/LuciferYang/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/LuciferYang/subscriptions",
    "organizations_url": "https://api.github.com/users/LuciferYang/orgs",
    "repos_url": "https://api.github.com/users/LuciferYang/repos",
    "events_url": "https://api.github.com/users/LuciferYang/events{/privacy}",
    "received_events_url": "https://api.github.com/users/LuciferYang/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/apache/spark/issues/41282/reactions",
    "total_count": 1,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/apache/spark/issues/41282/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
