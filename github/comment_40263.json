[
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1465662299",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1465662299",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1465662299,
    "node_id": "IC_kwDOAQXtWs5XXDtb",
    "user": {
      "login": "zhengruifeng",
      "id": 7322292,
      "node_id": "MDQ6VXNlcjczMjIyOTI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7322292?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/zhengruifeng",
      "html_url": "https://github.com/zhengruifeng",
      "followers_url": "https://api.github.com/users/zhengruifeng/followers",
      "following_url": "https://api.github.com/users/zhengruifeng/following{/other_user}",
      "gists_url": "https://api.github.com/users/zhengruifeng/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/zhengruifeng/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/zhengruifeng/subscriptions",
      "organizations_url": "https://api.github.com/users/zhengruifeng/orgs",
      "repos_url": "https://api.github.com/users/zhengruifeng/repos",
      "events_url": "https://api.github.com/users/zhengruifeng/events{/privacy}",
      "received_events_url": "https://api.github.com/users/zhengruifeng/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-13T07:54:37Z",
    "updated_at": "2023-03-13T07:55:11Z",
    "author_association": "CONTRIBUTOR",
    "body": "I did a quick test with dataset `T10I4D100K` in http://fimi.uantwerpen.be/data/ \r\n\r\nfit:\r\n```\r\nscala> val df = sc.textFile(\"/Users/ruifeng.zheng/.dev/data/T10I4D100K.dat\").map(_.split(\" \")).toDF(\"items\")\r\ndf: org.apache.spark.sql.DataFrame = [items: array<string>]\r\n\r\nscala> df.count\r\nres16: Long = 100000\r\n\r\nscala> val model = new FPGrowth().setMinSupport(0.01).setMinConfidence(0.01).fit(df)\r\nmodel: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fpgrowth_92901252345a, numTrainingRecords=100000\r\n\r\nscala> model.freqItemsets.count\r\nres17: Long = 385                                                               \r\n\r\nscala> model.associationRules.count\r\nres18: Long = 21                                                                \r\n\r\nscala> model.save(\"/tmp/fpm.model\")\r\n```\r\n\r\n\r\ntransformation:\r\n```\r\nimport org.apache.spark.ml.fpm._\r\nval df = sc.textFile(\"/Users/ruifeng.zheng/.dev/data/T10I4D100K.dat\").map(_.split(\" \")).toDF(\"items\")\r\ndf.cache()\r\ndf.count()\r\n\r\nval model = FPGrowthModel.load(\"/tmp/fpm.model\")\r\nmodel.transform(df).explain(\"extended\")\r\nSeq.range(0, 100).foreach{i => model.transform(df).count()} // warms up\r\nval start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start\r\nval start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start\r\n```\r\n\r\nmaster:\r\n```\r\nscala> val model = FPGrowthModel.load(\"/tmp/fpm.model\")\r\nmodel: org.apache.spark.ml.fpm.FPGrowthModel = FPGrowthModel: uid=fpgrowth_92901252345a, numTrainingRecords=100000\r\n\r\nscala> model.transform(df).explain(\"extended\")\r\n== Parsed Logical Plan ==\r\n'Project [items#5, UDF('items) AS prediction#70]\r\n+- Project [value#2 AS items#5]\r\n   +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n      +- ExternalRDD [obj#1]\r\n\r\n== Analyzed Logical Plan ==\r\nitems: array<string>, prediction: array<string>\r\nProject [items#5, UDF(items#5) AS prediction#70]\r\n+- Project [value#2 AS items#5]\r\n   +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n      +- ExternalRDD [obj#1]\r\n\r\n== Optimized Logical Plan ==\r\nProject [items#5, UDF(items#5) AS prediction#70]\r\n+- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)\r\n      +- *(1) Project [value#2 AS items#5]\r\n         +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n            +- Scan[obj#1]\r\n\r\n== Physical Plan ==\r\n*(1) Project [items#5, UDF(items#5) AS prediction#70]\r\n+- InMemoryTableScan [items#5]\r\n      +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)\r\n            +- *(1) Project [value#2 AS items#5]\r\n               +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n                  +- Scan[obj#1]\r\n\r\n\r\nscala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up\r\n\r\nscala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start\r\nstart: Long = 1678692855532\r\nend: Long = 1678692860098\r\nres4: Long = 4566\r\n\r\nscala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start\r\nstart: Long = 1678692860277\r\nend: Long = 1678692862372\r\nres5: Long = 2095\r\n```\r\n\r\nthis PR:\r\n```\r\nscala> model.transform(df).explain(\"extended\")\r\n== Parsed Logical Plan ==\r\n'Project [items#5, CASE WHEN NOT isnull('items) THEN aggregate('prediction, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda 'y_1[antecedent], lambdafunction(array_contains('items, lambda 'x_2), lambda 'x_2, false)) THEN array_union(lambda 'x_0, array_except(lambda 'y_1[consequent], 'items)) ELSE lambda 'x_0 END, lambda 'x_0, lambda 'y_1, false), lambdafunction(lambda 'x_3, lambda 'x_3, false)) ELSE cast(array() as array<string>) END AS prediction#72]\r\n+- Join Cross\r\n   :- Project [value#2 AS items#5]\r\n   :  +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n   :     +- ExternalRDD [obj#1]\r\n   +- ResolvedHint (strategy=broadcast)\r\n      +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]\r\n         +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))\r\n            +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false\r\n\r\n== Analyzed Logical Plan ==\r\nitems: array<string>, prediction: array<string>\r\nProject [items#5, CASE WHEN NOT isnull(items#5) THEN aggregate(prediction#68, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE cast(array() as array<string>) END AS prediction#72]\r\n+- Join Cross\r\n   :- Project [value#2 AS items#5]\r\n   :  +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n   :     +- ExternalRDD [obj#1]\r\n   +- ResolvedHint (strategy=broadcast)\r\n      +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]\r\n         +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))\r\n            +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false\r\n\r\n== Optimized Logical Plan ==\r\nProject [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(prediction#68, [], lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE [] END AS prediction#72]\r\n+- Join Cross, rightHint=(strategy=broadcast)\r\n   :- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)\r\n   :     +- *(1) Project [value#2 AS items#5]\r\n   :        +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n   :           +- Scan[obj#1]\r\n   +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS prediction#68]\r\n      +- Project [antecedent#57, consequent#58]\r\n         +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false\r\n\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(prediction#68, [], lambdafunction(CASE WHEN forall(lambda y_1#74.antecedent, lambdafunction(array_contains(items#5, lambda x_2#76), lambda x_2#76, false)) THEN array_union(lambda x_0#73, array_except(lambda y_1#74.consequent, items#5)) ELSE lambda x_0#73 END, lambda x_0#73, lambda y_1#74, false), lambdafunction(lambda x_3#75, lambda x_3#75, false)) ELSE [] END AS prediction#72]\r\n   +- BroadcastNestedLoopJoin BuildRight, Cross\r\n      :- InMemoryTableScan [items#5]\r\n      :     +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)\r\n      :           +- *(1) Project [value#2 AS items#5]\r\n      :              +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n      :                 +- Scan[obj#1]\r\n      +- BroadcastExchange IdentityBroadcastMode, [plan_id=117]\r\n         +- ObjectHashAggregate(keys=[], functions=[collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[prediction#68])\r\n            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=114]\r\n               +- ObjectHashAggregate(keys=[], functions=[partial_collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[buf#95])\r\n                  +- Project [antecedent#57, consequent#58]\r\n                     +- Scan ExistingRDD[antecedent#57,consequent#58,confidence#59,lift#60,support#61]\r\n\r\n\r\nscala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up\r\n\r\nscala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start\r\nstart: Long = 1678693708534\r\nend: Long = 1678693713436\r\nres6: Long = 4902\r\n\r\nscala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start\r\nstart: Long = 1678693713596\r\nend: Long = 1678693713807\r\nres7: Long = 211\r\n```\r\n\r\n\r\nthe transformation is a bit slower 4566 -> 4902, but when we need to analyze the dataframe it will be much faster 2095 -> 211 since the `collect` execution is delayed.",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1465662299/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1466280043",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1466280043",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1466280043,
    "node_id": "IC_kwDOAQXtWs5XZahr",
    "user": {
      "login": "srowen",
      "id": 822522,
      "node_id": "MDQ6VXNlcjgyMjUyMg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/822522?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/srowen",
      "html_url": "https://github.com/srowen",
      "followers_url": "https://api.github.com/users/srowen/followers",
      "following_url": "https://api.github.com/users/srowen/following{/other_user}",
      "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/srowen/subscriptions",
      "organizations_url": "https://api.github.com/users/srowen/orgs",
      "repos_url": "https://api.github.com/users/srowen/repos",
      "events_url": "https://api.github.com/users/srowen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/srowen/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-13T14:45:50Z",
    "updated_at": "2023-03-13T14:45:50Z",
    "author_association": "MEMBER",
    "body": "So this seems slower on a medium-sized data set. I don't know if delaying the collect() matters much; the overall execution time matters. I'm worried that this gets much slower on 1M or 10M records. Does this buy us other benefits, like, is this necessary to support \"Safe Spark\" or something?",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1466280043/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1467821846",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1467821846",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1467821846,
    "node_id": "IC_kwDOAQXtWs5XfS8W",
    "user": {
      "login": "zhengruifeng",
      "id": 7322292,
      "node_id": "MDQ6VXNlcjczMjIyOTI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7322292?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/zhengruifeng",
      "html_url": "https://github.com/zhengruifeng",
      "followers_url": "https://api.github.com/users/zhengruifeng/followers",
      "following_url": "https://api.github.com/users/zhengruifeng/following{/other_user}",
      "gists_url": "https://api.github.com/users/zhengruifeng/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/zhengruifeng/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/zhengruifeng/subscriptions",
      "organizations_url": "https://api.github.com/users/zhengruifeng/orgs",
      "repos_url": "https://api.github.com/users/zhengruifeng/repos",
      "events_url": "https://api.github.com/users/zhengruifeng/events{/privacy}",
      "received_events_url": "https://api.github.com/users/zhengruifeng/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-14T10:26:54Z",
    "updated_at": "2023-03-17T03:05:18Z",
    "author_association": "CONTRIBUTOR",
    "body": "yes, the `BroadcastNestedLoopJoin` is slower.\r\n\r\nThen I have another try with subquery, and it's faster in both execution and analysis, but I have to create temp view and write the sql query then, see https://github.com/apache/spark/pull/40263/commits/63595ba03d9f18fe0b43bfb09f974ea50cb2c651\r\n\r\n`model.transform(df).count()`: 4566 -> 3046\r\n`model.transform(df).schema`: 2095 -> 298\r\n\r\nSo I'm trying to add a new method `Dataset.withScalarSubquery` in https://github.com/apache/spark/pull/40263/commits/c41ac094eb40520948d95108a78431694a33772d \r\n\r\nnot sure whether it is the correct way to support `ScalarSubquery` in DataFrame APIs, but it is actually a pain point to me.\r\n\r\n```\r\nscala> model.transform(df).explain(\"extended\")\r\n== Parsed Logical Plan ==\r\n'Project [items#5, CASE WHEN NOT isnull('items) THEN aggregate('prediction, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda 'y_1[antecedent], lambdafunction(array_contains('items, lambda 'x_2), lambda 'x_2, false)) THEN array_union(lambda 'x_0, array_except(lambda 'y_1[consequent], 'items)) ELSE lambda 'x_0 END, lambda 'x_0, lambda 'y_1, false), lambdafunction(lambda 'x_3, lambda 'x_3, false)) ELSE cast(array() as array<string>) END AS prediction#74]\r\n+- Project [items#5, scalar-subquery#70 [] AS prediction#71]\r\n   :  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]\r\n   :     +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))\r\n   :        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false\r\n   +- Project [value#2 AS items#5]\r\n      +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n         +- ExternalRDD [obj#1]\r\n\r\n== Analyzed Logical Plan ==\r\nitems: array<string>, prediction: array<string>\r\nProject [items#5, CASE WHEN NOT isnull(items#5) THEN aggregate(prediction#71, cast(array() as array<string>), lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE cast(array() as array<string>) END AS prediction#74]\r\n+- Project [items#5, scalar-subquery#70 [] AS prediction#71]\r\n   :  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]\r\n   :     +- Filter (NOT isnull(antecedent#57) AND NOT isnull(consequent#58))\r\n   :        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false\r\n   +- Project [value#2 AS items#5]\r\n      +- SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n         +- ExternalRDD [obj#1]\r\n\r\n== Optimized Logical Plan ==\r\nProject [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(scalar-subquery#70 [], [], lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE [] END AS prediction#74]\r\n:  +- Aggregate [collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0) AS collect_list(struct(antecedent, consequent))#68]\r\n:     +- Project [antecedent#57, consequent#58]\r\n:        +- LogicalRDD [antecedent#57, consequent#58, confidence#59, lift#60, support#61], false\r\n+- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)\r\n      +- *(1) Project [value#2 AS items#5]\r\n         +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n            +- Scan[obj#1]\r\n\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- Project [items#5, CASE WHEN isnotnull(items#5) THEN aggregate(Subquery subquery#70, [id=#105], [], lambdafunction(CASE WHEN forall(lambda y_1#76.antecedent, lambdafunction(array_contains(items#5, lambda x_2#78), lambda x_2#78, false)) THEN array_union(lambda x_0#75, array_except(lambda y_1#76.consequent, items#5)) ELSE lambda x_0#75 END, lambda x_0#75, lambda y_1#76, false), lambdafunction(lambda x_3#77, lambda x_3#77, false)) ELSE [] END AS prediction#74]\r\n   :  +- Subquery subquery#70, [id=#105]\r\n   :     +- AdaptiveSparkPlan isFinalPlan=false\r\n   :        +- ObjectHashAggregate(keys=[], functions=[collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[collect_list(struct(antecedent, consequent))#68])\r\n   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=103]\r\n   :              +- ObjectHashAggregate(keys=[], functions=[partial_collect_list(struct(antecedent, antecedent#57, consequent, consequent#58), 0, 0)], output=[buf#97])\r\n   :                 +- Project [antecedent#57, consequent#58]\r\n   :                    +- Scan ExistingRDD[antecedent#57,consequent#58,confidence#59,lift#60,support#61]\r\n   +- InMemoryTableScan [items#5]\r\n         +- InMemoryRelation [items#5], StorageLevel(disk, memory, deserialized, 1 replicas)\r\n               +- *(1) Project [value#2 AS items#5]\r\n                  +- *(1) SerializeFromObject [mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), input[0, [Ljava.lang.String;, true], None) AS value#2]\r\n                     +- Scan[obj#1]\r\n\r\n\r\nscala> Seq.range(0, 100).foreach{i => model.transform(df).count()} // warms up\r\n\r\nscala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).count()}; val end = System.currentTimeMillis; end - start\r\nstart: Long = 1678788928604\r\nend: Long = 1678788931650\r\nres4: Long = 3046\r\n\r\nscala> val start = System.currentTimeMillis; Seq.range(0, 100).foreach{i => model.transform(df).schema}; val end = System.currentTimeMillis; end - start\r\nstart: Long = 1678788931785\r\nend: Long = 1678788932083\r\nres5: Long = 298\r\n```",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1467821846/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1473064231",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1473064231",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1473064231,
    "node_id": "IC_kwDOAQXtWs5XzS0n",
    "user": {
      "login": "zhengruifeng",
      "id": 7322292,
      "node_id": "MDQ6VXNlcjczMjIyOTI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7322292?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/zhengruifeng",
      "html_url": "https://github.com/zhengruifeng",
      "followers_url": "https://api.github.com/users/zhengruifeng/followers",
      "following_url": "https://api.github.com/users/zhengruifeng/following{/other_user}",
      "gists_url": "https://api.github.com/users/zhengruifeng/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/zhengruifeng/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/zhengruifeng/subscriptions",
      "organizations_url": "https://api.github.com/users/zhengruifeng/orgs",
      "repos_url": "https://api.github.com/users/zhengruifeng/repos",
      "events_url": "https://api.github.com/users/zhengruifeng/events{/privacy}",
      "received_events_url": "https://api.github.com/users/zhengruifeng/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-17T03:20:45Z",
    "updated_at": "2023-03-17T03:20:45Z",
    "author_association": "CONTRIBUTOR",
    "body": "@srowen if the latest performance test seems fine, then I'd ask the SQL guys whether we can have a subquery method in DataFrame APIs.",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1473064231/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1473071461",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1473071461",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1473071461,
    "node_id": "IC_kwDOAQXtWs5XzUll",
    "user": {
      "login": "srowen",
      "id": 822522,
      "node_id": "MDQ6VXNlcjgyMjUyMg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/822522?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/srowen",
      "html_url": "https://github.com/srowen",
      "followers_url": "https://api.github.com/users/srowen/followers",
      "following_url": "https://api.github.com/users/srowen/following{/other_user}",
      "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/srowen/subscriptions",
      "organizations_url": "https://api.github.com/users/srowen/orgs",
      "repos_url": "https://api.github.com/users/srowen/repos",
      "events_url": "https://api.github.com/users/srowen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/srowen/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-17T03:29:28Z",
    "updated_at": "2023-03-17T03:29:28Z",
    "author_association": "MEMBER",
    "body": "If it's faster and gives the right answers, sure",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1473071461/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1477193966",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1477193966",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1477193966,
    "node_id": "IC_kwDOAQXtWs5YDDDu",
    "user": {
      "login": "zhengruifeng",
      "id": 7322292,
      "node_id": "MDQ6VXNlcjczMjIyOTI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7322292?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/zhengruifeng",
      "html_url": "https://github.com/zhengruifeng",
      "followers_url": "https://api.github.com/users/zhengruifeng/followers",
      "following_url": "https://api.github.com/users/zhengruifeng/following{/other_user}",
      "gists_url": "https://api.github.com/users/zhengruifeng/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/zhengruifeng/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/zhengruifeng/subscriptions",
      "organizations_url": "https://api.github.com/users/zhengruifeng/orgs",
      "repos_url": "https://api.github.com/users/zhengruifeng/repos",
      "events_url": "https://api.github.com/users/zhengruifeng/events{/privacy}",
      "received_events_url": "https://api.github.com/users/zhengruifeng/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-21T02:26:50Z",
    "updated_at": "2023-03-21T02:26:50Z",
    "author_association": "CONTRIBUTOR",
    "body": "TL;DR  I want to apply scalar subquery to optimize `FPGrowthModel.transform`, there are two options:\r\n\r\n1, create temp views and use `spark.sql`, see https://github.com/apache/spark/commit/63595ba03d9f18fe0b43bfb09f974ea50cb2c651;\r\n\r\n2, add `private[spark] def withScalarSubquery(colName: String, subquery: Dataset[_]): DataFrame`, it seems much more convenient but not sure whether it is a proper way.\r\n\r\ncc @cloud-fan @HyukjinKwon ",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1477193966/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1477246959",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1477246959",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1477246959,
    "node_id": "IC_kwDOAQXtWs5YDP_v",
    "user": {
      "login": "srowen",
      "id": 822522,
      "node_id": "MDQ6VXNlcjgyMjUyMg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/822522?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/srowen",
      "html_url": "https://github.com/srowen",
      "followers_url": "https://api.github.com/users/srowen/followers",
      "following_url": "https://api.github.com/users/srowen/following{/other_user}",
      "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/srowen/subscriptions",
      "organizations_url": "https://api.github.com/users/srowen/orgs",
      "repos_url": "https://api.github.com/users/srowen/repos",
      "events_url": "https://api.github.com/users/srowen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/srowen/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-21T03:57:56Z",
    "updated_at": "2023-03-21T03:57:56Z",
    "author_association": "MEMBER",
    "body": "I don't know enough to say whether it's worth a new method. Can we start with the change that needs no new API, is it a big enough win?",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1477246959/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/apache/spark/issues/comments/1478764552",
    "html_url": "https://github.com/apache/spark/pull/40263#issuecomment-1478764552",
    "issue_url": "https://api.github.com/repos/apache/spark/issues/40263",
    "id": 1478764552,
    "node_id": "IC_kwDOAQXtWs5YJCgI",
    "user": {
      "login": "zhengruifeng",
      "id": 7322292,
      "node_id": "MDQ6VXNlcjczMjIyOTI=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7322292?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/zhengruifeng",
      "html_url": "https://github.com/zhengruifeng",
      "followers_url": "https://api.github.com/users/zhengruifeng/followers",
      "following_url": "https://api.github.com/users/zhengruifeng/following{/other_user}",
      "gists_url": "https://api.github.com/users/zhengruifeng/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/zhengruifeng/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/zhengruifeng/subscriptions",
      "organizations_url": "https://api.github.com/users/zhengruifeng/orgs",
      "repos_url": "https://api.github.com/users/zhengruifeng/repos",
      "events_url": "https://api.github.com/users/zhengruifeng/events{/privacy}",
      "received_events_url": "https://api.github.com/users/zhengruifeng/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-22T00:34:37Z",
    "updated_at": "2023-03-22T00:34:37Z",
    "author_association": "CONTRIBUTOR",
    "body": "@srowen sounds reasonable",
    "reactions": {
      "url": "https://api.github.com/repos/apache/spark/issues/comments/1478764552/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
